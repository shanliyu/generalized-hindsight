{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7c6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60449e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000_2021_05_24_22_28_26_0000--s-10/progress.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e12349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[14830]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21460778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214e1740). One of the two will be used. Which one is undefined.\n",
      "objc[14830]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21460700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214e1768). One of the two will be used. Which one is undefined.\n",
      "objc[14830]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a214607a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214e17b8). One of the two will be used. Which one is undefined.\n",
      "objc[14830]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a21460818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a214e1830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 09:48:49.991433 PDT | Variant:\n",
      "2021-05-25 09:48:49.992207 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 09:49:51.858545 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      23.2985\n",
      "trainer/QF2 Loss                                      23.2842\n",
      "trainer/Policy Loss                                   -5.32932\n",
      "trainer/Q1 Predictions Mean                            0.00278101\n",
      "trainer/Q1 Predictions Std                             0.00349027\n",
      "trainer/Q1 Predictions Max                             0.0153041\n",
      "trainer/Q1 Predictions Min                            -0.00784241\n",
      "trainer/Q2 Predictions Mean                            0.00432815\n",
      "trainer/Q2 Predictions Std                             0.00316395\n",
      "trainer/Q2 Predictions Max                             0.0169291\n",
      "trainer/Q2 Predictions Min                            -0.00194247\n",
      "trainer/Q Targets Mean                                 4.74451\n",
      "trainer/Q Targets Std                                  0.902745\n",
      "trainer/Q Targets Max                                  7.25666\n",
      "trainer/Q Targets Min                                 -1.48978\n",
      "trainer/Log Pis Mean                                  -5.32788\n",
      "trainer/Log Pis Std                                    0.608678\n",
      "trainer/Log Pis Max                                   -3.46964\n",
      "trainer/Log Pis Min                                   -7.64423\n",
      "trainer/Policy mu Mean                                -0.000327822\n",
      "trainer/Policy mu Std                                  0.00207243\n",
      "trainer/Policy mu Max                                  0.00620543\n",
      "trainer/Policy mu Min                                 -0.00688817\n",
      "trainer/Policy log std Mean                           -5.93903e-05\n",
      "trainer/Policy log std Std                             0.00222875\n",
      "trainer/Policy log std Max                             0.00974909\n",
      "trainer/Policy log std Min                            -0.00742114\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            7\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.50978\n",
      "exploration/Rewards Std                                0.478424\n",
      "exploration/Rewards Max                                1.60556\n",
      "exploration/Rewards Min                               -1.86386\n",
      "exploration/Returns Mean                            -509.78\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -509.78\n",
      "exploration/Returns Min                             -509.78\n",
      "exploration/Actions Mean                               0.00458489\n",
      "exploration/Actions Std                                0.622418\n",
      "exploration/Actions Max                                0.999661\n",
      "exploration/Actions Min                               -0.999059\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -509.78\n",
      "exploration/env_infos/final/reward_forward Mean        0.320225\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.320225\n",
      "exploration/env_infos/final/reward_forward Min         0.320225\n",
      "exploration/env_infos/initial/reward_forward Mean      0.210146\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.210146\n",
      "exploration/env_infos/initial/reward_forward Min       0.210146\n",
      "exploration/env_infos/reward_forward Mean              0.0353598\n",
      "exploration/env_infos/reward_forward Std               0.383644\n",
      "exploration/env_infos/reward_forward Max               1.69822\n",
      "exploration/env_infos/reward_forward Min              -1.16106\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.34173\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.34173\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.34173\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.753\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.753\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.753\n",
      "exploration/env_infos/reward_ctrl Mean                -1.5497\n",
      "exploration/env_infos/reward_ctrl Std                  0.438042\n",
      "exploration/env_infos/reward_ctrl Max                 -0.478101\n",
      "exploration/env_infos/reward_ctrl Min                 -2.90006\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.144963\n",
      "exploration/env_infos/final/torso_velocity Std         0.124789\n",
      "exploration/env_infos/final/torso_velocity Max         0.320225\n",
      "exploration/env_infos/final/torso_velocity Min         0.0394182\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.167914\n",
      "exploration/env_infos/initial/torso_velocity Std       0.299996\n",
      "exploration/env_infos/initial/torso_velocity Max       0.512392\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.218797\n",
      "exploration/env_infos/torso_velocity Mean              0.0179897\n",
      "exploration/env_infos/torso_velocity Std               0.375555\n",
      "exploration/env_infos/torso_velocity Max               2.02847\n",
      "exploration/env_infos/torso_velocity Min              -2.98977\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00089\n",
      "evaluation/Rewards Std                                 0.0144767\n",
      "evaluation/Rewards Max                                 1.91771\n",
      "evaluation/Rewards Min                                 0.999985\n",
      "evaluation/Returns Mean                             1000.89\n",
      "evaluation/Returns Std                                 0.997637\n",
      "evaluation/Returns Max                              1004.63\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000131233\n",
      "evaluation/Actions Std                                 0.00116069\n",
      "evaluation/Actions Max                                 0.00373327\n",
      "evaluation/Actions Min                                -0.00370399\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.89\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000290596\n",
      "evaluation/env_infos/final/reward_forward Std          0.00029135\n",
      "evaluation/env_infos/final/reward_forward Max          0.00108824\n",
      "evaluation/env_infos/final/reward_forward Min         -3.82092e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0319382\n",
      "evaluation/env_infos/initial/reward_forward Std        0.135268\n",
      "evaluation/env_infos/initial/reward_forward Max        0.25181\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.29084\n",
      "evaluation/env_infos/reward_forward Mean               0.0027106\n",
      "evaluation/env_infos/reward_forward Std                0.0443503\n",
      "evaluation/env_infos/reward_forward Max                1.06717\n",
      "evaluation/env_infos/reward_forward Min               -1.00921\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.42438e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.82295e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.59292e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.58719e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.28675e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           5.82211e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.31355e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.23019e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.45767e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.10913e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -2.92732e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.48657e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000130762\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000304967\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00161653\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000146254\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137002\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.25385\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.62846\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.29084\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000187019\n",
      "evaluation/env_infos/torso_velocity Std                0.0509129\n",
      "evaluation/env_infos/torso_velocity Max                1.2333\n",
      "evaluation/env_infos/torso_velocity Min               -1.88592\n",
      "time/data storing (s)                                  0.0170728\n",
      "time/evaluation sampling (s)                          53.5237\n",
      "time/exploration sampling (s)                          1.9849\n",
      "time/logging (s)                                       0.274871\n",
      "time/saving (s)                                        0.0662348\n",
      "time/training (s)                                      3.88029\n",
      "time/epoch (s)                                        59.7471\n",
      "time/total (s)                                        65.6349\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:50:44.247511 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.601482\n",
      "trainer/QF2 Loss                                       0.606152\n",
      "trainer/Policy Loss                                   -9.45554\n",
      "trainer/Q1 Predictions Mean                            3.99925\n",
      "trainer/Q1 Predictions Std                             0.417647\n",
      "trainer/Q1 Predictions Max                             5.27142\n",
      "trainer/Q1 Predictions Min                             2.29521\n",
      "trainer/Q2 Predictions Mean                            3.99976\n",
      "trainer/Q2 Predictions Std                             0.426116\n",
      "trainer/Q2 Predictions Max                             5.41101\n",
      "trainer/Q2 Predictions Min                             2.32111\n",
      "trainer/Q Targets Mean                                 4.04545\n",
      "trainer/Q Targets Std                                  0.71656\n",
      "trainer/Q Targets Max                                  6.32517\n",
      "trainer/Q Targets Min                                 -0.70254\n",
      "trainer/Log Pis Mean                                  -5.47891\n",
      "trainer/Log Pis Std                                    0.443889\n",
      "trainer/Log Pis Max                                   -4.52507\n",
      "trainer/Log Pis Min                                   -9.29314\n",
      "trainer/Policy mu Mean                                 0.00707923\n",
      "trainer/Policy mu Std                                  0.0182569\n",
      "trainer/Policy mu Max                                  0.108261\n",
      "trainer/Policy mu Min                                 -0.0492571\n",
      "trainer/Policy log std Mean                           -0.105341\n",
      "trainer/Policy log std Std                             0.0226372\n",
      "trainer/Policy log std Max                            -0.0632686\n",
      "trainer/Policy log std Min                            -0.245728\n",
      "trainer/Alpha                                          0.738537\n",
      "trainer/Alpha Loss                                    -4.04478\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           17\n",
      "exploration/path length Mean                         100\n",
      "exploration/path length Std                          172.271\n",
      "exploration/path length Max                          613\n",
      "exploration/path length Min                           19\n",
      "exploration/Rewards Mean                              -0.388967\n",
      "exploration/Rewards Std                                0.462016\n",
      "exploration/Rewards Max                                1.5374\n",
      "exploration/Rewards Min                               -1.83875\n",
      "exploration/Returns Mean                             -38.8967\n",
      "exploration/Returns Std                               70.761\n",
      "exploration/Returns Max                               -5.56838\n",
      "exploration/Returns Min                             -249.822\n",
      "exploration/Actions Mean                              -0.00315363\n",
      "exploration/Actions Std                                0.600197\n",
      "exploration/Actions Max                                0.998138\n",
      "exploration/Actions Min                               -0.999028\n",
      "exploration/Num Paths                                 10\n",
      "exploration/Average Returns                          -38.8967\n",
      "exploration/env_infos/final/reward_forward Mean        0.176827\n",
      "exploration/env_infos/final/reward_forward Std         0.782812\n",
      "exploration/env_infos/final/reward_forward Max         1.93665\n",
      "exploration/env_infos/final/reward_forward Min        -1.05315\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0252464\n",
      "exploration/env_infos/initial/reward_forward Std       0.11956\n",
      "exploration/env_infos/initial/reward_forward Max       0.22188\n",
      "exploration/env_infos/initial/reward_forward Min      -0.136344\n",
      "exploration/env_infos/reward_forward Mean             -0.00238013\n",
      "exploration/env_infos/reward_forward Std               0.633461\n",
      "exploration/env_infos/reward_forward Max               2.63041\n",
      "exploration/env_infos/reward_forward Min              -2.2251\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.49021\n",
      "exploration/env_infos/final/reward_ctrl Std            0.369092\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.892402\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.15645\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.39841\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.502496\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.66236\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.17037\n",
      "exploration/env_infos/reward_ctrl Mean                -1.44098\n",
      "exploration/env_infos/reward_ctrl Std                  0.413861\n",
      "exploration/env_infos/reward_ctrl Max                 -0.415096\n",
      "exploration/env_infos/reward_ctrl Min                 -2.83875\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.462383\n",
      "exploration/env_infos/final/torso_velocity Std         0.94821\n",
      "exploration/env_infos/final/torso_velocity Max         2.43262\n",
      "exploration/env_infos/final/torso_velocity Min        -1.05315\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.167329\n",
      "exploration/env_infos/initial/torso_velocity Std       0.264925\n",
      "exploration/env_infos/initial/torso_velocity Max       0.597058\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.432204\n",
      "exploration/env_infos/torso_velocity Mean              0.0253032\n",
      "exploration/env_infos/torso_velocity Std               0.631189\n",
      "exploration/env_infos/torso_velocity Max               3.29249\n",
      "exploration/env_infos/torso_velocity Min              -2.2251\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999801\n",
      "evaluation/Rewards Std                                 0.0123382\n",
      "evaluation/Rewards Max                                 1.65217\n",
      "evaluation/Rewards Min                                 0.994745\n",
      "evaluation/Returns Mean                              999.801\n",
      "evaluation/Returns Std                                 0.751716\n",
      "evaluation/Returns Max                              1002.48\n",
      "evaluation/Returns Min                               999.184\n",
      "evaluation/Actions Mean                                0.00426625\n",
      "evaluation/Actions Std                                 0.0131177\n",
      "evaluation/Actions Max                                 0.0843885\n",
      "evaluation/Actions Min                                -0.0213145\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.801\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.86819e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.67795e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.67931e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.11645e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0120558\n",
      "evaluation/env_infos/initial/reward_forward Std        0.120433\n",
      "evaluation/env_infos/initial/reward_forward Max        0.191876\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.283201\n",
      "evaluation/env_infos/reward_forward Mean              -0.000264131\n",
      "evaluation/env_infos/reward_forward Std                0.050195\n",
      "evaluation/env_infos/reward_forward Max                1.23063\n",
      "evaluation/env_infos/reward_forward Min               -1.41841\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.000749554\n",
      "evaluation/env_infos/final/reward_ctrl Std             4.34098e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000670094\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.000814797\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.000620636\n",
      "evaluation/env_infos/initial/reward_ctrl Std           3.02969e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000563876\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00067987\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0007611\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000140393\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000563876\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00525537\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.22864e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.75307e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.27986e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.16927e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.136341\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.231253\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.542531\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.283201\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000957425\n",
      "evaluation/env_infos/torso_velocity Std                0.050109\n",
      "evaluation/env_infos/torso_velocity Max                1.23063\n",
      "evaluation/env_infos/torso_velocity Min               -1.73818\n",
      "time/data storing (s)                                  0.0173402\n",
      "time/evaluation sampling (s)                          46.3224\n",
      "time/exploration sampling (s)                          1.88477\n",
      "time/logging (s)                                       0.284462\n",
      "time/saving (s)                                        0.0267126\n",
      "time/training (s)                                      3.59055\n",
      "time/epoch (s)                                        52.1263\n",
      "time/total (s)                                       118.033\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:51:37.059043 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.336746\n",
      "trainer/QF2 Loss                                       0.343354\n",
      "trainer/Policy Loss                                   -9.17367\n",
      "trainer/Q1 Predictions Mean                            3.73858\n",
      "trainer/Q1 Predictions Std                             0.328434\n",
      "trainer/Q1 Predictions Max                             4.95015\n",
      "trainer/Q1 Predictions Min                             2.65333\n",
      "trainer/Q2 Predictions Mean                            3.73159\n",
      "trainer/Q2 Predictions Std                             0.330753\n",
      "trainer/Q2 Predictions Max                             4.88203\n",
      "trainer/Q2 Predictions Min                             2.62298\n",
      "trainer/Q Targets Mean                                 3.80409\n",
      "trainer/Q Targets Std                                  0.521576\n",
      "trainer/Q Targets Max                                  5.74226\n",
      "trainer/Q Targets Min                                  2.57279\n",
      "trainer/Log Pis Mean                                  -5.45943\n",
      "trainer/Log Pis Std                                    0.34924\n",
      "trainer/Log Pis Max                                   -4.52082\n",
      "trainer/Log Pis Min                                   -7.93882\n",
      "trainer/Policy mu Mean                                -0.00586425\n",
      "trainer/Policy mu Std                                  0.0219526\n",
      "trainer/Policy mu Max                                  0.0775513\n",
      "trainer/Policy mu Min                                 -0.0857895\n",
      "trainer/Policy log std Mean                           -0.123261\n",
      "trainer/Policy log std Std                             0.0158553\n",
      "trainer/Policy log std Max                            -0.0824867\n",
      "trainer/Policy log std Min                            -0.195469\n",
      "trainer/Alpha                                          0.547069\n",
      "trainer/Alpha Loss                                    -8.07805\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           25\n",
      "exploration/path length Mean                         125\n",
      "exploration/path length Std                           96.9201\n",
      "exploration/path length Max                          352\n",
      "exploration/path length Min                           39\n",
      "exploration/Rewards Mean                              -0.332139\n",
      "exploration/Rewards Std                                0.501766\n",
      "exploration/Rewards Max                                1.90154\n",
      "exploration/Rewards Min                               -1.72893\n",
      "exploration/Returns Mean                             -41.5173\n",
      "exploration/Returns Std                               36.9717\n",
      "exploration/Returns Max                              -11.4251\n",
      "exploration/Returns Min                             -132.388\n",
      "exploration/Actions Mean                              -0.0039074\n",
      "exploration/Actions Std                                0.592932\n",
      "exploration/Actions Max                                0.998029\n",
      "exploration/Actions Min                               -0.996283\n",
      "exploration/Num Paths                                  8\n",
      "exploration/Average Returns                          -41.5173\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0898442\n",
      "exploration/env_infos/final/reward_forward Std         0.731214\n",
      "exploration/env_infos/final/reward_forward Max         1.00422\n",
      "exploration/env_infos/final/reward_forward Min        -1.25729\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.057115\n",
      "exploration/env_infos/initial/reward_forward Std       0.18315\n",
      "exploration/env_infos/initial/reward_forward Max       0.27661\n",
      "exploration/env_infos/initial/reward_forward Min      -0.259633\n",
      "exploration/env_infos/reward_forward Mean              0.0198746\n",
      "exploration/env_infos/reward_forward Std               0.779577\n",
      "exploration/env_infos/reward_forward Max               2.84258\n",
      "exploration/env_infos/reward_forward Min              -2.3847\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.26963\n",
      "exploration/env_infos/final/reward_ctrl Std            0.37342\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.77447\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.89301\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.37312\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.321056\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.805517\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.83687\n",
      "exploration/env_infos/reward_ctrl Mean                -1.40633\n",
      "exploration/env_infos/reward_ctrl Std                  0.423237\n",
      "exploration/env_infos/reward_ctrl Max                 -0.23374\n",
      "exploration/env_infos/reward_ctrl Min                 -2.72893\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.370373\n",
      "exploration/env_infos/final/torso_velocity Std         1.02273\n",
      "exploration/env_infos/final/torso_velocity Max         2.33906\n",
      "exploration/env_infos/final/torso_velocity Min        -1.50921\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.109151\n",
      "exploration/env_infos/initial/torso_velocity Std       0.288468\n",
      "exploration/env_infos/initial/torso_velocity Max       0.515203\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.383537\n",
      "exploration/env_infos/torso_velocity Mean              0.0431584\n",
      "exploration/env_infos/torso_velocity Std               0.832964\n",
      "exploration/env_infos/torso_velocity Max               3.03195\n",
      "exploration/env_infos/torso_velocity Min              -2.3847\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999192\n",
      "evaluation/Rewards Std                                 0.0177754\n",
      "evaluation/Rewards Max                                 2.12594\n",
      "evaluation/Rewards Min                                 0.995967\n",
      "evaluation/Returns Mean                              999.192\n",
      "evaluation/Returns Std                                 0.812211\n",
      "evaluation/Returns Max                              1000.71\n",
      "evaluation/Returns Min                               998.038\n",
      "evaluation/Actions Mean                               -0.00478605\n",
      "evaluation/Actions Std                                 0.0196838\n",
      "evaluation/Actions Max                                 0.0495892\n",
      "evaluation/Actions Min                                -0.0747151\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.192\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00036766\n",
      "evaluation/env_infos/final/reward_forward Std          0.000971354\n",
      "evaluation/env_infos/final/reward_forward Max          0.00386994\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00127239\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0135608\n",
      "evaluation/env_infos/initial/reward_forward Std        0.109867\n",
      "evaluation/env_infos/initial/reward_forward Max        0.190101\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.220883\n",
      "evaluation/env_infos/reward_forward Mean              -0.00102881\n",
      "evaluation/env_infos/reward_forward Std                0.0496202\n",
      "evaluation/env_infos/reward_forward Max                0.826572\n",
      "evaluation/env_infos/reward_forward Min               -1.26406\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00163719\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000280471\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0011118\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0019731\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0012491\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000273256\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000783748\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00163589\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00164143\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000299363\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000783748\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00403336\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000173155\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00061999\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00386994\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00127239\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.140141\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.215789\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.571978\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.220883\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00240377\n",
      "evaluation/env_infos/torso_velocity Std                0.0562643\n",
      "evaluation/env_infos/torso_velocity Max                1.52159\n",
      "evaluation/env_infos/torso_velocity Min               -1.74867\n",
      "time/data storing (s)                                  0.0171373\n",
      "time/evaluation sampling (s)                          46.6315\n",
      "time/exploration sampling (s)                          1.99418\n",
      "time/logging (s)                                       0.285606\n",
      "time/saving (s)                                        0.0282074\n",
      "time/training (s)                                      3.65979\n",
      "time/epoch (s)                                        52.6165\n",
      "time/total (s)                                       170.846\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:52:29.452689 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.394117\n",
      "trainer/QF2 Loss                                        0.402123\n",
      "trainer/Policy Loss                                    -9.21535\n",
      "trainer/Q1 Predictions Mean                             3.76517\n",
      "trainer/Q1 Predictions Std                              0.290646\n",
      "trainer/Q1 Predictions Max                              4.73926\n",
      "trainer/Q1 Predictions Min                              2.78099\n",
      "trainer/Q2 Predictions Mean                             3.77146\n",
      "trainer/Q2 Predictions Std                              0.304009\n",
      "trainer/Q2 Predictions Max                              4.96941\n",
      "trainer/Q2 Predictions Min                              2.76663\n",
      "trainer/Q Targets Mean                                  3.76258\n",
      "trainer/Q Targets Std                                   0.656254\n",
      "trainer/Q Targets Max                                   6.60086\n",
      "trainer/Q Targets Min                                  -0.893006\n",
      "trainer/Log Pis Mean                                   -5.47563\n",
      "trainer/Log Pis Std                                     0.437862\n",
      "trainer/Log Pis Max                                    -4.71276\n",
      "trainer/Log Pis Min                                    -9.74764\n",
      "trainer/Policy mu Mean                                 -0.00675654\n",
      "trainer/Policy mu Std                                   0.0286755\n",
      "trainer/Policy mu Max                                   0.104847\n",
      "trainer/Policy mu Min                                  -0.120145\n",
      "trainer/Policy log std Mean                            -0.138616\n",
      "trainer/Policy log std Std                              0.0155365\n",
      "trainer/Policy log std Max                             -0.0859327\n",
      "trainer/Policy log std Min                             -0.225927\n",
      "trainer/Alpha                                           0.405271\n",
      "trainer/Alpha Loss                                    -12.1307\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            41\n",
      "exploration/path length Mean                           62.5\n",
      "exploration/path length Std                            63.7838\n",
      "exploration/path length Max                           283\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                               -0.279431\n",
      "exploration/Rewards Std                                 0.564046\n",
      "exploration/Rewards Max                                 3.43773\n",
      "exploration/Rewards Min                                -1.68912\n",
      "exploration/Returns Mean                              -17.4644\n",
      "exploration/Returns Std                                23.5203\n",
      "exploration/Returns Max                                15.8018\n",
      "exploration/Returns Min                               -93.751\n",
      "exploration/Actions Mean                                0.000433325\n",
      "exploration/Actions Std                                 0.585609\n",
      "exploration/Actions Max                                 0.997091\n",
      "exploration/Actions Min                                -0.994256\n",
      "exploration/Num Paths                                  16\n",
      "exploration/Average Returns                           -17.4644\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0425051\n",
      "exploration/env_infos/final/reward_forward Std          0.713739\n",
      "exploration/env_infos/final/reward_forward Max          1.39671\n",
      "exploration/env_infos/final/reward_forward Min         -1.44065\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0195027\n",
      "exploration/env_infos/initial/reward_forward Std        0.193712\n",
      "exploration/env_infos/initial/reward_forward Max        0.293535\n",
      "exploration/env_infos/initial/reward_forward Min       -0.353118\n",
      "exploration/env_infos/reward_forward Mean               0.134576\n",
      "exploration/env_infos/reward_forward Std                0.874392\n",
      "exploration/env_infos/reward_forward Max                3.4531\n",
      "exploration/env_infos/reward_forward Min               -2.42417\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.36023\n",
      "exploration/env_infos/final/reward_ctrl Std             0.412695\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.762823\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.12451\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.30117\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.413712\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.330143\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.0584\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37175\n",
      "exploration/env_infos/reward_ctrl Std                   0.405116\n",
      "exploration/env_infos/reward_ctrl Max                  -0.330143\n",
      "exploration/env_infos/reward_ctrl Min                  -2.68912\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.394082\n",
      "exploration/env_infos/final/torso_velocity Std          1.15617\n",
      "exploration/env_infos/final/torso_velocity Max          3.81664\n",
      "exploration/env_infos/final/torso_velocity Min         -2.4335\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0953822\n",
      "exploration/env_infos/initial/torso_velocity Std        0.278382\n",
      "exploration/env_infos/initial/torso_velocity Max        0.559616\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.568974\n",
      "exploration/env_infos/torso_velocity Mean               0.0554389\n",
      "exploration/env_infos/torso_velocity Std                0.835101\n",
      "exploration/env_infos/torso_velocity Max                4.03047\n",
      "exploration/env_infos/torso_velocity Min               -2.93337\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.998573\n",
      "evaluation/Rewards Std                                  0.0163681\n",
      "evaluation/Rewards Max                                  1.91283\n",
      "evaluation/Rewards Min                                  0.996412\n",
      "evaluation/Returns Mean                               998.573\n",
      "evaluation/Returns Std                                  1.46682\n",
      "evaluation/Returns Max                               1003.06\n",
      "evaluation/Returns Min                                997.133\n",
      "evaluation/Actions Mean                                -0.00561695\n",
      "evaluation/Actions Std                                  0.0231236\n",
      "evaluation/Actions Max                                  0.0634237\n",
      "evaluation/Actions Min                                 -0.0526515\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            998.573\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000110893\n",
      "evaluation/env_infos/final/reward_forward Std           0.00023553\n",
      "evaluation/env_infos/final/reward_forward Max           0.00100163\n",
      "evaluation/env_infos/final/reward_forward Min          -9.6282e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00264583\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139306\n",
      "evaluation/env_infos/initial/reward_forward Max         0.298569\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.373396\n",
      "evaluation/env_infos/reward_forward Mean                0.00337231\n",
      "evaluation/env_infos/reward_forward Std                 0.0463905\n",
      "evaluation/env_infos/reward_forward Max                 1.02716\n",
      "evaluation/env_infos/reward_forward Min                -1.24009\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00226128\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000352088\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00181862\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00287642\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00202489\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000147096\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00176898\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00246133\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00226501\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000355439\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000722592\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0035876\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.93112e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000161238\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00100163\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000308482\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.120652\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224519\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.498942\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.373396\n",
      "evaluation/env_infos/torso_velocity Mean                0.000630454\n",
      "evaluation/env_infos/torso_velocity Std                 0.0494534\n",
      "evaluation/env_infos/torso_velocity Max                 1.18776\n",
      "evaluation/env_infos/torso_velocity Min                -1.8463\n",
      "time/data storing (s)                                   0.0186578\n",
      "time/evaluation sampling (s)                           46.5051\n",
      "time/exploration sampling (s)                           1.87276\n",
      "time/logging (s)                                        0.270176\n",
      "time/saving (s)                                         0.0318946\n",
      "time/training (s)                                       3.46312\n",
      "time/epoch (s)                                         52.1617\n",
      "time/total (s)                                        223.223\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:53:21.388924 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.340055\n",
      "trainer/QF2 Loss                                        0.351047\n",
      "trainer/Policy Loss                                    -9.11457\n",
      "trainer/Q1 Predictions Mean                             3.73031\n",
      "trainer/Q1 Predictions Std                              0.314968\n",
      "trainer/Q1 Predictions Max                              4.81416\n",
      "trainer/Q1 Predictions Min                              2.84094\n",
      "trainer/Q2 Predictions Mean                             3.70629\n",
      "trainer/Q2 Predictions Std                              0.313749\n",
      "trainer/Q2 Predictions Max                              4.70405\n",
      "trainer/Q2 Predictions Min                              2.85233\n",
      "trainer/Q Targets Mean                                  3.77327\n",
      "trainer/Q Targets Std                                   0.644283\n",
      "trainer/Q Targets Max                                   7.86337\n",
      "trainer/Q Targets Min                                  -0.432922\n",
      "trainer/Log Pis Mean                                   -5.4267\n",
      "trainer/Log Pis Std                                     0.375842\n",
      "trainer/Log Pis Max                                    -4.3578\n",
      "trainer/Log Pis Min                                    -7.5222\n",
      "trainer/Policy mu Mean                                 -0.00923514\n",
      "trainer/Policy mu Std                                   0.0496347\n",
      "trainer/Policy mu Max                                   0.161032\n",
      "trainer/Policy mu Min                                  -0.172677\n",
      "trainer/Policy log std Mean                            -0.13467\n",
      "trainer/Policy log std Std                              0.017405\n",
      "trainer/Policy log std Max                             -0.0594356\n",
      "trainer/Policy log std Min                             -0.21379\n",
      "trainer/Alpha                                           0.300217\n",
      "trainer/Alpha Loss                                    -16.1154\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            50\n",
      "exploration/path length Mean                          111.111\n",
      "exploration/path length Std                           182.551\n",
      "exploration/path length Max                           619\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                               -0.332896\n",
      "exploration/Rewards Std                                 0.452744\n",
      "exploration/Rewards Max                                 1.78211\n",
      "exploration/Rewards Min                                -1.86021\n",
      "exploration/Returns Mean                              -36.9884\n",
      "exploration/Returns Std                                62.5402\n",
      "exploration/Returns Max                                -2.08883\n",
      "exploration/Returns Min                              -211.189\n",
      "exploration/Actions Mean                               -0.0172435\n",
      "exploration/Actions Std                                 0.586357\n",
      "exploration/Actions Max                                 0.995427\n",
      "exploration/Actions Min                                -0.999021\n",
      "exploration/Num Paths                                   9\n",
      "exploration/Average Returns                           -36.9884\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00637881\n",
      "exploration/env_infos/final/reward_forward Std          0.948319\n",
      "exploration/env_infos/final/reward_forward Max          1.87754\n",
      "exploration/env_infos/final/reward_forward Min         -1.51914\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00494249\n",
      "exploration/env_infos/initial/reward_forward Std        0.0745126\n",
      "exploration/env_infos/initial/reward_forward Max        0.0999046\n",
      "exploration/env_infos/initial/reward_forward Min       -0.102049\n",
      "exploration/env_infos/reward_forward Mean              -0.0421586\n",
      "exploration/env_infos/reward_forward Std                0.54121\n",
      "exploration/env_infos/reward_forward Max                2.57933\n",
      "exploration/env_infos/reward_forward Min               -2.02836\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.30441\n",
      "exploration/env_infos/final/reward_ctrl Std             0.508837\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.54647\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.00876\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.12827\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.225503\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.694538\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.42057\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37645\n",
      "exploration/env_infos/reward_ctrl Std                   0.404352\n",
      "exploration/env_infos/reward_ctrl Max                  -0.262512\n",
      "exploration/env_infos/reward_ctrl Min                  -2.86021\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.506505\n",
      "exploration/env_infos/final/torso_velocity Std          1.18665\n",
      "exploration/env_infos/final/torso_velocity Max          2.63791\n",
      "exploration/env_infos/final/torso_velocity Min         -1.51914\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146528\n",
      "exploration/env_infos/initial/torso_velocity Std        0.237101\n",
      "exploration/env_infos/initial/torso_velocity Max        0.549\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.501407\n",
      "exploration/env_infos/torso_velocity Mean              -0.0163682\n",
      "exploration/env_infos/torso_velocity Std                0.593077\n",
      "exploration/env_infos/torso_velocity Max                3.59867\n",
      "exploration/env_infos/torso_velocity Min               -2.52337\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.995475\n",
      "evaluation/Rewards Std                                  0.0201928\n",
      "evaluation/Rewards Max                                  1.99986\n",
      "evaluation/Rewards Min                                  0.977921\n",
      "evaluation/Returns Mean                               995.475\n",
      "evaluation/Returns Std                                  1.72089\n",
      "evaluation/Returns Max                                998.803\n",
      "evaluation/Returns Min                                991.932\n",
      "evaluation/Actions Mean                                -0.00557729\n",
      "evaluation/Actions Std                                  0.0369746\n",
      "evaluation/Actions Max                                  0.120277\n",
      "evaluation/Actions Min                                 -0.112436\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            995.475\n",
      "evaluation/env_infos/final/reward_forward Mean          2.50033e-05\n",
      "evaluation/env_infos/final/reward_forward Std           7.97008e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000377272\n",
      "evaluation/env_infos/final/reward_forward Min          -7.47868e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0403393\n",
      "evaluation/env_infos/initial/reward_forward Std         0.123753\n",
      "evaluation/env_infos/initial/reward_forward Max         0.136167\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.238067\n",
      "evaluation/env_infos/reward_forward Mean                0.00245981\n",
      "evaluation/env_infos/reward_forward Std                 0.0568496\n",
      "evaluation/env_infos/reward_forward Max                 0.943432\n",
      "evaluation/env_infos/reward_forward Min                -1.24957\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00555856\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00101235\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00451122\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00805753\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00577844\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00120986\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00442967\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00852733\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00559291\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00109146\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00419049\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0220794\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.64602e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           6.72486e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000377272\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000299976\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151365\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.270911\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.739931\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.238067\n",
      "evaluation/env_infos/torso_velocity Mean                0.000350137\n",
      "evaluation/env_infos/torso_velocity Std                 0.0570948\n",
      "evaluation/env_infos/torso_velocity Max                 1.13857\n",
      "evaluation/env_infos/torso_velocity Min                -2.17105\n",
      "time/data storing (s)                                   0.0165857\n",
      "time/evaluation sampling (s)                           46.0311\n",
      "time/exploration sampling (s)                           1.91664\n",
      "time/logging (s)                                        0.277919\n",
      "time/saving (s)                                         0.0262002\n",
      "time/training (s)                                       3.47477\n",
      "time/epoch (s)                                         51.7432\n",
      "time/total (s)                                        275.167\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:54:14.565057 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.349423\n",
      "trainer/QF2 Loss                                        0.355237\n",
      "trainer/Policy Loss                                    -9.19342\n",
      "trainer/Q1 Predictions Mean                             3.78803\n",
      "trainer/Q1 Predictions Std                              0.36919\n",
      "trainer/Q1 Predictions Max                              5.0136\n",
      "trainer/Q1 Predictions Min                              2.9175\n",
      "trainer/Q2 Predictions Mean                             3.76405\n",
      "trainer/Q2 Predictions Std                              0.343646\n",
      "trainer/Q2 Predictions Max                              4.92867\n",
      "trainer/Q2 Predictions Min                              2.96963\n",
      "trainer/Q Targets Mean                                  3.85403\n",
      "trainer/Q Targets Std                                   0.633327\n",
      "trainer/Q Targets Max                                   6.25775\n",
      "trainer/Q Targets Min                                  -0.421069\n",
      "trainer/Log Pis Mean                                   -5.46342\n",
      "trainer/Log Pis Std                                     0.569012\n",
      "trainer/Log Pis Max                                    -4.24044\n",
      "trainer/Log Pis Min                                    -9.75507\n",
      "trainer/Policy mu Mean                                 -0.0210567\n",
      "trainer/Policy mu Std                                   0.0831056\n",
      "trainer/Policy mu Max                                   0.245576\n",
      "trainer/Policy mu Min                                  -0.418583\n",
      "trainer/Policy log std Mean                            -0.136534\n",
      "trainer/Policy log std Std                              0.017892\n",
      "trainer/Policy log std Max                             -0.0432455\n",
      "trainer/Policy log std Min                             -0.234893\n",
      "trainer/Alpha                                           0.222469\n",
      "trainer/Alpha Loss                                    -20.1947\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            59\n",
      "exploration/path length Mean                          111.111\n",
      "exploration/path length Std                            77.9607\n",
      "exploration/path length Max                           243\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.358684\n",
      "exploration/Rewards Std                                 0.471497\n",
      "exploration/Rewards Max                                 1.52435\n",
      "exploration/Rewards Min                                -1.97139\n",
      "exploration/Returns Mean                              -39.8538\n",
      "exploration/Returns Std                                28.6522\n",
      "exploration/Returns Max                                -3.07299\n",
      "exploration/Returns Min                               -86.202\n",
      "exploration/Actions Mean                               -0.0145241\n",
      "exploration/Actions Std                                 0.592704\n",
      "exploration/Actions Max                                 0.994831\n",
      "exploration/Actions Min                                -0.997722\n",
      "exploration/Num Paths                                   9\n",
      "exploration/Average Returns                           -39.8538\n",
      "exploration/env_infos/final/reward_forward Mean        -0.346258\n",
      "exploration/env_infos/final/reward_forward Std          0.443963\n",
      "exploration/env_infos/final/reward_forward Max          0.597858\n",
      "exploration/env_infos/final/reward_forward Min         -0.881961\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0110759\n",
      "exploration/env_infos/initial/reward_forward Std        0.162059\n",
      "exploration/env_infos/initial/reward_forward Max        0.290644\n",
      "exploration/env_infos/initial/reward_forward Min       -0.280078\n",
      "exploration/env_infos/reward_forward Mean               0.0492937\n",
      "exploration/env_infos/reward_forward Std                0.784164\n",
      "exploration/env_infos/reward_forward Max                2.38044\n",
      "exploration/env_infos/reward_forward Min               -2.82509\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.16467\n",
      "exploration/env_infos/final/reward_ctrl Std             0.342107\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.715303\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.59522\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.22094\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.186105\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.894518\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.5104\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.40604\n",
      "exploration/env_infos/reward_ctrl Std                   0.422271\n",
      "exploration/env_infos/reward_ctrl Max                  -0.285627\n",
      "exploration/env_infos/reward_ctrl Min                  -2.97139\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.279146\n",
      "exploration/env_infos/final/torso_velocity Std          0.934278\n",
      "exploration/env_infos/final/torso_velocity Max          1.93526\n",
      "exploration/env_infos/final/torso_velocity Min         -1.9001\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.119859\n",
      "exploration/env_infos/initial/torso_velocity Std        0.253377\n",
      "exploration/env_infos/initial/torso_velocity Max        0.634702\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.280078\n",
      "exploration/env_infos/torso_velocity Mean               0.0459547\n",
      "exploration/env_infos/torso_velocity Std                0.756057\n",
      "exploration/env_infos/torso_velocity Max                2.86351\n",
      "exploration/env_infos/torso_velocity Min               -2.82509\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.987498\n",
      "evaluation/Rewards Std                                  0.0212678\n",
      "evaluation/Rewards Max                                  2.46489\n",
      "evaluation/Rewards Min                                  0.857201\n",
      "evaluation/Returns Mean                               987.498\n",
      "evaluation/Returns Std                                  2.91806\n",
      "evaluation/Returns Max                                993.163\n",
      "evaluation/Returns Min                                980.644\n",
      "evaluation/Actions Mean                                -0.0138632\n",
      "evaluation/Actions Std                                  0.0554483\n",
      "evaluation/Actions Max                                  0.253656\n",
      "evaluation/Actions Min                                 -0.282368\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            987.498\n",
      "evaluation/env_infos/final/reward_forward Mean          3.37233e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000164513\n",
      "evaluation/env_infos/final/reward_forward Max           0.000839663\n",
      "evaluation/env_infos/final/reward_forward Min          -1.02163e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0548627\n",
      "evaluation/env_infos/initial/reward_forward Std         0.113776\n",
      "evaluation/env_infos/initial/reward_forward Max         0.183503\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.253034\n",
      "evaluation/env_infos/reward_forward Mean               -0.002687\n",
      "evaluation/env_infos/reward_forward Std                 0.049415\n",
      "evaluation/env_infos/reward_forward Max                 0.763226\n",
      "evaluation/env_infos/reward_forward Min                -1.68494\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0129152\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00274163\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00915876\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0195096\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0145548\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00510384\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00864705\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0259686\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0130668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/reward_ctrl Std                    0.00452263\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00606664\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.142799\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.17976e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000130455\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000839663\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00075594\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137482\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.259278\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.714952\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.253034\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00204377\n",
      "evaluation/env_infos/torso_velocity Std                 0.0500013\n",
      "evaluation/env_infos/torso_velocity Max                 1.29516\n",
      "evaluation/env_infos/torso_velocity Min                -1.79695\n",
      "time/data storing (s)                                   0.0175555\n",
      "time/evaluation sampling (s)                           47.3638\n",
      "time/exploration sampling (s)                           1.83101\n",
      "time/logging (s)                                        0.278656\n",
      "time/saving (s)                                         0.0267103\n",
      "time/training (s)                                       3.45182\n",
      "time/epoch (s)                                         52.9696\n",
      "time/total (s)                                        328.343\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 09:55:07.966067 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.328272\n",
      "trainer/QF2 Loss                                        0.341654\n",
      "trainer/Policy Loss                                    -9.19133\n",
      "trainer/Q1 Predictions Mean                             3.82689\n",
      "trainer/Q1 Predictions Std                              0.383247\n",
      "trainer/Q1 Predictions Max                              4.93662\n",
      "trainer/Q1 Predictions Min                              2.63469\n",
      "trainer/Q2 Predictions Mean                             3.79409\n",
      "trainer/Q2 Predictions Std                              0.363701\n",
      "trainer/Q2 Predictions Max                              5.00084\n",
      "trainer/Q2 Predictions Min                              2.57612\n",
      "trainer/Q Targets Mean                                  3.81657\n",
      "trainer/Q Targets Std                                   0.674656\n",
      "trainer/Q Targets Max                                   7.38763\n",
      "trainer/Q Targets Min                                  -0.720052\n",
      "trainer/Log Pis Mean                                   -5.43022\n",
      "trainer/Log Pis Std                                     0.62939\n",
      "trainer/Log Pis Max                                    -3.98039\n",
      "trainer/Log Pis Min                                   -10.9436\n",
      "trainer/Policy mu Mean                                 -0.000603549\n",
      "trainer/Policy mu Std                                   0.123061\n",
      "trainer/Policy mu Max                                   0.57399\n",
      "trainer/Policy mu Min                                  -0.469281\n",
      "trainer/Policy log std Mean                            -0.176645\n",
      "trainer/Policy log std Std                              0.0302955\n",
      "trainer/Policy log std Max                             -0.113671\n",
      "trainer/Policy log std Min                             -0.312814\n",
      "trainer/Alpha                                           0.164963\n",
      "trainer/Alpha Loss                                    -24.1617\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            65\n",
      "exploration/path length Mean                          166.667\n",
      "exploration/path length Std                           116.506\n",
      "exploration/path length Max                           403\n",
      "exploration/path length Min                            57\n",
      "exploration/Rewards Mean                               -0.259866\n",
      "exploration/Rewards Std                                 0.497207\n",
      "exploration/Rewards Max                                 2.65993\n",
      "exploration/Rewards Min                                -1.5765\n",
      "exploration/Returns Mean                              -43.3109\n",
      "exploration/Returns Std                                24.0536\n",
      "exploration/Returns Max                               -15.6376\n",
      "exploration/Returns Min                               -82.3908\n",
      "exploration/Actions Mean                               -0.00585384\n",
      "exploration/Actions Std                                 0.576042\n",
      "exploration/Actions Max                                 0.99787\n",
      "exploration/Actions Min                                -0.993597\n",
      "exploration/Num Paths                                   6\n",
      "exploration/Average Returns                           -43.3109\n",
      "exploration/env_infos/final/reward_forward Mean         0.316618\n",
      "exploration/env_infos/final/reward_forward Std          0.483676\n",
      "exploration/env_infos/final/reward_forward Max          0.972669\n",
      "exploration/env_infos/final/reward_forward Min         -0.538399\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.118843\n",
      "exploration/env_infos/initial/reward_forward Std        0.0798695\n",
      "exploration/env_infos/initial/reward_forward Max        0.0384839\n",
      "exploration/env_infos/initial/reward_forward Min       -0.208808\n",
      "exploration/env_infos/reward_forward Mean               0.00968964\n",
      "exploration/env_infos/reward_forward Std                0.809515\n",
      "exploration/env_infos/reward_forward Max                2.67361\n",
      "exploration/env_infos/reward_forward Min               -2.54278\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.44398\n",
      "exploration/env_infos/final/reward_ctrl Std             0.361579\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.986241\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.99442\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.38885\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.490072\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.723149\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.21388\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.32743\n",
      "exploration/env_infos/reward_ctrl Std                   0.40902\n",
      "exploration/env_infos/reward_ctrl Max                  -0.220287\n",
      "exploration/env_infos/reward_ctrl Min                  -2.5765\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.430391\n",
      "exploration/env_infos/final/torso_velocity Std          0.640218\n",
      "exploration/env_infos/final/torso_velocity Max          1.74463\n",
      "exploration/env_infos/final/torso_velocity Min         -0.658453\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0814975\n",
      "exploration/env_infos/initial/torso_velocity Std        0.25474\n",
      "exploration/env_infos/initial/torso_velocity Max        0.515658\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.334635\n",
      "exploration/env_infos/torso_velocity Mean               0.00936401\n",
      "exploration/env_infos/torso_velocity Std                0.797589\n",
      "exploration/env_infos/torso_velocity Max                3.39927\n",
      "exploration/env_infos/torso_velocity Min               -2.91692\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979669\n",
      "evaluation/Rewards Std                                  0.0268492\n",
      "evaluation/Rewards Max                                  2.35134\n",
      "evaluation/Rewards Min                                  0.802029\n",
      "evaluation/Returns Mean                               979.669\n",
      "evaluation/Returns Std                                  2.51298\n",
      "evaluation/Returns Max                                985.808\n",
      "evaluation/Returns Min                                974.776\n",
      "evaluation/Actions Mean                                -0.0124698\n",
      "evaluation/Actions Std                                  0.0727398\n",
      "evaluation/Actions Max                                  0.435017\n",
      "evaluation/Actions Min                                 -0.288799\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.669\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000805038\n",
      "evaluation/env_infos/final/reward_forward Std           0.00395893\n",
      "evaluation/env_infos/final/reward_forward Max           0.0201996\n",
      "evaluation/env_infos/final/reward_forward Min          -7.51978e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00838141\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115751\n",
      "evaluation/env_infos/initial/reward_forward Max         0.288304\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.186898\n",
      "evaluation/env_infos/reward_forward Mean               -0.00187471\n",
      "evaluation/env_infos/reward_forward Std                 0.0557493\n",
      "evaluation/env_infos/reward_forward Max                 0.8556\n",
      "evaluation/env_infos/reward_forward Min                -1.53257\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0215802\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00245984\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0159621\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0258441\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0200388\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00614244\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0137648\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0360967\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0217863\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0052325\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.008841\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.233812\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000269217\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00528142\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0201996\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0410955\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128759\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.257648\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.657235\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.360734\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00181562\n",
      "evaluation/env_infos/torso_velocity Std                 0.0557572\n",
      "evaluation/env_infos/torso_velocity Max                 0.8556\n",
      "evaluation/env_infos/torso_velocity Min                -1.7955\n",
      "time/data storing (s)                                   0.0174047\n",
      "time/evaluation sampling (s)                           47.3455\n",
      "time/exploration sampling (s)                           1.93707\n",
      "time/logging (s)                                        0.384671\n",
      "time/saving (s)                                         0.0307656\n",
      "time/training (s)                                       3.56552\n",
      "time/epoch (s)                                         53.2809\n",
      "time/total (s)                                        381.85\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:56:02.681479 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.337913\n",
      "trainer/QF2 Loss                                        0.393724\n",
      "trainer/Policy Loss                                    -8.56905\n",
      "trainer/Q1 Predictions Mean                             3.58149\n",
      "trainer/Q1 Predictions Std                              0.483766\n",
      "trainer/Q1 Predictions Max                              5.16041\n",
      "trainer/Q1 Predictions Min                              0.975572\n",
      "trainer/Q2 Predictions Mean                             3.51254\n",
      "trainer/Q2 Predictions Std                              0.440501\n",
      "trainer/Q2 Predictions Max                              4.79378\n",
      "trainer/Q2 Predictions Min                              1.31736\n",
      "trainer/Q Targets Mean                                  3.69157\n",
      "trainer/Q Targets Std                                   0.800568\n",
      "trainer/Q Targets Max                                   6.25165\n",
      "trainer/Q Targets Min                                  -1.48978\n",
      "trainer/Log Pis Mean                                   -5.01492\n",
      "trainer/Log Pis Std                                     0.877846\n",
      "trainer/Log Pis Max                                    -1.94275\n",
      "trainer/Log Pis Min                                    -9.46688\n",
      "trainer/Policy mu Mean                                  0.0352043\n",
      "trainer/Policy mu Std                                   0.206814\n",
      "trainer/Policy mu Max                                   0.786072\n",
      "trainer/Policy mu Min                                  -0.684531\n",
      "trainer/Policy log std Mean                            -0.30211\n",
      "trainer/Policy log std Std                              0.0895366\n",
      "trainer/Policy log std Max                             -0.160597\n",
      "trainer/Policy log std Min                             -0.7077\n",
      "trainer/Alpha                                           0.122631\n",
      "trainer/Alpha Loss                                    -27.2747\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                            69.848\n",
      "exploration/path length Max                           221\n",
      "exploration/path length Min                            19\n",
      "exploration/Rewards Mean                               -0.131906\n",
      "exploration/Rewards Std                                 0.486998\n",
      "exploration/Rewards Max                                 2.52864\n",
      "exploration/Rewards Min                                -1.5805\n",
      "exploration/Returns Mean                              -16.4882\n",
      "exploration/Returns Std                                11.2025\n",
      "exploration/Returns Max                                -1.4791\n",
      "exploration/Returns Min                               -34.8705\n",
      "exploration/Actions Mean                                0.0403526\n",
      "exploration/Actions Std                                 0.545524\n",
      "exploration/Actions Max                                 0.99014\n",
      "exploration/Actions Min                                -0.989869\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                           -16.4882\n",
      "exploration/env_infos/final/reward_forward Mean         0.276265\n",
      "exploration/env_infos/final/reward_forward Std          1.38003\n",
      "exploration/env_infos/final/reward_forward Max          2.02205\n",
      "exploration/env_infos/final/reward_forward Min         -2.02101\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.024031\n",
      "exploration/env_infos/initial/reward_forward Std        0.150091\n",
      "exploration/env_infos/initial/reward_forward Max        0.15831\n",
      "exploration/env_infos/initial/reward_forward Min       -0.305024\n",
      "exploration/env_infos/reward_forward Mean              -0.115505\n",
      "exploration/env_infos/reward_forward Std                0.852537\n",
      "exploration/env_infos/reward_forward Max                2.40576\n",
      "exploration/env_infos/reward_forward Min               -2.77348\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.3915\n",
      "exploration/env_infos/final/reward_ctrl Std             0.28256\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.931386\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.90435\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.20875\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.549045\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.442151\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.14827\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.1969\n",
      "exploration/env_infos/reward_ctrl Std                   0.383099\n",
      "exploration/env_infos/reward_ctrl Max                  -0.183847\n",
      "exploration/env_infos/reward_ctrl Min                  -2.5805\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.774672\n",
      "exploration/env_infos/final/torso_velocity Std          1.06439\n",
      "exploration/env_infos/final/torso_velocity Max          2.07297\n",
      "exploration/env_infos/final/torso_velocity Min         -2.02101\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146107\n",
      "exploration/env_infos/initial/torso_velocity Std        0.246385\n",
      "exploration/env_infos/initial/torso_velocity Max        0.660947\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.305024\n",
      "exploration/env_infos/torso_velocity Mean               0.00858116\n",
      "exploration/env_infos/torso_velocity Std                0.836049\n",
      "exploration/env_infos/torso_velocity Max                2.83174\n",
      "exploration/env_infos/torso_velocity Min               -2.77348\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.922319\n",
      "evaluation/Rewards Std                                  0.0480307\n",
      "evaluation/Rewards Max                                  1.71202\n",
      "evaluation/Rewards Min                                  0.584937\n",
      "evaluation/Returns Mean                               922.319\n",
      "evaluation/Returns Std                                 19.8167\n",
      "evaluation/Returns Max                                981.404\n",
      "evaluation/Returns Min                                908.238\n",
      "evaluation/Actions Mean                                 0.0653512\n",
      "evaluation/Actions Std                                  0.130243\n",
      "evaluation/Actions Max                                  0.538091\n",
      "evaluation/Actions Min                                 -0.481935\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            922.319\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0300958\n",
      "evaluation/env_infos/final/reward_forward Std           0.0927097\n",
      "evaluation/env_infos/final/reward_forward Max           0.133445\n",
      "evaluation/env_infos/final/reward_forward Min          -0.254892\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0217793\n",
      "evaluation/env_infos/initial/reward_forward Std         0.105697\n",
      "evaluation/env_infos/initial/reward_forward Max         0.337421\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.170174\n",
      "evaluation/env_infos/reward_forward Mean               -0.00752767\n",
      "evaluation/env_infos/reward_forward Std                 0.113845\n",
      "evaluation/env_infos/reward_forward Max                 0.945742\n",
      "evaluation/env_infos/reward_forward Min                -1.3553\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0835011\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00979351\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0616038\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0959207\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0814013\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0213103\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0363292\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.12416\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0849359\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0117662\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0363292\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.415063\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0181074\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0875874\n",
      "evaluation/env_infos/final/torso_velocity Max           0.227286\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.254892\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.16979\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.23466\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.610549\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.286277\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00202521\n",
      "evaluation/env_infos/torso_velocity Std                 0.122018\n",
      "evaluation/env_infos/torso_velocity Max                 1.43075\n",
      "evaluation/env_infos/torso_velocity Min                -1.79271\n",
      "time/data storing (s)                                   0.0177111\n",
      "time/evaluation sampling (s)                           47.7413\n",
      "time/exploration sampling (s)                           2.05775\n",
      "time/logging (s)                                        0.282333\n",
      "time/saving (s)                                         0.027615\n",
      "time/training (s)                                       4.24915\n",
      "time/epoch (s)                                         54.3758\n",
      "time/total (s)                                        436.462\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:56:56.841379 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.401015\n",
      "trainer/QF2 Loss                                        0.421594\n",
      "trainer/Policy Loss                                    -7.59222\n",
      "trainer/Q1 Predictions Mean                             3.52123\n",
      "trainer/Q1 Predictions Std                              0.51661\n",
      "trainer/Q1 Predictions Max                              5.72658\n",
      "trainer/Q1 Predictions Min                              1.71623\n",
      "trainer/Q2 Predictions Mean                             3.4177\n",
      "trainer/Q2 Predictions Std                              0.493465\n",
      "trainer/Q2 Predictions Max                              5.5211\n",
      "trainer/Q2 Predictions Min                              1.7223\n",
      "trainer/Q Targets Mean                                  3.63731\n",
      "trainer/Q Targets Std                                   0.808517\n",
      "trainer/Q Targets Max                                   7.48901\n",
      "trainer/Q Targets Min                                  -0.893006\n",
      "trainer/Log Pis Mean                                   -3.97411\n",
      "trainer/Log Pis Std                                     1.33915\n",
      "trainer/Log Pis Max                                    -0.259202\n",
      "trainer/Log Pis Min                                    -9.78089\n",
      "trainer/Policy mu Mean                                  0.0539539\n",
      "trainer/Policy mu Std                                   0.204862\n",
      "trainer/Policy mu Max                                   0.666458\n",
      "trainer/Policy mu Min                                  -1.05638\n",
      "trainer/Policy log std Mean                            -0.611025\n",
      "trainer/Policy log std Std                              0.18512\n",
      "trainer/Policy log std Max                             -0.234306\n",
      "trainer/Policy log std Min                             -1.17782\n",
      "trainer/Alpha                                           0.0920611\n",
      "trainer/Alpha Loss                                    -28.529\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            77\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           190.751\n",
      "exploration/path length Max                           556\n",
      "exploration/path length Min                            54\n",
      "exploration/Rewards Mean                                0.206975\n",
      "exploration/Rewards Std                                 0.401866\n",
      "exploration/Rewards Max                                 2.49114\n",
      "exploration/Rewards Min                                -0.74942\n",
      "exploration/Returns Mean                               51.7436\n",
      "exploration/Returns Std                                37.8698\n",
      "exploration/Returns Max                               109.936\n",
      "exploration/Returns Min                                12.7881\n",
      "exploration/Actions Mean                                0.0539608\n",
      "exploration/Actions Std                                 0.463774\n",
      "exploration/Actions Max                                 0.993769\n",
      "exploration/Actions Min                                -0.980427\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                            51.7436\n",
      "exploration/env_infos/final/reward_forward Mean         0.466657\n",
      "exploration/env_infos/final/reward_forward Std          1.11395\n",
      "exploration/env_infos/final/reward_forward Max          2.1168\n",
      "exploration/env_infos/final/reward_forward Min         -1.01953\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0514469\n",
      "exploration/env_infos/initial/reward_forward Std        0.102603\n",
      "exploration/env_infos/initial/reward_forward Max        0.028411\n",
      "exploration/env_infos/initial/reward_forward Min       -0.226862\n",
      "exploration/env_infos/reward_forward Mean              -0.0303606\n",
      "exploration/env_infos/reward_forward Std                0.743349\n",
      "exploration/env_infos/reward_forward Max                2.1168\n",
      "exploration/env_infos/reward_forward Min               -2.61749\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.583011\n",
      "exploration/env_infos/final/reward_ctrl Std             0.134441\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.3884\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.767737\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.732654\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.184123\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.466419\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.980347\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.871992\n",
      "exploration/env_infos/reward_ctrl Std                   0.300944\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0551692\n",
      "exploration/env_infos/reward_ctrl Min                  -1.90417\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.624052\n",
      "exploration/env_infos/final/torso_velocity Std          0.805657\n",
      "exploration/env_infos/final/torso_velocity Max          2.1168\n",
      "exploration/env_infos/final/torso_velocity Min         -1.01953\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.125892\n",
      "exploration/env_infos/initial/torso_velocity Std        0.253808\n",
      "exploration/env_infos/initial/torso_velocity Max        0.54494\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.252377\n",
      "exploration/env_infos/torso_velocity Mean               0.00368016\n",
      "exploration/env_infos/torso_velocity Std                0.768394\n",
      "exploration/env_infos/torso_velocity Max                3.14128\n",
      "exploration/env_infos/torso_velocity Min               -2.61749\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.950968\n",
      "evaluation/Rewards Std                                  0.18618\n",
      "evaluation/Rewards Max                                  2.55752\n",
      "evaluation/Rewards Min                                  0.643423\n",
      "evaluation/Returns Mean                               950.968\n",
      "evaluation/Returns Std                                 21.1945\n",
      "evaluation/Returns Max                               1009.19\n",
      "evaluation/Returns Min                                930.44\n",
      "evaluation/Actions Mean                                 0.072182\n",
      "evaluation/Actions Std                                  0.134018\n",
      "evaluation/Actions Max                                  0.424483\n",
      "evaluation/Actions Min                                 -0.523833\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            950.968\n",
      "evaluation/env_infos/final/reward_forward Mean          0.013075\n",
      "evaluation/env_infos/final/reward_forward Std           0.0523703\n",
      "evaluation/env_infos/final/reward_forward Max           0.139924\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0982859\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0531839\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112319\n",
      "evaluation/env_infos/initial/reward_forward Max         0.248527\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.226759\n",
      "evaluation/env_infos/reward_forward Mean               -0.00147211\n",
      "evaluation/env_infos/reward_forward Std                 0.397686\n",
      "evaluation/env_infos/reward_forward Max                 1.56248\n",
      "evaluation/env_infos/reward_forward Min                -1.48855\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.091981\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0183503\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.063269\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.131859\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0455309\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00545179\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0370379\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0602417\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0926841\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0345516\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0279692\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.356577\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.0159013\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0554207\n",
      "evaluation/env_infos/final/torso_velocity Max           0.240139\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0982859\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.20302\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.237339\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.736297\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.226759\n",
      "evaluation/env_infos/torso_velocity Mean                0.00342719\n",
      "evaluation/env_infos/torso_velocity Std                 0.344726\n",
      "evaluation/env_infos/torso_velocity Max                 1.65162\n",
      "evaluation/env_infos/torso_velocity Min                -1.72623\n",
      "time/data storing (s)                                   0.0169248\n",
      "time/evaluation sampling (s)                           47.4319\n",
      "time/exploration sampling (s)                           1.88523\n",
      "time/logging (s)                                        0.286892\n",
      "time/saving (s)                                         0.0286798\n",
      "time/training (s)                                       4.28098\n",
      "time/epoch (s)                                         53.9306\n",
      "time/total (s)                                        490.627\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:57:48.698959 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_48_49_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.208092\n",
      "trainer/QF2 Loss                                        0.2239\n",
      "trainer/Policy Loss                                    -5.89736\n",
      "trainer/Q1 Predictions Mean                             3.66387\n",
      "trainer/Q1 Predictions Std                              0.553627\n",
      "trainer/Q1 Predictions Max                              5.20766\n",
      "trainer/Q1 Predictions Min                              2.07417\n",
      "trainer/Q2 Predictions Mean                             3.70887\n",
      "trainer/Q2 Predictions Std                              0.529341\n",
      "trainer/Q2 Predictions Max                              5.37801\n",
      "trainer/Q2 Predictions Min                              2.42975\n",
      "trainer/Q Targets Mean                                  3.74337\n",
      "trainer/Q Targets Std                                   0.703505\n",
      "trainer/Q Targets Max                                   6.30926\n",
      "trainer/Q Targets Min                                   2.09676\n",
      "trainer/Log Pis Mean                                   -1.73804\n",
      "trainer/Log Pis Std                                     1.60009\n",
      "trainer/Log Pis Max                                     1.62197\n",
      "trainer/Log Pis Min                                    -6.75169\n",
      "trainer/Policy mu Mean                                 -0.0300637\n",
      "trainer/Policy mu Std                                   0.185473\n",
      "trainer/Policy mu Max                                   0.861285\n",
      "trainer/Policy mu Min                                  -1.16262\n",
      "trainer/Policy log std Mean                            -1.05254\n",
      "trainer/Policy log std Std                              0.210528\n",
      "trainer/Policy log std Max                             -0.562157\n",
      "trainer/Policy log std Min                             -1.8132\n",
      "trainer/Alpha                                           0.071518\n",
      "trainer/Alpha Loss                                    -25.6645\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            79\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           260\n",
      "exploration/path length Max                           760\n",
      "exploration/path length Min                           240\n",
      "exploration/Rewards Mean                                0.598235\n",
      "exploration/Rewards Std                                 0.27972\n",
      "exploration/Rewards Max                                 2.54377\n",
      "exploration/Rewards Min                                -0.268227\n",
      "exploration/Returns Mean                              299.118\n",
      "exploration/Returns Std                               157.709\n",
      "exploration/Returns Max                               456.826\n",
      "exploration/Returns Min                               141.409\n",
      "exploration/Actions Mean                               -0.0166866\n",
      "exploration/Actions Std                                 0.337472\n",
      "exploration/Actions Max                                 0.937476\n",
      "exploration/Actions Min                                -0.943199\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                           299.118\n",
      "exploration/env_infos/final/reward_forward Mean         0.378011\n",
      "exploration/env_infos/final/reward_forward Std          0.787617\n",
      "exploration/env_infos/final/reward_forward Max          1.16563\n",
      "exploration/env_infos/final/reward_forward Min         -0.409607\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0466658\n",
      "exploration/env_infos/initial/reward_forward Std        0.201789\n",
      "exploration/env_infos/initial/reward_forward Max        0.155123\n",
      "exploration/env_infos/initial/reward_forward Min       -0.248454\n",
      "exploration/env_infos/reward_forward Mean               0.0646045\n",
      "exploration/env_infos/reward_forward Std                0.540715\n",
      "exploration/env_infos/reward_forward Max                2.03458\n",
      "exploration/env_infos/reward_forward Min               -1.52703\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.211391\n",
      "exploration/env_infos/final/reward_ctrl Std             0.00238147\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.20901\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.213773\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.46706\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0945329\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.372527\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.561593\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.456664\n",
      "exploration/env_infos/reward_ctrl Std                   0.194486\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0333477\n",
      "exploration/env_infos/reward_ctrl Min                  -1.26823\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.258735\n",
      "exploration/env_infos/final/torso_velocity Std          0.653675\n",
      "exploration/env_infos/final/torso_velocity Max          1.16563\n",
      "exploration/env_infos/final/torso_velocity Min         -0.409607\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.131985\n",
      "exploration/env_infos/initial/torso_velocity Std        0.253644\n",
      "exploration/env_infos/initial/torso_velocity Max        0.480193\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.248454\n",
      "exploration/env_infos/torso_velocity Mean               0.0254621\n",
      "exploration/env_infos/torso_velocity Std                0.538935\n",
      "exploration/env_infos/torso_velocity Max                2.21189\n",
      "exploration/env_infos/torso_velocity Min               -1.92261\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979961\n",
      "evaluation/Rewards Std                                  0.113373\n",
      "evaluation/Rewards Max                                  2.24665\n",
      "evaluation/Rewards Min                                  0.735509\n",
      "evaluation/Returns Mean                               979.961\n",
      "evaluation/Returns Std                                 34.6859\n",
      "evaluation/Returns Max                               1073.03\n",
      "evaluation/Returns Min                                944.678\n",
      "evaluation/Actions Mean                                -0.00909765\n",
      "evaluation/Actions Std                                  0.106662\n",
      "evaluation/Actions Max                                  0.527496\n",
      "evaluation/Actions Min                                 -0.434477\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.961\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00201409\n",
      "evaluation/env_infos/final/reward_forward Std           0.219482\n",
      "evaluation/env_infos/final/reward_forward Max           0.433815\n",
      "evaluation/env_infos/final/reward_forward Min          -0.61658\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0415769\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119397\n",
      "evaluation/env_infos/initial/reward_forward Max         0.33379\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.135632\n",
      "evaluation/env_infos/reward_forward Mean               -0.00763147\n",
      "evaluation/env_infos/reward_forward Std                 0.238317\n",
      "evaluation/env_infos/reward_forward Max                 1.63148\n",
      "evaluation/env_infos/reward_forward Min                -1.60728\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0450367\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00984847\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.030162\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.073325\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.057555\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0152248\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0386515\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0950978\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0458379\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0127179\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0220722\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.264491\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.0111612\n",
      "evaluation/env_infos/final/torso_velocity Std           0.218686\n",
      "evaluation/env_infos/final/torso_velocity Max           0.61977\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.783138\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14625\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228943\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.639741\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.276231\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0023403\n",
      "evaluation/env_infos/torso_velocity Std                 0.226141\n",
      "evaluation/env_infos/torso_velocity Max                 1.63148\n",
      "evaluation/env_infos/torso_velocity Min                -1.86183\n",
      "time/data storing (s)                                   0.0151889\n",
      "time/evaluation sampling (s)                           45.6577\n",
      "time/exploration sampling (s)                           1.76132\n",
      "time/logging (s)                                        0.269388\n",
      "time/saving (s)                                         0.0561074\n",
      "time/training (s)                                       3.82629\n",
      "time/epoch (s)                                         51.586\n",
      "time/total (s)                                        542.466\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[14978]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1a1bb778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a23c740). One of the two will be used. Which one is undefined.\n",
      "objc[14978]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1a1bb700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a23c768). One of the two will be used. Which one is undefined.\n",
      "objc[14978]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1a1bb7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a23c7b8). One of the two will be used. Which one is undefined.\n",
      "objc[14978]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1a1bb818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a23c830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 09:57:54.936028 PDT | Variant:\n",
      "2021-05-25 09:57:54.937021 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 1,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 09:58:49.347212 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      21.9245\n",
      "trainer/QF2 Loss                                      21.9056\n",
      "trainer/Policy Loss                                   -5.32935\n",
      "trainer/Q1 Predictions Mean                            0.00213752\n",
      "trainer/Q1 Predictions Std                             0.00401022\n",
      "trainer/Q1 Predictions Max                             0.0149149\n",
      "trainer/Q1 Predictions Min                            -0.0106003\n",
      "trainer/Q2 Predictions Mean                            0.00417708\n",
      "trainer/Q2 Predictions Std                             0.00328119\n",
      "trainer/Q2 Predictions Max                             0.0133902\n",
      "trainer/Q2 Predictions Min                            -0.00264082\n",
      "trainer/Q Targets Mean                                 4.58484\n",
      "trainer/Q Targets Std                                  0.960845\n",
      "trainer/Q Targets Max                                  6.48468\n",
      "trainer/Q Targets Min                                 -1.14557\n",
      "trainer/Log Pis Mean                                  -5.3285\n",
      "trainer/Log Pis Std                                    0.609576\n",
      "trainer/Log Pis Max                                   -3.4712\n",
      "trainer/Log Pis Min                                   -7.65618\n",
      "trainer/Policy mu Mean                                -0.000270115\n",
      "trainer/Policy mu Std                                  0.00206103\n",
      "trainer/Policy mu Max                                  0.00602639\n",
      "trainer/Policy mu Min                                 -0.00687459\n",
      "trainer/Policy log std Mean                           -4.08172e-05\n",
      "trainer/Policy log std Std                             0.00220226\n",
      "trainer/Policy log std Max                             0.00716723\n",
      "trainer/Policy log std Min                            -0.00662443\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           15\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.51781\n",
      "exploration/Rewards Std                                0.456387\n",
      "exploration/Rewards Max                                1.86897\n",
      "exploration/Rewards Min                               -1.90025\n",
      "exploration/Returns Mean                            -517.81\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -517.81\n",
      "exploration/Returns Min                             -517.81\n",
      "exploration/Actions Mean                               0.00459306\n",
      "exploration/Actions Std                                0.622421\n",
      "exploration/Actions Max                                0.999664\n",
      "exploration/Actions Min                               -0.999062\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -517.81\n",
      "exploration/env_infos/final/reward_forward Mean       -0.497396\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.497396\n",
      "exploration/env_infos/final/reward_forward Min        -0.497396\n",
      "exploration/env_infos/initial/reward_forward Mean      0.284803\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.284803\n",
      "exploration/env_infos/initial/reward_forward Min       0.284803\n",
      "exploration/env_infos/reward_forward Mean              0.0171128\n",
      "exploration/env_infos/reward_forward Std               0.365117\n",
      "exploration/env_infos/reward_forward Max               1.48408\n",
      "exploration/env_infos/reward_forward Min              -1.2585\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.34189\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.34189\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.34189\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.75311\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.75311\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.75311\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54972\n",
      "exploration/env_infos/reward_ctrl Std                  0.438055\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477806\n",
      "exploration/env_infos/reward_ctrl Min                 -2.90025\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.262303\n",
      "exploration/env_infos/final/torso_velocity Std         0.169696\n",
      "exploration/env_infos/final/torso_velocity Max        -0.103004\n",
      "exploration/env_infos/final/torso_velocity Min        -0.497396\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.142836\n",
      "exploration/env_infos/initial/torso_velocity Std       0.378187\n",
      "exploration/env_infos/initial/torso_velocity Max       0.518419\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.374715\n",
      "exploration/env_infos/torso_velocity Mean             -0.00158226\n",
      "exploration/env_infos/torso_velocity Std               0.36274\n",
      "exploration/env_infos/torso_velocity Max               1.48408\n",
      "exploration/env_infos/torso_velocity Min              -2.06649\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.0012\n",
      "evaluation/Rewards Std                                 0.0220175\n",
      "evaluation/Rewards Max                                 2.27934\n",
      "evaluation/Rewards Min                                 0.999985\n",
      "evaluation/Returns Mean                             1001.2\n",
      "evaluation/Returns Std                                 1.2289\n",
      "evaluation/Returns Max                              1004.33\n",
      "evaluation/Returns Min                               999.993\n",
      "evaluation/Actions Mean                               -0.000133337\n",
      "evaluation/Actions Std                                 0.00115468\n",
      "evaluation/Actions Max                                 0.00366438\n",
      "evaluation/Actions Min                                -0.00394578\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1001.2\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000278864\n",
      "evaluation/env_infos/final/reward_forward Std          0.000348322\n",
      "evaluation/env_infos/final/reward_forward Max          0.000983935\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000561206\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0246761\n",
      "evaluation/env_infos/initial/reward_forward Std        0.118016\n",
      "evaluation/env_infos/initial/reward_forward Max        0.203236\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.250379\n",
      "evaluation/env_infos/reward_forward Mean               0.00198237\n",
      "evaluation/env_infos/reward_forward Std                0.0452778\n",
      "evaluation/env_infos/reward_forward Max                1.24515\n",
      "evaluation/env_infos/reward_forward Min               -1.00289\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.38048e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             4.98878e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.59247e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.49572e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.37677e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           5.33372e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.50826e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.27701e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.40424e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   6.42822e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -3.24066e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.49802e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000163189\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000374255\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00164246\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000561206\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.166308\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.221115\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.582374\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.250379\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000330363\n",
      "evaluation/env_infos/torso_velocity Std                0.0528065\n",
      "evaluation/env_infos/torso_velocity Max                1.24515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/torso_velocity Min               -2.04041\n",
      "time/data storing (s)                                  0.0315492\n",
      "time/evaluation sampling (s)                          46.8784\n",
      "time/exploration sampling (s)                          1.89007\n",
      "time/logging (s)                                       0.393987\n",
      "time/saving (s)                                        0.0674955\n",
      "time/training (s)                                      3.40034\n",
      "time/epoch (s)                                        52.6618\n",
      "time/total (s)                                        58.1673\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 09:59:41.819580 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.571968\n",
      "trainer/QF2 Loss                                       0.565698\n",
      "trainer/Policy Loss                                   -9.37513\n",
      "trainer/Q1 Predictions Mean                            3.94253\n",
      "trainer/Q1 Predictions Std                             0.407784\n",
      "trainer/Q1 Predictions Max                             5.29463\n",
      "trainer/Q1 Predictions Min                             2.08445\n",
      "trainer/Q2 Predictions Mean                            3.92891\n",
      "trainer/Q2 Predictions Std                             0.407352\n",
      "trainer/Q2 Predictions Max                             5.20685\n",
      "trainer/Q2 Predictions Min                             2.09047\n",
      "trainer/Q Targets Mean                                 4.05242\n",
      "trainer/Q Targets Std                                  0.634167\n",
      "trainer/Q Targets Max                                  5.90538\n",
      "trainer/Q Targets Min                                 -0.450851\n",
      "trainer/Log Pis Mean                                  -5.45676\n",
      "trainer/Log Pis Std                                    0.453778\n",
      "trainer/Log Pis Max                                   -4.48714\n",
      "trainer/Log Pis Min                                   -9.59206\n",
      "trainer/Policy mu Mean                                -0.0108972\n",
      "trainer/Policy mu Std                                  0.0301198\n",
      "trainer/Policy mu Max                                  0.0709077\n",
      "trainer/Policy mu Min                                 -0.111835\n",
      "trainer/Policy log std Mean                           -0.109858\n",
      "trainer/Policy log std Std                             0.0233252\n",
      "trainer/Policy log std Max                            -0.0610878\n",
      "trainer/Policy log std Min                            -0.233556\n",
      "trainer/Alpha                                          0.738535\n",
      "trainer/Alpha Loss                                    -4.03818\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           25\n",
      "exploration/path length Mean                         100\n",
      "exploration/path length Std                           92.1401\n",
      "exploration/path length Max                          305\n",
      "exploration/path length Min                           22\n",
      "exploration/Rewards Mean                              -0.405631\n",
      "exploration/Rewards Std                                0.453765\n",
      "exploration/Rewards Max                                1.92411\n",
      "exploration/Rewards Min                               -1.7525\n",
      "exploration/Returns Mean                             -40.5631\n",
      "exploration/Returns Std                               35.412\n",
      "exploration/Returns Max                               -7.16738\n",
      "exploration/Returns Min                             -117.141\n",
      "exploration/Actions Mean                              -0.0142647\n",
      "exploration/Actions Std                                0.599325\n",
      "exploration/Actions Max                                0.998148\n",
      "exploration/Actions Min                               -0.998883\n",
      "exploration/Num Paths                                 10\n",
      "exploration/Average Returns                          -40.5631\n",
      "exploration/env_infos/final/reward_forward Mean        0.15575\n",
      "exploration/env_infos/final/reward_forward Std         0.412538\n",
      "exploration/env_infos/final/reward_forward Max         0.920705\n",
      "exploration/env_infos/final/reward_forward Min        -0.398096\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0718107\n",
      "exploration/env_infos/initial/reward_forward Std       0.0740022\n",
      "exploration/env_infos/initial/reward_forward Max       0.0868763\n",
      "exploration/env_infos/initial/reward_forward Min      -0.170768\n",
      "exploration/env_infos/reward_forward Mean             -0.0931876\n",
      "exploration/env_infos/reward_forward Std               0.789161\n",
      "exploration/env_infos/reward_forward Max               2.38519\n",
      "exploration/env_infos/reward_forward Min              -2.45366\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.80673\n",
      "exploration/env_infos/final/reward_ctrl Std            0.382056\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.889563\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.27011\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.54314\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.349976\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.872096\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.01709\n",
      "exploration/env_infos/reward_ctrl Mean                -1.43758\n",
      "exploration/env_infos/reward_ctrl Std                  0.410974\n",
      "exploration/env_infos/reward_ctrl Max                 -0.390329\n",
      "exploration/env_infos/reward_ctrl Min                 -2.7525\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.339685\n",
      "exploration/env_infos/final/torso_velocity Std         1.14445\n",
      "exploration/env_infos/final/torso_velocity Max         3.50372\n",
      "exploration/env_infos/final/torso_velocity Min        -1.72809\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.128898\n",
      "exploration/env_infos/initial/torso_velocity Std       0.261484\n",
      "exploration/env_infos/initial/torso_velocity Max       0.577046\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.170768\n",
      "exploration/env_infos/torso_velocity Mean             -0.0276916\n",
      "exploration/env_infos/torso_velocity Std               0.800235\n",
      "exploration/env_infos/torso_velocity Max               4.2744\n",
      "exploration/env_infos/torso_velocity Min              -2.45366\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.998733\n",
      "evaluation/Rewards Std                                 0.0285386\n",
      "evaluation/Rewards Max                                 2.43948\n",
      "evaluation/Rewards Min                                 0.993926\n",
      "evaluation/Returns Mean                              998.733\n",
      "evaluation/Returns Std                                 1.56242\n",
      "evaluation/Returns Max                              1004.23\n",
      "evaluation/Returns Min                               997.416\n",
      "evaluation/Actions Mean                               -0.00993893\n",
      "evaluation/Actions Std                                 0.0225603\n",
      "evaluation/Actions Max                                 0.0341032\n",
      "evaluation/Actions Min                                -0.0873282\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           998.733\n",
      "evaluation/env_infos/final/reward_forward Mean         2.7033e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.8034e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.38296e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.49869e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0145593\n",
      "evaluation/env_infos/initial/reward_forward Std        0.116704\n",
      "evaluation/env_infos/initial/reward_forward Max        0.263329\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.176002\n",
      "evaluation/env_infos/reward_forward Mean               0.00339504\n",
      "evaluation/env_infos/reward_forward Std                0.0509897\n",
      "evaluation/env_infos/reward_forward Max                1.42614\n",
      "evaluation/env_infos/reward_forward Min               -1.15724\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00241984\n",
      "evaluation/env_infos/final/reward_ctrl Std             8.94937e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00230142\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00259376\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00188415\n",
      "evaluation/env_infos/initial/reward_ctrl Std           9.41566e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00168269\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00212628\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00243099\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000163297\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00150294\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00623927\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.77614e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.59477e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          5.38296e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.49869e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.151568\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.238862\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.670481\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.2839\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000634666\n",
      "evaluation/env_infos/torso_velocity Std                0.0604237\n",
      "evaluation/env_infos/torso_velocity Max                1.43432\n",
      "evaluation/env_infos/torso_velocity Min               -1.83953\n",
      "time/data storing (s)                                  0.0359312\n",
      "time/evaluation sampling (s)                          46.4468\n",
      "time/exploration sampling (s)                          1.87273\n",
      "time/logging (s)                                       0.273772\n",
      "time/saving (s)                                        0.0258915\n",
      "time/training (s)                                      3.40165\n",
      "time/epoch (s)                                        52.0568\n",
      "time/total (s)                                       110.519\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:00:32.827786 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.48717\n",
      "trainer/QF2 Loss                                       0.486197\n",
      "trainer/Policy Loss                                   -9.19571\n",
      "trainer/Q1 Predictions Mean                            3.7518\n",
      "trainer/Q1 Predictions Std                             0.32034\n",
      "trainer/Q1 Predictions Max                             4.83868\n",
      "trainer/Q1 Predictions Min                             2.58427\n",
      "trainer/Q2 Predictions Mean                            3.74952\n",
      "trainer/Q2 Predictions Std                             0.325163\n",
      "trainer/Q2 Predictions Max                             4.80562\n",
      "trainer/Q2 Predictions Min                             2.55646\n",
      "trainer/Q Targets Mean                                 3.76905\n",
      "trainer/Q Targets Std                                  0.739317\n",
      "trainer/Q Targets Max                                  7.44555\n",
      "trainer/Q Targets Min                                 -1.21241\n",
      "trainer/Log Pis Mean                                  -5.46016\n",
      "trainer/Log Pis Std                                    0.354311\n",
      "trainer/Log Pis Max                                   -4.50815\n",
      "trainer/Log Pis Min                                   -7.91578\n",
      "trainer/Policy mu Mean                                -0.0111043\n",
      "trainer/Policy mu Std                                  0.0312047\n",
      "trainer/Policy mu Max                                  0.0854397\n",
      "trainer/Policy mu Min                                 -0.128231\n",
      "trainer/Policy log std Mean                           -0.121613\n",
      "trainer/Policy log std Std                             0.0151424\n",
      "trainer/Policy log std Max                            -0.0885434\n",
      "trainer/Policy log std Min                            -0.208291\n",
      "trainer/Alpha                                          0.547082\n",
      "trainer/Alpha Loss                                    -8.07819\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           28\n",
      "exploration/path length Mean                         333.333\n",
      "exploration/path length Std                          385.345\n",
      "exploration/path length Max                          877\n",
      "exploration/path length Min                           29\n",
      "exploration/Rewards Mean                              -0.362143\n",
      "exploration/Rewards Std                                0.460332\n",
      "exploration/Rewards Max                                1.82464\n",
      "exploration/Rewards Min                               -1.77013\n",
      "exploration/Returns Mean                            -120.714\n",
      "exploration/Returns Std                              139.554\n",
      "exploration/Returns Max                               -9.42055\n",
      "exploration/Returns Min                             -317.511\n",
      "exploration/Actions Mean                              -0.00767992\n",
      "exploration/Actions Std                                0.593105\n",
      "exploration/Actions Max                                0.997855\n",
      "exploration/Actions Min                               -0.995788\n",
      "exploration/Num Paths                                  3\n",
      "exploration/Average Returns                         -120.714\n",
      "exploration/env_infos/final/reward_forward Mean       -0.576747\n",
      "exploration/env_infos/final/reward_forward Std         0.0616195\n",
      "exploration/env_infos/final/reward_forward Max        -0.503009\n",
      "exploration/env_infos/final/reward_forward Min        -0.653834\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0451349\n",
      "exploration/env_infos/initial/reward_forward Std       0.177232\n",
      "exploration/env_infos/initial/reward_forward Max       0.219939\n",
      "exploration/env_infos/initial/reward_forward Min      -0.197828\n",
      "exploration/env_infos/reward_forward Mean             -0.022507\n",
      "exploration/env_infos/reward_forward Std               0.450978\n",
      "exploration/env_infos/reward_forward Max               1.7498\n",
      "exploration/env_infos/reward_forward Min              -2.88454\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.40478\n",
      "exploration/env_infos/final/reward_ctrl Std            0.250448\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.08455\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.69596\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.24707\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.3897\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.791739\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.74363\n",
      "exploration/env_infos/reward_ctrl Mean                -1.40733\n",
      "exploration/env_infos/reward_ctrl Std                  0.422704\n",
      "exploration/env_infos/reward_ctrl Max                 -0.23889\n",
      "exploration/env_infos/reward_ctrl Min                 -2.77013\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.12435\n",
      "exploration/env_infos/final/torso_velocity Std         0.686968\n",
      "exploration/env_infos/final/torso_velocity Max         1.59522\n",
      "exploration/env_infos/final/torso_velocity Min        -0.653834\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0519487\n",
      "exploration/env_infos/initial/torso_velocity Std       0.325171\n",
      "exploration/env_infos/initial/torso_velocity Max       0.492027\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.504662\n",
      "exploration/env_infos/torso_velocity Mean             -0.0343666\n",
      "exploration/env_infos/torso_velocity Std               0.483233\n",
      "exploration/env_infos/torso_velocity Max               2.39756\n",
      "exploration/env_infos/torso_velocity Min              -2.88454\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.996512\n",
      "evaluation/Rewards Std                                 0.0236017\n",
      "evaluation/Rewards Max                                 2.2477\n",
      "evaluation/Rewards Min                                 0.993369\n",
      "evaluation/Returns Mean                              996.512\n",
      "evaluation/Returns Std                                 1.52083\n",
      "evaluation/Returns Max                              1000.94\n",
      "evaluation/Returns Min                               995.434\n",
      "evaluation/Actions Mean                               -0.0133415\n",
      "evaluation/Actions Std                                 0.0295936\n",
      "evaluation/Actions Max                                 0.0678015\n",
      "evaluation/Actions Min                                -0.0744642\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           996.512\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.41019e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.03553e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.09598e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.45205e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0162657\n",
      "evaluation/env_infos/initial/reward_forward Std        0.112599\n",
      "evaluation/env_infos/initial/reward_forward Max        0.193197\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.275161\n",
      "evaluation/env_infos/reward_forward Mean              -0.00456051\n",
      "evaluation/env_infos/reward_forward Std                0.0605766\n",
      "evaluation/env_infos/reward_forward Max                0.897086\n",
      "evaluation/env_infos/reward_forward Min               -1.50397\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00421381\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00029477\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00364291\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00456939\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0034816\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000334389\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0028268\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00401145\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00421511\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000302474\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00249568\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00663085\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.61317e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.79699e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.2628e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.0981e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.146769\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.234574\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.605608\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.275161\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00226314\n",
      "evaluation/env_infos/torso_velocity Std                0.0628062\n",
      "evaluation/env_infos/torso_velocity Max                1.40409\n",
      "evaluation/env_infos/torso_velocity Min               -1.79113\n",
      "time/data storing (s)                                  0.0316901\n",
      "time/evaluation sampling (s)                          45.1651\n",
      "time/exploration sampling (s)                          1.87402\n",
      "time/logging (s)                                       0.282805\n",
      "time/saving (s)                                        0.026921\n",
      "time/training (s)                                      3.44948\n",
      "time/epoch (s)                                        50.83\n",
      "time/total (s)                                       161.536\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:01:23.582621 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.400678\n",
      "trainer/QF2 Loss                                        0.405598\n",
      "trainer/Policy Loss                                    -9.20301\n",
      "trainer/Q1 Predictions Mean                             3.74682\n",
      "trainer/Q1 Predictions Std                              0.253127\n",
      "trainer/Q1 Predictions Max                              4.41642\n",
      "trainer/Q1 Predictions Min                              3.11596\n",
      "trainer/Q2 Predictions Mean                             3.74361\n",
      "trainer/Q2 Predictions Std                              0.25378\n",
      "trainer/Q2 Predictions Max                              4.40245\n",
      "trainer/Q2 Predictions Min                              3.0196\n",
      "trainer/Q Targets Mean                                  3.74578\n",
      "trainer/Q Targets Std                                   0.631819\n",
      "trainer/Q Targets Max                                   5.48589\n",
      "trainer/Q Targets Min                                  -1.03824\n",
      "trainer/Log Pis Mean                                   -5.48274\n",
      "trainer/Log Pis Std                                     0.425215\n",
      "trainer/Log Pis Max                                    -4.62198\n",
      "trainer/Log Pis Min                                    -9.55747\n",
      "trainer/Policy mu Mean                                 -0.00617848\n",
      "trainer/Policy mu Std                                   0.0313709\n",
      "trainer/Policy mu Max                                   0.0906833\n",
      "trainer/Policy mu Min                                  -0.112549\n",
      "trainer/Policy log std Mean                            -0.140139\n",
      "trainer/Policy log std Std                              0.0166882\n",
      "trainer/Policy log std Max                             -0.107265\n",
      "trainer/Policy log std Min                             -0.217777\n",
      "trainer/Alpha                                           0.405267\n",
      "trainer/Alpha Loss                                    -12.1373\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            29\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.337528\n",
      "exploration/Rewards Std                                 0.425829\n",
      "exploration/Rewards Max                                 1.25998\n",
      "exploration/Rewards Min                                -1.66749\n",
      "exploration/Returns Mean                             -337.528\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -337.528\n",
      "exploration/Returns Min                              -337.528\n",
      "exploration/Actions Mean                                0.00131552\n",
      "exploration/Actions Std                                 0.585163\n",
      "exploration/Actions Max                                 0.997245\n",
      "exploration/Actions Min                                -0.994873\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -337.528\n",
      "exploration/env_infos/final/reward_forward Mean        -0.577748\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.577748\n",
      "exploration/env_infos/final/reward_forward Min         -0.577748\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0035466\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0035466\n",
      "exploration/env_infos/initial/reward_forward Min        0.0035466\n",
      "exploration/env_infos/reward_forward Mean              -0.0103051\n",
      "exploration/env_infos/reward_forward Std                0.479076\n",
      "exploration/env_infos/reward_forward Max                1.4773\n",
      "exploration/env_infos/reward_forward Min               -2.15757\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.806769\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.806769\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.806769\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.3407\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.3407\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.3407\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.36967\n",
      "exploration/env_infos/reward_ctrl Std                   0.404258\n",
      "exploration/env_infos/reward_ctrl Max                  -0.320913\n",
      "exploration/env_infos/reward_ctrl Min                  -2.66749\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.419209\n",
      "exploration/env_infos/final/torso_velocity Std          0.342734\n",
      "exploration/env_infos/final/torso_velocity Max          0.0567324\n",
      "exploration/env_infos/final/torso_velocity Min         -0.736613\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.130525\n",
      "exploration/env_infos/initial/torso_velocity Std        0.257204\n",
      "exploration/env_infos/initial/torso_velocity Max        0.489206\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.101179\n",
      "exploration/env_infos/torso_velocity Mean               0.0108458\n",
      "exploration/env_infos/torso_velocity Std                0.465102\n",
      "exploration/env_infos/torso_velocity Max                2.66034\n",
      "exploration/env_infos/torso_velocity Min               -3.08268\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.996379\n",
      "evaluation/Rewards Std                                  0.0207632\n",
      "evaluation/Rewards Max                                  1.95315\n",
      "evaluation/Rewards Min                                  0.99466\n",
      "evaluation/Returns Mean                               996.379\n",
      "evaluation/Returns Std                                  1.52654\n",
      "evaluation/Returns Max                               1000.8\n",
      "evaluation/Returns Min                                994.999\n",
      "evaluation/Actions Mean                                -0.00980733\n",
      "evaluation/Actions Std                                  0.0318005\n",
      "evaluation/Actions Max                                  0.0415141\n",
      "evaluation/Actions Min                                 -0.0869922\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            996.379\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.13765e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.17476e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.10245e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.91733e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00826398\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124016\n",
      "evaluation/env_infos/initial/reward_forward Max         0.277605\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.188401\n",
      "evaluation/env_infos/reward_forward Mean                0.00117451\n",
      "evaluation/env_infos/reward_forward Std                 0.0566527\n",
      "evaluation/env_infos/reward_forward Max                 1.1225\n",
      "evaluation/env_infos/reward_forward Min                -1.09415\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00443026\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000591432\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00335925\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00512907\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00325433\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000471997\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0023352\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00396572\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00442983\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000597306\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0023352\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00534027\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.24247e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.94951e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.35942e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.12102e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.132629\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235169\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.654417\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.235831\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00149249\n",
      "evaluation/env_infos/torso_velocity Std                 0.0575139\n",
      "evaluation/env_infos/torso_velocity Max                 1.32971\n",
      "evaluation/env_infos/torso_velocity Min                -1.67973\n",
      "time/data storing (s)                                   0.0325922\n",
      "time/evaluation sampling (s)                           44.9586\n",
      "time/exploration sampling (s)                           1.87379\n",
      "time/logging (s)                                        0.277914\n",
      "time/saving (s)                                         0.0256232\n",
      "time/training (s)                                       3.38256\n",
      "time/epoch (s)                                         50.5511\n",
      "time/total (s)                                        212.286\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:02:14.808127 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.35907\n",
      "trainer/QF2 Loss                                        0.361616\n",
      "trainer/Policy Loss                                    -9.24829\n",
      "trainer/Q1 Predictions Mean                             3.82497\n",
      "trainer/Q1 Predictions Std                              0.297865\n",
      "trainer/Q1 Predictions Max                              4.68358\n",
      "trainer/Q1 Predictions Min                              2.63472\n",
      "trainer/Q2 Predictions Mean                             3.83319\n",
      "trainer/Q2 Predictions Std                              0.279023\n",
      "trainer/Q2 Predictions Max                              4.64354\n",
      "trainer/Q2 Predictions Min                              2.9332\n",
      "trainer/Q Targets Mean                                  3.83063\n",
      "trainer/Q Targets Std                                   0.629733\n",
      "trainer/Q Targets Max                                   5.67468\n",
      "trainer/Q Targets Min                                  -1.21676\n",
      "trainer/Log Pis Mean                                   -5.44913\n",
      "trainer/Log Pis Std                                     0.354639\n",
      "trainer/Log Pis Max                                    -4.6472\n",
      "trainer/Log Pis Min                                    -7.22903\n",
      "trainer/Policy mu Mean                                 -0.00285225\n",
      "trainer/Policy mu Std                                   0.0318254\n",
      "trainer/Policy mu Max                                   0.0845301\n",
      "trainer/Policy mu Min                                  -0.115524\n",
      "trainer/Policy log std Mean                            -0.127263\n",
      "trainer/Policy log std Std                              0.0131846\n",
      "trainer/Policy log std Max                             -0.0918926\n",
      "trainer/Policy log std Min                             -0.186868\n",
      "trainer/Alpha                                           0.300191\n",
      "trainer/Alpha Loss                                    -16.1435\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            42\n",
      "exploration/path length Mean                           76.9231\n",
      "exploration/path length Std                            39.1102\n",
      "exploration/path length Max                           140\n",
      "exploration/path length Min                            14\n",
      "exploration/Rewards Mean                               -0.326362\n",
      "exploration/Rewards Std                                 0.475249\n",
      "exploration/Rewards Max                                 1.6627\n",
      "exploration/Rewards Min                                -1.80198\n",
      "exploration/Returns Mean                              -25.1048\n",
      "exploration/Returns Std                                15.0019\n",
      "exploration/Returns Max                                -2.89457\n",
      "exploration/Returns Min                               -51.3362\n",
      "exploration/Actions Mean                               -0.0139856\n",
      "exploration/Actions Std                                 0.588575\n",
      "exploration/Actions Max                                 0.996247\n",
      "exploration/Actions Min                                -0.999\n",
      "exploration/Num Paths                                  13\n",
      "exploration/Average Returns                           -25.1048\n",
      "exploration/env_infos/final/reward_forward Mean         0.285164\n",
      "exploration/env_infos/final/reward_forward Std          0.846784\n",
      "exploration/env_infos/final/reward_forward Max          1.61945\n",
      "exploration/env_infos/final/reward_forward Min         -1.07058\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.107162\n",
      "exploration/env_infos/initial/reward_forward Std        0.224775\n",
      "exploration/env_infos/initial/reward_forward Max        0.237155\n",
      "exploration/env_infos/initial/reward_forward Min       -0.640946\n",
      "exploration/env_infos/reward_forward Mean               0.0422718\n",
      "exploration/env_infos/reward_forward Std                0.708821\n",
      "exploration/env_infos/reward_forward Max                1.86585\n",
      "exploration/env_infos/reward_forward Min               -2.32217\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.34693\n",
      "exploration/env_infos/final/reward_ctrl Std             0.323177\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.746806\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.868\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.16852\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.341344\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.605548\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.89333\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38646\n",
      "exploration/env_infos/reward_ctrl Std                   0.404376\n",
      "exploration/env_infos/reward_ctrl Max                  -0.314537\n",
      "exploration/env_infos/reward_ctrl Min                  -2.80198\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.321308\n",
      "exploration/env_infos/final/torso_velocity Std          0.950368\n",
      "exploration/env_infos/final/torso_velocity Max          1.82474\n",
      "exploration/env_infos/final/torso_velocity Min         -2.30831\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.108841\n",
      "exploration/env_infos/initial/torso_velocity Std        0.313424\n",
      "exploration/env_infos/initial/torso_velocity Max        0.755021\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.640946\n",
      "exploration/env_infos/torso_velocity Mean               0.0375346\n",
      "exploration/env_infos/torso_velocity Std                0.856418\n",
      "exploration/env_infos/torso_velocity Max                3.22366\n",
      "exploration/env_infos/torso_velocity Min               -3.69284\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.995835\n",
      "evaluation/Rewards Std                                  0.024174\n",
      "evaluation/Rewards Max                                  2.23376\n",
      "evaluation/Rewards Min                                  0.993551\n",
      "evaluation/Returns Mean                               995.835\n",
      "evaluation/Returns Std                                  1.74241\n",
      "evaluation/Returns Max                               1001.08\n",
      "evaluation/Returns Min                                994.136\n",
      "evaluation/Actions Mean                                -0.00701571\n",
      "evaluation/Actions Std                                  0.0350907\n",
      "evaluation/Actions Max                                  0.0492942\n",
      "evaluation/Actions Min                                 -0.0730553\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            995.835\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.000118793\n",
      "evaluation/env_infos/final/reward_forward Std           0.000402953\n",
      "evaluation/env_infos/final/reward_forward Max           2.02795e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00155759\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0152216\n",
      "evaluation/env_infos/initial/reward_forward Std         0.126204\n",
      "evaluation/env_infos/initial/reward_forward Max         0.346248\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.254958\n",
      "evaluation/env_infos/reward_forward Mean                0.00201372\n",
      "evaluation/env_infos/reward_forward Std                 0.057969\n",
      "evaluation/env_infos/reward_forward Max                 1.06965\n",
      "evaluation/env_infos/reward_forward Min                -1.15215\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00514897\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000658295\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00372863\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00604724\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00396513\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000646154\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00285119\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00484086\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00512231\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000648865\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00236146\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00644937\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.06671e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000461542\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00274038\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00196434\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146093\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246711\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.637315\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.326821\n",
      "evaluation/env_infos/torso_velocity Mean                0.000138242\n",
      "evaluation/env_infos/torso_velocity Std                 0.0605849\n",
      "evaluation/env_infos/torso_velocity Max                 1.65876\n",
      "evaluation/env_infos/torso_velocity Min                -1.72801\n",
      "time/data storing (s)                                   0.0363155\n",
      "time/evaluation sampling (s)                           45.4509\n",
      "time/exploration sampling (s)                           1.81582\n",
      "time/logging (s)                                        0.286919\n",
      "time/saving (s)                                         0.0277497\n",
      "time/training (s)                                       3.41062\n",
      "time/epoch (s)                                         51.0284\n",
      "time/total (s)                                        263.52\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:03:05.899858 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.28764\n",
      "trainer/QF2 Loss                                        0.295045\n",
      "trainer/Policy Loss                                    -9.20853\n",
      "trainer/Q1 Predictions Mean                             3.775\n",
      "trainer/Q1 Predictions Std                              0.277859\n",
      "trainer/Q1 Predictions Max                              4.44989\n",
      "trainer/Q1 Predictions Min                              3.0247\n",
      "trainer/Q2 Predictions Mean                             3.76157\n",
      "trainer/Q2 Predictions Std                              0.283051\n",
      "trainer/Q2 Predictions Max                              4.48197\n",
      "trainer/Q2 Predictions Min                              2.94461\n",
      "trainer/Q Targets Mean                                  3.79157\n",
      "trainer/Q Targets Std                                   0.565025\n",
      "trainer/Q Targets Max                                   5.31769\n",
      "trainer/Q Targets Min                                  -0.177234\n",
      "trainer/Log Pis Mean                                   -5.4829\n",
      "trainer/Log Pis Std                                     0.526594\n",
      "trainer/Log Pis Max                                    -4.05289\n",
      "trainer/Log Pis Min                                    -9.72565\n",
      "trainer/Policy mu Mean                                  0.00722635\n",
      "trainer/Policy mu Std                                   0.0647851\n",
      "trainer/Policy mu Max                                   0.279784\n",
      "trainer/Policy mu Min                                  -0.350773\n",
      "trainer/Policy log std Mean                            -0.134476\n",
      "trainer/Policy log std Std                              0.0151266\n",
      "trainer/Policy log std Max                             -0.0687173\n",
      "trainer/Policy log std Min                             -0.208807\n",
      "trainer/Alpha                                           0.222423\n",
      "trainer/Alpha Loss                                    -20.2267\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            52\n",
      "exploration/path length Mean                          100\n",
      "exploration/path length Std                           118.551\n",
      "exploration/path length Max                           430\n",
      "exploration/path length Min                            23\n",
      "exploration/Rewards Mean                               -0.359451\n",
      "exploration/Rewards Std                                 0.469126\n",
      "exploration/Rewards Max                                 1.88972\n",
      "exploration/Rewards Min                                -2.04949\n",
      "exploration/Returns Mean                              -35.9451\n",
      "exploration/Returns Std                                45.7433\n",
      "exploration/Returns Max                                -3.53084\n",
      "exploration/Returns Min                              -162.731\n",
      "exploration/Actions Mean                                0.000317447\n",
      "exploration/Actions Std                                 0.593017\n",
      "exploration/Actions Max                                 0.996036\n",
      "exploration/Actions Min                                -0.99759\n",
      "exploration/Num Paths                                  10\n",
      "exploration/Average Returns                           -35.9451\n",
      "exploration/env_infos/final/reward_forward Mean         0.0369245\n",
      "exploration/env_infos/final/reward_forward Std          0.947273\n",
      "exploration/env_infos/final/reward_forward Max          2.76361\n",
      "exploration/env_infos/final/reward_forward Min         -0.681792\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0313986\n",
      "exploration/env_infos/initial/reward_forward Std        0.127526\n",
      "exploration/env_infos/initial/reward_forward Max        0.202301\n",
      "exploration/env_infos/initial/reward_forward Min       -0.124722\n",
      "exploration/env_infos/reward_forward Mean              -0.0750438\n",
      "exploration/env_infos/reward_forward Std                0.712857\n",
      "exploration/env_infos/reward_forward Max                3.15706\n",
      "exploration/env_infos/reward_forward Min               -2.42265\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.4097\n",
      "exploration/env_infos/final/reward_ctrl Std             0.443729\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.867072\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.16644\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.51075\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.405503\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.661115\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.09346\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.40668\n",
      "exploration/env_infos/reward_ctrl Std                   0.424014\n",
      "exploration/env_infos/reward_ctrl Max                  -0.262034\n",
      "exploration/env_infos/reward_ctrl Min                  -3.04949\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.293281\n",
      "exploration/env_infos/final/torso_velocity Std          0.933149\n",
      "exploration/env_infos/final/torso_velocity Max          2.76361\n",
      "exploration/env_infos/final/torso_velocity Min         -1.62835\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146769\n",
      "exploration/env_infos/initial/torso_velocity Std        0.253561\n",
      "exploration/env_infos/initial/torso_velocity Max        0.596953\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.354045\n",
      "exploration/env_infos/torso_velocity Mean              -0.0106733\n",
      "exploration/env_infos/torso_velocity Std                0.70683\n",
      "exploration/env_infos/torso_velocity Max                3.15706\n",
      "exploration/env_infos/torso_velocity Min               -2.77617\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.988826\n",
      "evaluation/Rewards Std                                  0.0193261\n",
      "evaluation/Rewards Max                                  2.31118\n",
      "evaluation/Rewards Min                                  0.967662\n",
      "evaluation/Returns Mean                               988.826\n",
      "evaluation/Returns Std                                  1.69054\n",
      "evaluation/Returns Max                                992.756\n",
      "evaluation/Returns Min                                987.114\n",
      "evaluation/Actions Mean                                -0.00337505\n",
      "evaluation/Actions Std                                  0.0539787\n",
      "evaluation/Actions Max                                  0.143435\n",
      "evaluation/Actions Min                                 -0.222932\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            988.826\n",
      "evaluation/env_infos/final/reward_forward Mean          4.36901e-07\n",
      "evaluation/env_infos/final/reward_forward Std           8.60741e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.88097e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.01081e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.000168727\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127071\n",
      "evaluation/env_infos/initial/reward_forward Max         0.216391\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.372027\n",
      "evaluation/env_infos/reward_forward Mean               -0.00300885\n",
      "evaluation/env_infos/reward_forward Std                 0.0498578\n",
      "evaluation/env_infos/reward_forward Max                 0.857469\n",
      "evaluation/env_infos/reward_forward Min                -1.3786\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0117031\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00113063\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00903804\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0129051\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00878553\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00186158\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00574084\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0111718\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0117004\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00117343\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00574084\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0323382\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          7.94176e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.92346e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.58827e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.01081e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144404\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219729\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.561371\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.372027\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00246492\n",
      "evaluation/env_infos/torso_velocity Std                 0.054291\n",
      "evaluation/env_infos/torso_velocity Max                 1.40887\n",
      "evaluation/env_infos/torso_velocity Min                -2.19306\n",
      "time/data storing (s)                                   0.0353029\n",
      "time/evaluation sampling (s)                           45.1825\n",
      "time/exploration sampling (s)                           1.8074\n",
      "time/logging (s)                                        0.288482\n",
      "time/saving (s)                                         0.0287226\n",
      "time/training (s)                                       3.5345\n",
      "time/epoch (s)                                         50.877\n",
      "time/total (s)                                        314.613\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:03:57.354428 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.306725\n",
      "trainer/QF2 Loss                                        0.314224\n",
      "trainer/Policy Loss                                    -9.16897\n",
      "trainer/Q1 Predictions Mean                             3.79655\n",
      "trainer/Q1 Predictions Std                              0.290136\n",
      "trainer/Q1 Predictions Max                              4.76397\n",
      "trainer/Q1 Predictions Min                              3.11103\n",
      "trainer/Q2 Predictions Mean                             3.77553\n",
      "trainer/Q2 Predictions Std                              0.289171\n",
      "trainer/Q2 Predictions Max                              4.82162\n",
      "trainer/Q2 Predictions Min                              3.04623\n",
      "trainer/Q Targets Mean                                  3.77874\n",
      "trainer/Q Targets Std                                   0.627073\n",
      "trainer/Q Targets Max                                   7.66051\n",
      "trainer/Q Targets Min                                  -0.680198\n",
      "trainer/Log Pis Mean                                   -5.41895\n",
      "trainer/Log Pis Std                                     0.604801\n",
      "trainer/Log Pis Max                                    -3.96108\n",
      "trainer/Log Pis Min                                   -11.2544\n",
      "trainer/Policy mu Mean                                 -0.015938\n",
      "trainer/Policy mu Std                                   0.101306\n",
      "trainer/Policy mu Max                                   0.360862\n",
      "trainer/Policy mu Min                                  -0.410485\n",
      "trainer/Policy log std Mean                            -0.169528\n",
      "trainer/Policy log std Std                              0.026906\n",
      "trainer/Policy log std Max                             -0.0954363\n",
      "trainer/Policy log std Min                             -0.298838\n",
      "trainer/Alpha                                           0.164855\n",
      "trainer/Alpha Loss                                    -24.1501\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            63\n",
      "exploration/path length Mean                           90.9091\n",
      "exploration/path length Std                            58.0712\n",
      "exploration/path length Max                           187\n",
      "exploration/path length Min                            19\n",
      "exploration/Rewards Mean                               -0.186235\n",
      "exploration/Rewards Std                                 0.595439\n",
      "exploration/Rewards Max                                 2.65303\n",
      "exploration/Rewards Min                                -1.57343\n",
      "exploration/Returns Mean                              -16.9304\n",
      "exploration/Returns Std                                11.8907\n",
      "exploration/Returns Max                                 1.36811\n",
      "exploration/Returns Min                               -41.2196\n",
      "exploration/Actions Mean                               -0.0180058\n",
      "exploration/Actions Std                                 0.576162\n",
      "exploration/Actions Max                                 0.997035\n",
      "exploration/Actions Min                                -0.994692\n",
      "exploration/Num Paths                                  11\n",
      "exploration/Average Returns                           -16.9304\n",
      "exploration/env_infos/final/reward_forward Mean         0.0529675\n",
      "exploration/env_infos/final/reward_forward Std          1.13824\n",
      "exploration/env_infos/final/reward_forward Max          2.08631\n",
      "exploration/env_infos/final/reward_forward Min         -2.16633\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0867723\n",
      "exploration/env_infos/initial/reward_forward Std        0.141755\n",
      "exploration/env_infos/initial/reward_forward Max        0.0952156\n",
      "exploration/env_infos/initial/reward_forward Min       -0.362385\n",
      "exploration/env_infos/reward_forward Mean               0.0464638\n",
      "exploration/env_infos/reward_forward Std                0.782666\n",
      "exploration/env_infos/reward_forward Max                2.51326\n",
      "exploration/env_infos/reward_forward Min               -2.16633\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.30689\n",
      "exploration/env_infos/final/reward_ctrl Std             0.388044\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.618029\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.91838\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.36356\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.4432\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.72254\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.21555\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.32915\n",
      "exploration/env_infos/reward_ctrl Std                   0.410538\n",
      "exploration/env_infos/reward_ctrl Max                  -0.141664\n",
      "exploration/env_infos/reward_ctrl Min                  -2.57343\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.381516\n",
      "exploration/env_infos/final/torso_velocity Std          1.18687\n",
      "exploration/env_infos/final/torso_velocity Max          3.33941\n",
      "exploration/env_infos/final/torso_velocity Min         -2.16633\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.113574\n",
      "exploration/env_infos/initial/torso_velocity Std        0.286274\n",
      "exploration/env_infos/initial/torso_velocity Max        0.628899\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.386732\n",
      "exploration/env_infos/torso_velocity Mean               0.0687723\n",
      "exploration/env_infos/torso_velocity Std                0.824221\n",
      "exploration/env_infos/torso_velocity Max                3.33941\n",
      "exploration/env_infos/torso_velocity Min               -2.2956\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969381\n",
      "evaluation/Rewards Std                                  0.0195476\n",
      "evaluation/Rewards Max                                  1.92681\n",
      "evaluation/Rewards Min                                  0.766408\n",
      "evaluation/Returns Mean                               969.381\n",
      "evaluation/Returns Std                                  3.72351\n",
      "evaluation/Returns Max                                978.612\n",
      "evaluation/Returns Min                                965.532\n",
      "evaluation/Actions Mean                                -0.0285589\n",
      "evaluation/Actions Std                                  0.0837442\n",
      "evaluation/Actions Max                                  0.340146\n",
      "evaluation/Actions Min                                 -0.376598\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.381\n",
      "evaluation/env_infos/final/reward_forward Mean          2.42743e-08\n",
      "evaluation/env_infos/final/reward_forward Std           1.03945e-06\n",
      "evaluation/env_infos/final/reward_forward Max           7.66614e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.76663e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00692412\n",
      "evaluation/env_infos/initial/reward_forward Std         0.143579\n",
      "evaluation/env_infos/initial/reward_forward Max         0.257402\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.214324\n",
      "evaluation/env_infos/reward_forward Mean                0.000361474\n",
      "evaluation/env_infos/reward_forward Std                 0.0476613\n",
      "evaluation/env_infos/reward_forward Max                 1.24296\n",
      "evaluation/env_infos/reward_forward Min                -1.2142\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0310849\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00330199\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0226079\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0341821\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0221976\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00578061\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0096486\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0289397\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0313148\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00674418\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0096486\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.233592\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.68693e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.74976e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           3.72713e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.35174e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138996\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218411\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.697269\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.214324\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00037741\n",
      "evaluation/env_infos/torso_velocity Std                 0.0527646\n",
      "evaluation/env_infos/torso_velocity Max                 1.65104\n",
      "evaluation/env_infos/torso_velocity Min                -1.76397\n",
      "time/data storing (s)                                   0.0384664\n",
      "time/evaluation sampling (s)                           45.32\n",
      "time/exploration sampling (s)                           1.89724\n",
      "time/logging (s)                                        0.293151\n",
      "time/saving (s)                                         0.0292755\n",
      "time/training (s)                                       3.65396\n",
      "time/epoch (s)                                         51.2321\n",
      "time/total (s)                                        366.072\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:04:51.514644 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.374359\n",
      "trainer/QF2 Loss                                        0.393061\n",
      "trainer/Policy Loss                                    -8.87142\n",
      "trainer/Q1 Predictions Mean                             3.61013\n",
      "trainer/Q1 Predictions Std                              0.368294\n",
      "trainer/Q1 Predictions Max                              4.85916\n",
      "trainer/Q1 Predictions Min                              2.50486\n",
      "trainer/Q2 Predictions Mean                             3.62641\n",
      "trainer/Q2 Predictions Std                              0.337006\n",
      "trainer/Q2 Predictions Max                              4.87764\n",
      "trainer/Q2 Predictions Min                              2.67183\n",
      "trainer/Q Targets Mean                                  3.74207\n",
      "trainer/Q Targets Std                                   0.673231\n",
      "trainer/Q Targets Max                                   6.61962\n",
      "trainer/Q Targets Min                                  -0.370076\n",
      "trainer/Log Pis Mean                                   -5.27303\n",
      "trainer/Log Pis Std                                     0.606746\n",
      "trainer/Log Pis Max                                    -3.21234\n",
      "trainer/Log Pis Min                                    -7.88015\n",
      "trainer/Policy mu Mean                                  0.0507152\n",
      "trainer/Policy mu Std                                   0.138799\n",
      "trainer/Policy mu Max                                   0.642327\n",
      "trainer/Policy mu Min                                  -0.40286\n",
      "trainer/Policy log std Mean                            -0.227511\n",
      "trainer/Policy log std Std                              0.0497386\n",
      "trainer/Policy log std Max                             -0.133648\n",
      "trainer/Policy log std Min                             -0.537319\n",
      "trainer/Alpha                                           0.122352\n",
      "trainer/Alpha Loss                                    -27.8453\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            67\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           240.116\n",
      "exploration/path length Max                           635\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.230329\n",
      "exploration/Rewards Std                                 0.489992\n",
      "exploration/Rewards Max                                 2.69652\n",
      "exploration/Rewards Min                                -1.69906\n",
      "exploration/Returns Mean                              -57.5822\n",
      "exploration/Returns Std                                64.2449\n",
      "exploration/Returns Max                                 5.55752\n",
      "exploration/Returns Min                              -157.198\n",
      "exploration/Actions Mean                                0.0376117\n",
      "exploration/Actions Std                                 0.565875\n",
      "exploration/Actions Max                                 0.992624\n",
      "exploration/Actions Min                                -0.989936\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                           -57.5822\n",
      "exploration/env_infos/final/reward_forward Mean        -0.589776\n",
      "exploration/env_infos/final/reward_forward Std          0.826098\n",
      "exploration/env_infos/final/reward_forward Max          0.733842\n",
      "exploration/env_infos/final/reward_forward Min         -1.53419\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0886382\n",
      "exploration/env_infos/initial/reward_forward Std        0.0843102\n",
      "exploration/env_infos/initial/reward_forward Max        0.183391\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0476674\n",
      "exploration/env_infos/reward_forward Mean               0.0520449\n",
      "exploration/env_infos/reward_forward Std                0.629176\n",
      "exploration/env_infos/reward_forward Max                2.46927\n",
      "exploration/env_infos/reward_forward Min               -2.32461\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.12797\n",
      "exploration/env_infos/final/reward_ctrl Std             0.421436\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.805084\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.83873\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.07501\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.277108\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.641269\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.41246\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.28652\n",
      "exploration/env_infos/reward_ctrl Std                   0.405742\n",
      "exploration/env_infos/reward_ctrl Max                  -0.226066\n",
      "exploration/env_infos/reward_ctrl Min                  -2.69906\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.113313\n",
      "exploration/env_infos/final/torso_velocity Std          1.21288\n",
      "exploration/env_infos/final/torso_velocity Max          2.42615\n",
      "exploration/env_infos/final/torso_velocity Min         -1.53419\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.212946\n",
      "exploration/env_infos/initial/torso_velocity Std        0.199832\n",
      "exploration/env_infos/initial/torso_velocity Max        0.544796\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0476674\n",
      "exploration/env_infos/torso_velocity Mean              -0.0028995\n",
      "exploration/env_infos/torso_velocity Std                0.638254\n",
      "exploration/env_infos/torso_velocity Max                3.0177\n",
      "exploration/env_infos/torso_velocity Min               -2.93797\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.974334\n",
      "evaluation/Rewards Std                                  0.0255482\n",
      "evaluation/Rewards Max                                  2.30793\n",
      "evaluation/Rewards Min                                  0.866076\n",
      "evaluation/Returns Mean                               974.334\n",
      "evaluation/Returns Std                                  5.37766\n",
      "evaluation/Returns Max                                985.708\n",
      "evaluation/Returns Min                                967.401\n",
      "evaluation/Actions Mean                                 0.0250572\n",
      "evaluation/Actions Std                                  0.0777768\n",
      "evaluation/Actions Max                                  0.344277\n",
      "evaluation/Actions Min                                 -0.273738\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            974.334\n",
      "evaluation/env_infos/final/reward_forward Mean          3.89973e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000248746\n",
      "evaluation/env_infos/final/reward_forward Max           0.00123093\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000263584\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.000533516\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128978\n",
      "evaluation/env_infos/initial/reward_forward Max         0.197904\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.252617\n",
      "evaluation/env_infos/reward_forward Mean               -0.00261715\n",
      "evaluation/env_infos/reward_forward Std                 0.0687884\n",
      "evaluation/env_infos/reward_forward Max                 0.748898\n",
      "evaluation/env_infos/reward_forward Min                -1.58553\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0266619\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00544819\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0159675\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0329346\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0294721\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0103259\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0119799\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0463781\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0267084\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00657173\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00869898\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.133924\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.10918e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000184197\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00123093\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000852569\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.155346\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22005\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.532011\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.252617\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00227563\n",
      "evaluation/env_infos/torso_velocity Std                 0.0684124\n",
      "evaluation/env_infos/torso_velocity Max                 1.14614\n",
      "evaluation/env_infos/torso_velocity Min                -1.89963\n",
      "time/data storing (s)                                   0.0341819\n",
      "time/evaluation sampling (s)                           47.5988\n",
      "time/exploration sampling (s)                           1.8745\n",
      "time/logging (s)                                        0.289972\n",
      "time/saving (s)                                         0.0283563\n",
      "time/training (s)                                       4.0922\n",
      "time/epoch (s)                                         53.918\n",
      "time/total (s)                                        420.228\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:05:43.668807 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.422936\n",
      "trainer/QF2 Loss                                        0.455031\n",
      "trainer/Policy Loss                                    -8.1977\n",
      "trainer/Q1 Predictions Mean                             3.55818\n",
      "trainer/Q1 Predictions Std                              0.399756\n",
      "trainer/Q1 Predictions Max                              4.63864\n",
      "trainer/Q1 Predictions Min                              2.02548\n",
      "trainer/Q2 Predictions Mean                             3.56483\n",
      "trainer/Q2 Predictions Std                              0.35348\n",
      "trainer/Q2 Predictions Max                              4.48583\n",
      "trainer/Q2 Predictions Min                              2.27714\n",
      "trainer/Q Targets Mean                                  3.5828\n",
      "trainer/Q Targets Std                                   0.809592\n",
      "trainer/Q Targets Max                                   5.89074\n",
      "trainer/Q Targets Min                                  -1.21676\n",
      "trainer/Log Pis Mean                                   -4.56087\n",
      "trainer/Log Pis Std                                     1.12013\n",
      "trainer/Log Pis Max                                    -0.0972333\n",
      "trainer/Log Pis Min                                    -8.88031\n",
      "trainer/Policy mu Mean                                  0.037037\n",
      "trainer/Policy mu Std                                   0.223274\n",
      "trainer/Policy mu Max                                   1.03679\n",
      "trainer/Policy mu Min                                  -0.856248\n",
      "trainer/Policy log std Mean                            -0.460722\n",
      "trainer/Policy log std Std                              0.112619\n",
      "trainer/Policy log std Max                             -0.222358\n",
      "trainer/Policy log std Min                             -0.921149\n",
      "trainer/Alpha                                           0.0913809\n",
      "trainer/Alpha Loss                                    -30.0188\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            72\n",
      "exploration/path length Mean                          200\n",
      "exploration/path length Std                           134.368\n",
      "exploration/path length Max                           354\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                                0.00591008\n",
      "exploration/Rewards Std                                 0.389928\n",
      "exploration/Rewards Max                                 1.64792\n",
      "exploration/Rewards Min                                -1.23179\n",
      "exploration/Returns Mean                                1.18202\n",
      "exploration/Returns Std                                 7.32945\n",
      "exploration/Returns Max                                12.2083\n",
      "exploration/Returns Min                                -7.73941\n",
      "exploration/Actions Mean                                0.0314322\n",
      "exploration/Actions Std                                 0.507013\n",
      "exploration/Actions Max                                 0.990675\n",
      "exploration/Actions Min                                -0.986604\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                             1.18202\n",
      "exploration/env_infos/final/reward_forward Mean        -0.415752\n",
      "exploration/env_infos/final/reward_forward Std          0.604996\n",
      "exploration/env_infos/final/reward_forward Max          0.159115\n",
      "exploration/env_infos/final/reward_forward Min         -1.46896\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0317581\n",
      "exploration/env_infos/initial/reward_forward Std        0.0819967\n",
      "exploration/env_infos/initial/reward_forward Max        0.0699999\n",
      "exploration/env_infos/initial/reward_forward Min       -0.163586\n",
      "exploration/env_infos/reward_forward Mean               0.115605\n",
      "exploration/env_infos/reward_forward Std                0.625015\n",
      "exploration/env_infos/reward_forward Max                2.3479\n",
      "exploration/env_infos/reward_forward Min               -1.67635\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.8184\n",
      "exploration/env_infos/final/reward_ctrl Std             0.259052\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.526163\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.22582\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.93952\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.377335\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.275617\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.29834\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.0322\n",
      "exploration/env_infos/reward_ctrl Std                   0.34514\n",
      "exploration/env_infos/reward_ctrl Max                  -0.129408\n",
      "exploration/env_infos/reward_ctrl Min                  -2.23179\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0658752\n",
      "exploration/env_infos/final/torso_velocity Std          0.850443\n",
      "exploration/env_infos/final/torso_velocity Max          1.63593\n",
      "exploration/env_infos/final/torso_velocity Min         -1.46896\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.143367\n",
      "exploration/env_infos/initial/torso_velocity Std        0.220929\n",
      "exploration/env_infos/initial/torso_velocity Max        0.59987\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.163586\n",
      "exploration/env_infos/torso_velocity Mean               0.0559594\n",
      "exploration/env_infos/torso_velocity Std                0.711568\n",
      "exploration/env_infos/torso_velocity Max                3.06981\n",
      "exploration/env_infos/torso_velocity Min               -2.44297\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979889\n",
      "evaluation/Rewards Std                                  0.189078\n",
      "evaluation/Rewards Max                                  3.02308\n",
      "evaluation/Rewards Min                                  0.31072\n",
      "evaluation/Returns Mean                               979.889\n",
      "evaluation/Returns Std                                 34.5818\n",
      "evaluation/Returns Max                               1073.1\n",
      "evaluation/Returns Min                                936.367\n",
      "evaluation/Actions Mean                                 0.0285421\n",
      "evaluation/Actions Std                                  0.122412\n",
      "evaluation/Actions Max                                  0.583638\n",
      "evaluation/Actions Min                                 -0.532026\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.889\n",
      "evaluation/env_infos/final/reward_forward Mean          0.089276\n",
      "evaluation/env_infos/final/reward_forward Std           0.297078\n",
      "evaluation/env_infos/final/reward_forward Max           1.09477\n",
      "evaluation/env_infos/final/reward_forward Min          -0.594696\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00482321\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119431\n",
      "evaluation/env_infos/initial/reward_forward Max         0.217638\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.243963\n",
      "evaluation/env_infos/reward_forward Mean                0.059143\n",
      "evaluation/env_infos/reward_forward Std                 0.470461\n",
      "evaluation/env_infos/reward_forward Max                 1.86078\n",
      "evaluation/env_infos/reward_forward Min                -2.01298\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.06052\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0265823\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0389644\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.160404\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0514777\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00953627\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.038905\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0791078\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0631975\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0355889\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0296493\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.68928\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.0688899\n",
      "evaluation/env_infos/final/torso_velocity Std           0.327222\n",
      "evaluation/env_infos/final/torso_velocity Max           1.09477\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.638811\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.179253\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234974\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.712505\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.243963\n",
      "evaluation/env_infos/torso_velocity Mean                0.011893\n",
      "evaluation/env_infos/torso_velocity Std                 0.396683\n",
      "evaluation/env_infos/torso_velocity Max                 1.98261\n",
      "evaluation/env_infos/torso_velocity Min                -2.01298\n",
      "time/data storing (s)                                   0.0328512\n",
      "time/evaluation sampling (s)                           45.7\n",
      "time/exploration sampling (s)                           1.90055\n",
      "time/logging (s)                                        0.288226\n",
      "time/saving (s)                                         0.0285568\n",
      "time/training (s)                                       3.9391\n",
      "time/epoch (s)                                         51.8893\n",
      "time/total (s)                                        472.38\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:06:36.251370 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_09_57_54_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.233238\n",
      "trainer/QF2 Loss                                        0.287726\n",
      "trainer/Policy Loss                                    -6.60679\n",
      "trainer/Q1 Predictions Mean                             3.38022\n",
      "trainer/Q1 Predictions Std                              0.445867\n",
      "trainer/Q1 Predictions Max                              4.6843\n",
      "trainer/Q1 Predictions Min                              2.18946\n",
      "trainer/Q2 Predictions Mean                             3.40319\n",
      "trainer/Q2 Predictions Std                              0.386581\n",
      "trainer/Q2 Predictions Max                              4.38084\n",
      "trainer/Q2 Predictions Min                              2.3025\n",
      "trainer/Q Targets Mean                                  3.50171\n",
      "trainer/Q Targets Std                                   0.698657\n",
      "trainer/Q Targets Max                                   5.16197\n",
      "trainer/Q Targets Min                                  -0.933175\n",
      "trainer/Log Pis Mean                                   -2.91622\n",
      "trainer/Log Pis Std                                     1.4331\n",
      "trainer/Log Pis Max                                     0.317134\n",
      "trainer/Log Pis Min                                    -7.47791\n",
      "trainer/Policy mu Mean                                 -0.00637992\n",
      "trainer/Policy mu Std                                   0.163501\n",
      "trainer/Policy mu Max                                   0.645899\n",
      "trainer/Policy mu Min                                  -0.592963\n",
      "trainer/Policy log std Mean                            -0.87131\n",
      "trainer/Policy log std Std                              0.153263\n",
      "trainer/Policy log std Max                             -0.477441\n",
      "trainer/Policy log std Min                             -1.3313\n",
      "trainer/Alpha                                           0.0696985\n",
      "trainer/Alpha Loss                                    -29.0484\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.440259\n",
      "exploration/Rewards Std                                 0.263465\n",
      "exploration/Rewards Max                                 1.90981\n",
      "exploration/Rewards Min                                -0.537789\n",
      "exploration/Returns Mean                              440.259\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               440.259\n",
      "exploration/Returns Min                               440.259\n",
      "exploration/Actions Mean                                0.00347004\n",
      "exploration/Actions Std                                 0.382962\n",
      "exploration/Actions Max                                 0.964367\n",
      "exploration/Actions Min                                -0.982728\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           440.259\n",
      "exploration/env_infos/final/reward_forward Mean         0.282155\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.282155\n",
      "exploration/env_infos/final/reward_forward Min          0.282155\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.258554\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.258554\n",
      "exploration/env_infos/initial/reward_forward Min       -0.258554\n",
      "exploration/env_infos/reward_forward Mean               0.00230182\n",
      "exploration/env_infos/reward_forward Std                0.293788\n",
      "exploration/env_infos/reward_forward Max                1.69793\n",
      "exploration/env_infos/reward_forward Min               -1.12985\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.395709\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.395709\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.395709\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.458195\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.458195\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.458195\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.586689\n",
      "exploration/env_infos/reward_ctrl Std                   0.239847\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0938504\n",
      "exploration/env_infos/reward_ctrl Min                  -1.53779\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.318712\n",
      "exploration/env_infos/final/torso_velocity Std          0.277299\n",
      "exploration/env_infos/final/torso_velocity Max          0.675132\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00115036\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0133306\n",
      "exploration/env_infos/initial/torso_velocity Std        0.261427\n",
      "exploration/env_infos/initial/torso_velocity Max        0.366241\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.258554\n",
      "exploration/env_infos/torso_velocity Mean              -0.00453615\n",
      "exploration/env_infos/torso_velocity Std                0.295019\n",
      "exploration/env_infos/torso_velocity Max                1.95552\n",
      "exploration/env_infos/torso_velocity Min               -2.69341\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.976068\n",
      "evaluation/Rewards Std                                  0.0787953\n",
      "evaluation/Rewards Max                                  2.49374\n",
      "evaluation/Rewards Min                                  0.8707\n",
      "evaluation/Returns Mean                               976.068\n",
      "evaluation/Returns Std                                 22.7681\n",
      "evaluation/Returns Max                               1038.54\n",
      "evaluation/Returns Min                                945.596\n",
      "evaluation/Actions Mean                                -0.0447582\n",
      "evaluation/Actions Std                                  0.0882467\n",
      "evaluation/Actions Max                                  0.341792\n",
      "evaluation/Actions Min                                 -0.403191\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            976.068\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0109395\n",
      "evaluation/env_infos/final/reward_forward Std           0.0451278\n",
      "evaluation/env_infos/final/reward_forward Max           0.069669\n",
      "evaluation/env_infos/final/reward_forward Min          -0.195933\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0243129\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132237\n",
      "evaluation/env_infos/initial/reward_forward Max         0.243355\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.261634\n",
      "evaluation/env_infos/reward_forward Mean               -0.00110985\n",
      "evaluation/env_infos/reward_forward Std                 0.115295\n",
      "evaluation/env_infos/reward_forward Max                 1.29766\n",
      "evaluation/env_infos/reward_forward Min                -1.55668\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0393694\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00620962\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0334412\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0545521\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0483723\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0180083\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0321489\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0910914\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0391631\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00616688\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0276163\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.1293\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00197828\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0730845\n",
      "evaluation/env_infos/final/torso_velocity Max           0.524674\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.195933\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.123341\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248209\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.689802\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.261634\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00315533\n",
      "evaluation/env_infos/torso_velocity Std                 0.119306\n",
      "evaluation/env_infos/torso_velocity Max                 1.29766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/torso_velocity Min                -1.94276\n",
      "time/data storing (s)                                   0.0365572\n",
      "time/evaluation sampling (s)                           46.3253\n",
      "time/exploration sampling (s)                           1.87327\n",
      "time/logging (s)                                        0.285728\n",
      "time/saving (s)                                         0.0562104\n",
      "time/training (s)                                       3.75624\n",
      "time/epoch (s)                                         52.3333\n",
      "time/total (s)                                        524.96\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n",
      "doodad not detected\n",
      "objc[15083]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a19f9d778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a01e740). One of the two will be used. Which one is undefined.\n",
      "objc[15083]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a19f9d700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a01e768). One of the two will be used. Which one is undefined.\n",
      "objc[15083]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a19f9d7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a01e7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15083]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a19f9d818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1a01e830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:06:42.425138 PDT | Variant:\n",
      "2021-05-25 10:06:42.425840 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:07:37.022482 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      23.2751\n",
      "trainer/QF2 Loss                                      23.2609\n",
      "trainer/Policy Loss                                   -5.32888\n",
      "trainer/Q1 Predictions Mean                            0.00272439\n",
      "trainer/Q1 Predictions Std                             0.00357522\n",
      "trainer/Q1 Predictions Max                             0.014834\n",
      "trainer/Q1 Predictions Min                            -0.0073335\n",
      "trainer/Q2 Predictions Mean                            0.0042744\n",
      "trainer/Q2 Predictions Std                             0.00337541\n",
      "trainer/Q2 Predictions Max                             0.0137971\n",
      "trainer/Q2 Predictions Min                            -0.00341369\n",
      "trainer/Q Targets Mean                                 4.75435\n",
      "trainer/Q Targets Std                                  0.835053\n",
      "trainer/Q Targets Max                                  7.27812\n",
      "trainer/Q Targets Min                                 -0.673563\n",
      "trainer/Log Pis Mean                                  -5.32762\n",
      "trainer/Log Pis Std                                    0.609687\n",
      "trainer/Log Pis Max                                   -3.46312\n",
      "trainer/Log Pis Min                                   -7.66189\n",
      "trainer/Policy mu Mean                                -0.000273722\n",
      "trainer/Policy mu Std                                  0.00208453\n",
      "trainer/Policy mu Max                                  0.00603716\n",
      "trainer/Policy mu Min                                 -0.00772994\n",
      "trainer/Policy log std Mean                            1.707e-05\n",
      "trainer/Policy log std Std                             0.0022324\n",
      "trainer/Policy log std Max                             0.0100611\n",
      "trainer/Policy log std Min                            -0.00693846\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           21\n",
      "exploration/path length Mean                          50\n",
      "exploration/path length Std                           40.1447\n",
      "exploration/path length Max                          180\n",
      "exploration/path length Min                           13\n",
      "exploration/Rewards Mean                              -0.465948\n",
      "exploration/Rewards Std                                0.51198\n",
      "exploration/Rewards Max                                1.78502\n",
      "exploration/Rewards Min                               -1.89999\n",
      "exploration/Returns Mean                             -23.2974\n",
      "exploration/Returns Std                               16.5372\n",
      "exploration/Returns Max                               -3.80113\n",
      "exploration/Returns Min                              -74.7012\n",
      "exploration/Actions Mean                               0.00462186\n",
      "exploration/Actions Std                                0.62244\n",
      "exploration/Actions Max                                0.999669\n",
      "exploration/Actions Min                               -0.999052\n",
      "exploration/Num Paths                                 20\n",
      "exploration/Average Returns                          -23.2974\n",
      "exploration/env_infos/final/reward_forward Mean       -0.230756\n",
      "exploration/env_infos/final/reward_forward Std         0.927217\n",
      "exploration/env_infos/final/reward_forward Max         1.62326\n",
      "exploration/env_infos/final/reward_forward Min        -1.89653\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00380296\n",
      "exploration/env_infos/initial/reward_forward Std       0.177228\n",
      "exploration/env_infos/initial/reward_forward Max       0.299144\n",
      "exploration/env_infos/initial/reward_forward Min      -0.30947\n",
      "exploration/env_infos/reward_forward Mean              0.00698264\n",
      "exploration/env_infos/reward_forward Std               0.812176\n",
      "exploration/env_infos/reward_forward Max               2.71973\n",
      "exploration/env_infos/reward_forward Min              -2.27195\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.5355\n",
      "exploration/env_infos/final/reward_ctrl Std            0.441022\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.839464\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.36598\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.76779\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.42095\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.969955\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.8356\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54981\n",
      "exploration/env_infos/reward_ctrl Std                  0.438071\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477854\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89999\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.514555\n",
      "exploration/env_infos/final/torso_velocity Std         1.24755\n",
      "exploration/env_infos/final/torso_velocity Max         3.39845\n",
      "exploration/env_infos/final/torso_velocity Min        -1.89653\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.111241\n",
      "exploration/env_infos/initial/torso_velocity Std       0.282181\n",
      "exploration/env_infos/initial/torso_velocity Max       0.718938\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.357388\n",
      "exploration/env_infos/torso_velocity Mean              0.0496514\n",
      "exploration/env_infos/torso_velocity Std               0.856068\n",
      "exploration/env_infos/torso_velocity Max               4.28732\n",
      "exploration/env_infos/torso_velocity Min              -2.59117\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00065\n",
      "evaluation/Rewards Std                                 0.00999451\n",
      "evaluation/Rewards Max                                 1.89588\n",
      "evaluation/Rewards Min                                 0.999985\n",
      "evaluation/Returns Mean                             1000.65\n",
      "evaluation/Returns Std                                 0.892257\n",
      "evaluation/Returns Max                              1003.67\n",
      "evaluation/Returns Min                               999.993\n",
      "evaluation/Actions Mean                               -0.000147635\n",
      "evaluation/Actions Std                                 0.00116946\n",
      "evaluation/Actions Max                                 0.00323085\n",
      "evaluation/Actions Min                                -0.00380121\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.65\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000272364\n",
      "evaluation/env_infos/final/reward_forward Std          0.000463769\n",
      "evaluation/env_infos/final/reward_forward Max          0.00100403\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00108037\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00670754\n",
      "evaluation/env_infos/initial/reward_forward Std        0.136078\n",
      "evaluation/env_infos/initial/reward_forward Max        0.228123\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.2849\n",
      "evaluation/env_infos/reward_forward Mean               0.00212288\n",
      "evaluation/env_infos/reward_forward Std                0.0428918\n",
      "evaluation/env_infos/reward_forward Max                1.0482\n",
      "evaluation/env_infos/reward_forward Min               -1.19995\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.53964e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             6.21468e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.66576e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.81539e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.32864e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           5.25073e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.49552e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.26526e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.55769e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.4496e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -3.44369e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.48491e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000181529\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000457039\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00212297\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00108037\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145944\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.230568\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.679296\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.2849\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000137916\n",
      "evaluation/env_infos/torso_velocity Std                0.0514513\n",
      "evaluation/env_infos/torso_velocity Max                1.28616\n",
      "evaluation/env_infos/torso_velocity Min               -1.92644\n",
      "time/data storing (s)                                  0.630436\n",
      "time/evaluation sampling (s)                          46.539\n",
      "time/exploration sampling (s)                          1.75755\n",
      "time/logging (s)                                       0.278709\n",
      "time/saving (s)                                        0.0678079\n",
      "time/training (s)                                      3.15419\n",
      "time/epoch (s)                                        52.4277\n",
      "time/total (s)                                        58.2196\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:08:26.222325 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  5960\n",
      "trainer/QF1 Loss                                       0.606661\n",
      "trainer/QF2 Loss                                       0.600573\n",
      "trainer/Policy Loss                                   -9.45069\n",
      "trainer/Q1 Predictions Mean                            4.00315\n",
      "trainer/Q1 Predictions Std                             0.401554\n",
      "trainer/Q1 Predictions Max                             5.19129\n",
      "trainer/Q1 Predictions Min                             2.70376\n",
      "trainer/Q2 Predictions Mean                            4.01453\n",
      "trainer/Q2 Predictions Std                             0.397649\n",
      "trainer/Q2 Predictions Max                             5.19274\n",
      "trainer/Q2 Predictions Min                             2.74258\n",
      "trainer/Q Targets Mean                                 4.05947\n",
      "trainer/Q Targets Std                                  0.751303\n",
      "trainer/Q Targets Max                                  6.66238\n",
      "trainer/Q Targets Min                                 -0.382221\n",
      "trainer/Log Pis Mean                                  -5.47276\n",
      "trainer/Log Pis Std                                    0.445783\n",
      "trainer/Log Pis Max                                   -4.61577\n",
      "trainer/Log Pis Min                                   -9.27703\n",
      "trainer/Policy mu Mean                                 0.00463625\n",
      "trainer/Policy mu Std                                  0.0210524\n",
      "trainer/Policy mu Max                                  0.113994\n",
      "trainer/Policy mu Min                                 -0.0374403\n",
      "trainer/Policy log std Mean                           -0.103158\n",
      "trainer/Policy log std Std                             0.0195432\n",
      "trainer/Policy log std Max                            -0.0549816\n",
      "trainer/Policy log std Min                            -0.218862\n",
      "trainer/Alpha                                          0.738539\n",
      "trainer/Alpha Loss                                    -4.0429\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           31\n",
      "exploration/path length Mean                         100\n",
      "exploration/path length Std                           61.5305\n",
      "exploration/path length Max                          219\n",
      "exploration/path length Min                           24\n",
      "exploration/Rewards Mean                              -0.36101\n",
      "exploration/Rewards Std                                0.529001\n",
      "exploration/Rewards Max                                2.60712\n",
      "exploration/Rewards Min                               -1.79919\n",
      "exploration/Returns Mean                             -36.101\n",
      "exploration/Returns Std                               26.5541\n",
      "exploration/Returns Max                                1.97838\n",
      "exploration/Returns Min                              -81.3449\n",
      "exploration/Actions Mean                              -0.00470157\n",
      "exploration/Actions Std                                0.601284\n",
      "exploration/Actions Max                                0.998584\n",
      "exploration/Actions Min                               -0.998602\n",
      "exploration/Num Paths                                 10\n",
      "exploration/Average Returns                          -36.101\n",
      "exploration/env_infos/final/reward_forward Mean        0.174626\n",
      "exploration/env_infos/final/reward_forward Std         1.09913\n",
      "exploration/env_infos/final/reward_forward Max         2.59472\n",
      "exploration/env_infos/final/reward_forward Min        -1.31743\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0422485\n",
      "exploration/env_infos/initial/reward_forward Std       0.171999\n",
      "exploration/env_infos/initial/reward_forward Max       0.28739\n",
      "exploration/env_infos/initial/reward_forward Min      -0.28085\n",
      "exploration/env_infos/reward_forward Mean             -0.0274589\n",
      "exploration/env_infos/reward_forward Std               0.739277\n",
      "exploration/env_infos/reward_forward Max               2.82652\n",
      "exploration/env_infos/reward_forward Min              -2.54545\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.59768\n",
      "exploration/env_infos/final/reward_ctrl Std            0.366256\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.919628\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.09892\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.77343\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.482376\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.864698\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.32113\n",
      "exploration/env_infos/reward_ctrl Mean                -1.44626\n",
      "exploration/env_infos/reward_ctrl Std                  0.412908\n",
      "exploration/env_infos/reward_ctrl Max                 -0.418043\n",
      "exploration/env_infos/reward_ctrl Min                 -2.79919\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.692648\n",
      "exploration/env_infos/final/torso_velocity Std         1.24112\n",
      "exploration/env_infos/final/torso_velocity Max         3.47562\n",
      "exploration/env_infos/final/torso_velocity Min        -1.63013\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.146734\n",
      "exploration/env_infos/initial/torso_velocity Std       0.294039\n",
      "exploration/env_infos/initial/torso_velocity Max       0.706789\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.28085\n",
      "exploration/env_infos/torso_velocity Mean             -0.0628055\n",
      "exploration/env_infos/torso_velocity Std               0.83904\n",
      "exploration/env_infos/torso_velocity Max               3.7672\n",
      "exploration/env_infos/torso_velocity Min              -2.72817\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999733\n",
      "evaluation/Rewards Std                                 0.0230866\n",
      "evaluation/Rewards Max                                 2.24665\n",
      "evaluation/Rewards Min                                 0.996494\n",
      "evaluation/Returns Mean                              999.733\n",
      "evaluation/Returns Std                                 1.52953\n",
      "evaluation/Returns Max                              1004.66\n",
      "evaluation/Returns Min                               998.612\n",
      "evaluation/Actions Mean                                0.00521002\n",
      "evaluation/Actions Std                                 0.0172195\n",
      "evaluation/Actions Max                                 0.0754709\n",
      "evaluation/Actions Min                                -0.030306\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.733\n",
      "evaluation/env_infos/final/reward_forward Mean        -8.51242e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000333083\n",
      "evaluation/env_infos/final/reward_forward Max          0.000220238\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00143727\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00600977\n",
      "evaluation/env_infos/initial/reward_forward Std        0.106019\n",
      "evaluation/env_infos/initial/reward_forward Max        0.183218\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.245731\n",
      "evaluation/env_infos/reward_forward Mean               0.00366355\n",
      "evaluation/env_infos/reward_forward Std                0.0485846\n",
      "evaluation/env_infos/reward_forward Max                1.28667\n",
      "evaluation/env_infos/reward_forward Min               -1.20178\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00128525\n",
      "evaluation/env_infos/final/reward_ctrl Std             8.18171e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00112077\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0013866\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00104441\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00013614\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000823047\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00123019\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00129463\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000102514\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000823047\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00350583\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.11061e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000303606\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00165283\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00143727\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.136155\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.234884\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.653618\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.336383\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00178048\n",
      "evaluation/env_infos/torso_velocity Std                0.056066\n",
      "evaluation/env_infos/torso_velocity Max                1.28667\n",
      "evaluation/env_infos/torso_velocity Min               -1.89253\n",
      "time/data storing (s)                                  0.47802\n",
      "time/evaluation sampling (s)                          42.7758\n",
      "time/exploration sampling (s)                          1.92555\n",
      "time/logging (s)                                       0.283404\n",
      "time/saving (s)                                        0.0271875\n",
      "time/training (s)                                      3.42055\n",
      "time/epoch (s)                                        48.9105\n",
      "time/total (s)                                       107.424\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:404: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_rewards\": np.array(self.relabeled_rewards)}\n",
      "2021-05-25 10:09:19.063494 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  7960\n",
      "trainer/QF1 Loss                                       0.491026\n",
      "trainer/QF2 Loss                                       0.482832\n",
      "trainer/Policy Loss                                   -9.12392\n",
      "trainer/Q1 Predictions Mean                            3.68386\n",
      "trainer/Q1 Predictions Std                             0.320672\n",
      "trainer/Q1 Predictions Max                             4.91914\n",
      "trainer/Q1 Predictions Min                             2.83376\n",
      "trainer/Q2 Predictions Mean                            3.69489\n",
      "trainer/Q2 Predictions Std                             0.320046\n",
      "trainer/Q2 Predictions Max                             4.98148\n",
      "trainer/Q2 Predictions Min                             2.91217\n",
      "trainer/Q Targets Mean                                 3.83508\n",
      "trainer/Q Targets Std                                  0.695949\n",
      "trainer/Q Targets Max                                  6.09225\n",
      "trainer/Q Targets Min                                 -1.02873\n",
      "trainer/Log Pis Mean                                  -5.46269\n",
      "trainer/Log Pis Std                                    0.354446\n",
      "trainer/Log Pis Max                                   -4.54924\n",
      "trainer/Log Pis Min                                   -7.86738\n",
      "trainer/Policy mu Mean                                -0.00793672\n",
      "trainer/Policy mu Std                                  0.0181331\n",
      "trainer/Policy mu Max                                  0.10696\n",
      "trainer/Policy mu Min                                 -0.0734644\n",
      "trainer/Policy log std Mean                           -0.117159\n",
      "trainer/Policy log std Std                             0.0160061\n",
      "trainer/Policy log std Max                            -0.0706939\n",
      "trainer/Policy log std Min                            -0.17494\n",
      "trainer/Alpha                                          0.547074\n",
      "trainer/Alpha Loss                                    -8.07988\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           34\n",
      "exploration/path length Mean                         333.333\n",
      "exploration/path length Std                          299.071\n",
      "exploration/path length Max                          752\n",
      "exploration/path length Min                           72\n",
      "exploration/Rewards Mean                              -0.352906\n",
      "exploration/Rewards Std                                0.481872\n",
      "exploration/Rewards Max                                1.79166\n",
      "exploration/Rewards Min                               -1.78905\n",
      "exploration/Returns Mean                            -117.635\n",
      "exploration/Returns Std                              121.079\n",
      "exploration/Returns Max                              -20.9831\n",
      "exploration/Returns Min                             -288.37\n",
      "exploration/Actions Mean                              -0.00607527\n",
      "exploration/Actions Std                                0.594759\n",
      "exploration/Actions Max                                0.998057\n",
      "exploration/Actions Min                               -0.996675\n",
      "exploration/Num Paths                                  3\n",
      "exploration/Average Returns                         -117.635\n",
      "exploration/env_infos/final/reward_forward Mean        0.0260295\n",
      "exploration/env_infos/final/reward_forward Std         0.216415\n",
      "exploration/env_infos/final/reward_forward Max         0.331754\n",
      "exploration/env_infos/final/reward_forward Min        -0.139189\n",
      "exploration/env_infos/initial/reward_forward Mean      0.165617\n",
      "exploration/env_infos/initial/reward_forward Std       0.100985\n",
      "exploration/env_infos/initial/reward_forward Max       0.305932\n",
      "exploration/env_infos/initial/reward_forward Min       0.0724203\n",
      "exploration/env_infos/reward_forward Mean              0.0131183\n",
      "exploration/env_infos/reward_forward Std               0.520132\n",
      "exploration/env_infos/reward_forward Max               2.75351\n",
      "exploration/env_infos/reward_forward Min              -2.26832\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.47372\n",
      "exploration/env_infos/final/reward_ctrl Std            0.170074\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.33409\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.71314\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.16453\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.788224\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.442524\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.26105\n",
      "exploration/env_infos/reward_ctrl Mean                -1.4151\n",
      "exploration/env_infos/reward_ctrl Std                  0.424473\n",
      "exploration/env_infos/reward_ctrl Max                 -0.216539\n",
      "exploration/env_infos/reward_ctrl Min                 -2.78905\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.14769\n",
      "exploration/env_infos/final/torso_velocity Std         0.802092\n",
      "exploration/env_infos/final/torso_velocity Max         2.12439\n",
      "exploration/env_infos/final/torso_velocity Min        -0.588929\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.213107\n",
      "exploration/env_infos/initial/torso_velocity Std       0.278249\n",
      "exploration/env_infos/initial/torso_velocity Max       0.622644\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.182251\n",
      "exploration/env_infos/torso_velocity Mean             -0.0362784\n",
      "exploration/env_infos/torso_velocity Std               0.541359\n",
      "exploration/env_infos/torso_velocity Max               3.65051\n",
      "exploration/env_infos/torso_velocity Min              -2.89837\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00012\n",
      "evaluation/Rewards Std                                 0.0222171\n",
      "evaluation/Rewards Max                                 2.26578\n",
      "evaluation/Rewards Min                                 0.98954\n",
      "evaluation/Returns Mean                             1000.12\n",
      "evaluation/Returns Std                                 1.97699\n",
      "evaluation/Returns Max                              1008.28\n",
      "evaluation/Returns Min                               998.79\n",
      "evaluation/Actions Mean                               -0.00908437\n",
      "evaluation/Actions Std                                 0.0131466\n",
      "evaluation/Actions Max                                 0.115584\n",
      "evaluation/Actions Min                                -0.0690711\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.12\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.000144665\n",
      "evaluation/env_infos/final/reward_forward Std          0.000514041\n",
      "evaluation/env_infos/final/reward_forward Max          2.72968e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00244248\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0195101\n",
      "evaluation/env_infos/initial/reward_forward Std        0.127732\n",
      "evaluation/env_infos/initial/reward_forward Max        0.288764\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.256293\n",
      "evaluation/env_infos/reward_forward Mean              -0.00126688\n",
      "evaluation/env_infos/reward_forward Std                0.0525427\n",
      "evaluation/env_infos/reward_forward Max                1.37039\n",
      "evaluation/env_infos/reward_forward Min               -1.27563\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00100617\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000154249\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000780106\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0012378\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.000799324\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000224\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000451976\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00114596\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00102144\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000337467\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000396297\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0104603\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         7.55426e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000547068\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00228713\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00244248\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.156765\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237746\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.608195\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.3113\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00365065\n",
      "evaluation/env_infos/torso_velocity Std                0.0584335\n",
      "evaluation/env_infos/torso_velocity Max                1.37039\n",
      "evaluation/env_infos/torso_velocity Min               -2.04388\n",
      "time/data storing (s)                                  0.336676\n",
      "time/evaluation sampling (s)                          46.9632\n",
      "time/exploration sampling (s)                          1.77115\n",
      "time/logging (s)                                       0.275892\n",
      "time/saving (s)                                        0.0261653\n",
      "time/training (s)                                      3.25952\n",
      "time/epoch (s)                                        52.6326\n",
      "time/total (s)                                       160.257\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:10:11.493185 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9960\n",
      "trainer/QF1 Loss                                        0.58135\n",
      "trainer/QF2 Loss                                        0.580899\n",
      "trainer/Policy Loss                                    -9.31884\n",
      "trainer/Q1 Predictions Mean                             3.87536\n",
      "trainer/Q1 Predictions Std                              0.324157\n",
      "trainer/Q1 Predictions Max                              4.80967\n",
      "trainer/Q1 Predictions Min                              2.81359\n",
      "trainer/Q2 Predictions Mean                             3.87601\n",
      "trainer/Q2 Predictions Std                              0.322654\n",
      "trainer/Q2 Predictions Max                              4.77729\n",
      "trainer/Q2 Predictions Min                              2.75568\n",
      "trainer/Q Targets Mean                                  3.85681\n",
      "trainer/Q Targets Std                                   0.799748\n",
      "trainer/Q Targets Max                                   6.14468\n",
      "trainer/Q Targets Min                                  -1.09892\n",
      "trainer/Log Pis Mean                                   -5.47552\n",
      "trainer/Log Pis Std                                     0.432604\n",
      "trainer/Log Pis Max                                    -4.70945\n",
      "trainer/Log Pis Min                                    -9.68682\n",
      "trainer/Policy mu Mean                                 -0.000515436\n",
      "trainer/Policy mu Std                                   0.0285416\n",
      "trainer/Policy mu Max                                   0.109975\n",
      "trainer/Policy mu Min                                  -0.133351\n",
      "trainer/Policy log std Mean                            -0.13871\n",
      "trainer/Policy log std Std                              0.0184498\n",
      "trainer/Policy log std Max                             -0.0726963\n",
      "trainer/Policy log std Min                             -0.248881\n",
      "trainer/Alpha                                           0.405284\n",
      "trainer/Alpha Loss                                    -12.1302\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            37\n",
      "exploration/path length Mean                          333.333\n",
      "exploration/path length Std                           339.357\n",
      "exploration/path length Max                           806\n",
      "exploration/path length Min                            25\n",
      "exploration/Rewards Mean                               -0.333133\n",
      "exploration/Rewards Std                                 0.428625\n",
      "exploration/Rewards Max                                 1.47366\n",
      "exploration/Rewards Min                                -1.59819\n",
      "exploration/Returns Mean                             -111.044\n",
      "exploration/Returns Std                               107.641\n",
      "exploration/Returns Max                                -9.58166\n",
      "exploration/Returns Min                              -260.055\n",
      "exploration/Actions Mean                                0.00345235\n",
      "exploration/Actions Std                                 0.585727\n",
      "exploration/Actions Max                                 0.997043\n",
      "exploration/Actions Min                                -0.995365\n",
      "exploration/Num Paths                                   3\n",
      "exploration/Average Returns                          -111.044\n",
      "exploration/env_infos/final/reward_forward Mean         0.505063\n",
      "exploration/env_infos/final/reward_forward Std          1.04479\n",
      "exploration/env_infos/final/reward_forward Max          1.83923\n",
      "exploration/env_infos/final/reward_forward Min         -0.711914\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0374076\n",
      "exploration/env_infos/initial/reward_forward Std        0.0550328\n",
      "exploration/env_infos/initial/reward_forward Max        0.115156\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00451985\n",
      "exploration/env_infos/reward_forward Mean               0.0247619\n",
      "exploration/env_infos/reward_forward Std                0.56567\n",
      "exploration/env_infos/reward_forward Max                2.41757\n",
      "exploration/env_infos/reward_forward Min               -2.00958\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.51172\n",
      "exploration/env_infos/final/reward_ctrl Std             0.530359\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.766459\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.95756\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.40609\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.133228\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.30316\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.59422\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37235\n",
      "exploration/env_infos/reward_ctrl Std                   0.403891\n",
      "exploration/env_infos/reward_ctrl Max                  -0.322384\n",
      "exploration/env_infos/reward_ctrl Min                  -2.68808\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.492263\n",
      "exploration/env_infos/final/torso_velocity Std          1.01065\n",
      "exploration/env_infos/final/torso_velocity Max          2.46522\n",
      "exploration/env_infos/final/torso_velocity Min         -0.711914\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.147651\n",
      "exploration/env_infos/initial/torso_velocity Std        0.244213\n",
      "exploration/env_infos/initial/torso_velocity Max        0.486305\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.252226\n",
      "exploration/env_infos/torso_velocity Mean              -0.0160884\n",
      "exploration/env_infos/torso_velocity Std                0.56476\n",
      "exploration/env_infos/torso_velocity Max                2.55363\n",
      "exploration/env_infos/torso_velocity Min               -2.99128\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.996818\n",
      "evaluation/Rewards Std                                  0.0174375\n",
      "evaluation/Rewards Max                                  2.06245\n",
      "evaluation/Rewards Min                                  0.984436\n",
      "evaluation/Returns Mean                               996.818\n",
      "evaluation/Returns Std                                  2.14616\n",
      "evaluation/Returns Max                                999.955\n",
      "evaluation/Returns Min                                994.258\n",
      "evaluation/Actions Mean                                 0.00274776\n",
      "evaluation/Actions Std                                  0.0311704\n",
      "evaluation/Actions Max                                  0.111625\n",
      "evaluation/Actions Min                                 -0.0849739\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            996.818\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000553764\n",
      "evaluation/env_infos/final/reward_forward Std           0.000782883\n",
      "evaluation/env_infos/final/reward_forward Max           0.0031464\n",
      "evaluation/env_infos/final/reward_forward Min          -6.56336e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0136552\n",
      "evaluation/env_infos/initial/reward_forward Std         0.10766\n",
      "evaluation/env_infos/initial/reward_forward Max         0.218857\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.219569\n",
      "evaluation/env_infos/reward_forward Mean                0.00598478\n",
      "evaluation/env_infos/reward_forward Std                 0.0676774\n",
      "evaluation/env_infos/reward_forward Max                 1.45171\n",
      "evaluation/env_infos/reward_forward Min                -1.01156\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00386788\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00212236\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000749628\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00605535\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00191675\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00118303\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000532086\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00366403\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00391659\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00212946\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000491329\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0155641\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.36513e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00075405\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0031464\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00193098\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138823\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242347\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.637311\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.35284\n",
      "evaluation/env_infos/torso_velocity Mean                0.00138671\n",
      "evaluation/env_infos/torso_velocity Std                 0.0648509\n",
      "evaluation/env_infos/torso_velocity Max                 1.45171\n",
      "evaluation/env_infos/torso_velocity Min                -1.77861\n",
      "time/data storing (s)                                   0.369022\n",
      "time/evaluation sampling (s)                           46.6606\n",
      "time/exploration sampling (s)                           1.85214\n",
      "time/logging (s)                                        0.273622\n",
      "time/saving (s)                                         0.0298972\n",
      "time/training (s)                                       3.04788\n",
      "time/epoch (s)                                         52.2331\n",
      "time/total (s)                                        212.683\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:10:58.648416 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11960\n",
      "trainer/QF1 Loss                                        0.400671\n",
      "trainer/QF2 Loss                                        0.408628\n",
      "trainer/Policy Loss                                    -9.24234\n",
      "trainer/Q1 Predictions Mean                             3.81842\n",
      "trainer/Q1 Predictions Std                              0.329701\n",
      "trainer/Q1 Predictions Max                              4.99339\n",
      "trainer/Q1 Predictions Min                              2.99393\n",
      "trainer/Q2 Predictions Mean                             3.81941\n",
      "trainer/Q2 Predictions Std                              0.348147\n",
      "trainer/Q2 Predictions Max                              5.26952\n",
      "trainer/Q2 Predictions Min                              3.02269\n",
      "trainer/Q Targets Mean                                  3.84655\n",
      "trainer/Q Targets Std                                   0.687318\n",
      "trainer/Q Targets Max                                   8.03313\n",
      "trainer/Q Targets Min                                  -1.15945\n",
      "trainer/Log Pis Mean                                   -5.45007\n",
      "trainer/Log Pis Std                                     0.401396\n",
      "trainer/Log Pis Max                                    -4.49408\n",
      "trainer/Log Pis Min                                    -7.30965\n",
      "trainer/Policy mu Mean                                 -0.0152581\n",
      "trainer/Policy mu Std                                   0.0606269\n",
      "trainer/Policy mu Max                                   0.125398\n",
      "trainer/Policy mu Min                                  -0.294535\n",
      "trainer/Policy log std Mean                            -0.134036\n",
      "trainer/Policy log std Std                              0.0161603\n",
      "trainer/Policy log std Max                             -0.079412\n",
      "trainer/Policy log std Min                             -0.208251\n",
      "trainer/Alpha                                           0.300222\n",
      "trainer/Alpha Loss                                    -16.1432\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            38\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.346827\n",
      "exploration/Rewards Std                                 0.431391\n",
      "exploration/Rewards Max                                 1.0989\n",
      "exploration/Rewards Min                                -1.84742\n",
      "exploration/Returns Mean                             -346.827\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -346.827\n",
      "exploration/Returns Min                              -346.827\n",
      "exploration/Actions Mean                               -0.0224789\n",
      "exploration/Actions Std                                 0.587464\n",
      "exploration/Actions Max                                 0.99564\n",
      "exploration/Actions Min                                -0.998724\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -346.827\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0478668\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0478668\n",
      "exploration/env_infos/final/reward_forward Min         -0.0478668\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0702786\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0702786\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0702786\n",
      "exploration/env_infos/reward_forward Mean               0.00843357\n",
      "exploration/env_infos/reward_forward Std                0.380122\n",
      "exploration/env_infos/reward_forward Max                1.41347\n",
      "exploration/env_infos/reward_forward Min               -1.61798\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.770485\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.770485\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.770485\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.23949\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.23949\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.23949\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38248\n",
      "exploration/env_infos/reward_ctrl Std                   0.406111\n",
      "exploration/env_infos/reward_ctrl Max                  -0.301914\n",
      "exploration/env_infos/reward_ctrl Min                  -2.84742\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0197029\n",
      "exploration/env_infos/final/torso_velocity Std          0.246835\n",
      "exploration/env_infos/final/torso_velocity Max          0.35008\n",
      "exploration/env_infos/final/torso_velocity Min         -0.243105\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.100429\n",
      "exploration/env_infos/initial/torso_velocity Std        0.136052\n",
      "exploration/env_infos/initial/torso_velocity Max        0.262656\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0702786\n",
      "exploration/env_infos/torso_velocity Mean               0.000356352\n",
      "exploration/env_infos/torso_velocity Std                0.384222\n",
      "exploration/env_infos/torso_velocity Max                2.04034\n",
      "exploration/env_infos/torso_velocity Min               -2.82355\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.988095\n",
      "evaluation/Rewards Std                                  0.0243369\n",
      "evaluation/Rewards Max                                  2.35554\n",
      "evaluation/Rewards Min                                  0.964412\n",
      "evaluation/Returns Mean                               988.095\n",
      "evaluation/Returns Std                                  3.85191\n",
      "evaluation/Returns Max                                994.131\n",
      "evaluation/Returns Min                                981.02\n",
      "evaluation/Actions Mean                                -0.0113606\n",
      "evaluation/Actions Std                                  0.0551285\n",
      "evaluation/Actions Max                                  0.0894437\n",
      "evaluation/Actions Min                                 -0.203701\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            988.095\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.61313e-05\n",
      "evaluation/env_infos/final/reward_forward Std           7.99408e-05\n",
      "evaluation/env_infos/final/reward_forward Max           8.65444e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000407752\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0136987\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122687\n",
      "evaluation/env_infos/initial/reward_forward Max         0.262152\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.221143\n",
      "evaluation/env_infos/reward_forward Mean                0.0010923\n",
      "evaluation/env_infos/reward_forward Std                 0.0485954\n",
      "evaluation/env_infos/reward_forward Max                 1.10862\n",
      "evaluation/env_infos/reward_forward Min                -1.2064\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.012632\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0039235\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00735597\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0195103\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0114752\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00570828\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0044452\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0199601\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0126729\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00397372\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0044452\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0355875\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.26565e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           6.61915e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000386759\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000407752\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.142071\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224965\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.600986\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.240667\n",
      "evaluation/env_infos/torso_velocity Mean                0.00012142\n",
      "evaluation/env_infos/torso_velocity Std                 0.0559828\n",
      "evaluation/env_infos/torso_velocity Max                 1.48156\n",
      "evaluation/env_infos/torso_velocity Min                -1.80689\n",
      "time/data storing (s)                                   0.270713\n",
      "time/evaluation sampling (s)                           41.802\n",
      "time/exploration sampling (s)                           1.53832\n",
      "time/logging (s)                                        0.269013\n",
      "time/saving (s)                                         0.0247334\n",
      "time/training (s)                                       3.04926\n",
      "time/epoch (s)                                         46.9541\n",
      "time/total (s)                                        259.833\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:11:46.518898 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  13960\n",
      "trainer/QF1 Loss                                        0.341927\n",
      "trainer/QF2 Loss                                        0.349431\n",
      "trainer/Policy Loss                                    -9.25603\n",
      "trainer/Q1 Predictions Mean                             3.82226\n",
      "trainer/Q1 Predictions Std                              0.373523\n",
      "trainer/Q1 Predictions Max                              5.49165\n",
      "trainer/Q1 Predictions Min                              2.8275\n",
      "trainer/Q2 Predictions Mean                             3.79208\n",
      "trainer/Q2 Predictions Std                              0.371924\n",
      "trainer/Q2 Predictions Max                              5.32949\n",
      "trainer/Q2 Predictions Min                              2.99165\n",
      "trainer/Q Targets Mean                                  3.84943\n",
      "trainer/Q Targets Std                                   0.625799\n",
      "trainer/Q Targets Max                                   6.63592\n",
      "trainer/Q Targets Min                                  -0.195125\n",
      "trainer/Log Pis Mean                                   -5.47335\n",
      "trainer/Log Pis Std                                     0.547334\n",
      "trainer/Log Pis Max                                    -3.72655\n",
      "trainer/Log Pis Min                                    -9.97013\n",
      "trainer/Policy mu Mean                                 -0.0185753\n",
      "trainer/Policy mu Std                                   0.0569026\n",
      "trainer/Policy mu Max                                   0.160682\n",
      "trainer/Policy mu Min                                  -0.346896\n",
      "trainer/Policy log std Mean                            -0.131601\n",
      "trainer/Policy log std Std                              0.0149303\n",
      "trainer/Policy log std Max                             -0.0916655\n",
      "trainer/Policy log std Min                             -0.198755\n",
      "trainer/Alpha                                           0.222441\n",
      "trainer/Alpha Loss                                    -20.2113\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            50\n",
      "exploration/path length Mean                           83.3333\n",
      "exploration/path length Std                            58.5553\n",
      "exploration/path length Max                           181\n",
      "exploration/path length Min                             8\n",
      "exploration/Rewards Mean                               -0.348094\n",
      "exploration/Rewards Std                                 0.49378\n",
      "exploration/Rewards Max                                 2.44599\n",
      "exploration/Rewards Min                                -1.9855\n",
      "exploration/Returns Mean                              -29.0078\n",
      "exploration/Returns Std                                20.4781\n",
      "exploration/Returns Max                                -2.08667\n",
      "exploration/Returns Min                               -66.769\n",
      "exploration/Actions Mean                               -0.0149091\n",
      "exploration/Actions Std                                 0.593221\n",
      "exploration/Actions Max                                 0.995716\n",
      "exploration/Actions Min                                -0.997534\n",
      "exploration/Num Paths                                  12\n",
      "exploration/Average Returns                           -29.0078\n",
      "exploration/env_infos/final/reward_forward Mean        -0.109043\n",
      "exploration/env_infos/final/reward_forward Std          0.68608\n",
      "exploration/env_infos/final/reward_forward Max          0.737227\n",
      "exploration/env_infos/final/reward_forward Min         -1.72844\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0422563\n",
      "exploration/env_infos/initial/reward_forward Std        0.158515\n",
      "exploration/env_infos/initial/reward_forward Max        0.220664\n",
      "exploration/env_infos/initial/reward_forward Min       -0.303915\n",
      "exploration/env_infos/reward_forward Mean               0.00836081\n",
      "exploration/env_infos/reward_forward Std                0.791069\n",
      "exploration/env_infos/reward_forward Max                2.07756\n",
      "exploration/env_infos/reward_forward Min               -3.13836\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.55366\n",
      "exploration/env_infos/final/reward_ctrl Std             0.470579\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.746095\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.28957\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.40834\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.325001\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.846844\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.11872\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.40853\n",
      "exploration/env_infos/reward_ctrl Std                   0.427177\n",
      "exploration/env_infos/reward_ctrl Max                  -0.248111\n",
      "exploration/env_infos/reward_ctrl Min                  -2.9855\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.38847\n",
      "exploration/env_infos/final/torso_velocity Std          1.16325\n",
      "exploration/env_infos/final/torso_velocity Max          3.40745\n",
      "exploration/env_infos/final/torso_velocity Min         -1.76538\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.116223\n",
      "exploration/env_infos/initial/torso_velocity Std        0.260426\n",
      "exploration/env_infos/initial/torso_velocity Max        0.661187\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.303915\n",
      "exploration/env_infos/torso_velocity Mean               0.067223\n",
      "exploration/env_infos/torso_velocity Std                0.854841\n",
      "exploration/env_infos/torso_velocity Max                3.71827\n",
      "exploration/env_infos/torso_velocity Min               -3.13836\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.993128\n",
      "evaluation/Rewards Std                                  0.0253714\n",
      "evaluation/Rewards Max                                  2.26198\n",
      "evaluation/Rewards Min                                  0.952803\n",
      "evaluation/Returns Mean                               993.128\n",
      "evaluation/Returns Std                                  2.56846\n",
      "evaluation/Returns Max                                998.661\n",
      "evaluation/Returns Min                                989.67\n",
      "evaluation/Actions Mean                                -0.0182757\n",
      "evaluation/Actions Std                                  0.0404414\n",
      "evaluation/Actions Max                                  0.0712869\n",
      "evaluation/Actions Min                                 -0.230153\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            993.128\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.69085e-05\n",
      "evaluation/env_infos/final/reward_forward Std           9.04812e-05\n",
      "evaluation/env_infos/final/reward_forward Max           8.68565e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000436962\n",
      "evaluation/env_infos/initial/reward_forward Mean        9.11531e-05\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0941847\n",
      "evaluation/env_infos/initial/reward_forward Max         0.199574\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.129699\n",
      "evaluation/env_infos/reward_forward Mean               -0.000527072\n",
      "evaluation/env_infos/reward_forward Std                 0.0526826\n",
      "evaluation/env_infos/reward_forward Max                 1.412\n",
      "evaluation/env_infos/reward_forward Min                -1.20259\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00781819\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00206222\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00421164\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0109026\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00727551\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00329183\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00252909\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.015101\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00787803\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00229538\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00181303\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0471968\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.85108e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000140757\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00047552\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000839045\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146024\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228466\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.634075\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.298523\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000974215\n",
      "evaluation/env_infos/torso_velocity Std                 0.0537243\n",
      "evaluation/env_infos/torso_velocity Max                 1.412\n",
      "evaluation/env_infos/torso_velocity Min                -1.93838\n",
      "time/data storing (s)                                   0.52254\n",
      "time/evaluation sampling (s)                           41.8888\n",
      "time/exploration sampling (s)                           1.81942\n",
      "time/logging (s)                                        0.227727\n",
      "time/saving (s)                                         0.0258067\n",
      "time/training (s)                                       3.13641\n",
      "time/epoch (s)                                         47.6207\n",
      "time/total (s)                                        307.662\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:12:33.076096 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  15960\n",
      "trainer/QF1 Loss                                        0.423957\n",
      "trainer/QF2 Loss                                        0.412286\n",
      "trainer/Policy Loss                                    -9.27219\n",
      "trainer/Q1 Predictions Mean                             3.87488\n",
      "trainer/Q1 Predictions Std                              0.428324\n",
      "trainer/Q1 Predictions Max                              5.71259\n",
      "trainer/Q1 Predictions Min                              2.88378\n",
      "trainer/Q2 Predictions Mean                             3.87734\n",
      "trainer/Q2 Predictions Std                              0.432519\n",
      "trainer/Q2 Predictions Max                              5.9047\n",
      "trainer/Q2 Predictions Min                              2.87602\n",
      "trainer/Q Targets Mean                                  3.88121\n",
      "trainer/Q Targets Std                                   0.750095\n",
      "trainer/Q Targets Max                                   7.56248\n",
      "trainer/Q Targets Min                                  -0.892589\n",
      "trainer/Log Pis Mean                                   -5.44918\n",
      "trainer/Log Pis Std                                     0.557584\n",
      "trainer/Log Pis Max                                    -4.11326\n",
      "trainer/Log Pis Min                                   -10.8296\n",
      "trainer/Policy mu Mean                                  0.017737\n",
      "trainer/Policy mu Std                                   0.0895034\n",
      "trainer/Policy mu Max                                   0.347952\n",
      "trainer/Policy mu Min                                  -0.266146\n",
      "trainer/Policy log std Mean                            -0.166714\n",
      "trainer/Policy log std Std                              0.0254856\n",
      "trainer/Policy log std Max                             -0.0896014\n",
      "trainer/Policy log std Min                             -0.27436\n",
      "trainer/Alpha                                           0.164885\n",
      "trainer/Alpha Loss                                    -24.202\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            56\n",
      "exploration/path length Mean                          166.667\n",
      "exploration/path length Std                           230.111\n",
      "exploration/path length Max                           675\n",
      "exploration/path length Min                            19\n",
      "exploration/Rewards Mean                               -0.249002\n",
      "exploration/Rewards Std                                 0.541565\n",
      "exploration/Rewards Max                                 3.04311\n",
      "exploration/Rewards Min                                -1.58114\n",
      "exploration/Returns Mean                              -41.5003\n",
      "exploration/Returns Std                                62.928\n",
      "exploration/Returns Max                                -6.03876\n",
      "exploration/Returns Min                              -181.646\n",
      "exploration/Actions Mean                               -0.0037282\n",
      "exploration/Actions Std                                 0.57751\n",
      "exploration/Actions Max                                 0.997156\n",
      "exploration/Actions Min                                -0.994719\n",
      "exploration/Num Paths                                   6\n",
      "exploration/Average Returns                           -41.5003\n",
      "exploration/env_infos/final/reward_forward Mean        -0.212891\n",
      "exploration/env_infos/final/reward_forward Std          0.938146\n",
      "exploration/env_infos/final/reward_forward Max          0.948129\n",
      "exploration/env_infos/final/reward_forward Min         -1.73728\n",
      "exploration/env_infos/initial/reward_forward Mean       0.116832\n",
      "exploration/env_infos/initial/reward_forward Std        0.203561\n",
      "exploration/env_infos/initial/reward_forward Max        0.382411\n",
      "exploration/env_infos/initial/reward_forward Min       -0.243244\n",
      "exploration/env_infos/reward_forward Mean              -0.0754502\n",
      "exploration/env_infos/reward_forward Std                0.774884\n",
      "exploration/env_infos/reward_forward Max                2.69886\n",
      "exploration/env_infos/reward_forward Min               -2.13304\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.54536\n",
      "exploration/env_infos/final/reward_ctrl Std             0.481031\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.823631\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.06251\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.4984\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.563375\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.554805\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.22606\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.33413\n",
      "exploration/env_infos/reward_ctrl Std                   0.416724\n",
      "exploration/env_infos/reward_ctrl Max                  -0.135387\n",
      "exploration/env_infos/reward_ctrl Min                  -2.58114\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.464438\n",
      "exploration/env_infos/final/torso_velocity Std          1.14127\n",
      "exploration/env_infos/final/torso_velocity Max          2.73374\n",
      "exploration/env_infos/final/torso_velocity Min         -1.73728\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164258\n",
      "exploration/env_infos/initial/torso_velocity Std        0.279052\n",
      "exploration/env_infos/initial/torso_velocity Max        0.664647\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.253497\n",
      "exploration/env_infos/torso_velocity Mean              -0.0220093\n",
      "exploration/env_infos/torso_velocity Std                0.751178\n",
      "exploration/env_infos/torso_velocity Max                3.70398\n",
      "exploration/env_infos/torso_velocity Min               -2.13304\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.98701\n",
      "evaluation/Rewards Std                                  0.0396835\n",
      "evaluation/Rewards Max                                  2.72595\n",
      "evaluation/Rewards Min                                  0.759778\n",
      "evaluation/Returns Mean                               987.01\n",
      "evaluation/Returns Std                                  8.47919\n",
      "evaluation/Returns Max                                998.909\n",
      "evaluation/Returns Min                                963.497\n",
      "evaluation/Actions Mean                                 0.0149963\n",
      "evaluation/Actions Std                                  0.0587006\n",
      "evaluation/Actions Max                                  0.307072\n",
      "evaluation/Actions Min                                 -0.302835\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            987.01\n",
      "evaluation/env_infos/final/reward_forward Mean          1.2256e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.46601e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.43228e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.67566e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0121978\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14095\n",
      "evaluation/env_infos/initial/reward_forward Max         0.299432\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.248281\n",
      "evaluation/env_infos/reward_forward Mean                0.00345763\n",
      "evaluation/env_infos/reward_forward Std                 0.0548803\n",
      "evaluation/env_infos/reward_forward Max                 1.62879\n",
      "evaluation/env_infos/reward_forward Min                -0.976462\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0145247\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0073222\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00804192\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0361149\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0171261\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0110846\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0044897\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0432121\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0146826\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0078751\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00382044\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.240222\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          5.9866e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.41793e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.46055e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.00203e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.15915\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.241651\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.67922\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.248281\n",
      "evaluation/env_infos/torso_velocity Mean                0.000358137\n",
      "evaluation/env_infos/torso_velocity Std                 0.0613741\n",
      "evaluation/env_infos/torso_velocity Max                 1.62879\n",
      "evaluation/env_infos/torso_velocity Min                -1.69532\n",
      "time/data storing (s)                                   0.4137\n",
      "time/evaluation sampling (s)                           40.7929\n",
      "time/exploration sampling (s)                           1.79954\n",
      "time/logging (s)                                        0.224757\n",
      "time/saving (s)                                         0.0246451\n",
      "time/training (s)                                       3.17479\n",
      "time/epoch (s)                                         46.4303\n",
      "time/total (s)                                        354.216\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:13:22.071646 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  17960\n",
      "trainer/QF1 Loss                                        0.384086\n",
      "trainer/QF2 Loss                                        0.403895\n",
      "trainer/Policy Loss                                    -9.15421\n",
      "trainer/Q1 Predictions Mean                             3.82136\n",
      "trainer/Q1 Predictions Std                              0.519796\n",
      "trainer/Q1 Predictions Max                              6.33165\n",
      "trainer/Q1 Predictions Min                              2.40693\n",
      "trainer/Q2 Predictions Mean                             3.85719\n",
      "trainer/Q2 Predictions Std                              0.543148\n",
      "trainer/Q2 Predictions Max                              6.95915\n",
      "trainer/Q2 Predictions Min                              2.4765\n",
      "trainer/Q Targets Mean                                  3.90396\n",
      "trainer/Q Targets Std                                   0.829657\n",
      "trainer/Q Targets Max                                   7.87143\n",
      "trainer/Q Targets Min                                   2.32391\n",
      "trainer/Log Pis Mean                                   -5.3564\n",
      "trainer/Log Pis Std                                     0.665668\n",
      "trainer/Log Pis Max                                    -3.43008\n",
      "trainer/Log Pis Min                                   -10.3877\n",
      "trainer/Policy mu Mean                                 -0.0224191\n",
      "trainer/Policy mu Std                                   0.142804\n",
      "trainer/Policy mu Max                                   0.692285\n",
      "trainer/Policy mu Min                                  -0.703672\n",
      "trainer/Policy log std Mean                            -0.215391\n",
      "trainer/Policy log std Std                              0.0549546\n",
      "trainer/Policy log std Max                             -0.101685\n",
      "trainer/Policy log std Min                             -0.461704\n",
      "trainer/Alpha                                           0.122387\n",
      "trainer/Alpha Loss                                    -28.0163\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            68\n",
      "exploration/path length Mean                           83.3333\n",
      "exploration/path length Std                            49.9622\n",
      "exploration/path length Max                           152\n",
      "exploration/path length Min                             9\n",
      "exploration/Rewards Mean                               -0.238359\n",
      "exploration/Rewards Std                                 0.482323\n",
      "exploration/Rewards Max                                 2.65289\n",
      "exploration/Rewards Min                                -1.72432\n",
      "exploration/Returns Mean                              -19.8632\n",
      "exploration/Returns Std                                13.6439\n",
      "exploration/Returns Max                                -1.34462\n",
      "exploration/Returns Min                               -41.9105\n",
      "exploration/Actions Mean                               -0.00710326\n",
      "exploration/Actions Std                                 0.568799\n",
      "exploration/Actions Max                                 0.993494\n",
      "exploration/Actions Min                                -0.994955\n",
      "exploration/Num Paths                                  12\n",
      "exploration/Average Returns                           -19.8632\n",
      "exploration/env_infos/final/reward_forward Mean         0.310921\n",
      "exploration/env_infos/final/reward_forward Std          0.900634\n",
      "exploration/env_infos/final/reward_forward Max          2.08228\n",
      "exploration/env_infos/final/reward_forward Min         -1.13401\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0462664\n",
      "exploration/env_infos/initial/reward_forward Std        0.142315\n",
      "exploration/env_infos/initial/reward_forward Max        0.129472\n",
      "exploration/env_infos/initial/reward_forward Min       -0.292417\n",
      "exploration/env_infos/reward_forward Mean               0.00701719\n",
      "exploration/env_infos/reward_forward Std                0.879571\n",
      "exploration/env_infos/reward_forward Max                3.05947\n",
      "exploration/env_infos/reward_forward Min               -2.68095\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.32085\n",
      "exploration/env_infos/final/reward_ctrl Std             0.396971\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.34331\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.82156\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.19023\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.473614\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.301793\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.07819\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.29433\n",
      "exploration/env_infos/reward_ctrl Std                   0.405868\n",
      "exploration/env_infos/reward_ctrl Max                  -0.225478\n",
      "exploration/env_infos/reward_ctrl Min                  -2.72432\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.554245\n",
      "exploration/env_infos/final/torso_velocity Std          1.00827\n",
      "exploration/env_infos/final/torso_velocity Max          2.34701\n",
      "exploration/env_infos/final/torso_velocity Min         -1.51704\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.139725\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254143\n",
      "exploration/env_infos/initial/torso_velocity Max        0.621303\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.292417\n",
      "exploration/env_infos/torso_velocity Mean               0.0341194\n",
      "exploration/env_infos/torso_velocity Std                0.840726\n",
      "exploration/env_infos/torso_velocity Max                3.83369\n",
      "exploration/env_infos/torso_velocity Min               -2.68095\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969855\n",
      "evaluation/Rewards Std                                  0.0385554\n",
      "evaluation/Rewards Max                                  2.58542\n",
      "evaluation/Rewards Min                                  0.759982\n",
      "evaluation/Returns Mean                               969.855\n",
      "evaluation/Returns Std                                 18.2928\n",
      "evaluation/Returns Max                                996.872\n",
      "evaluation/Returns Min                                930.585\n",
      "evaluation/Actions Mean                                -0.020587\n",
      "evaluation/Actions Std                                  0.0877664\n",
      "evaluation/Actions Max                                  0.315837\n",
      "evaluation/Actions Min                                 -0.400138\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.855\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00884075\n",
      "evaluation/env_infos/final/reward_forward Std           0.038718\n",
      "evaluation/env_infos/final/reward_forward Max           0.00798348\n",
      "evaluation/env_infos/final/reward_forward Min          -0.195648\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0129409\n",
      "evaluation/env_infos/initial/reward_forward Std         0.137622\n",
      "evaluation/env_infos/initial/reward_forward Max         0.299737\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.21232\n",
      "evaluation/env_infos/reward_forward Mean               -0.00174119\n",
      "evaluation/env_infos/reward_forward Std                 0.068903\n",
      "evaluation/env_infos/reward_forward Max                 0.896648\n",
      "evaluation/env_infos/reward_forward Min                -1.05803\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.03232\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0174142\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0119816\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0697297\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0388513\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0250404\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00751934\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.1044\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0325071\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0180625\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00520341\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.240018\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00546335\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0296304\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0388023\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.195648\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.161422\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231659\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.653617\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.282189\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00138769\n",
      "evaluation/env_infos/torso_velocity Std                 0.0788484\n",
      "evaluation/env_infos/torso_velocity Max                 1.71985\n",
      "evaluation/env_infos/torso_velocity Min                -1.66385\n",
      "time/data storing (s)                                   0.491637\n",
      "time/evaluation sampling (s)                           42.7498\n",
      "time/exploration sampling (s)                           1.6982\n",
      "time/logging (s)                                        0.276431\n",
      "time/saving (s)                                         0.0252883\n",
      "time/training (s)                                       3.66988\n",
      "time/epoch (s)                                         48.9112\n",
      "time/total (s)                                        403.264\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:14:10.662392 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  19960\n",
      "trainer/QF1 Loss                                        0.375926\n",
      "trainer/QF2 Loss                                        0.379589\n",
      "trainer/Policy Loss                                    -8.62623\n",
      "trainer/Q1 Predictions Mean                             3.70063\n",
      "trainer/Q1 Predictions Std                              0.512177\n",
      "trainer/Q1 Predictions Max                              5.44739\n",
      "trainer/Q1 Predictions Min                              2.40749\n",
      "trainer/Q2 Predictions Mean                             3.67164\n",
      "trainer/Q2 Predictions Std                              0.526121\n",
      "trainer/Q2 Predictions Max                              5.74106\n",
      "trainer/Q2 Predictions Min                              2.13768\n",
      "trainer/Q Targets Mean                                  3.71475\n",
      "trainer/Q Targets Std                                   0.771212\n",
      "trainer/Q Targets Max                                   7.43389\n",
      "trainer/Q Targets Min                                  -1.02873\n",
      "trainer/Log Pis Mean                                   -4.93719\n",
      "trainer/Log Pis Std                                     0.926502\n",
      "trainer/Log Pis Max                                    -2.21397\n",
      "trainer/Log Pis Min                                    -8.32468\n",
      "trainer/Policy mu Mean                                  0.0128459\n",
      "trainer/Policy mu Std                                   0.190704\n",
      "trainer/Policy mu Max                                   0.90167\n",
      "trainer/Policy mu Min                                  -0.76457\n",
      "trainer/Policy log std Mean                            -0.353778\n",
      "trainer/Policy log std Std                              0.106851\n",
      "trainer/Policy log std Max                             -0.120434\n",
      "trainer/Policy log std Min                             -0.63899\n",
      "trainer/Alpha                                           0.091282\n",
      "trainer/Alpha Loss                                    -30.9317\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                          142.857\n",
      "exploration/path length Std                            51.5958\n",
      "exploration/path length Max                           204\n",
      "exploration/path length Min                            55\n",
      "exploration/Rewards Mean                               -0.11139\n",
      "exploration/Rewards Std                                 0.405191\n",
      "exploration/Rewards Max                                 1.84638\n",
      "exploration/Rewards Min                                -1.33924\n",
      "exploration/Returns Mean                              -15.9129\n",
      "exploration/Returns Std                                 7.21283\n",
      "exploration/Returns Max                                -6.92398\n",
      "exploration/Returns Min                               -29.0877\n",
      "exploration/Actions Mean                                0.0101811\n",
      "exploration/Actions Std                                 0.535409\n",
      "exploration/Actions Max                                 0.995182\n",
      "exploration/Actions Min                                -0.98914\n",
      "exploration/Num Paths                                   7\n",
      "exploration/Average Returns                           -15.9129\n",
      "exploration/env_infos/final/reward_forward Mean        -0.361841\n",
      "exploration/env_infos/final/reward_forward Std          0.406231\n",
      "exploration/env_infos/final/reward_forward Max          0.543484\n",
      "exploration/env_infos/final/reward_forward Min         -0.799204\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0131906\n",
      "exploration/env_infos/initial/reward_forward Std        0.109855\n",
      "exploration/env_infos/initial/reward_forward Max        0.172136\n",
      "exploration/env_infos/initial/reward_forward Min       -0.139852\n",
      "exploration/env_infos/reward_forward Mean              -0.237367\n",
      "exploration/env_infos/reward_forward Std                0.718496\n",
      "exploration/env_infos/reward_forward Max                2.01842\n",
      "exploration/env_infos/reward_forward Min               -2.62826\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.17817\n",
      "exploration/env_infos/final/reward_ctrl Std             0.454851\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.358395\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.76285\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.24284\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.163247\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.996256\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.52946\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.14707\n",
      "exploration/env_infos/reward_ctrl Std                   0.368841\n",
      "exploration/env_infos/reward_ctrl Max                  -0.146429\n",
      "exploration/env_infos/reward_ctrl Min                  -2.33924\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.457244\n",
      "exploration/env_infos/final/torso_velocity Std          1.14616\n",
      "exploration/env_infos/final/torso_velocity Max          3.21958\n",
      "exploration/env_infos/final/torso_velocity Min         -1.51225\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.172041\n",
      "exploration/env_infos/initial/torso_velocity Std        0.195997\n",
      "exploration/env_infos/initial/torso_velocity Max        0.604322\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.139852\n",
      "exploration/env_infos/torso_velocity Mean               0.000752585\n",
      "exploration/env_infos/torso_velocity Std                0.811846\n",
      "exploration/env_infos/torso_velocity Max                3.53972\n",
      "exploration/env_infos/torso_velocity Min               -2.62826\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.949581\n",
      "evaluation/Rewards Std                                  0.0335153\n",
      "evaluation/Rewards Max                                  2.15993\n",
      "evaluation/Rewards Min                                  0.536254\n",
      "evaluation/Returns Mean                               949.581\n",
      "evaluation/Returns Std                                 12.8084\n",
      "evaluation/Returns Max                                965.426\n",
      "evaluation/Returns Min                                922.337\n",
      "evaluation/Actions Mean                                 0.0476446\n",
      "evaluation/Actions Std                                  0.103329\n",
      "evaluation/Actions Max                                  0.59407\n",
      "evaluation/Actions Min                                 -0.473435\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            949.581\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.74428e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000204247\n",
      "evaluation/env_infos/final/reward_forward Max           8.57147e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00103457\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0430727\n",
      "evaluation/env_infos/initial/reward_forward Std         0.148929\n",
      "evaluation/env_infos/initial/reward_forward Max         0.354117\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.20136\n",
      "evaluation/env_infos/reward_forward Mean                0.00428649\n",
      "evaluation/env_infos/reward_forward Std                 0.070118\n",
      "evaluation/env_infos/reward_forward Max                 1.5848\n",
      "evaluation/env_infos/reward_forward Min                -1.46802\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0510131\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0136707\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0343853\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0814601\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0558797\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0231435\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0163145\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.1028\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0517879\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0194443\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0163145\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.494388\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.71562e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000184615\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000995057\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00103457\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.165675\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.216833\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.597741\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.274004\n",
      "evaluation/env_infos/torso_velocity Mean                0.000270305\n",
      "evaluation/env_infos/torso_velocity Std                 0.0768676\n",
      "evaluation/env_infos/torso_velocity Max                 1.5848\n",
      "evaluation/env_infos/torso_velocity Min                -1.75722\n",
      "time/data storing (s)                                   0.420608\n",
      "time/evaluation sampling (s)                           42.0779\n",
      "time/exploration sampling (s)                           1.83811\n",
      "time/logging (s)                                        0.266585\n",
      "time/saving (s)                                         0.0258859\n",
      "time/training (s)                                       3.71708\n",
      "time/epoch (s)                                         48.3462\n",
      "time/total (s)                                        451.844\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:14:59.480832 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_06_42_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  21960\n",
      "trainer/QF1 Loss                                        0.45465\n",
      "trainer/QF2 Loss                                        0.496987\n",
      "trainer/Policy Loss                                    -7.95346\n",
      "trainer/Q1 Predictions Mean                             3.84682\n",
      "trainer/Q1 Predictions Std                              0.668328\n",
      "trainer/Q1 Predictions Max                              6.68602\n",
      "trainer/Q1 Predictions Min                              2.39851\n",
      "trainer/Q2 Predictions Mean                             3.93755\n",
      "trainer/Q2 Predictions Std                              0.624323\n",
      "trainer/Q2 Predictions Max                              6.48217\n",
      "trainer/Q2 Predictions Min                              2.59113\n",
      "trainer/Q Targets Mean                                  3.74159\n",
      "trainer/Q Targets Std                                   0.843922\n",
      "trainer/Q Targets Max                                   6.85767\n",
      "trainer/Q Targets Min                                  -0.655714\n",
      "trainer/Log Pis Mean                                   -3.9119\n",
      "trainer/Log Pis Std                                     1.27729\n",
      "trainer/Log Pis Max                                    -0.564619\n",
      "trainer/Log Pis Min                                    -7.51854\n",
      "trainer/Policy mu Mean                                 -0.0207093\n",
      "trainer/Policy mu Std                                   0.208145\n",
      "trainer/Policy mu Max                                   0.728934\n",
      "trainer/Policy mu Min                                  -1.11719\n",
      "trainer/Policy log std Mean                            -0.628037\n",
      "trainer/Policy log std Std                              0.194285\n",
      "trainer/Policy log std Max                             -0.255864\n",
      "trainer/Policy log std Min                             -1.1499\n",
      "trainer/Alpha                                           0.0690709\n",
      "trainer/Alpha Loss                                    -31.8038\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            77\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           453\n",
      "exploration/path length Max                           953\n",
      "exploration/path length Min                            47\n",
      "exploration/Rewards Mean                                0.179129\n",
      "exploration/Rewards Std                                 0.344973\n",
      "exploration/Rewards Max                                 1.76593\n",
      "exploration/Rewards Min                                -0.985025\n",
      "exploration/Returns Mean                               89.5643\n",
      "exploration/Returns Std                                81.9008\n",
      "exploration/Returns Max                               171.465\n",
      "exploration/Returns Min                                 7.66349\n",
      "exploration/Actions Mean                               -0.00092077\n",
      "exploration/Actions Std                                 0.460077\n",
      "exploration/Actions Max                                 0.978602\n",
      "exploration/Actions Min                                -0.994963\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                            89.5643\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0838133\n",
      "exploration/env_infos/final/reward_forward Std          0.779927\n",
      "exploration/env_infos/final/reward_forward Max          0.696113\n",
      "exploration/env_infos/final/reward_forward Min         -0.86374\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0353332\n",
      "exploration/env_infos/initial/reward_forward Std        0.115575\n",
      "exploration/env_infos/initial/reward_forward Max        0.150908\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0802421\n",
      "exploration/env_infos/reward_forward Mean               0.00805014\n",
      "exploration/env_infos/reward_forward Std                0.713393\n",
      "exploration/env_infos/reward_forward Max                1.83821\n",
      "exploration/env_infos/reward_forward Min               -2.43636\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.02327\n",
      "exploration/env_infos/final/reward_ctrl Std             0.214699\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.808567\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.23796\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.66776\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.127082\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.540678\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.794842\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.846687\n",
      "exploration/env_infos/reward_ctrl Std                   0.312507\n",
      "exploration/env_infos/reward_ctrl Max                  -0.101976\n",
      "exploration/env_infos/reward_ctrl Min                  -1.98502\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.122207\n",
      "exploration/env_infos/final/torso_velocity Std          0.581703\n",
      "exploration/env_infos/final/torso_velocity Max          0.775267\n",
      "exploration/env_infos/final/torso_velocity Min         -0.86374\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.157152\n",
      "exploration/env_infos/initial/torso_velocity Std        0.230163\n",
      "exploration/env_infos/initial/torso_velocity Max        0.556284\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.11648\n",
      "exploration/env_infos/torso_velocity Mean              -0.0474761\n",
      "exploration/env_infos/torso_velocity Std                0.716994\n",
      "exploration/env_infos/torso_velocity Max                2.85799\n",
      "exploration/env_infos/torso_velocity Min               -2.43636\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.94406\n",
      "evaluation/Rewards Std                                  0.0764613\n",
      "evaluation/Rewards Max                                  2.5707\n",
      "evaluation/Rewards Min                                  0.471602\n",
      "evaluation/Returns Mean                               944.06\n",
      "evaluation/Returns Std                                 20.6243\n",
      "evaluation/Returns Max                               1010.91\n",
      "evaluation/Returns Min                                906.843\n",
      "evaluation/Actions Mean                                -0.0308572\n",
      "evaluation/Actions Std                                  0.123397\n",
      "evaluation/Actions Max                                  0.696507\n",
      "evaluation/Actions Min                                 -0.560545\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            944.06\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00363247\n",
      "evaluation/env_infos/final/reward_forward Std           0.0178096\n",
      "evaluation/env_infos/final/reward_forward Max           0.000783415\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0908752\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0133209\n",
      "evaluation/env_infos/initial/reward_forward Std         0.138009\n",
      "evaluation/env_infos/initial/reward_forward Max         0.200324\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.316404\n",
      "evaluation/env_infos/reward_forward Mean                0.00661686\n",
      "evaluation/env_infos/reward_forward Std                 0.122458\n",
      "evaluation/env_infos/reward_forward Max                 1.51718\n",
      "evaluation/env_infos/reward_forward Min                -1.02224\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0583192\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0147312\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0388349\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0927172\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0760705\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0396301\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0283014\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.148021\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0647157\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0282356\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0283014\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.528398\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00299648\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0211125\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0243928\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.158899\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.123375\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238624\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.72049\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.359031\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00329498\n",
      "evaluation/env_infos/torso_velocity Std                 0.163393\n",
      "evaluation/env_infos/torso_velocity Max                 1.59698\n",
      "evaluation/env_infos/torso_velocity Min                -2.08188\n",
      "time/data storing (s)                                   0.33182\n",
      "time/evaluation sampling (s)                           42.6936\n",
      "time/exploration sampling (s)                           1.75178\n",
      "time/logging (s)                                        0.209642\n",
      "time/saving (s)                                         0.0497563\n",
      "time/training (s)                                       3.49507\n",
      "time/epoch (s)                                         48.5317\n",
      "time/total (s)                                        500.605\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15291]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22e2d778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22eae740). One of the two will be used. Which one is undefined.\n",
      "objc[15291]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22e2d700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22eae768). One of the two will be used. Which one is undefined.\n",
      "objc[15291]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22e2d7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22eae7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15291]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22e2d818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a22eae830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:15:05.313481 PDT | Variant:\n",
      "2021-05-25 10:15:05.314168 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 10,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": true,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": true,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:15:57.239136 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      22.4351\n",
      "trainer/QF2 Loss                                      22.4117\n",
      "trainer/Policy Loss                                   -5.32914\n",
      "trainer/Q1 Predictions Mean                            0.00190269\n",
      "trainer/Q1 Predictions Std                             0.00397006\n",
      "trainer/Q1 Predictions Max                             0.0133711\n",
      "trainer/Q1 Predictions Min                            -0.00965401\n",
      "trainer/Q2 Predictions Mean                            0.00435157\n",
      "trainer/Q2 Predictions Std                             0.00314918\n",
      "trainer/Q2 Predictions Max                             0.0144262\n",
      "trainer/Q2 Predictions Min                            -0.0021354\n",
      "trainer/Q Targets Mean                                 4.65308\n",
      "trainer/Q Targets Std                                  0.894771\n",
      "trainer/Q Targets Max                                  7.32257\n",
      "trainer/Q Targets Min                                 -0.242608\n",
      "trainer/Log Pis Mean                                  -5.32828\n",
      "trainer/Log Pis Std                                    0.609736\n",
      "trainer/Log Pis Max                                   -3.46965\n",
      "trainer/Log Pis Min                                   -7.67235\n",
      "trainer/Policy mu Mean                                -0.000280699\n",
      "trainer/Policy mu Std                                  0.00202992\n",
      "trainer/Policy mu Max                                  0.00578159\n",
      "trainer/Policy mu Min                                 -0.00896693\n",
      "trainer/Policy log std Mean                           -2.47298e-05\n",
      "trainer/Policy log std Std                             0.00216897\n",
      "trainer/Policy log std Max                             0.0073935\n",
      "trainer/Policy log std Min                            -0.0058842\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           10\n",
      "exploration/path length Mean                         111.111\n",
      "exploration/path length Std                           72.756\n",
      "exploration/path length Max                          238\n",
      "exploration/path length Min                           28\n",
      "exploration/Rewards Mean                              -0.492163\n",
      "exploration/Rewards Std                                0.501246\n",
      "exploration/Rewards Max                                2.19494\n",
      "exploration/Rewards Min                               -1.89976\n",
      "exploration/Returns Mean                             -54.6848\n",
      "exploration/Returns Std                               34.6493\n",
      "exploration/Returns Max                              -16.3279\n",
      "exploration/Returns Min                             -114.069\n",
      "exploration/Actions Mean                               0.00459031\n",
      "exploration/Actions Std                                0.622429\n",
      "exploration/Actions Max                                0.999667\n",
      "exploration/Actions Min                               -0.999064\n",
      "exploration/Num Paths                                  9\n",
      "exploration/Average Returns                          -54.6848\n",
      "exploration/env_infos/final/reward_forward Mean        0.448118\n",
      "exploration/env_infos/final/reward_forward Std         0.513157\n",
      "exploration/env_infos/final/reward_forward Max         1.25864\n",
      "exploration/env_infos/final/reward_forward Min        -0.53702\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0556704\n",
      "exploration/env_infos/initial/reward_forward Std       0.15415\n",
      "exploration/env_infos/initial/reward_forward Max       0.29624\n",
      "exploration/env_infos/initial/reward_forward Min      -0.137223\n",
      "exploration/env_infos/reward_forward Mean              0.0456405\n",
      "exploration/env_infos/reward_forward Std               0.697938\n",
      "exploration/env_infos/reward_forward Max               1.984\n",
      "exploration/env_infos/reward_forward Min              -2.59125\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.39539\n",
      "exploration/env_infos/final/reward_ctrl Std            0.263575\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.02595\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.84215\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.62002\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.301312\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.25529\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.22974\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54975\n",
      "exploration/env_infos/reward_ctrl Std                  0.438035\n",
      "exploration/env_infos/reward_ctrl Max                 -0.47798\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89976\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.892663\n",
      "exploration/env_infos/final/torso_velocity Std         1.02881\n",
      "exploration/env_infos/final/torso_velocity Max         3.3555\n",
      "exploration/env_infos/final/torso_velocity Min        -0.723139\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.134228\n",
      "exploration/env_infos/initial/torso_velocity Std       0.233186\n",
      "exploration/env_infos/initial/torso_velocity Max       0.487442\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.283399\n",
      "exploration/env_infos/torso_velocity Mean              0.0594558\n",
      "exploration/env_infos/torso_velocity Std               0.765213\n",
      "exploration/env_infos/torso_velocity Max               3.49422\n",
      "exploration/env_infos/torso_velocity Min              -2.59125\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00139\n",
      "evaluation/Rewards Std                                 0.0175484\n",
      "evaluation/Rewards Max                                 1.86721\n",
      "evaluation/Rewards Min                                 0.999984\n",
      "evaluation/Returns Mean                             1001.39\n",
      "evaluation/Returns Std                                 1.68878\n",
      "evaluation/Returns Max                              1005.78\n",
      "evaluation/Returns Min                               999.993\n",
      "evaluation/Actions Mean                               -0.000149445\n",
      "evaluation/Actions Std                                 0.00116925\n",
      "evaluation/Actions Max                                 0.00311301\n",
      "evaluation/Actions Min                                -0.00384349\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1001.39\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000343119\n",
      "evaluation/env_infos/final/reward_forward Std          0.000372995\n",
      "evaluation/env_infos/final/reward_forward Max          0.00121199\n",
      "evaluation/env_infos/final/reward_forward Min         -1.62803e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0183562\n",
      "evaluation/env_infos/initial/reward_forward Std        0.125617\n",
      "evaluation/env_infos/initial/reward_forward Max        0.280311\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.19694\n",
      "evaluation/env_infos/reward_forward Mean               0.00128884\n",
      "evaluation/env_infos/reward_forward Std                0.0411362\n",
      "evaluation/env_infos/reward_forward Max                1.11561\n",
      "evaluation/env_infos/reward_forward Min               -1.10151\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.52393e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.45343e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.73706e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.74066e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.32649e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           5.4033e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.47104e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.35538e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.55792e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   6.80471e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -3.50326e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.61392e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000206656\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000439962\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00186611\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000173494\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.148068\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.2098\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.552152\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.222923\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000256749\n",
      "evaluation/env_infos/torso_velocity Std                0.0481635\n",
      "evaluation/env_infos/torso_velocity Max                1.11561\n",
      "evaluation/env_infos/torso_velocity Min               -1.62095\n",
      "time/data storing (s)                                  0.69184\n",
      "time/evaluation sampling (s)                          42.9174\n",
      "time/exploration sampling (s)                          1.70269\n",
      "time/logging (s)                                       0.26477\n",
      "time/saving (s)                                        0.535048\n",
      "time/training (s)                                      3.25681\n",
      "time/epoch (s)                                        49.3686\n",
      "time/total (s)                                        55.4688\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "2021-05-25 10:16:47.442445 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.447199\n",
      "trainer/QF2 Loss                                       0.449862\n",
      "trainer/Policy Loss                                   -9.41523\n",
      "trainer/Q1 Predictions Mean                            3.95835\n",
      "trainer/Q1 Predictions Std                             0.357298\n",
      "trainer/Q1 Predictions Max                             4.99993\n",
      "trainer/Q1 Predictions Min                             2.60872\n",
      "trainer/Q2 Predictions Mean                            3.95705\n",
      "trainer/Q2 Predictions Std                             0.356626\n",
      "trainer/Q2 Predictions Max                             5.0335\n",
      "trainer/Q2 Predictions Min                             2.64474\n",
      "trainer/Q Targets Mean                                 4.08641\n",
      "trainer/Q Targets Std                                  0.616045\n",
      "trainer/Q Targets Max                                  5.97867\n",
      "trainer/Q Targets Min                                 -0.0930805\n",
      "trainer/Log Pis Mean                                  -5.47334\n",
      "trainer/Log Pis Std                                    0.444973\n",
      "trainer/Log Pis Max                                   -4.58119\n",
      "trainer/Log Pis Min                                   -9.39418\n",
      "trainer/Policy mu Mean                                 7.85847e-05\n",
      "trainer/Policy mu Std                                  0.0225875\n",
      "trainer/Policy mu Max                                  0.104991\n",
      "trainer/Policy mu Min                                 -0.0803367\n",
      "trainer/Policy log std Mean                           -0.106016\n",
      "trainer/Policy log std Std                             0.0187748\n",
      "trainer/Policy log std Max                            -0.0672687\n",
      "trainer/Policy log std Min                            -0.193107\n",
      "trainer/Alpha                                          0.738539\n",
      "trainer/Alpha Loss                                    -4.04307\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           17\n",
      "exploration/path length Mean                         142.857\n",
      "exploration/path length Std                          144.664\n",
      "exploration/path length Max                          482\n",
      "exploration/path length Min                           23\n",
      "exploration/Rewards Mean                              -0.394587\n",
      "exploration/Rewards Std                                0.455284\n",
      "exploration/Rewards Max                                1.5766\n",
      "exploration/Rewards Min                               -1.74194\n",
      "exploration/Returns Mean                             -56.3696\n",
      "exploration/Returns Std                               61.5299\n",
      "exploration/Returns Max                              -11.2673\n",
      "exploration/Returns Min                             -201.609\n",
      "exploration/Actions Mean                              -0.00751101\n",
      "exploration/Actions Std                                0.600118\n",
      "exploration/Actions Max                                0.997928\n",
      "exploration/Actions Min                               -0.998768\n",
      "exploration/Num Paths                                  7\n",
      "exploration/Average Returns                          -56.3696\n",
      "exploration/env_infos/final/reward_forward Mean        0.549793\n",
      "exploration/env_infos/final/reward_forward Std         1.04431\n",
      "exploration/env_infos/final/reward_forward Max         2.19948\n",
      "exploration/env_infos/final/reward_forward Min        -1.013\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00662982\n",
      "exploration/env_infos/initial/reward_forward Std       0.142974\n",
      "exploration/env_infos/initial/reward_forward Max       0.191573\n",
      "exploration/env_infos/initial/reward_forward Min      -0.256314\n",
      "exploration/env_infos/reward_forward Mean             -0.0188896\n",
      "exploration/env_infos/reward_forward Std               0.683799\n",
      "exploration/env_infos/reward_forward Max               3.00945\n",
      "exploration/env_infos/reward_forward Min              -2.37128\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.22838\n",
      "exploration/env_infos/final/reward_ctrl Std            0.262585\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.01361\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.77339\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.21367\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.458556\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.512003\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.82384\n",
      "exploration/env_infos/reward_ctrl Mean                -1.44079\n",
      "exploration/env_infos/reward_ctrl Std                  0.411849\n",
      "exploration/env_infos/reward_ctrl Max                 -0.417657\n",
      "exploration/env_infos/reward_ctrl Min                 -2.74194\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.692281\n",
      "exploration/env_infos/final/torso_velocity Std         1.31453\n",
      "exploration/env_infos/final/torso_velocity Max         2.86967\n",
      "exploration/env_infos/final/torso_velocity Min        -2.04389\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.169615\n",
      "exploration/env_infos/initial/torso_velocity Std       0.244341\n",
      "exploration/env_infos/initial/torso_velocity Max       0.67855\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.256314\n",
      "exploration/env_infos/torso_velocity Mean             -0.00988395\n",
      "exploration/env_infos/torso_velocity Std               0.680233\n",
      "exploration/env_infos/torso_velocity Max               3.81466\n",
      "exploration/env_infos/torso_velocity Min              -3.07782\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999666\n",
      "evaluation/Rewards Std                                 0.0169272\n",
      "evaluation/Rewards Max                                 2.19328\n",
      "evaluation/Rewards Min                                 0.998256\n",
      "evaluation/Returns Mean                              999.666\n",
      "evaluation/Returns Std                                 1.00043\n",
      "evaluation/Returns Max                              1002.88\n",
      "evaluation/Returns Min                               999.016\n",
      "evaluation/Actions Mean                               -0.000313368\n",
      "evaluation/Actions Std                                 0.0151695\n",
      "evaluation/Actions Max                                 0.0410746\n",
      "evaluation/Actions Min                                -0.0380536\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.666\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.000240781\n",
      "evaluation/env_infos/final/reward_forward Std          0.000478878\n",
      "evaluation/env_infos/final/reward_forward Max         -9.51747e-08\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00185597\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.013554\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0791312\n",
      "evaluation/env_infos/initial/reward_forward Max        0.196731\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.227212\n",
      "evaluation/env_infos/reward_forward Mean              -0.00329945\n",
      "evaluation/env_infos/reward_forward Std                0.0452989\n",
      "evaluation/env_infos/reward_forward Max                1.24873\n",
      "evaluation/env_infos/reward_forward Min               -1.31327\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00091925\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00010042\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000711856\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00102319\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.000834908\n",
      "evaluation/env_infos/initial/reward_ctrl Std           6.29874e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000749403\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0010063\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.000920852\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000108678\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000616849\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00174403\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.14086e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000969777\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00607481\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00185597\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.142525\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.226273\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.640586\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.227212\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0016417\n",
      "evaluation/env_infos/torso_velocity Std                0.0557631\n",
      "evaluation/env_infos/torso_velocity Max                1.24873\n",
      "evaluation/env_infos/torso_velocity Min               -2.02573\n",
      "time/data storing (s)                                  1.005\n",
      "time/evaluation sampling (s)                          42.5656\n",
      "time/exploration sampling (s)                          1.74168\n",
      "time/logging (s)                                       0.285867\n",
      "time/saving (s)                                        0.420137\n",
      "time/training (s)                                      3.91448\n",
      "time/epoch (s)                                        49.9328\n",
      "time/total (s)                                       105.693\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:17:35.617319 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.363951\n",
      "trainer/QF2 Loss                                       0.361489\n",
      "trainer/Policy Loss                                   -9.2823\n",
      "trainer/Q1 Predictions Mean                            3.84069\n",
      "trainer/Q1 Predictions Std                             0.303333\n",
      "trainer/Q1 Predictions Max                             4.88014\n",
      "trainer/Q1 Predictions Min                             2.70561\n",
      "trainer/Q2 Predictions Mean                            3.84183\n",
      "trainer/Q2 Predictions Std                             0.304917\n",
      "trainer/Q2 Predictions Max                             4.98999\n",
      "trainer/Q2 Predictions Min                             2.69857\n",
      "trainer/Q Targets Mean                                 3.89848\n",
      "trainer/Q Targets Std                                  0.562469\n",
      "trainer/Q Targets Max                                  6.97337\n",
      "trainer/Q Targets Min                                  2.39379\n",
      "trainer/Log Pis Mean                                  -5.46942\n",
      "trainer/Log Pis Std                                    0.344122\n",
      "trainer/Log Pis Max                                   -4.59324\n",
      "trainer/Log Pis Min                                   -7.89713\n",
      "trainer/Policy mu Mean                                -0.0111163\n",
      "trainer/Policy mu Std                                  0.0217398\n",
      "trainer/Policy mu Max                                  0.0663106\n",
      "trainer/Policy mu Min                                 -0.0859686\n",
      "trainer/Policy log std Mean                           -0.118911\n",
      "trainer/Policy log std Std                             0.0149809\n",
      "trainer/Policy log std Max                            -0.0738938\n",
      "trainer/Policy log std Min                            -0.199072\n",
      "trainer/Alpha                                          0.54707\n",
      "trainer/Alpha Loss                                    -8.08403\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           23\n",
      "exploration/path length Mean                         166.667\n",
      "exploration/path length Std                          190.944\n",
      "exploration/path length Max                          585\n",
      "exploration/path length Min                           29\n",
      "exploration/Rewards Mean                              -0.351693\n",
      "exploration/Rewards Std                                0.487369\n",
      "exploration/Rewards Max                                2.14658\n",
      "exploration/Rewards Min                               -1.74571\n",
      "exploration/Returns Mean                             -58.6154\n",
      "exploration/Returns Std                               76.6886\n",
      "exploration/Returns Max                               -9.57702\n",
      "exploration/Returns Min                             -228.801\n",
      "exploration/Actions Mean                              -0.00801079\n",
      "exploration/Actions Std                                0.59333\n",
      "exploration/Actions Max                                0.998181\n",
      "exploration/Actions Min                               -0.996221\n",
      "exploration/Num Paths                                  6\n",
      "exploration/Average Returns                          -58.6154\n",
      "exploration/env_infos/final/reward_forward Mean       -0.124729\n",
      "exploration/env_infos/final/reward_forward Std         0.410782\n",
      "exploration/env_infos/final/reward_forward Max         0.683958\n",
      "exploration/env_infos/final/reward_forward Min        -0.668488\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0372192\n",
      "exploration/env_infos/initial/reward_forward Std       0.178078\n",
      "exploration/env_infos/initial/reward_forward Max       0.295479\n",
      "exploration/env_infos/initial/reward_forward Min      -0.31104\n",
      "exploration/env_infos/reward_forward Mean              0.130056\n",
      "exploration/env_infos/reward_forward Std               0.672124\n",
      "exploration/env_infos/reward_forward Max               3.14475\n",
      "exploration/env_infos/reward_forward Min              -1.61715\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.53747\n",
      "exploration/env_infos/final/reward_ctrl Std            0.410671\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.07614\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.16477\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.0104\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.308213\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.531249\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.44679\n",
      "exploration/env_infos/reward_ctrl Mean                -1.40842\n",
      "exploration/env_infos/reward_ctrl Std                  0.422262\n",
      "exploration/env_infos/reward_ctrl Max                 -0.229513\n",
      "exploration/env_infos/reward_ctrl Min                 -2.74571\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.55897\n",
      "exploration/env_infos/final/torso_velocity Std         1.0421\n",
      "exploration/env_infos/final/torso_velocity Max         3.22568\n",
      "exploration/env_infos/final/torso_velocity Min        -0.668488\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.143893\n",
      "exploration/env_infos/initial/torso_velocity Std       0.286454\n",
      "exploration/env_infos/initial/torso_velocity Max       0.707268\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.31104\n",
      "exploration/env_infos/torso_velocity Mean              0.0687614\n",
      "exploration/env_infos/torso_velocity Std               0.731187\n",
      "exploration/env_infos/torso_velocity Max               3.36153\n",
      "exploration/env_infos/torso_velocity Min              -3.29053\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.99789\n",
      "evaluation/Rewards Std                                 0.0265802\n",
      "evaluation/Rewards Max                                 2.3159\n",
      "evaluation/Rewards Min                                 0.99556\n",
      "evaluation/Returns Mean                              997.89\n",
      "evaluation/Returns Std                                 1.84428\n",
      "evaluation/Returns Max                              1004.33\n",
      "evaluation/Returns Min                               996.147\n",
      "evaluation/Actions Mean                               -0.011843\n",
      "evaluation/Actions Std                                 0.0260811\n",
      "evaluation/Actions Max                                 0.0430498\n",
      "evaluation/Actions Min                                -0.0630201\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           997.89\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.02443e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.74187e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.86605e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.69286e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0105004\n",
      "evaluation/env_infos/initial/reward_forward Std        0.11475\n",
      "evaluation/env_infos/initial/reward_forward Max        0.234929\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.239733\n",
      "evaluation/env_infos/reward_forward Mean              -0.000143256\n",
      "evaluation/env_infos/reward_forward Std                0.0391871\n",
      "evaluation/env_infos/reward_forward Max                0.672967\n",
      "evaluation/env_infos/reward_forward Min               -1.09118\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00329098\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000517632\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00228709\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00388044\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00218104\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000339987\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00157009\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00285797\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00328192\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000526678\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00114234\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00443996\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.29557e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          2.95184e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          4.86605e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.21462e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.14315\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.231068\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.582882\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.298281\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00114711\n",
      "evaluation/env_infos/torso_velocity Std                0.050087\n",
      "evaluation/env_infos/torso_velocity Max                1.72062\n",
      "evaluation/env_infos/torso_velocity Min               -1.66361\n",
      "time/data storing (s)                                  1.27847\n",
      "time/evaluation sampling (s)                          41.1819\n",
      "time/exploration sampling (s)                          1.71416\n",
      "time/logging (s)                                       0.263067\n",
      "time/saving (s)                                        0.486164\n",
      "time/training (s)                                      3.03404\n",
      "time/epoch (s)                                        47.9578\n",
      "time/total (s)                                       153.844\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:18:27.448683 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.580424\n",
      "trainer/QF2 Loss                                        0.579147\n",
      "trainer/Policy Loss                                    -9.37391\n",
      "trainer/Q1 Predictions Mean                             3.94244\n",
      "trainer/Q1 Predictions Std                              0.330046\n",
      "trainer/Q1 Predictions Max                              5.22226\n",
      "trainer/Q1 Predictions Min                              3.2339\n",
      "trainer/Q2 Predictions Mean                             3.9396\n",
      "trainer/Q2 Predictions Std                              0.323695\n",
      "trainer/Q2 Predictions Max                              5.19803\n",
      "trainer/Q2 Predictions Min                              3.32554\n",
      "trainer/Q Targets Mean                                  3.79842\n",
      "trainer/Q Targets Std                                   0.799927\n",
      "trainer/Q Targets Max                                   6.38935\n",
      "trainer/Q Targets Min                                  -1.18117\n",
      "trainer/Log Pis Mean                                   -5.45462\n",
      "trainer/Log Pis Std                                     0.448272\n",
      "trainer/Log Pis Max                                    -4.68311\n",
      "trainer/Log Pis Min                                    -9.86122\n",
      "trainer/Policy mu Mean                                 -0.00121513\n",
      "trainer/Policy mu Std                                   0.0303841\n",
      "trainer/Policy mu Max                                   0.149681\n",
      "trainer/Policy mu Min                                  -0.145238\n",
      "trainer/Policy log std Mean                            -0.146087\n",
      "trainer/Policy log std Std                              0.0255104\n",
      "trainer/Policy log std Max                             -0.0969156\n",
      "trainer/Policy log std Min                             -0.278355\n",
      "trainer/Alpha                                           0.405272\n",
      "trainer/Alpha Loss                                    -12.1118\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            31\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                           177.785\n",
      "exploration/path length Max                           587\n",
      "exploration/path length Min                            10\n",
      "exploration/Rewards Mean                               -0.303934\n",
      "exploration/Rewards Std                                 0.470176\n",
      "exploration/Rewards Max                                 2.04116\n",
      "exploration/Rewards Min                                -1.71499\n",
      "exploration/Returns Mean                              -37.9917\n",
      "exploration/Returns Std                                61.2139\n",
      "exploration/Returns Max                                -1.73191\n",
      "exploration/Returns Min                              -197.615\n",
      "exploration/Actions Mean                                0.00464686\n",
      "exploration/Actions Std                                 0.584093\n",
      "exploration/Actions Max                                 0.996445\n",
      "exploration/Actions Min                                -0.994153\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                           -37.9917\n",
      "exploration/env_infos/final/reward_forward Mean         0.598751\n",
      "exploration/env_infos/final/reward_forward Std          1.29086\n",
      "exploration/env_infos/final/reward_forward Max          2.00085\n",
      "exploration/env_infos/final/reward_forward Min         -2.2232\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0629839\n",
      "exploration/env_infos/initial/reward_forward Std        0.222618\n",
      "exploration/env_infos/initial/reward_forward Max        0.272333\n",
      "exploration/env_infos/initial/reward_forward Min       -0.357909\n",
      "exploration/env_infos/reward_forward Mean               0.0305317\n",
      "exploration/env_infos/reward_forward Std                0.670222\n",
      "exploration/env_infos/reward_forward Max                3.45074\n",
      "exploration/env_infos/reward_forward Min               -2.2232\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.45491\n",
      "exploration/env_infos/final/reward_ctrl Std             0.358251\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.81649\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.18117\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.28162\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.22391\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.860184\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.63678\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.36475\n",
      "exploration/env_infos/reward_ctrl Std                   0.404032\n",
      "exploration/env_infos/reward_ctrl Max                  -0.324274\n",
      "exploration/env_infos/reward_ctrl Min                  -2.71499\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.833493\n",
      "exploration/env_infos/final/torso_velocity Std          1.19652\n",
      "exploration/env_infos/final/torso_velocity Max          2.61444\n",
      "exploration/env_infos/final/torso_velocity Min         -2.2232\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.121408\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254701\n",
      "exploration/env_infos/initial/torso_velocity Max        0.484665\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.357909\n",
      "exploration/env_infos/torso_velocity Mean               0.00149628\n",
      "exploration/env_infos/torso_velocity Std                0.638095\n",
      "exploration/env_infos/torso_velocity Max                3.45074\n",
      "exploration/env_infos/torso_velocity Min               -2.35448\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.998228\n",
      "evaluation/Rewards Std                                  0.0162802\n",
      "evaluation/Rewards Max                                  2.06938\n",
      "evaluation/Rewards Min                                  0.995463\n",
      "evaluation/Returns Mean                               998.228\n",
      "evaluation/Returns Std                                  3.11441\n",
      "evaluation/Returns Max                               1009.71\n",
      "evaluation/Returns Min                                996.474\n",
      "evaluation/Actions Mean                                 0.00245988\n",
      "evaluation/Actions Std                                  0.027213\n",
      "evaluation/Actions Max                                  0.0854839\n",
      "evaluation/Actions Min                                 -0.0423704\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            998.228\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.49619e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000221548\n",
      "evaluation/env_infos/final/reward_forward Max           0.000427346\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00102531\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0186536\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124955\n",
      "evaluation/env_infos/initial/reward_forward Max         0.391075\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.174041\n",
      "evaluation/env_infos/reward_forward Mean               -0.00157163\n",
      "evaluation/env_infos/reward_forward Std                 0.0437628\n",
      "evaluation/env_infos/reward_forward Max                 0.972981\n",
      "evaluation/env_infos/reward_forward Min                -1.25957\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00303059\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000428419\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00236129\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00371292\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00167929\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000118923\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00146622\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00195262\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0029864\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000411113\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00146622\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00453683\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.08443e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000146097\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000427346\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00102531\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.172355\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22957\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.649504\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.232725\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0020133\n",
      "evaluation/env_infos/torso_velocity Std                 0.0510988\n",
      "evaluation/env_infos/torso_velocity Max                 1.15913\n",
      "evaluation/env_infos/torso_velocity Min                -1.77049\n",
      "time/data storing (s)                                   1.86199\n",
      "time/evaluation sampling (s)                           44.1267\n",
      "time/exploration sampling (s)                           1.90323\n",
      "time/logging (s)                                        0.1457\n",
      "time/saving (s)                                         0.414913\n",
      "time/training (s)                                       3.07426\n",
      "time/epoch (s)                                         51.5268\n",
      "time/total (s)                                        205.558\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:19:08.830849 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.331579\n",
      "trainer/QF2 Loss                                        0.325632\n",
      "trainer/Policy Loss                                    -9.17245\n",
      "trainer/Q1 Predictions Mean                             3.74506\n",
      "trainer/Q1 Predictions Std                              0.348976\n",
      "trainer/Q1 Predictions Max                              4.85774\n",
      "trainer/Q1 Predictions Min                              3.03316\n",
      "trainer/Q2 Predictions Mean                             3.76571\n",
      "trainer/Q2 Predictions Std                              0.336222\n",
      "trainer/Q2 Predictions Max                              4.94669\n",
      "trainer/Q2 Predictions Min                              3.0702\n",
      "trainer/Q Targets Mean                                  3.88141\n",
      "trainer/Q Targets Std                                   0.591207\n",
      "trainer/Q Targets Max                                   6.74389\n",
      "trainer/Q Targets Min                                   2.5221\n",
      "trainer/Log Pis Mean                                   -5.44601\n",
      "trainer/Log Pis Std                                     0.361382\n",
      "trainer/Log Pis Max                                    -4.59992\n",
      "trainer/Log Pis Min                                    -7.25924\n",
      "trainer/Policy mu Mean                                 -0.01284\n",
      "trainer/Policy mu Std                                   0.0416717\n",
      "trainer/Policy mu Max                                   0.112863\n",
      "trainer/Policy mu Min                                  -0.180179\n",
      "trainer/Policy log std Mean                            -0.126452\n",
      "trainer/Policy log std Std                              0.0140785\n",
      "trainer/Policy log std Max                             -0.0941771\n",
      "trainer/Policy log std Min                             -0.185801\n",
      "trainer/Alpha                                           0.300205\n",
      "trainer/Alpha Loss                                    -16.1391\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            36\n",
      "exploration/path length Mean                          200\n",
      "exploration/path length Std                           247.478\n",
      "exploration/path length Max                           654\n",
      "exploration/path length Min                            15\n",
      "exploration/Rewards Mean                               -0.316838\n",
      "exploration/Rewards Std                                 0.491837\n",
      "exploration/Rewards Max                                 2.01807\n",
      "exploration/Rewards Min                                -1.86738\n",
      "exploration/Returns Mean                              -63.3676\n",
      "exploration/Returns Std                                75.998\n",
      "exploration/Returns Max                                -6.01585\n",
      "exploration/Returns Min                              -201.333\n",
      "exploration/Actions Mean                               -0.0199207\n",
      "exploration/Actions Std                                 0.588941\n",
      "exploration/Actions Max                                 0.996241\n",
      "exploration/Actions Min                                -0.998667\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -63.3676\n",
      "exploration/env_infos/final/reward_forward Mean         0.500018\n",
      "exploration/env_infos/final/reward_forward Std          0.559906\n",
      "exploration/env_infos/final/reward_forward Max          1.28402\n",
      "exploration/env_infos/final/reward_forward Min         -0.0782704\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0380813\n",
      "exploration/env_infos/initial/reward_forward Std        0.197838\n",
      "exploration/env_infos/initial/reward_forward Max        0.194432\n",
      "exploration/env_infos/initial/reward_forward Min       -0.332515\n",
      "exploration/env_infos/reward_forward Mean              -0.029615\n",
      "exploration/env_infos/reward_forward Std                0.696431\n",
      "exploration/env_infos/reward_forward Max                2.21509\n",
      "exploration/env_infos/reward_forward Min               -2.46968\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.23937\n",
      "exploration/env_infos/final/reward_ctrl Std             0.288604\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.772768\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.65326\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.07165\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.25261\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.666982\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.40159\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38899\n",
      "exploration/env_infos/reward_ctrl Std                   0.406564\n",
      "exploration/env_infos/reward_ctrl Max                  -0.292877\n",
      "exploration/env_infos/reward_ctrl Min                  -2.86738\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.242584\n",
      "exploration/env_infos/final/torso_velocity Std          0.977391\n",
      "exploration/env_infos/final/torso_velocity Max          1.28402\n",
      "exploration/env_infos/final/torso_velocity Min         -2.70262\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.163071\n",
      "exploration/env_infos/initial/torso_velocity Std        0.282238\n",
      "exploration/env_infos/initial/torso_velocity Max        0.580274\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.332515\n",
      "exploration/env_infos/torso_velocity Mean               0.00265299\n",
      "exploration/env_infos/torso_velocity Std                0.653999\n",
      "exploration/env_infos/torso_velocity Max                2.7718\n",
      "exploration/env_infos/torso_velocity Min               -2.70262\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.993123\n",
      "evaluation/Rewards Std                                  0.0192982\n",
      "evaluation/Rewards Max                                  2.62549\n",
      "evaluation/Rewards Min                                  0.983194\n",
      "evaluation/Returns Mean                               993.123\n",
      "evaluation/Returns Std                                  1.4115\n",
      "evaluation/Returns Max                                998.448\n",
      "evaluation/Returns Min                                991.523\n",
      "evaluation/Actions Mean                                -0.0131621\n",
      "evaluation/Actions Std                                  0.0412864\n",
      "evaluation/Actions Max                                  0.0924969\n",
      "evaluation/Actions Min                                 -0.133841\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            993.123\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.0975e-05\n",
      "evaluation/env_infos/final/reward_forward Std           4.42306e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000117221\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000141268\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0256594\n",
      "evaluation/env_infos/initial/reward_forward Std         0.144271\n",
      "evaluation/env_infos/initial/reward_forward Max         0.337708\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.272079\n",
      "evaluation/env_infos/reward_forward Mean                6.35981e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0462371\n",
      "evaluation/env_infos/reward_forward Max                 1.19772\n",
      "evaluation/env_infos/reward_forward Min                -1.12447\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00751413\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000741264\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00658122\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00853913\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00609242\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000432081\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00527986\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00743281\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00751123\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00075388\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.005031\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0168062\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.3002e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           6.22581e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000157237\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000249086\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.162107\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.263112\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.671467\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.379748\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000849017\n",
      "evaluation/env_infos/torso_velocity Std                 0.0536916\n",
      "evaluation/env_infos/torso_velocity Max                 1.47126\n",
      "evaluation/env_infos/torso_velocity Min                -1.76407\n",
      "time/data storing (s)                                   0.96539\n",
      "time/evaluation sampling (s)                           36.0407\n",
      "time/exploration sampling (s)                           1.07598\n",
      "time/logging (s)                                        0.133998\n",
      "time/saving (s)                                         0.448095\n",
      "time/training (s)                                       2.58322\n",
      "time/epoch (s)                                         41.2474\n",
      "time/total (s)                                        246.928\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:19:40.272065 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.317236\n",
      "trainer/QF2 Loss                                        0.326991\n",
      "trainer/Policy Loss                                    -9.31879\n",
      "trainer/Q1 Predictions Mean                             3.86363\n",
      "trainer/Q1 Predictions Std                              0.361454\n",
      "trainer/Q1 Predictions Max                              5.5557\n",
      "trainer/Q1 Predictions Min                              2.58784\n",
      "trainer/Q2 Predictions Mean                             3.87371\n",
      "trainer/Q2 Predictions Std                              0.350196\n",
      "trainer/Q2 Predictions Max                              5.63544\n",
      "trainer/Q2 Predictions Min                              2.98805\n",
      "trainer/Q Targets Mean                                  3.91027\n",
      "trainer/Q Targets Std                                   0.628116\n",
      "trainer/Q Targets Max                                   6.74456\n",
      "trainer/Q Targets Min                                   2.49057\n",
      "trainer/Log Pis Mean                                   -5.48705\n",
      "trainer/Log Pis Std                                     0.51394\n",
      "trainer/Log Pis Max                                    -4.4242\n",
      "trainer/Log Pis Min                                    -9.82897\n",
      "trainer/Policy mu Mean                                  0.00166239\n",
      "trainer/Policy mu Std                                   0.0496143\n",
      "trainer/Policy mu Max                                   0.169663\n",
      "trainer/Policy mu Min                                  -0.137583\n",
      "trainer/Policy log std Mean                            -0.120429\n",
      "trainer/Policy log std Std                              0.0115049\n",
      "trainer/Policy log std Max                             -0.0874553\n",
      "trainer/Policy log std Min                             -0.176043\n",
      "trainer/Alpha                                           0.222435\n",
      "trainer/Alpha Loss                                    -20.2323\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            46\n",
      "exploration/path length Mean                          100\n",
      "exploration/path length Std                            66.7053\n",
      "exploration/path length Max                           214\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                               -0.314068\n",
      "exploration/Rewards Std                                 0.576442\n",
      "exploration/Rewards Max                                 2.67063\n",
      "exploration/Rewards Min                                -2.02714\n",
      "exploration/Returns Mean                              -31.4068\n",
      "exploration/Returns Std                                25.0879\n",
      "exploration/Returns Max                                -2.77041\n",
      "exploration/Returns Min                               -83.1638\n",
      "exploration/Actions Mean                               -0.00219145\n",
      "exploration/Actions Std                                 0.596973\n",
      "exploration/Actions Max                                 0.996094\n",
      "exploration/Actions Min                                -0.997812\n",
      "exploration/Num Paths                                  10\n",
      "exploration/Average Returns                           -31.4068\n",
      "exploration/env_infos/final/reward_forward Mean        -0.321063\n",
      "exploration/env_infos/final/reward_forward Std          1.27717\n",
      "exploration/env_infos/final/reward_forward Max          1.15538\n",
      "exploration/env_infos/final/reward_forward Min         -3.47433\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0158201\n",
      "exploration/env_infos/initial/reward_forward Std        0.164693\n",
      "exploration/env_infos/initial/reward_forward Max        0.363068\n",
      "exploration/env_infos/initial/reward_forward Min       -0.237872\n",
      "exploration/env_infos/reward_forward Mean               0.0650458\n",
      "exploration/env_infos/reward_forward Std                0.807047\n",
      "exploration/env_infos/reward_forward Max                2.03377\n",
      "exploration/env_infos/reward_forward Min               -3.47433\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.43626\n",
      "exploration/env_infos/final/reward_ctrl Std             0.503336\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.673198\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.19359\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.46088\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.549348\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.593388\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.32599\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.42553\n",
      "exploration/env_infos/reward_ctrl Std                   0.427723\n",
      "exploration/env_infos/reward_ctrl Max                  -0.275654\n",
      "exploration/env_infos/reward_ctrl Min                  -3.02714\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.396799\n",
      "exploration/env_infos/final/torso_velocity Std          1.22691\n",
      "exploration/env_infos/final/torso_velocity Max          2.69011\n",
      "exploration/env_infos/final/torso_velocity Min         -3.47433\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.16297\n",
      "exploration/env_infos/initial/torso_velocity Std        0.256073\n",
      "exploration/env_infos/initial/torso_velocity Max        0.712184\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.287036\n",
      "exploration/env_infos/torso_velocity Mean               0.0442816\n",
      "exploration/env_infos/torso_velocity Std                0.817758\n",
      "exploration/env_infos/torso_velocity Max                3.4149\n",
      "exploration/env_infos/torso_velocity Min               -3.47433\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.988407\n",
      "evaluation/Rewards Std                                  0.020875\n",
      "evaluation/Rewards Max                                  2.33092\n",
      "evaluation/Rewards Min                                  0.983534\n",
      "evaluation/Returns Mean                               988.407\n",
      "evaluation/Returns Std                                  2.41392\n",
      "evaluation/Returns Max                                993.988\n",
      "evaluation/Returns Min                                985.063\n",
      "evaluation/Actions Mean                                 0.00200204\n",
      "evaluation/Actions Std                                  0.055208\n",
      "evaluation/Actions Max                                  0.130059\n",
      "evaluation/Actions Min                                 -0.0883017\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            988.407\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.56675e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.80066e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.85504e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.9316e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00878093\n",
      "evaluation/env_infos/initial/reward_forward Std         0.107511\n",
      "evaluation/env_infos/initial/reward_forward Max         0.17907\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.159536\n",
      "evaluation/env_infos/reward_forward Mean               -0.00111196\n",
      "evaluation/env_infos/reward_forward Std                 0.0460314\n",
      "evaluation/env_infos/reward_forward Max                 0.717361\n",
      "evaluation/env_infos/reward_forward Min                -1.26997\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0122454\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0021852\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00919307\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0149858\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00969932\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00183935\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00751509\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.013806\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0122077\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0022011\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00602534\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0167471\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.07832e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.31213e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.36643e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.9316e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152171\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230856\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.604299\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.159536\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0015411\n",
      "evaluation/env_infos/torso_velocity Std                 0.0515118\n",
      "evaluation/env_infos/torso_velocity Max                 1.51253\n",
      "evaluation/env_infos/torso_velocity Min                -1.79543\n",
      "time/data storing (s)                                   1.19064\n",
      "time/evaluation sampling (s)                           25.7801\n",
      "time/exploration sampling (s)                           1.01573\n",
      "time/logging (s)                                        0.154339\n",
      "time/saving (s)                                         0.663951\n",
      "time/training (s)                                       2.54092\n",
      "time/epoch (s)                                         31.3457\n",
      "time/total (s)                                        278.39\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:20:13.942987 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.668987\n",
      "trainer/QF2 Loss                                        0.68054\n",
      "trainer/Policy Loss                                    -9.2705\n",
      "trainer/Q1 Predictions Mean                             3.87133\n",
      "trainer/Q1 Predictions Std                              0.440335\n",
      "trainer/Q1 Predictions Max                              5.46169\n",
      "trainer/Q1 Predictions Min                              2.47594\n",
      "trainer/Q2 Predictions Mean                             3.87382\n",
      "trainer/Q2 Predictions Std                              0.42783\n",
      "trainer/Q2 Predictions Max                              5.65755\n",
      "trainer/Q2 Predictions Min                              2.61159\n",
      "trainer/Q Targets Mean                                  3.7886\n",
      "trainer/Q Targets Std                                   0.884285\n",
      "trainer/Q Targets Max                                   7.33466\n",
      "trainer/Q Targets Min                                  -0.862937\n",
      "trainer/Log Pis Mean                                   -5.43007\n",
      "trainer/Log Pis Std                                     0.59804\n",
      "trainer/Log Pis Max                                    -3.80589\n",
      "trainer/Log Pis Min                                   -10.406\n",
      "trainer/Policy mu Mean                                  0.000180401\n",
      "trainer/Policy mu Std                                   0.115954\n",
      "trainer/Policy mu Max                                   0.583541\n",
      "trainer/Policy mu Min                                  -0.501414\n",
      "trainer/Policy log std Mean                            -0.165167\n",
      "trainer/Policy log std Std                              0.023155\n",
      "trainer/Policy log std Max                             -0.114665\n",
      "trainer/Policy log std Min                             -0.312288\n",
      "trainer/Alpha                                           0.16485\n",
      "trainer/Alpha Loss                                    -24.1705\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            59\n",
      "exploration/path length Mean                           76.9231\n",
      "exploration/path length Std                            70.9805\n",
      "exploration/path length Max                           314\n",
      "exploration/path length Min                            31\n",
      "exploration/Rewards Mean                               -0.283599\n",
      "exploration/Rewards Std                                 0.479757\n",
      "exploration/Rewards Max                                 2.29452\n",
      "exploration/Rewards Min                                -1.55626\n",
      "exploration/Returns Mean                              -21.8153\n",
      "exploration/Returns Std                                18.9645\n",
      "exploration/Returns Max                                -8.54175\n",
      "exploration/Returns Min                               -85.4173\n",
      "exploration/Actions Mean                               -0.00510209\n",
      "exploration/Actions Std                                 0.578362\n",
      "exploration/Actions Max                                 0.99594\n",
      "exploration/Actions Min                                -0.993775\n",
      "exploration/Num Paths                                  13\n",
      "exploration/Average Returns                           -21.8153\n",
      "exploration/env_infos/final/reward_forward Mean         0.0141503\n",
      "exploration/env_infos/final/reward_forward Std          0.669118\n",
      "exploration/env_infos/final/reward_forward Max          1.30488\n",
      "exploration/env_infos/final/reward_forward Min         -0.952452\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0799247\n",
      "exploration/env_infos/initial/reward_forward Std        0.155593\n",
      "exploration/env_infos/initial/reward_forward Max        0.402604\n",
      "exploration/env_infos/initial/reward_forward Min       -0.114103\n",
      "exploration/env_infos/reward_forward Mean              -0.0902718\n",
      "exploration/env_infos/reward_forward Std                0.809488\n",
      "exploration/env_infos/reward_forward Max                1.90696\n",
      "exploration/env_infos/reward_forward Min               -2.63325\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.518\n",
      "exploration/env_infos/final/reward_ctrl Std             0.39045\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.81054\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.03769\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.37316\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.380047\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.651696\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.31212\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.33812\n",
      "exploration/env_infos/reward_ctrl Std                   0.408868\n",
      "exploration/env_infos/reward_ctrl Max                  -0.18085\n",
      "exploration/env_infos/reward_ctrl Min                  -2.55626\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.438067\n",
      "exploration/env_infos/final/torso_velocity Std          0.835182\n",
      "exploration/env_infos/final/torso_velocity Max          2.00948\n",
      "exploration/env_infos/final/torso_velocity Min         -1.0456\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.179173\n",
      "exploration/env_infos/initial/torso_velocity Std        0.223248\n",
      "exploration/env_infos/initial/torso_velocity Max        0.563996\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.205433\n",
      "exploration/env_infos/torso_velocity Mean              -0.0263236\n",
      "exploration/env_infos/torso_velocity Std                0.818498\n",
      "exploration/env_infos/torso_velocity Max                3.30834\n",
      "exploration/env_infos/torso_velocity Min               -2.63325\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.967572\n",
      "evaluation/Rewards Std                                  0.0143585\n",
      "evaluation/Rewards Max                                  2.03739\n",
      "evaluation/Rewards Min                                  0.745858\n",
      "evaluation/Returns Mean                               967.572\n",
      "evaluation/Returns Std                                  6.85211\n",
      "evaluation/Returns Max                                979.175\n",
      "evaluation/Returns Min                                957.51\n",
      "evaluation/Actions Mean                                 0.00440494\n",
      "evaluation/Actions Std                                  0.090286\n",
      "evaluation/Actions Max                                  0.453017\n",
      "evaluation/Actions Min                                 -0.378428\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            967.572\n",
      "evaluation/env_infos/final/reward_forward Mean          9.07732e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.95893e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.45323e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -4.24636e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00677854\n",
      "evaluation/env_infos/initial/reward_forward Std         0.11898\n",
      "evaluation/env_infos/initial/reward_forward Max         0.275497\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.223772\n",
      "evaluation/env_infos/reward_forward Mean                0.000938699\n",
      "evaluation/env_infos/reward_forward Std                 0.0341587\n",
      "evaluation/env_infos/reward_forward Max                 1.22267\n",
      "evaluation/env_infos/reward_forward Min                -0.71477\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0324145\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00683483\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.020686\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0424651\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0221371\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00501214\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0155324\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0310385\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0326839\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00943506\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0155324\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.254142\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.27491e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           2.60968e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           5.45323e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000171185\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.140746\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.247212\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.599792\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.227545\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000922463\n",
      "evaluation/env_infos/torso_velocity Std                 0.0456478\n",
      "evaluation/env_infos/torso_velocity Max                 1.22267\n",
      "evaluation/env_infos/torso_velocity Min                -1.90708\n",
      "time/data storing (s)                                   1.39454\n",
      "time/evaluation sampling (s)                           27.8273\n",
      "time/exploration sampling (s)                           1.04229\n",
      "time/logging (s)                                        0.127399\n",
      "time/saving (s)                                         0.548289\n",
      "time/training (s)                                       2.57377\n",
      "time/epoch (s)                                         33.5136\n",
      "time/total (s)                                        312.034\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:20:46.048656 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.42099\n",
      "trainer/QF2 Loss                                        0.409213\n",
      "trainer/Policy Loss                                    -9.15164\n",
      "trainer/Q1 Predictions Mean                             3.90463\n",
      "trainer/Q1 Predictions Std                              0.472773\n",
      "trainer/Q1 Predictions Max                              6.16852\n",
      "trainer/Q1 Predictions Min                              2.78016\n",
      "trainer/Q2 Predictions Mean                             3.92816\n",
      "trainer/Q2 Predictions Std                              0.464553\n",
      "trainer/Q2 Predictions Max                              6.05174\n",
      "trainer/Q2 Predictions Min                              2.74671\n",
      "trainer/Q Targets Mean                                  3.82737\n",
      "trainer/Q Targets Std                                   0.730312\n",
      "trainer/Q Targets Max                                   8.32426\n",
      "trainer/Q Targets Min                                  -0.313064\n",
      "trainer/Log Pis Mean                                   -5.23897\n",
      "trainer/Log Pis Std                                     0.739887\n",
      "trainer/Log Pis Max                                    -3.07209\n",
      "trainer/Log Pis Min                                    -8.48479\n",
      "trainer/Policy mu Mean                                  0.0477529\n",
      "trainer/Policy mu Std                                   0.163265\n",
      "trainer/Policy mu Max                                   0.973583\n",
      "trainer/Policy mu Min                                  -0.414941\n",
      "trainer/Policy log std Mean                            -0.192002\n",
      "trainer/Policy log std Std                              0.0534836\n",
      "trainer/Policy log std Max                             -0.00771616\n",
      "trainer/Policy log std Min                             -0.385194\n",
      "trainer/Alpha                                           0.122315\n",
      "trainer/Alpha Loss                                    -27.7779\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            63\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           359.592\n",
      "exploration/path length Max                           872\n",
      "exploration/path length Min                            13\n",
      "exploration/Rewards Mean                               -0.284736\n",
      "exploration/Rewards Std                                 0.465507\n",
      "exploration/Rewards Max                                 1.58571\n",
      "exploration/Rewards Min                                -1.69769\n",
      "exploration/Returns Mean                              -71.184\n",
      "exploration/Returns Std                               103.614\n",
      "exploration/Returns Max                                -4.28877\n",
      "exploration/Returns Min                              -250.49\n",
      "exploration/Actions Mean                                0.0313703\n",
      "exploration/Actions Std                                 0.578147\n",
      "exploration/Actions Max                                 0.995826\n",
      "exploration/Actions Min                                -0.995621\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                           -71.184\n",
      "exploration/env_infos/final/reward_forward Mean         0.417787\n",
      "exploration/env_infos/final/reward_forward Std          1.28935\n",
      "exploration/env_infos/final/reward_forward Max          1.79628\n",
      "exploration/env_infos/final/reward_forward Min         -1.37414\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.136653\n",
      "exploration/env_infos/initial/reward_forward Std        0.136436\n",
      "exploration/env_infos/initial/reward_forward Max        0.0541719\n",
      "exploration/env_infos/initial/reward_forward Min       -0.307834\n",
      "exploration/env_infos/reward_forward Mean              -0.0825742\n",
      "exploration/env_infos/reward_forward Std                0.614854\n",
      "exploration/env_infos/reward_forward Max                1.94633\n",
      "exploration/env_infos/reward_forward Min               -2.47766\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.54464\n",
      "exploration/env_infos/final/reward_ctrl Std             0.46988\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.746827\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.9577\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.09779\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0770514\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.968155\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.16742\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.34095\n",
      "exploration/env_infos/reward_ctrl Std                   0.418506\n",
      "exploration/env_infos/reward_ctrl Max                  -0.243853\n",
      "exploration/env_infos/reward_ctrl Min                  -2.69769\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.335395\n",
      "exploration/env_infos/final/torso_velocity Std          1.18468\n",
      "exploration/env_infos/final/torso_velocity Max          1.89155\n",
      "exploration/env_infos/final/torso_velocity Min         -1.83044\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0848093\n",
      "exploration/env_infos/initial/torso_velocity Std        0.301204\n",
      "exploration/env_infos/initial/torso_velocity Max        0.62706\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.307834\n",
      "exploration/env_infos/torso_velocity Mean              -0.0288665\n",
      "exploration/env_infos/torso_velocity Std                0.595823\n",
      "exploration/env_infos/torso_velocity Max                3.39309\n",
      "exploration/env_infos/torso_velocity Min               -2.47766\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.958129\n",
      "evaluation/Rewards Std                                  0.0236428\n",
      "evaluation/Rewards Max                                  2.23046\n",
      "evaluation/Rewards Min                                  0.641218\n",
      "evaluation/Returns Mean                               958.129\n",
      "evaluation/Returns Std                                  5.45737\n",
      "evaluation/Returns Max                                970.191\n",
      "evaluation/Returns Min                                950.376\n",
      "evaluation/Actions Mean                                 0.0267156\n",
      "evaluation/Actions Std                                  0.100167\n",
      "evaluation/Actions Max                                  0.641477\n",
      "evaluation/Actions Min                                 -0.314668\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            958.129\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0029571\n",
      "evaluation/env_infos/final/reward_forward Std           0.0144857\n",
      "evaluation/env_infos/final/reward_forward Max           0.0739221\n",
      "evaluation/env_infos/final/reward_forward Min          -4.5221e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0106563\n",
      "evaluation/env_infos/initial/reward_forward Std         0.126864\n",
      "evaluation/env_infos/initial/reward_forward Max         0.257738\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.21232\n",
      "evaluation/env_infos/reward_forward Mean                0.000800293\n",
      "evaluation/env_infos/reward_forward Std                 0.0549599\n",
      "evaluation/env_infos/reward_forward Max                 1.33683\n",
      "evaluation/env_infos/reward_forward Min                -1.21715\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0423559\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00447371\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0331649\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0494648\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0398081\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00682935\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0319497\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0582532\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0429885\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0104402\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0292861\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.358782\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00146522\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0089746\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0739221\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.5221e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152806\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242532\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.600088\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.320491\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00014963\n",
      "evaluation/env_infos/torso_velocity Std                 0.0568174\n",
      "evaluation/env_infos/torso_velocity Max                 1.33683\n",
      "evaluation/env_infos/torso_velocity Min                -1.75921\n",
      "time/data storing (s)                                   1.56741\n",
      "time/evaluation sampling (s)                           25.4981\n",
      "time/exploration sampling (s)                           1.09596\n",
      "time/logging (s)                                        0.131229\n",
      "time/saving (s)                                         0.63354\n",
      "time/training (s)                                       3.0635\n",
      "time/epoch (s)                                         31.9897\n",
      "time/total (s)                                        344.143\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:21:18.190170 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.396444\n",
      "trainer/QF2 Loss                                        0.402444\n",
      "trainer/Policy Loss                                    -8.76092\n",
      "trainer/Q1 Predictions Mean                             3.56665\n",
      "trainer/Q1 Predictions Std                              0.468864\n",
      "trainer/Q1 Predictions Max                              5.43204\n",
      "trainer/Q1 Predictions Min                              2.61267\n",
      "trainer/Q2 Predictions Mean                             3.52211\n",
      "trainer/Q2 Predictions Std                              0.468181\n",
      "trainer/Q2 Predictions Max                              5.11167\n",
      "trainer/Q2 Predictions Min                              2.36732\n",
      "trainer/Q Targets Mean                                  3.69705\n",
      "trainer/Q Targets Std                                   0.773751\n",
      "trainer/Q Targets Max                                   6.59535\n",
      "trainer/Q Targets Min                                  -0.862937\n",
      "trainer/Log Pis Mean                                   -5.22396\n",
      "trainer/Log Pis Std                                     0.73602\n",
      "trainer/Log Pis Max                                    -2.69347\n",
      "trainer/Log Pis Min                                    -8.94223\n",
      "trainer/Policy mu Mean                                 -0.000351757\n",
      "trainer/Policy mu Std                                   0.158644\n",
      "trainer/Policy mu Max                                   0.599769\n",
      "trainer/Policy mu Min                                  -0.565742\n",
      "trainer/Policy log std Mean                            -0.288293\n",
      "trainer/Policy log std Std                              0.0727717\n",
      "trainer/Policy log std Max                             -0.133244\n",
      "trainer/Policy log std Min                             -0.648507\n",
      "trainer/Alpha                                           0.0910158\n",
      "trainer/Alpha Loss                                    -31.6552\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            71\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                           193.162\n",
      "exploration/path length Max                           625\n",
      "exploration/path length Min                            16\n",
      "exploration/Rewards Mean                               -0.149593\n",
      "exploration/Rewards Std                                 0.473978\n",
      "exploration/Rewards Max                                 2.24327\n",
      "exploration/Rewards Min                                -1.61714\n",
      "exploration/Returns Mean                              -18.6991\n",
      "exploration/Returns Std                                30.7277\n",
      "exploration/Returns Max                                 3.67549\n",
      "exploration/Returns Min                               -97.6501\n",
      "exploration/Actions Mean                                0.0247579\n",
      "exploration/Actions Std                                 0.55051\n",
      "exploration/Actions Max                                 0.997037\n",
      "exploration/Actions Min                                -0.995381\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                           -18.6991\n",
      "exploration/env_infos/final/reward_forward Mean         0.216216\n",
      "exploration/env_infos/final/reward_forward Std          0.375801\n",
      "exploration/env_infos/final/reward_forward Max          0.899387\n",
      "exploration/env_infos/final/reward_forward Min         -0.462175\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.085811\n",
      "exploration/env_infos/initial/reward_forward Std        0.152461\n",
      "exploration/env_infos/initial/reward_forward Max        0.128456\n",
      "exploration/env_infos/initial/reward_forward Min       -0.344976\n",
      "exploration/env_infos/reward_forward Mean               0.0694929\n",
      "exploration/env_infos/reward_forward Std                0.579222\n",
      "exploration/env_infos/reward_forward Max                2.28084\n",
      "exploration/env_infos/reward_forward Min               -2.12283\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.17857\n",
      "exploration/env_infos/final/reward_ctrl Std             0.315187\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.725006\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.65114\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.23907\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.11796\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.05165\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.46639\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.2147\n",
      "exploration/env_infos/reward_ctrl Std                   0.393265\n",
      "exploration/env_infos/reward_ctrl Max                  -0.109775\n",
      "exploration/env_infos/reward_ctrl Min                  -2.61714\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.536719\n",
      "exploration/env_infos/final/torso_velocity Std          0.927854\n",
      "exploration/env_infos/final/torso_velocity Max          4.04031\n",
      "exploration/env_infos/final/torso_velocity Min         -0.644177\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.122888\n",
      "exploration/env_infos/initial/torso_velocity Std        0.295052\n",
      "exploration/env_infos/initial/torso_velocity Max        0.647309\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.344976\n",
      "exploration/env_infos/torso_velocity Mean               0.0401317\n",
      "exploration/env_infos/torso_velocity Std                0.634691\n",
      "exploration/env_infos/torso_velocity Max                4.04031\n",
      "exploration/env_infos/torso_velocity Min               -2.82131\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.990909\n",
      "evaluation/Rewards Std                                  0.106338\n",
      "evaluation/Rewards Max                                  2.31131\n",
      "evaluation/Rewards Min                                  0.842788\n",
      "evaluation/Returns Mean                               990.909\n",
      "evaluation/Returns Std                                 19.6801\n",
      "evaluation/Returns Max                               1049.15\n",
      "evaluation/Returns Min                                962.378\n",
      "evaluation/Actions Mean                                -0.0219084\n",
      "evaluation/Actions Std                                  0.0902589\n",
      "evaluation/Actions Max                                  0.427053\n",
      "evaluation/Actions Min                                 -0.345305\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            990.909\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0145618\n",
      "evaluation/env_infos/final/reward_forward Std           0.176219\n",
      "evaluation/env_infos/final/reward_forward Max           0.399774\n",
      "evaluation/env_infos/final/reward_forward Min          -0.540986\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.055636\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119062\n",
      "evaluation/env_infos/initial/reward_forward Max         0.310517\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.125675\n",
      "evaluation/env_infos/reward_forward Mean               -0.00832174\n",
      "evaluation/env_infos/reward_forward Std                 0.215046\n",
      "evaluation/env_infos/reward_forward Max                 1.29189\n",
      "evaluation/env_infos/reward_forward Min                -1.53422\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0274027\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00886\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0070256\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0432554\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0409779\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0156824\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0148965\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0659618\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0345066\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0176347\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00595461\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.157212\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.0188384\n",
      "evaluation/env_infos/final/torso_velocity Std           0.163898\n",
      "evaluation/env_infos/final/torso_velocity Max           0.498891\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.556705\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.160695\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.232684\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.70954\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.226138\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0146409\n",
      "evaluation/env_infos/torso_velocity Std                 0.197646\n",
      "evaluation/env_infos/torso_velocity Max                 1.35108\n",
      "evaluation/env_infos/torso_velocity Min                -1.94596\n",
      "time/data storing (s)                                   1.66182\n",
      "time/evaluation sampling (s)                           25.592\n",
      "time/exploration sampling (s)                           1.02539\n",
      "time/logging (s)                                        0.127348\n",
      "time/saving (s)                                         0.643147\n",
      "time/training (s)                                       2.95761\n",
      "time/epoch (s)                                         32.0073\n",
      "time/total (s)                                        376.28\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:21:50.988603 PDT | [gher-antdirectionnewsparse-SAC-10e-1000s-disc0.99-horizon1000_2021_05_25_10_15_05_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.430466\n",
      "trainer/QF2 Loss                                        0.436317\n",
      "trainer/Policy Loss                                    -7.86785\n",
      "trainer/Q1 Predictions Mean                             3.83415\n",
      "trainer/Q1 Predictions Std                              0.52184\n",
      "trainer/Q1 Predictions Max                              5.6354\n",
      "trainer/Q1 Predictions Min                              2.61316\n",
      "trainer/Q2 Predictions Mean                             3.83969\n",
      "trainer/Q2 Predictions Std                              0.514828\n",
      "trainer/Q2 Predictions Max                              5.97078\n",
      "trainer/Q2 Predictions Min                              2.70984\n",
      "trainer/Q Targets Mean                                  3.59516\n",
      "trainer/Q Targets Std                                   0.774488\n",
      "trainer/Q Targets Max                                   7.38253\n",
      "trainer/Q Targets Min                                  -0.984132\n",
      "trainer/Log Pis Mean                                   -3.91751\n",
      "trainer/Log Pis Std                                     1.32833\n",
      "trainer/Log Pis Max                                     0.45672\n",
      "trainer/Log Pis Min                                    -8.29212\n",
      "trainer/Policy mu Mean                                  0.0430347\n",
      "trainer/Policy mu Std                                   0.286325\n",
      "trainer/Policy mu Max                                   1.23934\n",
      "trainer/Policy mu Min                                  -1.09717\n",
      "trainer/Policy log std Mean                            -0.586283\n",
      "trainer/Policy log std Std                              0.140878\n",
      "trainer/Policy log std Max                             -0.213349\n",
      "trainer/Policy log std Min                             -1.42636\n",
      "trainer/Alpha                                           0.0682497\n",
      "trainer/Alpha Loss                                    -31.9604\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                            13\n",
      "exploration/path length Max                           513\n",
      "exploration/path length Min                           487\n",
      "exploration/Rewards Mean                                0.102671\n",
      "exploration/Rewards Std                                 0.36326\n",
      "exploration/Rewards Max                                 1.7047\n",
      "exploration/Rewards Min                                -1.06009\n",
      "exploration/Returns Mean                               51.3353\n",
      "exploration/Returns Std                                 7.83454\n",
      "exploration/Returns Max                                59.1699\n",
      "exploration/Returns Min                                43.5008\n",
      "exploration/Actions Mean                                0.0460676\n",
      "exploration/Actions Std                                 0.481171\n",
      "exploration/Actions Max                                 0.984417\n",
      "exploration/Actions Min                                -0.99203\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                            51.3353\n",
      "exploration/env_infos/final/reward_forward Mean        -0.266076\n",
      "exploration/env_infos/final/reward_forward Std          0.530333\n",
      "exploration/env_infos/final/reward_forward Max          0.264257\n",
      "exploration/env_infos/final/reward_forward Min         -0.79641\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.139505\n",
      "exploration/env_infos/initial/reward_forward Std        0.196282\n",
      "exploration/env_infos/initial/reward_forward Max        0.0567771\n",
      "exploration/env_infos/initial/reward_forward Min       -0.335787\n",
      "exploration/env_infos/reward_forward Mean               0.0867353\n",
      "exploration/env_infos/reward_forward Std                0.531929\n",
      "exploration/env_infos/reward_forward Max                1.63171\n",
      "exploration/env_infos/reward_forward Min               -1.51788\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.619159\n",
      "exploration/env_infos/final/reward_ctrl Std             0.00577837\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.613381\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.624938\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.817547\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.11396\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.703587\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.931507\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.934592\n",
      "exploration/env_infos/reward_ctrl Std                   0.32267\n",
      "exploration/env_infos/reward_ctrl Max                  -0.155799\n",
      "exploration/env_infos/reward_ctrl Min                  -2.06009\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.10963\n",
      "exploration/env_infos/final/torso_velocity Std          0.625142\n",
      "exploration/env_infos/final/torso_velocity Max          1.25271\n",
      "exploration/env_infos/final/torso_velocity Min         -0.79641\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.108686\n",
      "exploration/env_infos/initial/torso_velocity Std        0.322861\n",
      "exploration/env_infos/initial/torso_velocity Max        0.638025\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.335787\n",
      "exploration/env_infos/torso_velocity Mean               0.0167094\n",
      "exploration/env_infos/torso_velocity Std                0.547706\n",
      "exploration/env_infos/torso_velocity Max                2.61021\n",
      "exploration/env_infos/torso_velocity Min               -2.65281\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.815667\n",
      "evaluation/Rewards Std                                  0.0589755\n",
      "evaluation/Rewards Max                                  2.1059\n",
      "evaluation/Rewards Min                                  0.39296\n",
      "evaluation/Returns Mean                               815.667\n",
      "evaluation/Returns Std                                 29.7261\n",
      "evaluation/Returns Max                                879.363\n",
      "evaluation/Returns Min                                752.717\n",
      "evaluation/Actions Mean                                 0.0528368\n",
      "evaluation/Actions Std                                  0.211285\n",
      "evaluation/Actions Max                                  0.642225\n",
      "evaluation/Actions Min                                 -0.587894\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            815.667\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00241633\n",
      "evaluation/env_infos/final/reward_forward Std           0.0621564\n",
      "evaluation/env_infos/final/reward_forward Max           0.141536\n",
      "evaluation/env_infos/final/reward_forward Min          -0.208982\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00238391\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0867446\n",
      "evaluation/env_infos/initial/reward_forward Max         0.136362\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.157863\n",
      "evaluation/env_infos/reward_forward Mean                0.00166386\n",
      "evaluation/env_infos/reward_forward Std                 0.0937577\n",
      "evaluation/env_infos/reward_forward Max                 1.30729\n",
      "evaluation/env_infos/reward_forward Min                -1.26949\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.184394\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0302407\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.134383\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.262433\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.162903\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0417873\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.101383\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.22919\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.189733\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0334397\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0847395\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.60704\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00112156\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0404572\n",
      "evaluation/env_infos/final/torso_velocity Max           0.141536\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.208982\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.119097\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.247573\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.674403\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.251596\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00266549\n",
      "evaluation/env_infos/torso_velocity Std                 0.0916931\n",
      "evaluation/env_infos/torso_velocity Max                 1.30729\n",
      "evaluation/env_infos/torso_velocity Min                -1.8935\n",
      "time/data storing (s)                                   1.8021\n",
      "time/evaluation sampling (s)                           25.6992\n",
      "time/exploration sampling (s)                           0.985627\n",
      "time/logging (s)                                        0.130335\n",
      "time/saving (s)                                         1.27132\n",
      "time/training (s)                                       2.78873\n",
      "time/epoch (s)                                         32.6773\n",
      "time/total (s)                                        409.082\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 10 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 10 --relabel --n_sampled_latents 1 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 10 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 10 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49979370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
