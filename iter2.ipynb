{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15508]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225740). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225768). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a47a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a242257b8). One of the two will be used. Which one is undefined.\n",
      "objc[15508]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a241a4818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a24225830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:24:55.667664 PDT | Variant:\n",
      "2021-05-25 10:24:55.668232 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:25:26.013173 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      16.1022\n",
      "trainer/QF2 Loss                                      16.0024\n",
      "trainer/Policy Loss                                   -4.03239\n",
      "trainer/Q1 Predictions Mean                           -0.00868505\n",
      "trainer/Q1 Predictions Std                             0.00569892\n",
      "trainer/Q1 Predictions Max                             0.00276847\n",
      "trainer/Q1 Predictions Min                            -0.0293377\n",
      "trainer/Q2 Predictions Mean                            0.00402752\n",
      "trainer/Q2 Predictions Std                             0.00448522\n",
      "trainer/Q2 Predictions Max                             0.0155763\n",
      "trainer/Q2 Predictions Min                            -0.0079983\n",
      "trainer/Q Targets Mean                                 3.86128\n",
      "trainer/Q Targets Std                                  1.06061\n",
      "trainer/Q Targets Max                                  7.74404\n",
      "trainer/Q Targets Min                                  1.06643\n",
      "trainer/Log Pis Mean                                  -4.04116\n",
      "trainer/Log Pis Std                                    0.507125\n",
      "trainer/Log Pis Max                                   -2.36207\n",
      "trainer/Log Pis Min                                   -5.49049\n",
      "trainer/Policy mu Mean                                -0.000921182\n",
      "trainer/Policy mu Std                                  0.00241126\n",
      "trainer/Policy mu Max                                  0.00701357\n",
      "trainer/Policy mu Min                                 -0.00715128\n",
      "trainer/Policy log std Mean                           -0.00134487\n",
      "trainer/Policy log std Std                             0.00179297\n",
      "trainer/Policy log std Max                             0.00386933\n",
      "trainer/Policy log std Min                            -0.00782172\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.180531\n",
      "exploration/Rewards Std                                0.575729\n",
      "exploration/Rewards Max                                1.67508\n",
      "exploration/Rewards Min                               -2.07869\n",
      "exploration/Returns Mean                            -180.531\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -180.531\n",
      "exploration/Returns Min                             -180.531\n",
      "exploration/Actions Mean                               0.00115617\n",
      "exploration/Actions Std                                0.627115\n",
      "exploration/Actions Max                                0.999103\n",
      "exploration/Actions Min                               -0.999803\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -180.531\n",
      "exploration/env_infos/final/reward_run Mean           -0.193592\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.193592\n",
      "exploration/env_infos/final/reward_run Min            -0.193592\n",
      "exploration/env_infos/initial/reward_run Mean         -0.17967\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.17967\n",
      "exploration/env_infos/initial/reward_run Min          -0.17967\n",
      "exploration/env_infos/reward_run Mean                 -0.0684477\n",
      "exploration/env_infos/reward_run Std                   0.705377\n",
      "exploration/env_infos/reward_run Max                   2.22754\n",
      "exploration/env_infos/reward_run Min                  -2.34565\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.11272\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.11272\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.11272\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144624\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144624\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144624\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235964\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750503\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181909\n",
      "exploration/env_infos/reward_ctrl Min                 -0.476008\n",
      "exploration/env_infos/final/height Mean                0.00225554\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                 0.00225554\n",
      "exploration/env_infos/final/height Min                 0.00225554\n",
      "exploration/env_infos/initial/height Mean             -0.0179407\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0179407\n",
      "exploration/env_infos/initial/height Min              -0.0179407\n",
      "exploration/env_infos/height Mean                     -0.0662649\n",
      "exploration/env_infos/height Std                       0.0781177\n",
      "exploration/env_infos/height Max                       0.161863\n",
      "exploration/env_infos/height Min                      -0.372448\n",
      "exploration/env_infos/final/reward_angular Mean       -2.32488\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -2.32488\n",
      "exploration/env_infos/final/reward_angular Min        -2.32488\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.113362\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.113362\n",
      "exploration/env_infos/initial/reward_angular Min      -0.113362\n",
      "exploration/env_infos/reward_angular Mean             -0.0205031\n",
      "exploration/env_infos/reward_angular Std               1.67749\n",
      "exploration/env_infos/reward_angular Max               5.52338\n",
      "exploration/env_infos/reward_angular Min              -6.63795\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0615665\n",
      "evaluation/Rewards Std                                 0.0479122\n",
      "evaluation/Rewards Max                                 1.31344\n",
      "evaluation/Rewards Min                                -1.96292\n",
      "evaluation/Returns Mean                              -61.5665\n",
      "evaluation/Returns Std                                38.0983\n",
      "evaluation/Returns Max                                -1.55696\n",
      "evaluation/Returns Min                              -127.68\n",
      "evaluation/Actions Mean                               -0.00032297\n",
      "evaluation/Actions Std                                 0.00121836\n",
      "evaluation/Actions Max                                 0.00379412\n",
      "evaluation/Actions Min                                -0.00324318\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.5665\n",
      "evaluation/env_infos/final/reward_run Mean             1.94289e-17\n",
      "evaluation/env_infos/final/reward_run Std              1.52365e-16\n",
      "evaluation/env_infos/final/reward_run Max              3.46945e-16\n",
      "evaluation/env_infos/final/reward_run Min             -5.55112e-16\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0216643\n",
      "evaluation/env_infos/initial/reward_run Std            0.0908698\n",
      "evaluation/env_infos/initial/reward_run Max            0.207972\n",
      "evaluation/env_infos/initial/reward_run Min           -0.21485\n",
      "evaluation/env_infos/reward_run Mean                  -6.01134e-05\n",
      "evaluation/env_infos/reward_run Std                    0.0129475\n",
      "evaluation/env_infos/reward_run Max                    0.297686\n",
      "evaluation/env_infos/reward_run Min                   -0.382699\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.00856e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.02247e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.64679e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.19671e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53235e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.68369e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -5.59195e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.16509e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean              -0.00261261\n",
      "evaluation/env_infos/initial/height Std                0.0507168\n",
      "evaluation/env_infos/initial/height Max                0.0808508\n",
      "evaluation/env_infos/initial/height Min               -0.0863121\n",
      "evaluation/env_infos/height Mean                      -0.132403\n",
      "evaluation/env_infos/height Std                        0.00598515\n",
      "evaluation/env_infos/height Max                        0.0808508\n",
      "evaluation/env_infos/height Min                       -0.147987\n",
      "evaluation/env_infos/final/reward_angular Mean         1.80369e-16\n",
      "evaluation/env_infos/final/reward_angular Std          1.16466e-15\n",
      "evaluation/env_infos/final/reward_angular Max          2.20667e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -2.39561e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0462222\n",
      "evaluation/env_infos/initial/reward_angular Std        0.203028\n",
      "evaluation/env_infos/initial/reward_angular Max        0.61123\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.258986\n",
      "evaluation/env_infos/reward_angular Mean               0.00137134\n",
      "evaluation/env_infos/reward_angular Std                0.0441042\n",
      "evaluation/env_infos/reward_angular Max                2.04155\n",
      "evaluation/env_infos/reward_angular Min               -1.34125\n",
      "time/data storing (s)                                  0.0162352\n",
      "time/evaluation sampling (s)                          24.0442\n",
      "time/exploration sampling (s)                          1.24839\n",
      "time/logging (s)                                       0.240108\n",
      "time/saving (s)                                        0.0758232\n",
      "time/training (s)                                      3.81875\n",
      "time/epoch (s)                                        29.4435\n",
      "time/total (s)                                        33.292\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:25:55.894479 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.678998\n",
      "trainer/QF2 Loss                                       0.681364\n",
      "trainer/Policy Loss                                   -7.30679\n",
      "trainer/Q1 Predictions Mean                            3.26513\n",
      "trainer/Q1 Predictions Std                             0.577677\n",
      "trainer/Q1 Predictions Max                             4.89418\n",
      "trainer/Q1 Predictions Min                             2.13402\n",
      "trainer/Q2 Predictions Mean                            3.27674\n",
      "trainer/Q2 Predictions Std                             0.576639\n",
      "trainer/Q2 Predictions Max                             4.89072\n",
      "trainer/Q2 Predictions Min                             2.17989\n",
      "trainer/Q Targets Mean                                 3.28808\n",
      "trainer/Q Targets Std                                  0.923946\n",
      "trainer/Q Targets Max                                  8.23753\n",
      "trainer/Q Targets Min                                  0.520468\n",
      "trainer/Log Pis Mean                                  -4.08131\n",
      "trainer/Log Pis Std                                    0.34747\n",
      "trainer/Log Pis Max                                   -3.30788\n",
      "trainer/Log Pis Min                                   -6.72451\n",
      "trainer/Policy mu Mean                                -0.04255\n",
      "trainer/Policy mu Std                                  0.0315773\n",
      "trainer/Policy mu Max                                  0.0126763\n",
      "trainer/Policy mu Min                                 -0.157892\n",
      "trainer/Policy log std Mean                           -0.11093\n",
      "trainer/Policy log std Std                             0.0199102\n",
      "trainer/Policy log std Max                            -0.0667875\n",
      "trainer/Policy log std Min                            -0.194263\n",
      "trainer/Alpha                                          0.738516\n",
      "trainer/Alpha Loss                                    -3.02552\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.212453\n",
      "exploration/Rewards Std                                0.470014\n",
      "exploration/Rewards Max                                1.69922\n",
      "exploration/Rewards Min                               -1.70513\n",
      "exploration/Returns Mean                            -212.453\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -212.453\n",
      "exploration/Returns Min                             -212.453\n",
      "exploration/Actions Mean                              -0.0328488\n",
      "exploration/Actions Std                                0.59676\n",
      "exploration/Actions Max                                0.995742\n",
      "exploration/Actions Min                               -0.998759\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -212.453\n",
      "exploration/env_infos/final/reward_run Mean           -0.210926\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.210926\n",
      "exploration/env_infos/final/reward_run Min            -0.210926\n",
      "exploration/env_infos/initial/reward_run Mean         -0.475005\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.475005\n",
      "exploration/env_infos/initial/reward_run Min          -0.475005\n",
      "exploration/env_infos/reward_run Mean                 -0.0793773\n",
      "exploration/env_infos/reward_run Std                   0.651602\n",
      "exploration/env_infos/reward_run Max                   1.81344\n",
      "exploration/env_infos/reward_run Min                  -2.30045\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.203473\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.203473\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.203473\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.2237\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.2237\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.2237\n",
      "exploration/env_infos/reward_ctrl Mean                -0.214321\n",
      "exploration/env_infos/reward_ctrl Std                  0.0715869\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0116574\n",
      "exploration/env_infos/reward_ctrl Min                 -0.496016\n",
      "exploration/env_infos/final/height Mean               -0.0337743\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0337743\n",
      "exploration/env_infos/final/height Min                -0.0337743\n",
      "exploration/env_infos/initial/height Mean              0.0839974\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0839974\n",
      "exploration/env_infos/initial/height Min               0.0839974\n",
      "exploration/env_infos/height Mean                     -0.0762302\n",
      "exploration/env_infos/height Std                       0.084257\n",
      "exploration/env_infos/height Max                       0.166745\n",
      "exploration/env_infos/height Min                      -0.327067\n",
      "exploration/env_infos/final/reward_angular Mean       -1.23031\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.23031\n",
      "exploration/env_infos/final/reward_angular Min        -1.23031\n",
      "exploration/env_infos/initial/reward_angular Mean      1.03283\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.03283\n",
      "exploration/env_infos/initial/reward_angular Min       1.03283\n",
      "exploration/env_infos/reward_angular Mean             -0.0116875\n",
      "exploration/env_infos/reward_angular Std               1.69053\n",
      "exploration/env_infos/reward_angular Max               5.89018\n",
      "exploration/env_infos/reward_angular Min              -5.68969\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0601582\n",
      "evaluation/Rewards Std                                 0.04255\n",
      "evaluation/Rewards Max                                 1.02703\n",
      "evaluation/Rewards Min                                -1.15343\n",
      "evaluation/Returns Mean                              -60.1582\n",
      "evaluation/Returns Std                                36.9065\n",
      "evaluation/Returns Max                                -0.695749\n",
      "evaluation/Returns Min                              -123.901\n",
      "evaluation/Actions Mean                               -0.0339672\n",
      "evaluation/Actions Std                                 0.0240189\n",
      "evaluation/Actions Max                                -0.00864913\n",
      "evaluation/Actions Min                                -0.0928537\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -60.1582\n",
      "evaluation/env_infos/final/reward_run Mean            -1.46515e-09\n",
      "evaluation/env_infos/final/reward_run Std              5.70146e-09\n",
      "evaluation/env_infos/final/reward_run Max              4.4929e-09\n",
      "evaluation/env_infos/final/reward_run Min             -2.11309e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0137592\n",
      "evaluation/env_infos/initial/reward_run Std            0.163507\n",
      "evaluation/env_infos/initial/reward_run Max            0.331616\n",
      "evaluation/env_infos/initial/reward_run Min           -0.387343\n",
      "evaluation/env_infos/reward_run Mean                  -0.000270794\n",
      "evaluation/env_infos/reward_run Std                    0.0171161\n",
      "evaluation/env_infos/reward_run Max                    0.352385\n",
      "evaluation/env_infos/reward_run Min                   -0.419241\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00103835\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.82639e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000861242\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00111939\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00102597\n",
      "evaluation/env_infos/initial/reward_ctrl Std           6.66741e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000879625\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00113864\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00103841\n",
      "evaluation/env_infos/reward_ctrl Std                   5.86909e-05\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000703211\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0012154\n",
      "evaluation/env_infos/final/height Mean                -0.129065\n",
      "evaluation/env_infos/final/height Std                  9.12761e-05\n",
      "evaluation/env_infos/final/height Max                 -0.128896\n",
      "evaluation/env_infos/final/height Min                 -0.129302\n",
      "evaluation/env_infos/initial/height Mean              -0.0138464\n",
      "evaluation/env_infos/initial/height Std                0.0516688\n",
      "evaluation/env_infos/initial/height Max                0.0824065\n",
      "evaluation/env_infos/initial/height Min               -0.093044\n",
      "evaluation/env_infos/height Mean                      -0.12866\n",
      "evaluation/env_infos/height Std                        0.00528021\n",
      "evaluation/env_infos/height Max                        0.0824065\n",
      "evaluation/env_infos/height Min                       -0.144338\n",
      "evaluation/env_infos/final/reward_angular Mean        -4.00339e-09\n",
      "evaluation/env_infos/final/reward_angular Std          2.78886e-08\n",
      "evaluation/env_infos/final/reward_angular Max          4.82978e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -1.31216e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.297131\n",
      "evaluation/env_infos/initial/reward_angular Std        0.340036\n",
      "evaluation/env_infos/initial/reward_angular Max        1.20393\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.447319\n",
      "evaluation/env_infos/reward_angular Mean               0.00118776\n",
      "evaluation/env_infos/reward_angular Std                0.0409047\n",
      "evaluation/env_infos/reward_angular Max                1.98845\n",
      "evaluation/env_infos/reward_angular Min               -1.49289\n",
      "time/data storing (s)                                  0.0157651\n",
      "time/evaluation sampling (s)                          24.7408\n",
      "time/exploration sampling (s)                          1.13584\n",
      "time/logging (s)                                       0.235015\n",
      "time/saving (s)                                        0.0411126\n",
      "time/training (s)                                      3.45743\n",
      "time/epoch (s)                                        29.626\n",
      "time/total (s)                                        63.1674\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:24.701505 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.404294\n",
      "trainer/QF2 Loss                                       0.403937\n",
      "trainer/Policy Loss                                   -7.04332\n",
      "trainer/Q1 Predictions Mean                            3.00057\n",
      "trainer/Q1 Predictions Std                             0.584367\n",
      "trainer/Q1 Predictions Max                             5.41582\n",
      "trainer/Q1 Predictions Min                             1.62855\n",
      "trainer/Q2 Predictions Mean                            3.01536\n",
      "trainer/Q2 Predictions Std                             0.567837\n",
      "trainer/Q2 Predictions Max                             5.29854\n",
      "trainer/Q2 Predictions Min                             1.62793\n",
      "trainer/Q Targets Mean                                 2.98985\n",
      "trainer/Q Targets Std                                  0.821935\n",
      "trainer/Q Targets Max                                  5.90788\n",
      "trainer/Q Targets Min                                  1.03996\n",
      "trainer/Log Pis Mean                                  -3.94463\n",
      "trainer/Log Pis Std                                    0.572421\n",
      "trainer/Log Pis Max                                   -1.57778\n",
      "trainer/Log Pis Min                                   -5.49842\n",
      "trainer/Policy mu Mean                                 0.0579283\n",
      "trainer/Policy mu Std                                  0.171058\n",
      "trainer/Policy mu Max                                  0.883871\n",
      "trainer/Policy mu Min                                 -0.193105\n",
      "trainer/Policy log std Mean                           -0.135114\n",
      "trainer/Policy log std Std                             0.0209456\n",
      "trainer/Policy log std Max                            -0.0761559\n",
      "trainer/Policy log std Min                            -0.225826\n",
      "trainer/Alpha                                          0.547506\n",
      "trainer/Alpha Loss                                    -5.96089\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.24663\n",
      "exploration/Rewards Std                                0.581159\n",
      "exploration/Rewards Max                                1.74305\n",
      "exploration/Rewards Min                               -2.30054\n",
      "exploration/Returns Mean                            -246.63\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -246.63\n",
      "exploration/Returns Min                             -246.63\n",
      "exploration/Actions Mean                               0.00726707\n",
      "exploration/Actions Std                                0.594888\n",
      "exploration/Actions Max                                0.997216\n",
      "exploration/Actions Min                               -0.996298\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -246.63\n",
      "exploration/env_infos/final/reward_run Mean            0.116645\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.116645\n",
      "exploration/env_infos/final/reward_run Min             0.116645\n",
      "exploration/env_infos/initial/reward_run Mean          0.0616765\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0616765\n",
      "exploration/env_infos/initial/reward_run Min           0.0616765\n",
      "exploration/env_infos/reward_run Mean                 -0.0188526\n",
      "exploration/env_infos/reward_run Std                   0.608925\n",
      "exploration/env_infos/reward_run Max                   1.75101\n",
      "exploration/env_infos/reward_run Min                  -2.38662\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.157908\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.157908\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.157908\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.160263\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.160263\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.160263\n",
      "exploration/env_infos/reward_ctrl Mean                -0.212367\n",
      "exploration/env_infos/reward_ctrl Std                  0.0734344\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0277211\n",
      "exploration/env_infos/reward_ctrl Min                 -0.42973\n",
      "exploration/env_infos/final/height Mean               -0.576053\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.576053\n",
      "exploration/env_infos/final/height Min                -0.576053\n",
      "exploration/env_infos/initial/height Mean             -0.0783618\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0783618\n",
      "exploration/env_infos/initial/height Min              -0.0783618\n",
      "exploration/env_infos/height Mean                     -0.329583\n",
      "exploration/env_infos/height Std                       0.236838\n",
      "exploration/env_infos/height Max                       0.166742\n",
      "exploration/env_infos/height Min                      -0.584316\n",
      "exploration/env_infos/final/reward_angular Mean       -0.00872479\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.00872479\n",
      "exploration/env_infos/final/reward_angular Min        -0.00872479\n",
      "exploration/env_infos/initial/reward_angular Mean      0.548147\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.548147\n",
      "exploration/env_infos/initial/reward_angular Min       0.548147\n",
      "exploration/env_infos/reward_angular Mean              0.0464629\n",
      "exploration/env_infos/reward_angular Std               1.41229\n",
      "exploration/env_infos/reward_angular Max               4.76513\n",
      "exploration/env_infos/reward_angular Min              -4.61424\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0631725\n",
      "evaluation/Rewards Std                                 0.0472528\n",
      "evaluation/Rewards Max                                 1.56586\n",
      "evaluation/Rewards Min                                -1.58833\n",
      "evaluation/Returns Mean                              -63.1725\n",
      "evaluation/Returns Std                                36.4916\n",
      "evaluation/Returns Max                                -3.80718\n",
      "evaluation/Returns Min                              -123.017\n",
      "evaluation/Actions Mean                                0.0722568\n",
      "evaluation/Actions Std                                 0.10843\n",
      "evaluation/Actions Max                                 0.471978\n",
      "evaluation/Actions Min                                -0.0753232\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -63.1725\n",
      "evaluation/env_infos/final/reward_run Mean            -1.47613e-09\n",
      "evaluation/env_infos/final/reward_run Std              3.34352e-08\n",
      "evaluation/env_infos/final/reward_run Max              5.64184e-08\n",
      "evaluation/env_infos/final/reward_run Min             -1.18418e-07\n",
      "evaluation/env_infos/initial/reward_run Mean           0.257676\n",
      "evaluation/env_infos/initial/reward_run Std            0.148793\n",
      "evaluation/env_infos/initial/reward_run Max            0.589325\n",
      "evaluation/env_infos/initial/reward_run Min           -0.06334\n",
      "evaluation/env_infos/reward_run Mean                  -0.000193345\n",
      "evaluation/env_infos/reward_run Std                    0.0219669\n",
      "evaluation/env_infos/reward_run Max                    0.721781\n",
      "evaluation/env_infos/reward_run Min                   -0.399405\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0101876\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00810318\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00079541\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.024873\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0156605\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0119967\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000774114\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0386431\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0101869\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00810298\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000716399\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0386431\n",
      "evaluation/env_infos/final/height Mean                -0.128694\n",
      "evaluation/env_infos/final/height Std                  0.000835328\n",
      "evaluation/env_infos/final/height Max                 -0.127758\n",
      "evaluation/env_infos/final/height Min                 -0.130298\n",
      "evaluation/env_infos/initial/height Mean              -0.0248016\n",
      "evaluation/env_infos/initial/height Std                0.0484514\n",
      "evaluation/env_infos/initial/height Max                0.0699546\n",
      "evaluation/env_infos/initial/height Min               -0.0921782\n",
      "evaluation/env_infos/height Mean                      -0.12822\n",
      "evaluation/env_infos/height Std                        0.00498882\n",
      "evaluation/env_infos/height Max                        0.0699546\n",
      "evaluation/env_infos/height Min                       -0.1444\n",
      "evaluation/env_infos/final/reward_angular Mean         1.16422e-08\n",
      "evaluation/env_infos/final/reward_angular Std          5.83257e-08\n",
      "evaluation/env_infos/final/reward_angular Max          2.49108e-07\n",
      "evaluation/env_infos/final/reward_angular Min         -7.37001e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.122197\n",
      "evaluation/env_infos/initial/reward_angular Std        0.477995\n",
      "evaluation/env_infos/initial/reward_angular Max        1.16753\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.903272\n",
      "evaluation/env_infos/reward_angular Mean               0.000826385\n",
      "evaluation/env_infos/reward_angular Std                0.0438158\n",
      "evaluation/env_infos/reward_angular Max                1.8137\n",
      "evaluation/env_infos/reward_angular Min               -1.36182\n",
      "time/data storing (s)                                  0.0151108\n",
      "time/evaluation sampling (s)                          23.7598\n",
      "time/exploration sampling (s)                          1.05998\n",
      "time/logging (s)                                       0.307348\n",
      "time/saving (s)                                        0.0277021\n",
      "time/training (s)                                      3.58953\n",
      "time/epoch (s)                                        28.7594\n",
      "time/total (s)                                        92.0462\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:55.923922 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.660069\n",
      "trainer/QF2 Loss                                        0.637088\n",
      "trainer/Policy Loss                                    -6.84251\n",
      "trainer/Q1 Predictions Mean                             2.79787\n",
      "trainer/Q1 Predictions Std                              0.677523\n",
      "trainer/Q1 Predictions Max                              4.76841\n",
      "trainer/Q1 Predictions Min                              0.851322\n",
      "trainer/Q2 Predictions Mean                             2.82682\n",
      "trainer/Q2 Predictions Std                              0.667765\n",
      "trainer/Q2 Predictions Max                              4.75247\n",
      "trainer/Q2 Predictions Min                              1.01812\n",
      "trainer/Q Targets Mean                                  3.04989\n",
      "trainer/Q Targets Std                                   0.920228\n",
      "trainer/Q Targets Max                                   8.43564\n",
      "trainer/Q Targets Min                                  -1.13101\n",
      "trainer/Log Pis Mean                                   -3.98263\n",
      "trainer/Log Pis Std                                     0.590446\n",
      "trainer/Log Pis Max                                    -1.90346\n",
      "trainer/Log Pis Min                                    -5.83776\n",
      "trainer/Policy mu Mean                                  0.0613373\n",
      "trainer/Policy mu Std                                   0.176744\n",
      "trainer/Policy mu Max                                   0.796341\n",
      "trainer/Policy mu Min                                  -0.44235\n",
      "trainer/Policy log std Mean                            -0.10363\n",
      "trainer/Policy log std Std                              0.0308055\n",
      "trainer/Policy log std Max                             -0.00845246\n",
      "trainer/Policy log std Min                             -0.267518\n",
      "trainer/Alpha                                           0.406715\n",
      "trainer/Alpha Loss                                     -8.95116\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0215991\n",
      "exploration/Rewards Std                                 0.515672\n",
      "exploration/Rewards Max                                 1.86304\n",
      "exploration/Rewards Min                                -1.51046\n",
      "exploration/Returns Mean                               21.5991\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                21.5991\n",
      "exploration/Returns Min                                21.5991\n",
      "exploration/Actions Mean                                0.0507274\n",
      "exploration/Actions Std                                 0.598589\n",
      "exploration/Actions Max                                 0.999479\n",
      "exploration/Actions Min                                -0.997621\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            21.5991\n",
      "exploration/env_infos/final/reward_run Mean             0.0147182\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0147182\n",
      "exploration/env_infos/final/reward_run Min              0.0147182\n",
      "exploration/env_infos/initial/reward_run Mean           0.440269\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.440269\n",
      "exploration/env_infos/initial/reward_run Min            0.440269\n",
      "exploration/env_infos/reward_run Mean                  -0.171141\n",
      "exploration/env_infos/reward_run Std                    0.683938\n",
      "exploration/env_infos/reward_run Max                    2.11781\n",
      "exploration/env_infos/reward_run Min                   -2.50504\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.175075\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.175075\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.175075\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.205215\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.205215\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.205215\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.216529\n",
      "exploration/env_infos/reward_ctrl Std                   0.0695732\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0481168\n",
      "exploration/env_infos/reward_ctrl Min                  -0.470154\n",
      "exploration/env_infos/final/height Mean                -0.148439\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.148439\n",
      "exploration/env_infos/final/height Min                 -0.148439\n",
      "exploration/env_infos/initial/height Mean              -0.0295987\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0295987\n",
      "exploration/env_infos/initial/height Min               -0.0295987\n",
      "exploration/env_infos/height Mean                      -0.0697129\n",
      "exploration/env_infos/height Std                        0.079027\n",
      "exploration/env_infos/height Max                        0.231921\n",
      "exploration/env_infos/height Min                       -0.32015\n",
      "exploration/env_infos/final/reward_angular Mean         0.688389\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.688389\n",
      "exploration/env_infos/final/reward_angular Min          0.688389\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.16192\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.16192\n",
      "exploration/env_infos/initial/reward_angular Min       -1.16192\n",
      "exploration/env_infos/reward_angular Mean              -0.0133176\n",
      "exploration/env_infos/reward_angular Std                1.68497\n",
      "exploration/env_infos/reward_angular Max                6.90645\n",
      "exploration/env_infos/reward_angular Min               -5.50978\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0638969\n",
      "evaluation/Rewards Std                                  0.0451971\n",
      "evaluation/Rewards Max                                  1.33637\n",
      "evaluation/Rewards Min                                 -0.747575\n",
      "evaluation/Returns Mean                               -63.8969\n",
      "evaluation/Returns Std                                 36.8872\n",
      "evaluation/Returns Max                                 -5.13772\n",
      "evaluation/Returns Min                               -123.338\n",
      "evaluation/Actions Mean                                 0.0973005\n",
      "evaluation/Actions Std                                  0.110045\n",
      "evaluation/Actions Max                                  0.438717\n",
      "evaluation/Actions Min                                 -0.0361656\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -63.8969\n",
      "evaluation/env_infos/final/reward_run Mean              1.43629e-11\n",
      "evaluation/env_infos/final/reward_run Std               2.57531e-08\n",
      "evaluation/env_infos/final/reward_run Max               8.06746e-08\n",
      "evaluation/env_infos/final/reward_run Min              -8.86879e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.225131\n",
      "evaluation/env_infos/initial/reward_run Std             0.164741\n",
      "evaluation/env_infos/initial/reward_run Max             0.731396\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0662965\n",
      "evaluation/env_infos/reward_run Mean                   -0.000766692\n",
      "evaluation/env_infos/reward_run Std                     0.0226042\n",
      "evaluation/env_infos/reward_run Max                     0.844874\n",
      "evaluation/env_infos/reward_run Min                    -0.513954\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.012957\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00923473\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000127909\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0293506\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0203973\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0121344\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000182907\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0397603\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0129464\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0092331\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000108798\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0397603\n",
      "evaluation/env_infos/final/height Mean                 -0.128347\n",
      "evaluation/env_infos/final/height Std                   0.00104404\n",
      "evaluation/env_infos/final/height Max                  -0.127473\n",
      "evaluation/env_infos/final/height Min                  -0.131804\n",
      "evaluation/env_infos/initial/height Mean               -0.00598214\n",
      "evaluation/env_infos/initial/height Std                 0.0478423\n",
      "evaluation/env_infos/initial/height Max                 0.0707485\n",
      "evaluation/env_infos/initial/height Min                -0.0803713\n",
      "evaluation/env_infos/height Mean                       -0.127887\n",
      "evaluation/env_infos/height Std                         0.00562686\n",
      "evaluation/env_infos/height Max                         0.0707485\n",
      "evaluation/env_infos/height Min                        -0.142209\n",
      "evaluation/env_infos/final/reward_angular Mean          9.75646e-09\n",
      "evaluation/env_infos/final/reward_angular Std           4.298e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.63495e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -6.65047e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.344683\n",
      "evaluation/env_infos/initial/reward_angular Std         0.310808\n",
      "evaluation/env_infos/initial/reward_angular Max         0.302222\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.888375\n",
      "evaluation/env_infos/reward_angular Mean                5.41934e-05\n",
      "evaluation/env_infos/reward_angular Std                 0.0425479\n",
      "evaluation/env_infos/reward_angular Max                 1.63764\n",
      "evaluation/env_infos/reward_angular Min                -1.29989\n",
      "time/data storing (s)                                   0.0151406\n",
      "time/evaluation sampling (s)                           26.3482\n",
      "time/exploration sampling (s)                           1.02265\n",
      "time/logging (s)                                        0.247237\n",
      "time/saving (s)                                         0.0291334\n",
      "time/training (s)                                       3.3256\n",
      "time/epoch (s)                                         30.988\n",
      "time/total (s)                                        123.208\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:25.408745 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.34782\n",
      "trainer/QF2 Loss                                        0.374001\n",
      "trainer/Policy Loss                                    -6.9377\n",
      "trainer/Q1 Predictions Mean                             2.9726\n",
      "trainer/Q1 Predictions Std                              0.571176\n",
      "trainer/Q1 Predictions Max                              4.96146\n",
      "trainer/Q1 Predictions Min                              1.90839\n",
      "trainer/Q2 Predictions Mean                             2.98138\n",
      "trainer/Q2 Predictions Std                              0.552342\n",
      "trainer/Q2 Predictions Max                              4.72368\n",
      "trainer/Q2 Predictions Min                              1.93048\n",
      "trainer/Q Targets Mean                                  3.04122\n",
      "trainer/Q Targets Std                                   0.842551\n",
      "trainer/Q Targets Max                                   5.83423\n",
      "trainer/Q Targets Min                                   0.735378\n",
      "trainer/Log Pis Mean                                   -3.8756\n",
      "trainer/Log Pis Std                                     0.930922\n",
      "trainer/Log Pis Max                                    -0.6194\n",
      "trainer/Log Pis Min                                    -8.30373\n",
      "trainer/Policy mu Mean                                  0.087058\n",
      "trainer/Policy mu Std                                   0.230402\n",
      "trainer/Policy mu Max                                   0.844121\n",
      "trainer/Policy mu Min                                  -0.817405\n",
      "trainer/Policy log std Mean                            -0.103249\n",
      "trainer/Policy log std Std                              0.0241596\n",
      "trainer/Policy log std Max                             -0.0518442\n",
      "trainer/Policy log std Min                             -0.218388\n",
      "trainer/Alpha                                           0.302416\n",
      "trainer/Alpha Loss                                    -11.7815\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.152268\n",
      "exploration/Rewards Std                                 0.785926\n",
      "exploration/Rewards Max                                 2.59785\n",
      "exploration/Rewards Min                                -3.1851\n",
      "exploration/Returns Mean                             -152.268\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -152.268\n",
      "exploration/Returns Min                              -152.268\n",
      "exploration/Actions Mean                                0.083698\n",
      "exploration/Actions Std                                 0.600483\n",
      "exploration/Actions Max                                 0.99743\n",
      "exploration/Actions Min                                -0.997158\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -152.268\n",
      "exploration/env_infos/final/reward_run Mean             1.46604\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.46604\n",
      "exploration/env_infos/final/reward_run Min              1.46604\n",
      "exploration/env_infos/initial/reward_run Mean           0.637536\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.637536\n",
      "exploration/env_infos/initial/reward_run Min            0.637536\n",
      "exploration/env_infos/reward_run Mean                  -0.0652612\n",
      "exploration/env_infos/reward_run Std                    0.732367\n",
      "exploration/env_infos/reward_run Max                    2.33612\n",
      "exploration/env_infos/reward_run Min                   -2.34585\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.306209\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.306209\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.306209\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.273272\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.273272\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.273272\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220551\n",
      "exploration/env_infos/reward_ctrl Std                   0.0739784\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0238213\n",
      "exploration/env_infos/reward_ctrl Min                  -0.510183\n",
      "exploration/env_infos/final/height Mean                -0.101705\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.101705\n",
      "exploration/env_infos/final/height Min                 -0.101705\n",
      "exploration/env_infos/initial/height Mean              -0.0427249\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0427249\n",
      "exploration/env_infos/initial/height Min               -0.0427249\n",
      "exploration/env_infos/height Mean                      -0.0683865\n",
      "exploration/env_infos/height Std                        0.0896722\n",
      "exploration/env_infos/height Max                        0.239235\n",
      "exploration/env_infos/height Min                       -0.375183\n",
      "exploration/env_infos/final/reward_angular Mean         1.92289\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.92289\n",
      "exploration/env_infos/final/reward_angular Min          1.92289\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.93187\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.93187\n",
      "exploration/env_infos/initial/reward_angular Min       -1.93187\n",
      "exploration/env_infos/reward_angular Mean              -0.0190962\n",
      "exploration/env_infos/reward_angular Std                1.83357\n",
      "exploration/env_infos/reward_angular Max                6.72595\n",
      "exploration/env_infos/reward_angular Min               -5.07088\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0662701\n",
      "evaluation/Rewards Std                                  0.0488911\n",
      "evaluation/Rewards Max                                  1.37551\n",
      "evaluation/Rewards Min                                 -1.55438\n",
      "evaluation/Returns Mean                               -66.2701\n",
      "evaluation/Returns Std                                 37.2317\n",
      "evaluation/Returns Max                                 -7.82187\n",
      "evaluation/Returns Min                               -124.825\n",
      "evaluation/Actions Mean                                 0.118131\n",
      "evaluation/Actions Std                                  0.120476\n",
      "evaluation/Actions Max                                  0.512526\n",
      "evaluation/Actions Min                                 -0.242226\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -66.2701\n",
      "evaluation/env_infos/final/reward_run Mean              3.89846e-09\n",
      "evaluation/env_infos/final/reward_run Std               7.80762e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.5864e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.75191e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.260463\n",
      "evaluation/env_infos/initial/reward_run Std             0.191691\n",
      "evaluation/env_infos/initial/reward_run Max             0.663177\n",
      "evaluation/env_infos/initial/reward_run Min            -0.127732\n",
      "evaluation/env_infos/reward_run Mean                   -0.00128514\n",
      "evaluation/env_infos/reward_run Std                     0.0336864\n",
      "evaluation/env_infos/reward_run Max                     0.846925\n",
      "evaluation/env_infos/reward_run Min                    -0.768794\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0170809\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0131514\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.000397432\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.041023\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0305576\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0197416\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000368635\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0607829\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0170816\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0131796\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000230521\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0643334\n",
      "evaluation/env_infos/final/height Mean                 -0.129784\n",
      "evaluation/env_infos/final/height Std                   0.00139696\n",
      "evaluation/env_infos/final/height Max                  -0.127934\n",
      "evaluation/env_infos/final/height Min                  -0.134536\n",
      "evaluation/env_infos/initial/height Mean               -0.00297703\n",
      "evaluation/env_infos/initial/height Std                 0.0521741\n",
      "evaluation/env_infos/initial/height Max                 0.0847405\n",
      "evaluation/env_infos/initial/height Min                -0.0977625\n",
      "evaluation/env_infos/height Mean                       -0.129395\n",
      "evaluation/env_infos/height Std                         0.00599436\n",
      "evaluation/env_infos/height Max                         0.0847405\n",
      "evaluation/env_infos/height Min                        -0.160827\n",
      "evaluation/env_infos/final/reward_angular Mean         -2.3107e-08\n",
      "evaluation/env_infos/final/reward_angular Std           9.49464e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.31381e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -3.63647e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.482479\n",
      "evaluation/env_infos/initial/reward_angular Std         0.391506\n",
      "evaluation/env_infos/initial/reward_angular Max         0.752706\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.989899\n",
      "evaluation/env_infos/reward_angular Mean                4.84121e-05\n",
      "evaluation/env_infos/reward_angular Std                 0.0565949\n",
      "evaluation/env_infos/reward_angular Max                 2.49137\n",
      "evaluation/env_infos/reward_angular Min                -1.32524\n",
      "time/data storing (s)                                   0.015292\n",
      "time/evaluation sampling (s)                           24.1823\n",
      "time/exploration sampling (s)                           1.09973\n",
      "time/logging (s)                                        0.244341\n",
      "time/saving (s)                                         0.0269703\n",
      "time/training (s)                                       3.77265\n",
      "time/epoch (s)                                         29.3412\n",
      "time/total (s)                                        152.689\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:54.098445 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.302022\n",
      "trainer/QF2 Loss                                        0.309897\n",
      "trainer/Policy Loss                                    -7.11827\n",
      "trainer/Q1 Predictions Mean                             3.21751\n",
      "trainer/Q1 Predictions Std                              0.681575\n",
      "trainer/Q1 Predictions Max                              6.34929\n",
      "trainer/Q1 Predictions Min                              1.43128\n",
      "trainer/Q2 Predictions Mean                             3.20145\n",
      "trainer/Q2 Predictions Std                              0.670372\n",
      "trainer/Q2 Predictions Max                              6.0328\n",
      "trainer/Q2 Predictions Min                              1.60019\n",
      "trainer/Q Targets Mean                                  3.11226\n",
      "trainer/Q Targets Std                                   0.861615\n",
      "trainer/Q Targets Max                                   6.29185\n",
      "trainer/Q Targets Min                                   0.263009\n",
      "trainer/Log Pis Mean                                   -3.84321\n",
      "trainer/Log Pis Std                                     0.947173\n",
      "trainer/Log Pis Max                                    -0.9439\n",
      "trainer/Log Pis Min                                    -8.45313\n",
      "trainer/Policy mu Mean                                  0.0650433\n",
      "trainer/Policy mu Std                                   0.299663\n",
      "trainer/Policy mu Max                                   1.09032\n",
      "trainer/Policy mu Min                                  -0.618818\n",
      "trainer/Policy log std Mean                            -0.130071\n",
      "trainer/Policy log std Std                              0.0459318\n",
      "trainer/Policy log std Max                             -0.0310924\n",
      "trainer/Policy log std Min                             -0.31297\n",
      "trainer/Alpha                                           0.225194\n",
      "trainer/Alpha Loss                                    -14.6453\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.133335\n",
      "exploration/Rewards Std                                 1.02307\n",
      "exploration/Rewards Max                                 2.71467\n",
      "exploration/Rewards Min                                -4.37094\n",
      "exploration/Returns Mean                             -133.335\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -133.335\n",
      "exploration/Returns Min                              -133.335\n",
      "exploration/Actions Mean                                0.0485146\n",
      "exploration/Actions Std                                 0.597525\n",
      "exploration/Actions Max                                 0.997636\n",
      "exploration/Actions Min                                -0.998246\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -133.335\n",
      "exploration/env_infos/final/reward_run Mean            -0.850672\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.850672\n",
      "exploration/env_infos/final/reward_run Min             -0.850672\n",
      "exploration/env_infos/initial/reward_run Mean           0.336019\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.336019\n",
      "exploration/env_infos/initial/reward_run Min            0.336019\n",
      "exploration/env_infos/reward_run Mean                  -0.196774\n",
      "exploration/env_infos/reward_run Std                    0.743209\n",
      "exploration/env_infos/reward_run Max                    2.02367\n",
      "exploration/env_infos/reward_run Min                   -2.59347\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.311993\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.311993\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.311993\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.195277\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.195277\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.195277\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.215634\n",
      "exploration/env_infos/reward_ctrl Std                   0.0727784\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0402273\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4578\n",
      "exploration/env_infos/final/height Mean                -0.529436\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.529436\n",
      "exploration/env_infos/final/height Min                 -0.529436\n",
      "exploration/env_infos/initial/height Mean               0.0138918\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0138918\n",
      "exploration/env_infos/initial/height Min                0.0138918\n",
      "exploration/env_infos/height Mean                      -0.0872805\n",
      "exploration/env_infos/height Std                        0.146697\n",
      "exploration/env_infos/height Max                        0.329546\n",
      "exploration/env_infos/height Min                       -0.577221\n",
      "exploration/env_infos/final/reward_angular Mean        -1.59787\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.59787\n",
      "exploration/env_infos/final/reward_angular Min         -1.59787\n",
      "exploration/env_infos/initial/reward_angular Mean       0.335933\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.335933\n",
      "exploration/env_infos/initial/reward_angular Min        0.335933\n",
      "exploration/env_infos/reward_angular Mean               0.0452737\n",
      "exploration/env_infos/reward_angular Std                1.63253\n",
      "exploration/env_infos/reward_angular Max                6.67946\n",
      "exploration/env_infos/reward_angular Min               -4.11736\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0642839\n",
      "evaluation/Rewards Std                                  0.0442269\n",
      "evaluation/Rewards Max                                  1.14336\n",
      "evaluation/Rewards Min                                 -1.33236\n",
      "evaluation/Returns Mean                               -64.2839\n",
      "evaluation/Returns Std                                 35.0916\n",
      "evaluation/Returns Max                                -11.8819\n",
      "evaluation/Returns Min                               -124.56\n",
      "evaluation/Actions Mean                                 0.0867836\n",
      "evaluation/Actions Std                                  0.149971\n",
      "evaluation/Actions Max                                  0.660345\n",
      "evaluation/Actions Min                                 -0.195225\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -64.2839\n",
      "evaluation/env_infos/final/reward_run Mean             -2.28889e-09\n",
      "evaluation/env_infos/final/reward_run Std               1.07538e-07\n",
      "evaluation/env_infos/final/reward_run Max               1.95302e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.60385e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.342047\n",
      "evaluation/env_infos/initial/reward_run Std             0.21554\n",
      "evaluation/env_infos/initial/reward_run Max             0.669853\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0732008\n",
      "evaluation/env_infos/reward_run Mean                   -0.000763219\n",
      "evaluation/env_infos/reward_run Std                     0.0320648\n",
      "evaluation/env_infos/reward_run Max                     0.697505\n",
      "evaluation/env_infos/reward_run Min                    -0.782973\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0180438\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0146508\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00234493\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0558265\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0338689\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.023959\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00153563\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0793432\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0180136\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0146364\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00119716\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0793432\n",
      "evaluation/env_infos/final/height Mean                 -0.124222\n",
      "evaluation/env_infos/final/height Std                   0.00369425\n",
      "evaluation/env_infos/final/height Max                  -0.119019\n",
      "evaluation/env_infos/final/height Min                  -0.135083\n",
      "evaluation/env_infos/initial/height Mean               -0.00864841\n",
      "evaluation/env_infos/initial/height Std                 0.0544218\n",
      "evaluation/env_infos/initial/height Max                 0.0888142\n",
      "evaluation/env_infos/initial/height Min                -0.106201\n",
      "evaluation/env_infos/height Mean                       -0.123863\n",
      "evaluation/env_infos/height Std                         0.00649149\n",
      "evaluation/env_infos/height Max                         0.0888142\n",
      "evaluation/env_infos/height Min                        -0.165325\n",
      "evaluation/env_infos/final/reward_angular Mean          4.43666e-10\n",
      "evaluation/env_infos/final/reward_angular Std           9.39996e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.9481e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.38687e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.424206\n",
      "evaluation/env_infos/initial/reward_angular Std         0.358403\n",
      "evaluation/env_infos/initial/reward_angular Max         0.538481\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.50301\n",
      "evaluation/env_infos/reward_angular Mean               -0.000451583\n",
      "evaluation/env_infos/reward_angular Std                 0.0407909\n",
      "evaluation/env_infos/reward_angular Max                 1.66636\n",
      "evaluation/env_infos/reward_angular Min                -1.50301\n",
      "time/data storing (s)                                   0.0152406\n",
      "time/evaluation sampling (s)                           23.4496\n",
      "time/exploration sampling (s)                           1.09141\n",
      "time/logging (s)                                        0.236815\n",
      "time/saving (s)                                         0.0281054\n",
      "time/training (s)                                       3.69812\n",
      "time/epoch (s)                                         28.5193\n",
      "time/total (s)                                        181.37\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:22.929369 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.311253\n",
      "trainer/QF2 Loss                                        0.305818\n",
      "trainer/Policy Loss                                    -6.5333\n",
      "trainer/Q1 Predictions Mean                             3.02075\n",
      "trainer/Q1 Predictions Std                              0.773871\n",
      "trainer/Q1 Predictions Max                              6.07187\n",
      "trainer/Q1 Predictions Min                              1.35424\n",
      "trainer/Q2 Predictions Mean                             3.04164\n",
      "trainer/Q2 Predictions Std                              0.753752\n",
      "trainer/Q2 Predictions Max                              6.04236\n",
      "trainer/Q2 Predictions Min                              1.46644\n",
      "trainer/Q Targets Mean                                  3.08249\n",
      "trainer/Q Targets Std                                   0.953505\n",
      "trainer/Q Targets Max                                   6.34373\n",
      "trainer/Q Targets Min                                   0.501439\n",
      "trainer/Log Pis Mean                                   -3.29842\n",
      "trainer/Log Pis Std                                     1.20823\n",
      "trainer/Log Pis Max                                     0.36734\n",
      "trainer/Log Pis Min                                    -6.5304\n",
      "trainer/Policy mu Mean                                  0.107687\n",
      "trainer/Policy mu Std                                   0.423264\n",
      "trainer/Policy mu Max                                   1.21434\n",
      "trainer/Policy mu Min                                  -1.2975\n",
      "trainer/Policy log std Mean                            -0.150765\n",
      "trainer/Policy log std Std                              0.0474662\n",
      "trainer/Policy log std Max                             -0.0623825\n",
      "trainer/Policy log std Min                             -0.415479\n",
      "trainer/Alpha                                           0.168506\n",
      "trainer/Alpha Loss                                    -16.5318\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.132583\n",
      "exploration/Rewards Std                                 0.916441\n",
      "exploration/Rewards Max                                 2.5542\n",
      "exploration/Rewards Min                                -2.79809\n",
      "exploration/Returns Mean                             -132.583\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -132.583\n",
      "exploration/Returns Min                              -132.583\n",
      "exploration/Actions Mean                                0.163069\n",
      "exploration/Actions Std                                 0.584393\n",
      "exploration/Actions Max                                 0.998728\n",
      "exploration/Actions Min                                -0.993812\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -132.583\n",
      "exploration/env_infos/final/reward_run Mean             0.393565\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.393565\n",
      "exploration/env_infos/final/reward_run Min              0.393565\n",
      "exploration/env_infos/initial/reward_run Mean           0.102906\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.102906\n",
      "exploration/env_infos/initial/reward_run Min            0.102906\n",
      "exploration/env_infos/reward_run Mean                   0.110471\n",
      "exploration/env_infos/reward_run Std                    0.595749\n",
      "exploration/env_infos/reward_run Max                    1.7497\n",
      "exploration/env_infos/reward_run Min                   -2.15104\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.243049\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.243049\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.243049\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0494102\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0494102\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0494102\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220864\n",
      "exploration/env_infos/reward_ctrl Std                   0.0726978\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0323928\n",
      "exploration/env_infos/reward_ctrl Min                  -0.445833\n",
      "exploration/env_infos/final/height Mean                -0.105356\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.105356\n",
      "exploration/env_infos/final/height Min                 -0.105356\n",
      "exploration/env_infos/initial/height Mean               0.0337205\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0337205\n",
      "exploration/env_infos/initial/height Min                0.0337205\n",
      "exploration/env_infos/height Mean                      -0.0906307\n",
      "exploration/env_infos/height Std                        0.0956253\n",
      "exploration/env_infos/height Max                        0.219754\n",
      "exploration/env_infos/height Min                       -0.357882\n",
      "exploration/env_infos/final/reward_angular Mean        -2.25245\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.25245\n",
      "exploration/env_infos/final/reward_angular Min         -2.25245\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.504082\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.504082\n",
      "exploration/env_infos/initial/reward_angular Min       -0.504082\n",
      "exploration/env_infos/reward_angular Mean               0.000221663\n",
      "exploration/env_infos/reward_angular Std                1.63122\n",
      "exploration/env_infos/reward_angular Max                4.7788\n",
      "exploration/env_infos/reward_angular Min               -4.81696\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0741872\n",
      "evaluation/Rewards Std                                  0.0540667\n",
      "evaluation/Rewards Max                                  1.38595\n",
      "evaluation/Rewards Min                                 -0.611058\n",
      "evaluation/Returns Mean                               -74.1872\n",
      "evaluation/Returns Std                                 34.4883\n",
      "evaluation/Returns Max                                 -1.82452\n",
      "evaluation/Returns Min                               -141.575\n",
      "evaluation/Actions Mean                                 0.13231\n",
      "evaluation/Actions Std                                  0.256171\n",
      "evaluation/Actions Max                                  0.725726\n",
      "evaluation/Actions Min                                 -0.795668\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -74.1872\n",
      "evaluation/env_infos/final/reward_run Mean              3.39553e-08\n",
      "evaluation/env_infos/final/reward_run Std               1.74019e-07\n",
      "evaluation/env_infos/final/reward_run Max               5.82459e-07\n",
      "evaluation/env_infos/final/reward_run Min              -3.0088e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.306446\n",
      "evaluation/env_infos/initial/reward_run Std             0.245852\n",
      "evaluation/env_infos/initial/reward_run Max             0.656701\n",
      "evaluation/env_infos/initial/reward_run Min            -0.209952\n",
      "evaluation/env_infos/reward_run Mean                    0.000176292\n",
      "evaluation/env_infos/reward_run Std                     0.0354188\n",
      "evaluation/env_infos/reward_run Max                     0.841495\n",
      "evaluation/env_infos/reward_run Min                    -0.555329\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0498743\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0199702\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0133719\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0756444\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0734294\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0259681\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0218386\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.116788\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0498778\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0199948\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0025786\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.130834\n",
      "evaluation/env_infos/final/height Mean                 -0.127698\n",
      "evaluation/env_infos/final/height Std                   0.0273535\n",
      "evaluation/env_infos/final/height Max                  -0.101489\n",
      "evaluation/env_infos/final/height Min                  -0.240458\n",
      "evaluation/env_infos/initial/height Mean               -0.0104864\n",
      "evaluation/env_infos/initial/height Std                 0.0560078\n",
      "evaluation/env_infos/initial/height Max                 0.0808589\n",
      "evaluation/env_infos/initial/height Min                -0.0924165\n",
      "evaluation/env_infos/height Mean                       -0.12735\n",
      "evaluation/env_infos/height Std                         0.0280139\n",
      "evaluation/env_infos/height Max                         0.0808589\n",
      "evaluation/env_infos/height Min                        -0.317924\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.35412e-08\n",
      "evaluation/env_infos/final/reward_angular Std           1.40932e-07\n",
      "evaluation/env_infos/final/reward_angular Max           3.54589e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.22701e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.297336\n",
      "evaluation/env_infos/initial/reward_angular Std         0.888196\n",
      "evaluation/env_infos/initial/reward_angular Max         1.53201\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.97585\n",
      "evaluation/env_infos/reward_angular Mean                0.000735248\n",
      "evaluation/env_infos/reward_angular Std                 0.0651814\n",
      "evaluation/env_infos/reward_angular Max                 1.85842\n",
      "evaluation/env_infos/reward_angular Min                -1.97585\n",
      "time/data storing (s)                                   0.0143119\n",
      "time/evaluation sampling (s)                           23.6739\n",
      "time/exploration sampling (s)                           1.05523\n",
      "time/logging (s)                                        0.230204\n",
      "time/saving (s)                                         0.0297492\n",
      "time/training (s)                                       3.67364\n",
      "time/epoch (s)                                         28.677\n",
      "time/total (s)                                        210.194\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:55.042511 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.366858\n",
      "trainer/QF2 Loss                                        0.390279\n",
      "trainer/Policy Loss                                    -6.50662\n",
      "trainer/Q1 Predictions Mean                             3.05197\n",
      "trainer/Q1 Predictions Std                              0.858733\n",
      "trainer/Q1 Predictions Max                              6.50648\n",
      "trainer/Q1 Predictions Min                              1.22083\n",
      "trainer/Q2 Predictions Mean                             2.98844\n",
      "trainer/Q2 Predictions Std                              0.839669\n",
      "trainer/Q2 Predictions Max                              6.3676\n",
      "trainer/Q2 Predictions Min                              0.98963\n",
      "trainer/Q Targets Mean                                  3.00652\n",
      "trainer/Q Targets Std                                   1.06152\n",
      "trainer/Q Targets Max                                   8.19994\n",
      "trainer/Q Targets Min                                   0.430129\n",
      "trainer/Log Pis Mean                                   -3.36148\n",
      "trainer/Log Pis Std                                     1.24214\n",
      "trainer/Log Pis Max                                     0.865553\n",
      "trainer/Log Pis Min                                    -6.99672\n",
      "trainer/Policy mu Mean                                  0.098193\n",
      "trainer/Policy mu Std                                   0.429879\n",
      "trainer/Policy mu Max                                   1.13697\n",
      "trainer/Policy mu Min                                  -1.46031\n",
      "trainer/Policy log std Mean                            -0.150965\n",
      "trainer/Policy log std Std                              0.0671134\n",
      "trainer/Policy log std Max                              0.00279409\n",
      "trainer/Policy log std Min                             -0.411634\n",
      "trainer/Alpha                                           0.127143\n",
      "trainer/Alpha Loss                                    -19.2816\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.331082\n",
      "exploration/Rewards Std                                 0.39926\n",
      "exploration/Rewards Max                                 1.22621\n",
      "exploration/Rewards Min                                -1.91359\n",
      "exploration/Returns Mean                             -331.082\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -331.082\n",
      "exploration/Returns Min                              -331.082\n",
      "exploration/Actions Mean                                0.0636036\n",
      "exploration/Actions Std                                 0.601142\n",
      "exploration/Actions Max                                 0.998308\n",
      "exploration/Actions Min                                -0.996476\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -331.082\n",
      "exploration/env_infos/final/reward_run Mean             0.701903\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.701903\n",
      "exploration/env_infos/final/reward_run Min              0.701903\n",
      "exploration/env_infos/initial/reward_run Mean           0.522772\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.522772\n",
      "exploration/env_infos/initial/reward_run Min            0.522772\n",
      "exploration/env_infos/reward_run Mean                   0.060462\n",
      "exploration/env_infos/reward_run Std                    0.563869\n",
      "exploration/env_infos/reward_run Max                    1.74384\n",
      "exploration/env_infos/reward_run Min                   -2.11935\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.313563\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.313563\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.313563\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.333007\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.333007\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.333007\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.21925\n",
      "exploration/env_infos/reward_ctrl Std                   0.0752098\n",
      "exploration/env_infos/reward_ctrl Max                  -0.025331\n",
      "exploration/env_infos/reward_ctrl Min                  -0.47343\n",
      "exploration/env_infos/final/height Mean                -0.517642\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.517642\n",
      "exploration/env_infos/final/height Min                 -0.517642\n",
      "exploration/env_infos/initial/height Mean              -0.0193259\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0193259\n",
      "exploration/env_infos/initial/height Min               -0.0193259\n",
      "exploration/env_infos/height Mean                      -0.392447\n",
      "exploration/env_infos/height Std                        0.223643\n",
      "exploration/env_infos/height Max                        0.20079\n",
      "exploration/env_infos/height Min                       -0.583908\n",
      "exploration/env_infos/final/reward_angular Mean         1.65226\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.65226\n",
      "exploration/env_infos/final/reward_angular Min          1.65226\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.67577\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.67577\n",
      "exploration/env_infos/initial/reward_angular Min       -1.67577\n",
      "exploration/env_infos/reward_angular Mean               0.0367101\n",
      "exploration/env_infos/reward_angular Std                1.36133\n",
      "exploration/env_infos/reward_angular Max                5.69306\n",
      "exploration/env_infos/reward_angular Min               -4.10553\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0893393\n",
      "evaluation/Rewards Std                                  0.0639081\n",
      "evaluation/Rewards Max                                  1.86875\n",
      "evaluation/Rewards Min                                 -0.894307\n",
      "evaluation/Returns Mean                               -89.3393\n",
      "evaluation/Returns Std                                 38.273\n",
      "evaluation/Returns Max                                 -4.731\n",
      "evaluation/Returns Min                               -160.043\n",
      "evaluation/Actions Mean                                 0.109719\n",
      "evaluation/Actions Std                                  0.341522\n",
      "evaluation/Actions Max                                  0.795235\n",
      "evaluation/Actions Min                                 -0.856884\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -89.3393\n",
      "evaluation/env_infos/final/reward_run Mean              0.00139953\n",
      "evaluation/env_infos/final/reward_run Std               0.0315586\n",
      "evaluation/env_infos/final/reward_run Max               0.0928237\n",
      "evaluation/env_infos/final/reward_run Min              -0.114559\n",
      "evaluation/env_infos/initial/reward_run Mean            0.154453\n",
      "evaluation/env_infos/initial/reward_run Std             0.335763\n",
      "evaluation/env_infos/initial/reward_run Max             0.831824\n",
      "evaluation/env_infos/initial/reward_run Min            -0.483821\n",
      "evaluation/env_infos/reward_run Mean                    0.00040014\n",
      "evaluation/env_infos/reward_run Std                     0.0490861\n",
      "evaluation/env_infos/reward_run Max                     0.962737\n",
      "evaluation/env_infos/reward_run Min                    -0.815609\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0772719\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0355419\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0336026\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.147406\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0928768\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0323537\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0398058\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.150094\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0772053\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0356049\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0193105\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.16944\n",
      "evaluation/env_infos/final/height Mean                 -0.141981\n",
      "evaluation/env_infos/final/height Std                   0.038087\n",
      "evaluation/env_infos/final/height Max                  -0.103836\n",
      "evaluation/env_infos/final/height Min                  -0.301845\n",
      "evaluation/env_infos/initial/height Mean               -0.00274296\n",
      "evaluation/env_infos/initial/height Std                 0.0556386\n",
      "evaluation/env_infos/initial/height Max                 0.0798893\n",
      "evaluation/env_infos/initial/height Min                -0.102651\n",
      "evaluation/env_infos/height Mean                       -0.141593\n",
      "evaluation/env_infos/height Std                         0.0384638\n",
      "evaluation/env_infos/height Max                         0.0798893\n",
      "evaluation/env_infos/height Min                        -0.391286\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0130875\n",
      "evaluation/env_infos/final/reward_angular Std           0.0627234\n",
      "evaluation/env_infos/final/reward_angular Max           0.260424\n",
      "evaluation/env_infos/final/reward_angular Min          -0.0944246\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.221766\n",
      "evaluation/env_infos/initial/reward_angular Std         1.16648\n",
      "evaluation/env_infos/initial/reward_angular Max         2.09881\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.03103\n",
      "evaluation/env_infos/reward_angular Mean                0.000934048\n",
      "evaluation/env_infos/reward_angular Std                 0.099894\n",
      "evaluation/env_infos/reward_angular Max                 2.19899\n",
      "evaluation/env_infos/reward_angular Min                -2.03103\n",
      "time/data storing (s)                                   0.0139589\n",
      "time/evaluation sampling (s)                           25.6578\n",
      "time/exploration sampling (s)                           1.01289\n",
      "time/logging (s)                                        0.236044\n",
      "time/saving (s)                                         0.0283792\n",
      "time/training (s)                                       5.01963\n",
      "time/epoch (s)                                         31.9687\n",
      "time/total (s)                                        242.312\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:25.640503 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.396492\n",
      "trainer/QF2 Loss                                        0.45555\n",
      "trainer/Policy Loss                                    -5.69391\n",
      "trainer/Q1 Predictions Mean                             2.85443\n",
      "trainer/Q1 Predictions Std                              0.866309\n",
      "trainer/Q1 Predictions Max                              5.13464\n",
      "trainer/Q1 Predictions Min                              0.855442\n",
      "trainer/Q2 Predictions Mean                             2.85276\n",
      "trainer/Q2 Predictions Std                              0.88765\n",
      "trainer/Q2 Predictions Max                              5.44078\n",
      "trainer/Q2 Predictions Min                              0.872045\n",
      "trainer/Q Targets Mean                                  2.98283\n",
      "trainer/Q Targets Std                                   1.03583\n",
      "trainer/Q Targets Max                                   6.37942\n",
      "trainer/Q Targets Min                                   0.154292\n",
      "trainer/Log Pis Mean                                   -2.64767\n",
      "trainer/Log Pis Std                                     1.83803\n",
      "trainer/Log Pis Max                                     3.66269\n",
      "trainer/Log Pis Min                                    -8.15564\n",
      "trainer/Policy mu Mean                                  0.17921\n",
      "trainer/Policy mu Std                                   0.589758\n",
      "trainer/Policy mu Max                                   1.83659\n",
      "trainer/Policy mu Min                                  -1.90115\n",
      "trainer/Policy log std Mean                            -0.193133\n",
      "trainer/Policy log std Std                              0.0955556\n",
      "trainer/Policy log std Max                              0.0207221\n",
      "trainer/Policy log std Min                             -0.609458\n",
      "trainer/Alpha                                           0.0964551\n",
      "trainer/Alpha Loss                                    -20.201\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.279427\n",
      "exploration/Rewards Std                                 0.629328\n",
      "exploration/Rewards Max                                 2.97378\n",
      "exploration/Rewards Min                                -2.31236\n",
      "exploration/Returns Mean                             -279.427\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -279.427\n",
      "exploration/Returns Min                              -279.427\n",
      "exploration/Actions Mean                               -0.093557\n",
      "exploration/Actions Std                                 0.606431\n",
      "exploration/Actions Max                                 0.997263\n",
      "exploration/Actions Min                                -0.998393\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -279.427\n",
      "exploration/env_infos/final/reward_run Mean            -0.959611\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.959611\n",
      "exploration/env_infos/final/reward_run Min             -0.959611\n",
      "exploration/env_infos/initial/reward_run Mean           0.871612\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.871612\n",
      "exploration/env_infos/initial/reward_run Min            0.871612\n",
      "exploration/env_infos/reward_run Mean                  -0.0163227\n",
      "exploration/env_infos/reward_run Std                    0.605483\n",
      "exploration/env_infos/reward_run Max                    2.59884\n",
      "exploration/env_infos/reward_run Min                   -1.8465\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.219076\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.219076\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.219076\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.409534\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.409534\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.409534\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.225907\n",
      "exploration/env_infos/reward_ctrl Std                   0.0760969\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0128957\n",
      "exploration/env_infos/reward_ctrl Min                  -0.482815\n",
      "exploration/env_infos/final/height Mean                -0.559632\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.559632\n",
      "exploration/env_infos/final/height Min                 -0.559632\n",
      "exploration/env_infos/initial/height Mean               0.0795178\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0795178\n",
      "exploration/env_infos/initial/height Min                0.0795178\n",
      "exploration/env_infos/height Mean                      -0.473104\n",
      "exploration/env_infos/height Std                        0.146229\n",
      "exploration/env_infos/height Max                        0.222094\n",
      "exploration/env_infos/height Min                       -0.58475\n",
      "exploration/env_infos/final/reward_angular Mean         0.477173\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.477173\n",
      "exploration/env_infos/final/reward_angular Min          0.477173\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.198617\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.198617\n",
      "exploration/env_infos/initial/reward_angular Min       -0.198617\n",
      "exploration/env_infos/reward_angular Mean               0.0685486\n",
      "exploration/env_infos/reward_angular Std                1.50792\n",
      "exploration/env_infos/reward_angular Max                6.55913\n",
      "exploration/env_infos/reward_angular Min               -4.80212\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.118042\n",
      "evaluation/Rewards Std                                  0.094573\n",
      "evaluation/Rewards Max                                  1.88904\n",
      "evaluation/Rewards Min                                 -1.03839\n",
      "evaluation/Returns Mean                              -118.042\n",
      "evaluation/Returns Std                                 57.6869\n",
      "evaluation/Returns Max                                -16.541\n",
      "evaluation/Returns Min                               -191.75\n",
      "evaluation/Actions Mean                                 0.155618\n",
      "evaluation/Actions Std                                  0.452071\n",
      "evaluation/Actions Max                                  0.949207\n",
      "evaluation/Actions Min                                 -0.925411\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -118.042\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00718322\n",
      "evaluation/env_infos/final/reward_run Std               0.0889627\n",
      "evaluation/env_infos/final/reward_run Max               0.21272\n",
      "evaluation/env_infos/final/reward_run Min              -0.3923\n",
      "evaluation/env_infos/initial/reward_run Mean            0.121926\n",
      "evaluation/env_infos/initial/reward_run Std             0.42735\n",
      "evaluation/env_infos/initial/reward_run Max             0.690743\n",
      "evaluation/env_infos/initial/reward_run Min            -0.658931\n",
      "evaluation/env_infos/reward_run Mean                    0.00119972\n",
      "evaluation/env_infos/reward_run Std                     0.0913823\n",
      "evaluation/env_infos/reward_run Max                     0.888153\n",
      "evaluation/env_infos/reward_run Min                    -0.964507\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.138539\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.057508\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0511635\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.234922\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.1441\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0448074\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0754737\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.220267\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.137151\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0577214\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0169002\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.277262\n",
      "evaluation/env_infos/final/height Mean                 -0.161097\n",
      "evaluation/env_infos/final/height Std                   0.0653437\n",
      "evaluation/env_infos/final/height Max                  -0.0797708\n",
      "evaluation/env_infos/final/height Min                  -0.390148\n",
      "evaluation/env_infos/initial/height Mean               -0.00271014\n",
      "evaluation/env_infos/initial/height Std                 0.0510249\n",
      "evaluation/env_infos/initial/height Max                 0.0745272\n",
      "evaluation/env_infos/initial/height Min                -0.0878226\n",
      "evaluation/env_infos/height Mean                       -0.158104\n",
      "evaluation/env_infos/height Std                         0.0591385\n",
      "evaluation/env_infos/height Max                         0.0745272\n",
      "evaluation/env_infos/height Min                        -0.420088\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0457603\n",
      "evaluation/env_infos/final/reward_angular Std           0.155593\n",
      "evaluation/env_infos/final/reward_angular Max           0.612036\n",
      "evaluation/env_infos/final/reward_angular Min          -1.32812e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.451353\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11919\n",
      "evaluation/env_infos/initial/reward_angular Max         1.63531\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.28871\n",
      "evaluation/env_infos/reward_angular Mean               -0.000442958\n",
      "evaluation/env_infos/reward_angular Std                 0.261367\n",
      "evaluation/env_infos/reward_angular Max                 1.86197\n",
      "evaluation/env_infos/reward_angular Min                -2.28871\n",
      "time/data storing (s)                                   0.0154268\n",
      "time/evaluation sampling (s)                           24.9184\n",
      "time/exploration sampling (s)                           1.06343\n",
      "time/logging (s)                                        0.237683\n",
      "time/saving (s)                                         0.0272585\n",
      "time/training (s)                                       4.17277\n",
      "time/epoch (s)                                         30.435\n",
      "time/total (s)                                        272.911\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:53.601068 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.440484\n",
      "trainer/QF2 Loss                                        0.4617\n",
      "trainer/Policy Loss                                    -5.41309\n",
      "trainer/Q1 Predictions Mean                             2.98194\n",
      "trainer/Q1 Predictions Std                              1.02691\n",
      "trainer/Q1 Predictions Max                              5.8588\n",
      "trainer/Q1 Predictions Min                              1.14559\n",
      "trainer/Q2 Predictions Mean                             2.92538\n",
      "trainer/Q2 Predictions Std                              1.02364\n",
      "trainer/Q2 Predictions Max                              6.17431\n",
      "trainer/Q2 Predictions Min                              1.08548\n",
      "trainer/Q Targets Mean                                  2.86781\n",
      "trainer/Q Targets Std                                   1.19284\n",
      "trainer/Q Targets Max                                   7.07663\n",
      "trainer/Q Targets Min                                  -0.692508\n",
      "trainer/Log Pis Mean                                   -2.21857\n",
      "trainer/Log Pis Std                                     1.96479\n",
      "trainer/Log Pis Max                                     3.85707\n",
      "trainer/Log Pis Min                                    -8.13553\n",
      "trainer/Policy mu Mean                                  0.134139\n",
      "trainer/Policy mu Std                                   0.648911\n",
      "trainer/Policy mu Max                                   1.97391\n",
      "trainer/Policy mu Min                                  -1.85443\n",
      "trainer/Policy log std Mean                            -0.22119\n",
      "trainer/Policy log std Std                              0.115862\n",
      "trainer/Policy log std Max                              0.033157\n",
      "trainer/Policy log std Min                             -0.708952\n",
      "trainer/Alpha                                           0.0738915\n",
      "trainer/Alpha Loss                                    -21.3893\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.150566\n",
      "exploration/Rewards Std                                 0.616007\n",
      "exploration/Rewards Max                                 1.96489\n",
      "exploration/Rewards Min                                -2.85181\n",
      "exploration/Returns Mean                             -150.566\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -150.566\n",
      "exploration/Returns Min                              -150.566\n",
      "exploration/Actions Mean                               -0.0873956\n",
      "exploration/Actions Std                                 0.582745\n",
      "exploration/Actions Max                                 0.995202\n",
      "exploration/Actions Min                                -0.997828\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -150.566\n",
      "exploration/env_infos/final/reward_run Mean            -0.504067\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.504067\n",
      "exploration/env_infos/final/reward_run Min             -0.504067\n",
      "exploration/env_infos/initial/reward_run Mean           0.203928\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.203928\n",
      "exploration/env_infos/initial/reward_run Min            0.203928\n",
      "exploration/env_infos/reward_run Mean                  -0.136499\n",
      "exploration/env_infos/reward_run Std                    0.680133\n",
      "exploration/env_infos/reward_run Max                    1.80255\n",
      "exploration/env_infos/reward_run Min                   -2.34348\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.233995\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.233995\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.233995\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.153462\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.153462\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.153462\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.208338\n",
      "exploration/env_infos/reward_ctrl Std                   0.0701441\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0163894\n",
      "exploration/env_infos/reward_ctrl Min                  -0.441796\n",
      "exploration/env_infos/final/height Mean                 0.00595382\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.00595382\n",
      "exploration/env_infos/final/height Min                  0.00595382\n",
      "exploration/env_infos/initial/height Mean              -0.00228377\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00228377\n",
      "exploration/env_infos/initial/height Min               -0.00228377\n",
      "exploration/env_infos/height Mean                      -0.0780336\n",
      "exploration/env_infos/height Std                        0.068569\n",
      "exploration/env_infos/height Max                        0.13261\n",
      "exploration/env_infos/height Min                       -0.266723\n",
      "exploration/env_infos/final/reward_angular Mean         1.48418\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.48418\n",
      "exploration/env_infos/final/reward_angular Min          1.48418\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.423474\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.423474\n",
      "exploration/env_infos/initial/reward_angular Min       -0.423474\n",
      "exploration/env_infos/reward_angular Mean              -0.0170365\n",
      "exploration/env_infos/reward_angular Std                1.62545\n",
      "exploration/env_infos/reward_angular Max                5.54911\n",
      "exploration/env_infos/reward_angular Min               -6.75835\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.132298\n",
      "evaluation/Rewards Std                                  0.283416\n",
      "evaluation/Rewards Max                                  2.91883\n",
      "evaluation/Rewards Min                                 -2.33025\n",
      "evaluation/Returns Mean                              -132.298\n",
      "evaluation/Returns Std                                 62.2532\n",
      "evaluation/Returns Max                                -35.3564\n",
      "evaluation/Returns Min                               -231.123\n",
      "evaluation/Actions Mean                                 0.114633\n",
      "evaluation/Actions Std                                  0.504517\n",
      "evaluation/Actions Max                                  0.982149\n",
      "evaluation/Actions Min                                 -0.947904\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -132.298\n",
      "evaluation/env_infos/final/reward_run Mean             -0.014843\n",
      "evaluation/env_infos/final/reward_run Std               0.111602\n",
      "evaluation/env_infos/final/reward_run Max               0.0807284\n",
      "evaluation/env_infos/final/reward_run Min              -0.551441\n",
      "evaluation/env_infos/initial/reward_run Mean            0.129341\n",
      "evaluation/env_infos/initial/reward_run Std             0.521533\n",
      "evaluation/env_infos/initial/reward_run Max             0.978732\n",
      "evaluation/env_infos/initial/reward_run Min            -0.628902\n",
      "evaluation/env_infos/reward_run Mean                    0.00284097\n",
      "evaluation/env_infos/reward_run Std                     0.130655\n",
      "evaluation/env_infos/reward_run Max                     1.12333\n",
      "evaluation/env_infos/reward_run Min                    -0.883157\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.166003\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0825414\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0427064\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.315\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.161476\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0662182\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0521294\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.290327\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.160607\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0775172\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0218503\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.334434\n",
      "evaluation/env_infos/final/height Mean                 -0.176591\n",
      "evaluation/env_infos/final/height Std                   0.0662451\n",
      "evaluation/env_infos/final/height Max                  -0.0875225\n",
      "evaluation/env_infos/final/height Min                  -0.356538\n",
      "evaluation/env_infos/initial/height Mean               -0.0176979\n",
      "evaluation/env_infos/initial/height Std                 0.0577566\n",
      "evaluation/env_infos/initial/height Max                 0.0751301\n",
      "evaluation/env_infos/initial/height Min                -0.0998241\n",
      "evaluation/env_infos/height Mean                       -0.172652\n",
      "evaluation/env_infos/height Std                         0.0693058\n",
      "evaluation/env_infos/height Max                         0.0751301\n",
      "evaluation/env_infos/height Min                        -0.451593\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0203959\n",
      "evaluation/env_infos/final/reward_angular Std           0.176406\n",
      "evaluation/env_infos/final/reward_angular Max           0.245222\n",
      "evaluation/env_infos/final/reward_angular Min          -0.849754\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.271615\n",
      "evaluation/env_infos/initial/reward_angular Std         1.44724\n",
      "evaluation/env_infos/initial/reward_angular Max         2.15185\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.42146\n",
      "evaluation/env_infos/reward_angular Mean                0.000354565\n",
      "evaluation/env_infos/reward_angular Std                 0.387838\n",
      "evaluation/env_infos/reward_angular Max                 2.46579\n",
      "evaluation/env_infos/reward_angular Min                -3.51152\n",
      "time/data storing (s)                                   0.0152336\n",
      "time/evaluation sampling (s)                           22.5513\n",
      "time/exploration sampling (s)                           1.05932\n",
      "time/logging (s)                                        0.236578\n",
      "time/saving (s)                                         0.0269645\n",
      "time/training (s)                                       3.90319\n",
      "time/epoch (s)                                         27.7926\n",
      "time/total (s)                                        300.869\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:22.796134 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.347352\n",
      "trainer/QF2 Loss                                        0.363084\n",
      "trainer/Policy Loss                                    -4.36417\n",
      "trainer/Q1 Predictions Mean                             2.80703\n",
      "trainer/Q1 Predictions Std                              1.07711\n",
      "trainer/Q1 Predictions Max                              7.37976\n",
      "trainer/Q1 Predictions Min                              0.798434\n",
      "trainer/Q2 Predictions Mean                             2.83167\n",
      "trainer/Q2 Predictions Std                              1.09118\n",
      "trainer/Q2 Predictions Max                              7.30804\n",
      "trainer/Q2 Predictions Min                              0.780649\n",
      "trainer/Q Targets Mean                                  2.7231\n",
      "trainer/Q Targets Std                                   1.24174\n",
      "trainer/Q Targets Max                                   8.40074\n",
      "trainer/Q Targets Min                                  -0.567778\n",
      "trainer/Log Pis Mean                                   -1.29696\n",
      "trainer/Log Pis Std                                     2.75919\n",
      "trainer/Log Pis Max                                     7.30614\n",
      "trainer/Log Pis Min                                    -6.36145\n",
      "trainer/Policy mu Mean                                  0.0708576\n",
      "trainer/Policy mu Std                                   0.839867\n",
      "trainer/Policy mu Max                                   2.76601\n",
      "trainer/Policy mu Min                                  -2.7229\n",
      "trainer/Policy log std Mean                            -0.279531\n",
      "trainer/Policy log std Std                              0.157806\n",
      "trainer/Policy log std Max                             -0.0415836\n",
      "trainer/Policy log std Min                             -1.02691\n",
      "trainer/Alpha                                           0.0572572\n",
      "trainer/Alpha Loss                                    -20.8534\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            12\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.256502\n",
      "exploration/Rewards Std                                 0.486424\n",
      "exploration/Rewards Max                                 1.39869\n",
      "exploration/Rewards Min                                -1.94202\n",
      "exploration/Returns Mean                             -256.502\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -256.502\n",
      "exploration/Returns Min                              -256.502\n",
      "exploration/Actions Mean                               -0.00579013\n",
      "exploration/Actions Std                                 0.671616\n",
      "exploration/Actions Max                                 0.998928\n",
      "exploration/Actions Min                                -0.999132\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -256.502\n",
      "exploration/env_infos/final/reward_run Mean             0.228027\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.228027\n",
      "exploration/env_infos/final/reward_run Min              0.228027\n",
      "exploration/env_infos/initial/reward_run Mean          -0.151863\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.151863\n",
      "exploration/env_infos/initial/reward_run Min           -0.151863\n",
      "exploration/env_infos/reward_run Mean                   0.0734391\n",
      "exploration/env_infos/reward_run Std                    0.450262\n",
      "exploration/env_infos/reward_run Max                    1.75293\n",
      "exploration/env_infos/reward_run Min                   -1.28313\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.417611\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.417611\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.417611\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.302015\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.302015\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.302015\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.270661\n",
      "exploration/env_infos/reward_ctrl Std                   0.0727398\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0306649\n",
      "exploration/env_infos/reward_ctrl Min                  -0.464515\n",
      "exploration/env_infos/final/height Mean                -0.130841\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.130841\n",
      "exploration/env_infos/final/height Min                 -0.130841\n",
      "exploration/env_infos/initial/height Mean              -0.0507589\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0507589\n",
      "exploration/env_infos/initial/height Min               -0.0507589\n",
      "exploration/env_infos/height Mean                      -0.141916\n",
      "exploration/env_infos/height Std                        0.0414614\n",
      "exploration/env_infos/height Max                       -0.00716628\n",
      "exploration/env_infos/height Min                       -0.275951\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0739635\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0739635\n",
      "exploration/env_infos/final/reward_angular Min         -0.0739635\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.547677\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.547677\n",
      "exploration/env_infos/initial/reward_angular Min       -0.547677\n",
      "exploration/env_infos/reward_angular Mean              -0.0240182\n",
      "exploration/env_infos/reward_angular Std                1.07733\n",
      "exploration/env_infos/reward_angular Max                3.45054\n",
      "exploration/env_infos/reward_angular Min               -3.65822\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.191591\n",
      "evaluation/Rewards Std                                  0.138817\n",
      "evaluation/Rewards Max                                  2.30994\n",
      "evaluation/Rewards Min                                 -1.5459\n",
      "evaluation/Returns Mean                              -191.591\n",
      "evaluation/Returns Std                                 86.7916\n",
      "evaluation/Returns Max                                -50.5056\n",
      "evaluation/Returns Min                               -356.541\n",
      "evaluation/Actions Mean                                -0.0594011\n",
      "evaluation/Actions Std                                  0.632702\n",
      "evaluation/Actions Max                                  0.996176\n",
      "evaluation/Actions Min                                 -0.990727\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -191.591\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00499401\n",
      "evaluation/env_infos/final/reward_run Std               0.0338081\n",
      "evaluation/env_infos/final/reward_run Max               0.0410231\n",
      "evaluation/env_infos/final/reward_run Min              -0.165877\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0765445\n",
      "evaluation/env_infos/initial/reward_run Std             0.622454\n",
      "evaluation/env_infos/initial/reward_run Max             0.903393\n",
      "evaluation/env_infos/initial/reward_run Min            -0.986574\n",
      "evaluation/env_infos/reward_run Mean                   -0.00132038\n",
      "evaluation/env_infos/reward_run Std                     0.125599\n",
      "evaluation/env_infos/reward_run Max                     1.39884\n",
      "evaluation/env_infos/reward_run Min                    -2.03435\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.240693\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0949835\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0633274\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.414877\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.226589\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.072847\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0565931\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.397624\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.242304\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0930547\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00592707\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.433391\n",
      "evaluation/env_infos/final/height Mean                 -0.255507\n",
      "evaluation/env_infos/final/height Std                   0.111019\n",
      "evaluation/env_infos/final/height Max                  -0.086954\n",
      "evaluation/env_infos/final/height Min                  -0.444274\n",
      "evaluation/env_infos/initial/height Mean               -0.00853817\n",
      "evaluation/env_infos/initial/height Std                 0.0564785\n",
      "evaluation/env_infos/initial/height Max                 0.0803523\n",
      "evaluation/env_infos/initial/height Min                -0.0876324\n",
      "evaluation/env_infos/height Mean                       -0.246829\n",
      "evaluation/env_infos/height Std                         0.110982\n",
      "evaluation/env_infos/height Max                         0.0836824\n",
      "evaluation/env_infos/height Min                        -0.589453\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.144693\n",
      "evaluation/env_infos/final/reward_angular Std           0.704218\n",
      "evaluation/env_infos/final/reward_angular Max           3.14399e-05\n",
      "evaluation/env_infos/final/reward_angular Min          -3.59457\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.123777\n",
      "evaluation/env_infos/initial/reward_angular Std         1.68156\n",
      "evaluation/env_infos/initial/reward_angular Max         2.46396\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.40719\n",
      "evaluation/env_infos/reward_angular Mean                0.00622323\n",
      "evaluation/env_infos/reward_angular Std                 0.321124\n",
      "evaluation/env_infos/reward_angular Max                 4.37271\n",
      "evaluation/env_infos/reward_angular Min                -4.97917\n",
      "time/data storing (s)                                   0.0150893\n",
      "time/evaluation sampling (s)                           23.2413\n",
      "time/exploration sampling (s)                           1.14123\n",
      "time/logging (s)                                        0.231701\n",
      "time/saving (s)                                         0.0279358\n",
      "time/training (s)                                       4.35625\n",
      "time/epoch (s)                                         29.0135\n",
      "time/total (s)                                        330.059\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:52.114202 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.62186\n",
      "trainer/QF2 Loss                                        0.641404\n",
      "trainer/Policy Loss                                    -3.34762\n",
      "trainer/Q1 Predictions Mean                             2.80112\n",
      "trainer/Q1 Predictions Std                              1.17322\n",
      "trainer/Q1 Predictions Max                              6.5256\n",
      "trainer/Q1 Predictions Min                              0.309959\n",
      "trainer/Q2 Predictions Mean                             2.76899\n",
      "trainer/Q2 Predictions Std                              1.14927\n",
      "trainer/Q2 Predictions Max                              6.87711\n",
      "trainer/Q2 Predictions Min                              0.422154\n",
      "trainer/Q Targets Mean                                  2.85016\n",
      "trainer/Q Targets Std                                   1.4288\n",
      "trainer/Q Targets Max                                   6.62139\n",
      "trainer/Q Targets Min                                  -1.42041\n",
      "trainer/Log Pis Mean                                   -0.223176\n",
      "trainer/Log Pis Std                                     3.0246\n",
      "trainer/Log Pis Max                                     8.99264\n",
      "trainer/Log Pis Min                                    -6.29994\n",
      "trainer/Policy mu Mean                                  0.0890124\n",
      "trainer/Policy mu Std                                   0.949379\n",
      "trainer/Policy mu Max                                   2.22471\n",
      "trainer/Policy mu Min                                  -3.30748\n",
      "trainer/Policy log std Mean                            -0.33648\n",
      "trainer/Policy log std Std                              0.144646\n",
      "trainer/Policy log std Max                             -0.00875626\n",
      "trainer/Policy log std Min                             -0.886248\n",
      "trainer/Alpha                                           0.0454195\n",
      "trainer/Alpha Loss                                    -19.2273\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            13\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.21107\n",
      "exploration/Rewards Std                                 0.683479\n",
      "exploration/Rewards Max                                 2.70811\n",
      "exploration/Rewards Min                                -2.71554\n",
      "exploration/Returns Mean                             -211.07\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -211.07\n",
      "exploration/Returns Min                              -211.07\n",
      "exploration/Actions Mean                                0.0340567\n",
      "exploration/Actions Std                                 0.667242\n",
      "exploration/Actions Max                                 0.99835\n",
      "exploration/Actions Min                                -0.999235\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -211.07\n",
      "exploration/env_infos/final/reward_run Mean            -0.273459\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.273459\n",
      "exploration/env_infos/final/reward_run Min             -0.273459\n",
      "exploration/env_infos/initial/reward_run Mean           0.572769\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.572769\n",
      "exploration/env_infos/initial/reward_run Min            0.572769\n",
      "exploration/env_infos/reward_run Mean                  -0.0137146\n",
      "exploration/env_infos/reward_run Std                    0.75633\n",
      "exploration/env_infos/reward_run Max                    2.65295\n",
      "exploration/env_infos/reward_run Min                   -2.37686\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.29803\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.29803\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.29803\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.285346\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.285346\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.285346\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.267823\n",
      "exploration/env_infos/reward_ctrl Std                   0.0832693\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0393823\n",
      "exploration/env_infos/reward_ctrl Min                  -0.552954\n",
      "exploration/env_infos/final/height Mean                -0.350062\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.350062\n",
      "exploration/env_infos/final/height Min                 -0.350062\n",
      "exploration/env_infos/initial/height Mean              -0.0576912\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0576912\n",
      "exploration/env_infos/initial/height Min               -0.0576912\n",
      "exploration/env_infos/height Mean                      -0.429672\n",
      "exploration/env_infos/height Std                        0.150929\n",
      "exploration/env_infos/height Max                        0.189633\n",
      "exploration/env_infos/height Min                       -0.589262\n",
      "exploration/env_infos/final/reward_angular Mean        -3.43719\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.43719\n",
      "exploration/env_infos/final/reward_angular Min         -3.43719\n",
      "exploration/env_infos/initial/reward_angular Mean       1.53315\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.53315\n",
      "exploration/env_infos/initial/reward_angular Min        1.53315\n",
      "exploration/env_infos/reward_angular Mean               0.103856\n",
      "exploration/env_infos/reward_angular Std                1.97122\n",
      "exploration/env_infos/reward_angular Max                7.26266\n",
      "exploration/env_infos/reward_angular Min               -6.98633\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.173491\n",
      "evaluation/Rewards Std                                  0.475206\n",
      "evaluation/Rewards Max                                  3.91446\n",
      "evaluation/Rewards Min                                 -3.28242\n",
      "evaluation/Returns Mean                              -173.491\n",
      "evaluation/Returns Std                                 86.3477\n",
      "evaluation/Returns Max                                -35.9942\n",
      "evaluation/Returns Min                               -332.804\n",
      "evaluation/Actions Mean                                -0.0668547\n",
      "evaluation/Actions Std                                  0.634571\n",
      "evaluation/Actions Max                                  0.982637\n",
      "evaluation/Actions Min                                 -0.998535\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -173.491\n",
      "evaluation/env_infos/final/reward_run Mean              0.0954628\n",
      "evaluation/env_infos/final/reward_run Std               0.233269\n",
      "evaluation/env_infos/final/reward_run Max               0.876388\n",
      "evaluation/env_infos/final/reward_run Min              -0.0447034\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.22386\n",
      "evaluation/env_infos/initial/reward_run Std             0.576193\n",
      "evaluation/env_infos/initial/reward_run Max             0.745999\n",
      "evaluation/env_infos/initial/reward_run Min            -0.931447\n",
      "evaluation/env_infos/reward_run Mean                    0.0836933\n",
      "evaluation/env_infos/reward_run Std                     0.299153\n",
      "evaluation/env_infos/reward_run Max                     1.89961\n",
      "evaluation/env_infos/reward_run Min                    -1.326\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.242221\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0877073\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.109499\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.411052\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.243753\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0770816\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0586705\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.378439\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.24429\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0856868\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0320418\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.435262\n",
      "evaluation/env_infos/final/height Mean                 -0.181629\n",
      "evaluation/env_infos/final/height Std                   0.0756955\n",
      "evaluation/env_infos/final/height Max                  -0.0740405\n",
      "evaluation/env_infos/final/height Min                  -0.395312\n",
      "evaluation/env_infos/initial/height Mean               -0.00711958\n",
      "evaluation/env_infos/initial/height Std                 0.0528391\n",
      "evaluation/env_infos/initial/height Max                 0.0865466\n",
      "evaluation/env_infos/initial/height Min                -0.0863864\n",
      "evaluation/env_infos/height Mean                       -0.181878\n",
      "evaluation/env_infos/height Std                         0.074944\n",
      "evaluation/env_infos/height Max                         0.0865466\n",
      "evaluation/env_infos/height Min                        -0.450342\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0816831\n",
      "evaluation/env_infos/final/reward_angular Std           0.416494\n",
      "evaluation/env_infos/final/reward_angular Max           1.09898\n",
      "evaluation/env_infos/final/reward_angular Min          -1.20715\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.323237\n",
      "evaluation/env_infos/initial/reward_angular Std         1.60142\n",
      "evaluation/env_infos/initial/reward_angular Max         2.84788\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.33052\n",
      "evaluation/env_infos/reward_angular Mean               -0.00663392\n",
      "evaluation/env_infos/reward_angular Std                 0.608382\n",
      "evaluation/env_infos/reward_angular Max                 3.9897\n",
      "evaluation/env_infos/reward_angular Min                -4.72252\n",
      "time/data storing (s)                                   0.0152453\n",
      "time/evaluation sampling (s)                           23.6584\n",
      "time/exploration sampling (s)                           1.01504\n",
      "time/logging (s)                                        0.230469\n",
      "time/saving (s)                                         0.0277838\n",
      "time/training (s)                                       4.18998\n",
      "time/epoch (s)                                         29.1369\n",
      "time/total (s)                                        359.374\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:20.721531 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.549177\n",
      "trainer/QF2 Loss                                        0.507934\n",
      "trainer/Policy Loss                                    -2.88674\n",
      "trainer/Q1 Predictions Mean                             2.95582\n",
      "trainer/Q1 Predictions Std                              1.28181\n",
      "trainer/Q1 Predictions Max                              7.07628\n",
      "trainer/Q1 Predictions Min                              0.628578\n",
      "trainer/Q2 Predictions Mean                             2.96642\n",
      "trainer/Q2 Predictions Std                              1.28397\n",
      "trainer/Q2 Predictions Max                              7.48487\n",
      "trainer/Q2 Predictions Min                              0.638272\n",
      "trainer/Q Targets Mean                                  2.80434\n",
      "trainer/Q Targets Std                                   1.42437\n",
      "trainer/Q Targets Max                                   8.18657\n",
      "trainer/Q Targets Min                                   0.0962379\n",
      "trainer/Log Pis Mean                                    0.38509\n",
      "trainer/Log Pis Std                                     3.02855\n",
      "trainer/Log Pis Max                                     9.63037\n",
      "trainer/Log Pis Min                                   -10.8615\n",
      "trainer/Policy mu Mean                                  0.249563\n",
      "trainer/Policy mu Std                                   0.994607\n",
      "trainer/Policy mu Max                                   2.68983\n",
      "trainer/Policy mu Min                                  -2.30509\n",
      "trainer/Policy log std Mean                            -0.380714\n",
      "trainer/Policy log std Std                              0.192551\n",
      "trainer/Policy log std Max                              0.0118805\n",
      "trainer/Policy log std Min                             -1.0596\n",
      "trainer/Alpha                                           0.0368297\n",
      "trainer/Alpha Loss                                    -18.5266\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            14\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0890633\n",
      "exploration/Rewards Std                                 0.481711\n",
      "exploration/Rewards Max                                 1.64833\n",
      "exploration/Rewards Min                                -1.68039\n",
      "exploration/Returns Mean                              -89.0633\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -89.0633\n",
      "exploration/Returns Min                               -89.0633\n",
      "exploration/Actions Mean                                0.477336\n",
      "exploration/Actions Std                                 0.533965\n",
      "exploration/Actions Max                                 0.997706\n",
      "exploration/Actions Min                                -0.994714\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -89.0633\n",
      "exploration/env_infos/final/reward_run Mean             0.0189792\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0189792\n",
      "exploration/env_infos/final/reward_run Min              0.0189792\n",
      "exploration/env_infos/initial/reward_run Mean           0.410402\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.410402\n",
      "exploration/env_infos/initial/reward_run Min            0.410402\n",
      "exploration/env_infos/reward_run Mean                  -0.00369599\n",
      "exploration/env_infos/reward_run Std                    0.252821\n",
      "exploration/env_infos/reward_run Max                    1.12982\n",
      "exploration/env_infos/reward_run Min                   -0.974741\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.349309\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.349309\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.349309\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.328851\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.328851\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.328851\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.307781\n",
      "exploration/env_infos/reward_ctrl Std                   0.0700938\n",
      "exploration/env_infos/reward_ctrl Max                  -0.118948\n",
      "exploration/env_infos/reward_ctrl Min                  -0.48654\n",
      "exploration/env_infos/final/height Mean                -0.344528\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.344528\n",
      "exploration/env_infos/final/height Min                 -0.344528\n",
      "exploration/env_infos/initial/height Mean              -0.0258072\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0258072\n",
      "exploration/env_infos/initial/height Min               -0.0258072\n",
      "exploration/env_infos/height Mean                      -0.324168\n",
      "exploration/env_infos/height Std                        0.0507742\n",
      "exploration/env_infos/height Max                       -0.0258072\n",
      "exploration/env_infos/height Min                       -0.441793\n",
      "exploration/env_infos/final/reward_angular Mean        -0.684543\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.684543\n",
      "exploration/env_infos/final/reward_angular Min         -0.684543\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.62545\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.62545\n",
      "exploration/env_infos/initial/reward_angular Min       -1.62545\n",
      "exploration/env_infos/reward_angular Mean              -0.0012423\n",
      "exploration/env_infos/reward_angular Std                0.80034\n",
      "exploration/env_infos/reward_angular Max                2.4117\n",
      "exploration/env_infos/reward_angular Min               -2.90075\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.2256\n",
      "evaluation/Rewards Std                                  0.387904\n",
      "evaluation/Rewards Max                                  3.12191\n",
      "evaluation/Rewards Min                                 -2.882\n",
      "evaluation/Returns Mean                              -225.6\n",
      "evaluation/Returns Std                                 96.2777\n",
      "evaluation/Returns Max                                -44.7712\n",
      "evaluation/Returns Min                               -473.946\n",
      "evaluation/Actions Mean                                -0.00787126\n",
      "evaluation/Actions Std                                  0.664645\n",
      "evaluation/Actions Max                                  0.997176\n",
      "evaluation/Actions Min                                 -0.993918\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -225.6\n",
      "evaluation/env_infos/final/reward_run Mean              0.0119646\n",
      "evaluation/env_infos/final/reward_run Std               0.107058\n",
      "evaluation/env_infos/final/reward_run Max               0.412049\n",
      "evaluation/env_infos/final/reward_run Min              -0.253395\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.190175\n",
      "evaluation/env_infos/initial/reward_run Std             0.564526\n",
      "evaluation/env_infos/initial/reward_run Max             0.99687\n",
      "evaluation/env_infos/initial/reward_run Min            -0.88276\n",
      "evaluation/env_infos/reward_run Mean                    0.0438064\n",
      "evaluation/env_infos/reward_run Std                     0.202783\n",
      "evaluation/env_infos/reward_run Max                     1.40322\n",
      "evaluation/env_infos/reward_run Min                    -1.72366\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.270911\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0754336\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.113904\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.401494\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.270615\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0849165\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0767672\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.461197\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.265089\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0828373\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0193374\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.461197\n",
      "evaluation/env_infos/final/height Mean                 -0.294315\n",
      "evaluation/env_infos/final/height Std                   0.137262\n",
      "evaluation/env_infos/final/height Max                  -0.127651\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00418314\n",
      "evaluation/env_infos/initial/height Std                 0.0581023\n",
      "evaluation/env_infos/initial/height Max                 0.0962184\n",
      "evaluation/env_infos/initial/height Min                -0.106613\n",
      "evaluation/env_infos/height Mean                       -0.287356\n",
      "evaluation/env_infos/height Std                         0.136773\n",
      "evaluation/env_infos/height Max                         0.233334\n",
      "evaluation/env_infos/height Min                        -0.578619\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0227311\n",
      "evaluation/env_infos/final/reward_angular Std           0.516879\n",
      "evaluation/env_infos/final/reward_angular Max           1.81372\n",
      "evaluation/env_infos/final/reward_angular Min          -1.32046\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.285508\n",
      "evaluation/env_infos/initial/reward_angular Std         1.76829\n",
      "evaluation/env_infos/initial/reward_angular Max         3.28255\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.0145\n",
      "evaluation/env_infos/reward_angular Mean                0.00700043\n",
      "evaluation/env_infos/reward_angular Std                 0.646872\n",
      "evaluation/env_infos/reward_angular Max                 5.37882\n",
      "evaluation/env_infos/reward_angular Min                -4.79748\n",
      "time/data storing (s)                                   0.0153795\n",
      "time/evaluation sampling (s)                           22.8627\n",
      "time/exploration sampling (s)                           1.12505\n",
      "time/logging (s)                                        0.234299\n",
      "time/saving (s)                                         0.0296286\n",
      "time/training (s)                                       4.15822\n",
      "time/epoch (s)                                         28.4253\n",
      "time/total (s)                                        387.985\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:50.344071 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.577147\n",
      "trainer/QF2 Loss                                        0.62556\n",
      "trainer/Policy Loss                                    -1.66617\n",
      "trainer/Q1 Predictions Mean                             2.75146\n",
      "trainer/Q1 Predictions Std                              1.40217\n",
      "trainer/Q1 Predictions Max                              7.60927\n",
      "trainer/Q1 Predictions Min                              0.247626\n",
      "trainer/Q2 Predictions Mean                             2.73843\n",
      "trainer/Q2 Predictions Std                              1.3904\n",
      "trainer/Q2 Predictions Max                              7.39643\n",
      "trainer/Q2 Predictions Min                             -0.369382\n",
      "trainer/Q Targets Mean                                  2.75468\n",
      "trainer/Q Targets Std                                   1.66063\n",
      "trainer/Q Targets Max                                  10.5376\n",
      "trainer/Q Targets Min                                  -0.707128\n",
      "trainer/Log Pis Mean                                    1.50785\n",
      "trainer/Log Pis Std                                     4.60254\n",
      "trainer/Log Pis Max                                    22.8324\n",
      "trainer/Log Pis Min                                    -7.13137\n",
      "trainer/Policy mu Mean                                 -0.056557\n",
      "trainer/Policy mu Std                                   1.16269\n",
      "trainer/Policy mu Max                                   3.78647\n",
      "trainer/Policy mu Min                                  -3.88354\n",
      "trainer/Policy log std Mean                            -0.460428\n",
      "trainer/Policy log std Std                              0.21314\n",
      "trainer/Policy log std Max                             -0.0139964\n",
      "trainer/Policy log std Min                             -1.27508\n",
      "trainer/Alpha                                           0.0306378\n",
      "trainer/Alpha Loss                                    -15.6494\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            15\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0278168\n",
      "exploration/Rewards Std                                 0.670919\n",
      "exploration/Rewards Max                                 2.3349\n",
      "exploration/Rewards Min                                -1.7654\n",
      "exploration/Returns Mean                              -27.8168\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -27.8168\n",
      "exploration/Returns Min                               -27.8168\n",
      "exploration/Actions Mean                                0.119004\n",
      "exploration/Actions Std                                 0.618165\n",
      "exploration/Actions Max                                 0.996166\n",
      "exploration/Actions Min                                -0.996919\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -27.8168\n",
      "exploration/env_infos/final/reward_run Mean            -0.254252\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.254252\n",
      "exploration/env_infos/final/reward_run Min             -0.254252\n",
      "exploration/env_infos/initial/reward_run Mean          -0.212611\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.212611\n",
      "exploration/env_infos/initial/reward_run Min           -0.212611\n",
      "exploration/env_infos/reward_run Mean                  -0.000383858\n",
      "exploration/env_infos/reward_run Std                    0.410062\n",
      "exploration/env_infos/reward_run Max                    1.33245\n",
      "exploration/env_infos/reward_run Min                   -1.1609\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.189041\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.189041\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.189041\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.283946\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.283946\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.283946\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.237774\n",
      "exploration/env_infos/reward_ctrl Std                   0.0768719\n",
      "exploration/env_infos/reward_ctrl Max                  -0.022046\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4591\n",
      "exploration/env_infos/final/height Mean                -0.31289\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.31289\n",
      "exploration/env_infos/final/height Min                 -0.31289\n",
      "exploration/env_infos/initial/height Mean              -0.0158694\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0158694\n",
      "exploration/env_infos/initial/height Min               -0.0158694\n",
      "exploration/env_infos/height Mean                      -0.250668\n",
      "exploration/env_infos/height Std                        0.0908399\n",
      "exploration/env_infos/height Max                        0.0562123\n",
      "exploration/env_infos/height Min                       -0.451531\n",
      "exploration/env_infos/final/reward_angular Mean         0.181549\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.181549\n",
      "exploration/env_infos/final/reward_angular Min          0.181549\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.91928\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.91928\n",
      "exploration/env_infos/initial/reward_angular Min       -0.91928\n",
      "exploration/env_infos/reward_angular Mean              -0.022187\n",
      "exploration/env_infos/reward_angular Std                1.13255\n",
      "exploration/env_infos/reward_angular Max                3.61859\n",
      "exploration/env_infos/reward_angular Min               -3.80514\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.206684\n",
      "evaluation/Rewards Std                                  0.773467\n",
      "evaluation/Rewards Max                                  8.00889\n",
      "evaluation/Rewards Min                                 -7.146\n",
      "evaluation/Returns Mean                              -206.684\n",
      "evaluation/Returns Std                                 94.7858\n",
      "evaluation/Returns Max                                -36.1494\n",
      "evaluation/Returns Min                               -382.976\n",
      "evaluation/Actions Mean                                -0.333514\n",
      "evaluation/Actions Std                                  0.617423\n",
      "evaluation/Actions Max                                  0.998597\n",
      "evaluation/Actions Min                                 -0.998637\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -206.684\n",
      "evaluation/env_infos/final/reward_run Mean             -0.120722\n",
      "evaluation/env_infos/final/reward_run Std               0.322446\n",
      "evaluation/env_infos/final/reward_run Max               0.364791\n",
      "evaluation/env_infos/final/reward_run Min              -0.999243\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.250662\n",
      "evaluation/env_infos/initial/reward_run Std             0.583556\n",
      "evaluation/env_infos/initial/reward_run Max             0.753415\n",
      "evaluation/env_infos/initial/reward_run Min            -1.04045\n",
      "evaluation/env_infos/reward_run Mean                    0.0177647\n",
      "evaluation/env_infos/reward_run Std                     0.478559\n",
      "evaluation/env_infos/reward_run Max                     3.66692\n",
      "evaluation/env_infos/reward_run Min                    -2.52735\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.301612\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0905276\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.171039\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.485625\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266347\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0822944\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.109426\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.38028\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.295465\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0823838\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0469743\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.543992\n",
      "evaluation/env_infos/final/height Mean                 -0.279483\n",
      "evaluation/env_infos/final/height Std                   0.118237\n",
      "evaluation/env_infos/final/height Max                  -0.123444\n",
      "evaluation/env_infos/final/height Min                  -0.556518\n",
      "evaluation/env_infos/initial/height Mean                0.00952989\n",
      "evaluation/env_infos/initial/height Std                 0.0541721\n",
      "evaluation/env_infos/initial/height Max                 0.0946882\n",
      "evaluation/env_infos/initial/height Min                -0.0811334\n",
      "evaluation/env_infos/height Mean                       -0.249891\n",
      "evaluation/env_infos/height Std                         0.114869\n",
      "evaluation/env_infos/height Max                         0.381994\n",
      "evaluation/env_infos/height Min                        -0.593067\n",
      "evaluation/env_infos/final/reward_angular Mean          0.144601\n",
      "evaluation/env_infos/final/reward_angular Std           1.34018\n",
      "evaluation/env_infos/final/reward_angular Max           5.01241\n",
      "evaluation/env_infos/final/reward_angular Min          -3.38418\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.966388\n",
      "evaluation/env_infos/initial/reward_angular Std         1.32543\n",
      "evaluation/env_infos/initial/reward_angular Max         3.97346\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.76722\n",
      "evaluation/env_infos/reward_angular Mean                0.0265442\n",
      "evaluation/env_infos/reward_angular Std                 1.34927\n",
      "evaluation/env_infos/reward_angular Max                 9.71043\n",
      "evaluation/env_infos/reward_angular Min                -8.77405\n",
      "time/data storing (s)                                   0.0148438\n",
      "time/evaluation sampling (s)                           23.3212\n",
      "time/exploration sampling (s)                           0.990002\n",
      "time/logging (s)                                        0.237899\n",
      "time/saving (s)                                         0.028773\n",
      "time/training (s)                                       4.83684\n",
      "time/epoch (s)                                         29.4296\n",
      "time/total (s)                                        417.61\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:18.908806 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.491169\n",
      "trainer/QF2 Loss                                        0.458346\n",
      "trainer/Policy Loss                                    -1.63522\n",
      "trainer/Q1 Predictions Mean                             2.81469\n",
      "trainer/Q1 Predictions Std                              1.43423\n",
      "trainer/Q1 Predictions Max                              7.34389\n",
      "trainer/Q1 Predictions Min                              0.062531\n",
      "trainer/Q2 Predictions Mean                             2.76964\n",
      "trainer/Q2 Predictions Std                              1.38745\n",
      "trainer/Q2 Predictions Max                              6.82272\n",
      "trainer/Q2 Predictions Min                             -0.336188\n",
      "trainer/Q Targets Mean                                  2.77112\n",
      "trainer/Q Targets Std                                   1.59817\n",
      "trainer/Q Targets Max                                   8.37291\n",
      "trainer/Q Targets Min                                  -1.05192\n",
      "trainer/Log Pis Mean                                    1.45395\n",
      "trainer/Log Pis Std                                     3.70353\n",
      "trainer/Log Pis Max                                    13.3107\n",
      "trainer/Log Pis Min                                    -7.61755\n",
      "trainer/Policy mu Mean                                  0.165463\n",
      "trainer/Policy mu Std                                   1.14121\n",
      "trainer/Policy mu Max                                   3.29922\n",
      "trainer/Policy mu Min                                  -3.00593\n",
      "trainer/Policy log std Mean                            -0.453192\n",
      "trainer/Policy log std Std                              0.20574\n",
      "trainer/Policy log std Max                              0.162698\n",
      "trainer/Policy log std Min                             -1.26236\n",
      "trainer/Alpha                                           0.0260164\n",
      "trainer/Alpha Loss                                    -16.581\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            16\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.198728\n",
      "exploration/Rewards Std                                 0.603783\n",
      "exploration/Rewards Max                                 1.67508\n",
      "exploration/Rewards Min                                -2.18602\n",
      "exploration/Returns Mean                             -198.728\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -198.728\n",
      "exploration/Returns Min                              -198.728\n",
      "exploration/Actions Mean                                0.22944\n",
      "exploration/Actions Std                                 0.632989\n",
      "exploration/Actions Max                                 0.998196\n",
      "exploration/Actions Min                                -0.997529\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -198.728\n",
      "exploration/env_infos/final/reward_run Mean            -0.383831\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.383831\n",
      "exploration/env_infos/final/reward_run Min             -0.383831\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0264874\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0264874\n",
      "exploration/env_infos/initial/reward_run Min           -0.0264874\n",
      "exploration/env_infos/reward_run Mean                   0.0154074\n",
      "exploration/env_infos/reward_run Std                    0.366945\n",
      "exploration/env_infos/reward_run Max                    1.3258\n",
      "exploration/env_infos/reward_run Min                   -1.1516\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.267353\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.267353\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.267353\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.333143\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.333143\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.333143\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.271991\n",
      "exploration/env_infos/reward_ctrl Std                   0.0730298\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0324728\n",
      "exploration/env_infos/reward_ctrl Min                  -0.499507\n",
      "exploration/env_infos/final/height Mean                -0.365404\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.365404\n",
      "exploration/env_infos/final/height Min                 -0.365404\n",
      "exploration/env_infos/initial/height Mean               0.0401786\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0401786\n",
      "exploration/env_infos/initial/height Min                0.0401786\n",
      "exploration/env_infos/height Mean                      -0.30245\n",
      "exploration/env_infos/height Std                        0.0626837\n",
      "exploration/env_infos/height Max                        0.0401786\n",
      "exploration/env_infos/height Min                       -0.428221\n",
      "exploration/env_infos/final/reward_angular Mean        -1.23734\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.23734\n",
      "exploration/env_infos/final/reward_angular Min         -1.23734\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.05504\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.05504\n",
      "exploration/env_infos/initial/reward_angular Min       -1.05504\n",
      "exploration/env_infos/reward_angular Mean              -0.0220392\n",
      "exploration/env_infos/reward_angular Std                1.04637\n",
      "exploration/env_infos/reward_angular Max                4.38598\n",
      "exploration/env_infos/reward_angular Min               -2.97391\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.183352\n",
      "evaluation/Rewards Std                                  0.614014\n",
      "evaluation/Rewards Max                                  3.62665\n",
      "evaluation/Rewards Min                                 -3.60486\n",
      "evaluation/Returns Mean                              -183.352\n",
      "evaluation/Returns Std                                105.76\n",
      "evaluation/Returns Max                                -27.0104\n",
      "evaluation/Returns Min                               -458.42\n",
      "evaluation/Actions Mean                                -0.195748\n",
      "evaluation/Actions Std                                  0.633004\n",
      "evaluation/Actions Max                                  0.996212\n",
      "evaluation/Actions Min                                 -0.998923\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -183.352\n",
      "evaluation/env_infos/final/reward_run Mean              0.00131965\n",
      "evaluation/env_infos/final/reward_run Std               0.206563\n",
      "evaluation/env_infos/final/reward_run Max               0.537152\n",
      "evaluation/env_infos/final/reward_run Min              -0.76737\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.274162\n",
      "evaluation/env_infos/initial/reward_run Std             0.487451\n",
      "evaluation/env_infos/initial/reward_run Max             0.795283\n",
      "evaluation/env_infos/initial/reward_run Min            -0.823373\n",
      "evaluation/env_infos/reward_run Mean                    0.00844339\n",
      "evaluation/env_infos/reward_run Std                     0.297454\n",
      "evaluation/env_infos/reward_run Max                     2.03182\n",
      "evaluation/env_infos/reward_run Min                    -2.58392\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.279832\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.090413\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0994533\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.460147\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.284042\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0674658\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0960828\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.38284\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.263407\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0928557\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.006108\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.501945\n",
      "evaluation/env_infos/final/height Mean                 -0.211148\n",
      "evaluation/env_infos/final/height Std                   0.11202\n",
      "evaluation/env_infos/final/height Max                   0.00985826\n",
      "evaluation/env_infos/final/height Min                  -0.519416\n",
      "evaluation/env_infos/initial/height Mean               -0.0127214\n",
      "evaluation/env_infos/initial/height Std                 0.0573272\n",
      "evaluation/env_infos/initial/height Max                 0.0921092\n",
      "evaluation/env_infos/initial/height Min                -0.112815\n",
      "evaluation/env_infos/height Mean                       -0.207064\n",
      "evaluation/env_infos/height Std                         0.0994032\n",
      "evaluation/env_infos/height Max                         0.143831\n",
      "evaluation/env_infos/height Min                        -0.594904\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0558546\n",
      "evaluation/env_infos/final/reward_angular Std           0.792966\n",
      "evaluation/env_infos/final/reward_angular Max           2.21868\n",
      "evaluation/env_infos/final/reward_angular Min          -1.59726\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.336481\n",
      "evaluation/env_infos/initial/reward_angular Std         1.28115\n",
      "evaluation/env_infos/initial/reward_angular Max         3.20401\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.73395\n",
      "evaluation/env_infos/reward_angular Mean                0.00131921\n",
      "evaluation/env_infos/reward_angular Std                 0.97358\n",
      "evaluation/env_infos/reward_angular Max                 6.75604\n",
      "evaluation/env_infos/reward_angular Min                -6.30449\n",
      "time/data storing (s)                                   0.0150071\n",
      "time/evaluation sampling (s)                           22.8385\n",
      "time/exploration sampling (s)                           1.15528\n",
      "time/logging (s)                                        0.234431\n",
      "time/saving (s)                                         0.0289808\n",
      "time/training (s)                                       4.05982\n",
      "time/epoch (s)                                         28.3321\n",
      "time/total (s)                                        446.17\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:47.556131 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  17000\n",
      "trainer/QF1 Loss                                        0.487767\n",
      "trainer/QF2 Loss                                        0.510808\n",
      "trainer/Policy Loss                                     0.40068\n",
      "trainer/Q1 Predictions Mean                             2.67481\n",
      "trainer/Q1 Predictions Std                              1.50279\n",
      "trainer/Q1 Predictions Max                             10.7883\n",
      "trainer/Q1 Predictions Min                              0.0572906\n",
      "trainer/Q2 Predictions Mean                             2.74056\n",
      "trainer/Q2 Predictions Std                              1.53948\n",
      "trainer/Q2 Predictions Max                             10.7192\n",
      "trainer/Q2 Predictions Min                             -0.0677769\n",
      "trainer/Q Targets Mean                                  2.65408\n",
      "trainer/Q Targets Std                                   1.66346\n",
      "trainer/Q Targets Max                                  12.4668\n",
      "trainer/Q Targets Min                                  -1.2054\n",
      "trainer/Log Pis Mean                                    3.5104\n",
      "trainer/Log Pis Std                                     4.37045\n",
      "trainer/Log Pis Max                                    19.69\n",
      "trainer/Log Pis Min                                    -5.54057\n",
      "trainer/Policy mu Mean                                 -0.310188\n",
      "trainer/Policy mu Std                                   1.28518\n",
      "trainer/Policy mu Max                                   3.09698\n",
      "trainer/Policy mu Min                                  -3.96767\n",
      "trainer/Policy log std Mean                            -0.606224\n",
      "trainer/Policy log std Std                              0.236122\n",
      "trainer/Policy log std Max                             -0.0941598\n",
      "trainer/Policy log std Min                             -1.56565\n",
      "trainer/Alpha                                           0.0225654\n",
      "trainer/Alpha Loss                                     -9.43609\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            17\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.174255\n",
      "exploration/Rewards Std                                 0.117138\n",
      "exploration/Rewards Max                                 0.535287\n",
      "exploration/Rewards Min                                -0.625845\n",
      "exploration/Returns Mean                             -174.255\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -174.255\n",
      "exploration/Returns Min                              -174.255\n",
      "exploration/Actions Mean                               -0.574097\n",
      "exploration/Actions Std                                 0.575534\n",
      "exploration/Actions Max                                 0.989937\n",
      "exploration/Actions Min                                -0.99706\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -174.255\n",
      "exploration/env_infos/final/reward_run Mean            -0.309725\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.309725\n",
      "exploration/env_infos/final/reward_run Min             -0.309725\n",
      "exploration/env_infos/initial/reward_run Mean          -0.833147\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.833147\n",
      "exploration/env_infos/initial/reward_run Min           -0.833147\n",
      "exploration/env_infos/reward_run Mean                  -0.0765004\n",
      "exploration/env_infos/reward_run Std                    0.169522\n",
      "exploration/env_infos/reward_run Max                    0.517506\n",
      "exploration/env_infos/reward_run Min                   -1.0301\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.340832\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.340832\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.340832\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.446242\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.446242\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.446242\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.396496\n",
      "exploration/env_infos/reward_ctrl Std                   0.0565271\n",
      "exploration/env_infos/reward_ctrl Max                  -0.16857\n",
      "exploration/env_infos/reward_ctrl Min                  -0.552172\n",
      "exploration/env_infos/final/height Mean                -0.227902\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.227902\n",
      "exploration/env_infos/final/height Min                 -0.227902\n",
      "exploration/env_infos/initial/height Mean              -0.00420185\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00420185\n",
      "exploration/env_infos/initial/height Min               -0.00420185\n",
      "exploration/env_infos/height Mean                      -0.210703\n",
      "exploration/env_infos/height Std                        0.0185301\n",
      "exploration/env_infos/height Max                       -0.00420185\n",
      "exploration/env_infos/height Min                       -0.254181\n",
      "exploration/env_infos/final/reward_angular Mean         0.713645\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.713645\n",
      "exploration/env_infos/final/reward_angular Min          0.713645\n",
      "exploration/env_infos/initial/reward_angular Mean       0.947772\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.947772\n",
      "exploration/env_infos/initial/reward_angular Min        0.947772\n",
      "exploration/env_infos/reward_angular Mean               0.0161348\n",
      "exploration/env_infos/reward_angular Std                0.610436\n",
      "exploration/env_infos/reward_angular Max                2.65306\n",
      "exploration/env_infos/reward_angular Min               -2.51692\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.236651\n",
      "evaluation/Rewards Std                                  0.414026\n",
      "evaluation/Rewards Max                                  4.032\n",
      "evaluation/Rewards Min                                 -3.69819\n",
      "evaluation/Returns Mean                              -236.651\n",
      "evaluation/Returns Std                                 93.0383\n",
      "evaluation/Returns Max                                -58.0871\n",
      "evaluation/Returns Min                               -480.477\n",
      "evaluation/Actions Mean                                -0.311994\n",
      "evaluation/Actions Std                                  0.709237\n",
      "evaluation/Actions Max                                  0.999776\n",
      "evaluation/Actions Min                                 -0.999909\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -236.651\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0373786\n",
      "evaluation/env_infos/final/reward_run Std               0.360317\n",
      "evaluation/env_infos/final/reward_run Max               1.21381\n",
      "evaluation/env_infos/final/reward_run Min              -0.901827\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.190386\n",
      "evaluation/env_infos/initial/reward_run Std             0.493886\n",
      "evaluation/env_infos/initial/reward_run Max             1.06073\n",
      "evaluation/env_infos/initial/reward_run Min            -1.10514\n",
      "evaluation/env_infos/reward_run Mean                    0.0436491\n",
      "evaluation/env_infos/reward_run Std                     0.378365\n",
      "evaluation/env_infos/reward_run Max                     2.81998\n",
      "evaluation/env_infos/reward_run Min                    -2.20625\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.364523\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101952\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.114489\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.502703\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.321136\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0938392\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.140246\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.485837\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.360215\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109766\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0300755\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.556528\n",
      "evaluation/env_infos/final/height Mean                 -0.24595\n",
      "evaluation/env_infos/final/height Std                   0.101514\n",
      "evaluation/env_infos/final/height Max                  -0.133139\n",
      "evaluation/env_infos/final/height Min                  -0.541593\n",
      "evaluation/env_infos/initial/height Mean               -0.00326818\n",
      "evaluation/env_infos/initial/height Std                 0.0553187\n",
      "evaluation/env_infos/initial/height Max                 0.090568\n",
      "evaluation/env_infos/initial/height Min                -0.112105\n",
      "evaluation/env_infos/height Mean                       -0.239686\n",
      "evaluation/env_infos/height Std                         0.0979602\n",
      "evaluation/env_infos/height Max                         0.192323\n",
      "evaluation/env_infos/height Min                        -0.597291\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.131632\n",
      "evaluation/env_infos/final/reward_angular Std           1.0435\n",
      "evaluation/env_infos/final/reward_angular Max           1.96469\n",
      "evaluation/env_infos/final/reward_angular Min          -4.68451\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.655111\n",
      "evaluation/env_infos/initial/reward_angular Std         0.742244\n",
      "evaluation/env_infos/initial/reward_angular Max         1.9156\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.740101\n",
      "evaluation/env_infos/reward_angular Mean                0.0069941\n",
      "evaluation/env_infos/reward_angular Std                 0.759907\n",
      "evaluation/env_infos/reward_angular Max                 6.41049\n",
      "evaluation/env_infos/reward_angular Min                -5.70705\n",
      "time/data storing (s)                                   0.0155025\n",
      "time/evaluation sampling (s)                           22.9195\n",
      "time/exploration sampling (s)                           1.20098\n",
      "time/logging (s)                                        0.234405\n",
      "time/saving (s)                                         0.0293477\n",
      "time/training (s)                                       4.03771\n",
      "time/epoch (s)                                         28.4374\n",
      "time/total (s)                                        474.816\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:15.722673 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.572728\n",
      "trainer/QF2 Loss                                        0.548747\n",
      "trainer/Policy Loss                                    -0.510618\n",
      "trainer/Q1 Predictions Mean                             2.78083\n",
      "trainer/Q1 Predictions Std                              1.57291\n",
      "trainer/Q1 Predictions Max                              8.13888\n",
      "trainer/Q1 Predictions Min                              0.110418\n",
      "trainer/Q2 Predictions Mean                             2.82488\n",
      "trainer/Q2 Predictions Std                              1.56997\n",
      "trainer/Q2 Predictions Max                              7.53182\n",
      "trainer/Q2 Predictions Min                              0.112843\n",
      "trainer/Q Targets Mean                                  2.73158\n",
      "trainer/Q Targets Std                                   1.73712\n",
      "trainer/Q Targets Max                                   8.08794\n",
      "trainer/Q Targets Min                                  -1.10875\n",
      "trainer/Log Pis Mean                                    2.70775\n",
      "trainer/Log Pis Std                                     4.16994\n",
      "trainer/Log Pis Max                                    21.2366\n",
      "trainer/Log Pis Min                                    -5.51415\n",
      "trainer/Policy mu Mean                                  0.0960156\n",
      "trainer/Policy mu Std                                   1.25513\n",
      "trainer/Policy mu Max                                   3.81087\n",
      "trainer/Policy mu Min                                  -2.8442\n",
      "trainer/Policy log std Mean                            -0.573248\n",
      "trainer/Policy log std Std                              0.23115\n",
      "trainer/Policy log std Max                              0.102688\n",
      "trainer/Policy log std Min                             -1.45299\n",
      "trainer/Alpha                                           0.0198029\n",
      "trainer/Alpha Loss                                    -12.9079\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            18\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.226407\n",
      "exploration/Rewards Std                                 1.35289\n",
      "exploration/Rewards Max                                 4.17049\n",
      "exploration/Rewards Min                                -3.93198\n",
      "exploration/Returns Mean                             -226.407\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -226.407\n",
      "exploration/Returns Min                              -226.407\n",
      "exploration/Actions Mean                                0.193894\n",
      "exploration/Actions Std                                 0.711934\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -0.999867\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -226.407\n",
      "exploration/env_infos/final/reward_run Mean             0.144763\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.144763\n",
      "exploration/env_infos/final/reward_run Min              0.144763\n",
      "exploration/env_infos/initial/reward_run Mean           0.787944\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.787944\n",
      "exploration/env_infos/initial/reward_run Min            0.787944\n",
      "exploration/env_infos/reward_run Mean                  -0.121743\n",
      "exploration/env_infos/reward_run Std                    0.949961\n",
      "exploration/env_infos/reward_run Max                    2.65537\n",
      "exploration/env_infos/reward_run Min                   -2.60802\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.153985\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.153985\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.153985\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.321866\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.321866\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.321866\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.326667\n",
      "exploration/env_infos/reward_ctrl Std                   0.104799\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0559081\n",
      "exploration/env_infos/reward_ctrl Min                  -0.586648\n",
      "exploration/env_infos/final/height Mean                -0.357671\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.357671\n",
      "exploration/env_infos/final/height Min                 -0.357671\n",
      "exploration/env_infos/initial/height Mean              -0.0153677\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0153677\n",
      "exploration/env_infos/initial/height Min               -0.0153677\n",
      "exploration/env_infos/height Mean                      -0.37967\n",
      "exploration/env_infos/height Std                        0.158577\n",
      "exploration/env_infos/height Max                        0.2181\n",
      "exploration/env_infos/height Min                       -0.587845\n",
      "exploration/env_infos/final/reward_angular Mean         2.58583\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.58583\n",
      "exploration/env_infos/final/reward_angular Min          2.58583\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.237863\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.237863\n",
      "exploration/env_infos/initial/reward_angular Min       -0.237863\n",
      "exploration/env_infos/reward_angular Mean               0.0246491\n",
      "exploration/env_infos/reward_angular Std                2.5916\n",
      "exploration/env_infos/reward_angular Max                8.05246\n",
      "exploration/env_infos/reward_angular Min               -6.41686\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.202419\n",
      "evaluation/Rewards Std                                  1.17097\n",
      "evaluation/Rewards Max                                  7.88136\n",
      "evaluation/Rewards Min                                 -7.46194\n",
      "evaluation/Returns Mean                              -202.419\n",
      "evaluation/Returns Std                                 95.963\n",
      "evaluation/Returns Max                                 -3.57365\n",
      "evaluation/Returns Min                               -382.119\n",
      "evaluation/Actions Mean                                -0.10722\n",
      "evaluation/Actions Std                                  0.708642\n",
      "evaluation/Actions Max                                  0.999966\n",
      "evaluation/Actions Min                                 -0.999944\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -202.419\n",
      "evaluation/env_infos/final/reward_run Mean              0.0364216\n",
      "evaluation/env_infos/final/reward_run Std               0.703934\n",
      "evaluation/env_infos/final/reward_run Max               2.49418\n",
      "evaluation/env_infos/final/reward_run Min              -1.66633\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.105231\n",
      "evaluation/env_infos/initial/reward_run Std             0.388031\n",
      "evaluation/env_infos/initial/reward_run Max             0.604375\n",
      "evaluation/env_infos/initial/reward_run Min            -0.786328\n",
      "evaluation/env_infos/reward_run Mean                   -0.00133738\n",
      "evaluation/env_infos/reward_run Std                     0.597259\n",
      "evaluation/env_infos/reward_run Max                     4.30194\n",
      "evaluation/env_infos/reward_run Min                    -3.12226\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.309136\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101272\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0825489\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.576061\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.247989\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.074812\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0837564\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.357023\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.308202\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0983888\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.011505\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596092\n",
      "evaluation/env_infos/final/height Mean                 -0.274526\n",
      "evaluation/env_infos/final/height Std                   0.119382\n",
      "evaluation/env_infos/final/height Max                  -0.092741\n",
      "evaluation/env_infos/final/height Min                  -0.520601\n",
      "evaluation/env_infos/initial/height Mean               -0.0125183\n",
      "evaluation/env_infos/initial/height Std                 0.05196\n",
      "evaluation/env_infos/initial/height Max                 0.0891309\n",
      "evaluation/env_infos/initial/height Min                -0.101204\n",
      "evaluation/env_infos/height Mean                       -0.261873\n",
      "evaluation/env_infos/height Std                         0.13271\n",
      "evaluation/env_infos/height Max                         0.392002\n",
      "evaluation/env_infos/height Min                        -0.596167\n",
      "evaluation/env_infos/final/reward_angular Mean          0.413185\n",
      "evaluation/env_infos/final/reward_angular Std           2.13542\n",
      "evaluation/env_infos/final/reward_angular Max           6.9724\n",
      "evaluation/env_infos/final/reward_angular Min          -6.68166\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.397276\n",
      "evaluation/env_infos/initial/reward_angular Std         0.93673\n",
      "evaluation/env_infos/initial/reward_angular Max         3.20156\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.35309\n",
      "evaluation/env_infos/reward_angular Mean                0.0418981\n",
      "evaluation/env_infos/reward_angular Std                 1.5876\n",
      "evaluation/env_infos/reward_angular Max                 8.8914\n",
      "evaluation/env_infos/reward_angular Min                -8.22886\n",
      "time/data storing (s)                                   0.0149804\n",
      "time/evaluation sampling (s)                           22.7664\n",
      "time/exploration sampling (s)                           1.00831\n",
      "time/logging (s)                                        0.230746\n",
      "time/saving (s)                                         0.0274281\n",
      "time/training (s)                                       3.89168\n",
      "time/epoch (s)                                         27.9396\n",
      "time/total (s)                                        502.978\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:43.858560 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  19000\n",
      "trainer/QF1 Loss                                        0.40942\n",
      "trainer/QF2 Loss                                        0.422275\n",
      "trainer/Policy Loss                                     0.525523\n",
      "trainer/Q1 Predictions Mean                             2.64444\n",
      "trainer/Q1 Predictions Std                              1.56305\n",
      "trainer/Q1 Predictions Max                              7.86381\n",
      "trainer/Q1 Predictions Min                              0.102036\n",
      "trainer/Q2 Predictions Mean                             2.59539\n",
      "trainer/Q2 Predictions Std                              1.55554\n",
      "trainer/Q2 Predictions Max                              7.73058\n",
      "trainer/Q2 Predictions Min                              0.126768\n",
      "trainer/Q Targets Mean                                  2.62273\n",
      "trainer/Q Targets Std                                   1.67199\n",
      "trainer/Q Targets Max                                   8.18254\n",
      "trainer/Q Targets Min                                  -0.761314\n",
      "trainer/Log Pis Mean                                    3.57829\n",
      "trainer/Log Pis Std                                     4.42207\n",
      "trainer/Log Pis Max                                    17.8167\n",
      "trainer/Log Pis Min                                    -5.87204\n",
      "trainer/Policy mu Mean                                 -0.31434\n",
      "trainer/Policy mu Std                                   1.26253\n",
      "trainer/Policy mu Max                                   3.4465\n",
      "trainer/Policy mu Min                                  -3.70878\n",
      "trainer/Policy log std Mean                            -0.691003\n",
      "trainer/Policy log std Std                              0.285043\n",
      "trainer/Policy log std Max                              0.0829471\n",
      "trainer/Policy log std Min                             -1.86183\n",
      "trainer/Alpha                                           0.0177091\n",
      "trainer/Alpha Loss                                     -9.76584\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            19\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.205947\n",
      "exploration/Rewards Std                                 0.19006\n",
      "exploration/Rewards Max                                 0.358404\n",
      "exploration/Rewards Min                                -0.945016\n",
      "exploration/Returns Mean                             -205.947\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -205.947\n",
      "exploration/Returns Min                              -205.947\n",
      "exploration/Actions Mean                               -0.224399\n",
      "exploration/Actions Std                                 0.5285\n",
      "exploration/Actions Max                                 0.992105\n",
      "exploration/Actions Min                                -0.991862\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -205.947\n",
      "exploration/env_infos/final/reward_run Mean            -0.00496155\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.00496155\n",
      "exploration/env_infos/final/reward_run Min             -0.00496155\n",
      "exploration/env_infos/initial/reward_run Mean          -0.335281\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.335281\n",
      "exploration/env_infos/initial/reward_run Min           -0.335281\n",
      "exploration/env_infos/reward_run Mean                  -0.0232906\n",
      "exploration/env_infos/reward_run Std                    0.465421\n",
      "exploration/env_infos/reward_run Max                    1.61714\n",
      "exploration/env_infos/reward_run Min                   -1.41515\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.144383\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.144383\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.144383\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.209083\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.209083\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.209083\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.1978\n",
      "exploration/env_infos/reward_ctrl Std                   0.0685047\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0232006\n",
      "exploration/env_infos/reward_ctrl Min                  -0.460132\n",
      "exploration/env_infos/final/height Mean                -0.165387\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.165387\n",
      "exploration/env_infos/final/height Min                 -0.165387\n",
      "exploration/env_infos/initial/height Mean               0.0792449\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0792449\n",
      "exploration/env_infos/initial/height Min                0.0792449\n",
      "exploration/env_infos/height Mean                      -0.125752\n",
      "exploration/env_infos/height Std                        0.0410335\n",
      "exploration/env_infos/height Max                        0.0792449\n",
      "exploration/env_infos/height Min                       -0.247189\n",
      "exploration/env_infos/final/reward_angular Mean         0.236228\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.236228\n",
      "exploration/env_infos/final/reward_angular Min          0.236228\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0708612\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0708612\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0708612\n",
      "exploration/env_infos/reward_angular Mean               0.00178092\n",
      "exploration/env_infos/reward_angular Std                0.932945\n",
      "exploration/env_infos/reward_angular Max                3.74919\n",
      "exploration/env_infos/reward_angular Min               -2.51673\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.246189\n",
      "evaluation/Rewards Std                                  0.918351\n",
      "evaluation/Rewards Max                                  7.93945\n",
      "evaluation/Rewards Min                                 -5.80787\n",
      "evaluation/Returns Mean                              -246.189\n",
      "evaluation/Returns Std                                138.129\n",
      "evaluation/Returns Max                                -47.1048\n",
      "evaluation/Returns Min                               -599.196\n",
      "evaluation/Actions Mean                                -0.177067\n",
      "evaluation/Actions Std                                  0.727333\n",
      "evaluation/Actions Max                                  0.999988\n",
      "evaluation/Actions Min                                 -0.999984\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -246.189\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0307523\n",
      "evaluation/env_infos/final/reward_run Std               0.33355\n",
      "evaluation/env_infos/final/reward_run Max               1.03044\n",
      "evaluation/env_infos/final/reward_run Min              -0.777504\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.340481\n",
      "evaluation/env_infos/initial/reward_run Std             0.447354\n",
      "evaluation/env_infos/initial/reward_run Max             0.772796\n",
      "evaluation/env_infos/initial/reward_run Min            -0.939478\n",
      "evaluation/env_infos/reward_run Mean                   -0.115995\n",
      "evaluation/env_infos/reward_run Std                     0.652391\n",
      "evaluation/env_infos/reward_run Max                     3.43078\n",
      "evaluation/env_infos/reward_run Min                    -3.82307\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.340883\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0881029\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.1068\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.466216\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266148\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107382\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0491941\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.394352\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.336219\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0955403\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00827807\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.588391\n",
      "evaluation/env_infos/final/height Mean                 -0.242305\n",
      "evaluation/env_infos/final/height Std                   0.124716\n",
      "evaluation/env_infos/final/height Max                  -0.118559\n",
      "evaluation/env_infos/final/height Min                  -0.574803\n",
      "evaluation/env_infos/initial/height Mean               -0.00721206\n",
      "evaluation/env_infos/initial/height Std                 0.0524488\n",
      "evaluation/env_infos/initial/height Max                 0.0951835\n",
      "evaluation/env_infos/initial/height Min                -0.107119\n",
      "evaluation/env_infos/height Mean                       -0.237119\n",
      "evaluation/env_infos/height Std                         0.137578\n",
      "evaluation/env_infos/height Max                         0.386115\n",
      "evaluation/env_infos/height Min                        -0.593109\n",
      "evaluation/env_infos/final/reward_angular Mean          0.415198\n",
      "evaluation/env_infos/final/reward_angular Std           1.293\n",
      "evaluation/env_infos/final/reward_angular Max           4.31146\n",
      "evaluation/env_infos/final/reward_angular Min          -2.51902\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.676895\n",
      "evaluation/env_infos/initial/reward_angular Std         0.704883\n",
      "evaluation/env_infos/initial/reward_angular Max         1.69465\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.21557\n",
      "evaluation/env_infos/reward_angular Mean                0.0380127\n",
      "evaluation/env_infos/reward_angular Std                 1.2408\n",
      "evaluation/env_infos/reward_angular Max                 9.12125\n",
      "evaluation/env_infos/reward_angular Min                -5.97525\n",
      "time/data storing (s)                                   0.0144383\n",
      "time/evaluation sampling (s)                           22.2264\n",
      "time/exploration sampling (s)                           1.10894\n",
      "time/logging (s)                                        0.232391\n",
      "time/saving (s)                                         0.0270515\n",
      "time/training (s)                                       4.31019\n",
      "time/epoch (s)                                         27.9194\n",
      "time/total (s)                                        531.114\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:15.610641 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.530851\n",
      "trainer/QF2 Loss                                        0.564681\n",
      "trainer/Policy Loss                                     0.116953\n",
      "trainer/Q1 Predictions Mean                             2.86826\n",
      "trainer/Q1 Predictions Std                              1.80761\n",
      "trainer/Q1 Predictions Max                              9.78724\n",
      "trainer/Q1 Predictions Min                              0.0296523\n",
      "trainer/Q2 Predictions Mean                             2.91105\n",
      "trainer/Q2 Predictions Std                              1.76634\n",
      "trainer/Q2 Predictions Max                              9.49061\n",
      "trainer/Q2 Predictions Min                              0.0742811\n",
      "trainer/Q Targets Mean                                  2.80842\n",
      "trainer/Q Targets Std                                   1.88354\n",
      "trainer/Q Targets Max                                   8.43046\n",
      "trainer/Q Targets Min                                  -0.972922\n",
      "trainer/Log Pis Mean                                    3.38335\n",
      "trainer/Log Pis Std                                     4.76186\n",
      "trainer/Log Pis Max                                    22.8404\n",
      "trainer/Log Pis Min                                    -7.34804\n",
      "trainer/Policy mu Mean                                 -0.216416\n",
      "trainer/Policy mu Std                                   1.28396\n",
      "trainer/Policy mu Max                                   5.09298\n",
      "trainer/Policy mu Min                                  -4.62136\n",
      "trainer/Policy log std Mean                            -0.649628\n",
      "trainer/Policy log std Std                              0.286127\n",
      "trainer/Policy log std Max                              0.086178\n",
      "trainer/Policy log std Min                             -1.89591\n",
      "trainer/Alpha                                           0.0158419\n",
      "trainer/Alpha Loss                                    -10.8434\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                            20\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.146314\n",
      "exploration/Rewards Std                                 0.415093\n",
      "exploration/Rewards Max                                 1.31976\n",
      "exploration/Rewards Min                                -1.85159\n",
      "exploration/Returns Mean                             -146.314\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -146.314\n",
      "exploration/Returns Min                              -146.314\n",
      "exploration/Actions Mean                               -0.205924\n",
      "exploration/Actions Std                                 0.711211\n",
      "exploration/Actions Max                                 0.994327\n",
      "exploration/Actions Min                                -0.996688\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -146.314\n",
      "exploration/env_infos/final/reward_run Mean            -0.000527322\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.000527322\n",
      "exploration/env_infos/final/reward_run Min             -0.000527322\n",
      "exploration/env_infos/initial/reward_run Mean          -0.925488\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.925488\n",
      "exploration/env_infos/initial/reward_run Min           -0.925488\n",
      "exploration/env_infos/reward_run Mean                  -0.039319\n",
      "exploration/env_infos/reward_run Std                    0.320927\n",
      "exploration/env_infos/reward_run Max                    1.01661\n",
      "exploration/env_infos/reward_run Min                   -1.50356\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.28108\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.28108\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.28108\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.325451\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.325451\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.325451\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.328935\n",
      "exploration/env_infos/reward_ctrl Std                   0.0687009\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0904627\n",
      "exploration/env_infos/reward_ctrl Min                  -0.529167\n",
      "exploration/env_infos/final/height Mean                -0.142478\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.142478\n",
      "exploration/env_infos/final/height Min                 -0.142478\n",
      "exploration/env_infos/initial/height Mean              -0.0361601\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0361601\n",
      "exploration/env_infos/initial/height Min               -0.0361601\n",
      "exploration/env_infos/height Mean                      -0.11185\n",
      "exploration/env_infos/height Std                        0.0298006\n",
      "exploration/env_infos/height Max                       -0.0166043\n",
      "exploration/env_infos/height Min                       -0.210846\n",
      "exploration/env_infos/final/reward_angular Mean         1.12544\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.12544\n",
      "exploration/env_infos/final/reward_angular Min          1.12544\n",
      "exploration/env_infos/initial/reward_angular Mean       0.70296\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.70296\n",
      "exploration/env_infos/initial/reward_angular Min        0.70296\n",
      "exploration/env_infos/reward_angular Mean               0.00143999\n",
      "exploration/env_infos/reward_angular Std                0.924435\n",
      "exploration/env_infos/reward_angular Max                3.64099\n",
      "exploration/env_infos/reward_angular Min               -3.73513\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.225533\n",
      "evaluation/Rewards Std                                  0.951533\n",
      "evaluation/Rewards Max                                  7.68467\n",
      "evaluation/Rewards Min                                 -6.47677\n",
      "evaluation/Returns Mean                              -225.533\n",
      "evaluation/Returns Std                                120.198\n",
      "evaluation/Returns Max                                 23.6546\n",
      "evaluation/Returns Min                               -435.636\n",
      "evaluation/Actions Mean                                -0.183564\n",
      "evaluation/Actions Std                                  0.73272\n",
      "evaluation/Actions Max                                  0.999992\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -225.533\n",
      "evaluation/env_infos/final/reward_run Mean             -0.140459\n",
      "evaluation/env_infos/final/reward_run Std               0.382935\n",
      "evaluation/env_infos/final/reward_run Max               0.399338\n",
      "evaluation/env_infos/final/reward_run Min              -1.40501\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.319172\n",
      "evaluation/env_infos/initial/reward_run Std             0.64444\n",
      "evaluation/env_infos/initial/reward_run Max             0.791865\n",
      "evaluation/env_infos/initial/reward_run Min            -1.1802\n",
      "evaluation/env_infos/reward_run Mean                   -0.093606\n",
      "evaluation/env_infos/reward_run Std                     0.585128\n",
      "evaluation/env_infos/reward_run Max                     2.98276\n",
      "evaluation/env_infos/reward_run Min                    -3.56248\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.348992\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0873392\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0982809\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.521402\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.261386\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0786875\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.130124\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.435516\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.342344\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0836626\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0259206\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.570217\n",
      "evaluation/env_infos/final/height Mean                 -0.266138\n",
      "evaluation/env_infos/final/height Std                   0.143364\n",
      "evaluation/env_infos/final/height Max                  -0.0452831\n",
      "evaluation/env_infos/final/height Min                  -0.575787\n",
      "evaluation/env_infos/initial/height Mean               -0.00907833\n",
      "evaluation/env_infos/initial/height Std                 0.0555739\n",
      "evaluation/env_infos/initial/height Max                 0.0844966\n",
      "evaluation/env_infos/initial/height Min                -0.0930511\n",
      "evaluation/env_infos/height Mean                       -0.247594\n",
      "evaluation/env_infos/height Std                         0.136537\n",
      "evaluation/env_infos/height Max                         0.404198\n",
      "evaluation/env_infos/height Min                        -0.594776\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.200094\n",
      "evaluation/env_infos/final/reward_angular Std           1.12697\n",
      "evaluation/env_infos/final/reward_angular Max           1.79797\n",
      "evaluation/env_infos/final/reward_angular Min          -3.96854\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.738224\n",
      "evaluation/env_infos/initial/reward_angular Std         1.14094\n",
      "evaluation/env_infos/initial/reward_angular Max         2.80822\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.58632\n",
      "evaluation/env_infos/reward_angular Mean                0.0446179\n",
      "evaluation/env_infos/reward_angular Std                 1.30543\n",
      "evaluation/env_infos/reward_angular Max                 8.6265\n",
      "evaluation/env_infos/reward_angular Min                -7.08539\n",
      "time/data storing (s)                                   0.0143853\n",
      "time/evaluation sampling (s)                           26.0707\n",
      "time/exploration sampling (s)                           1.13846\n",
      "time/logging (s)                                        0.239248\n",
      "time/saving (s)                                         0.0283032\n",
      "time/training (s)                                       4.0444\n",
      "time/epoch (s)                                         31.5355\n",
      "time/total (s)                                        562.872\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:44.181640 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  21000\n",
      "trainer/QF1 Loss                                        0.816884\n",
      "trainer/QF2 Loss                                        0.779302\n",
      "trainer/Policy Loss                                     1.55165\n",
      "trainer/Q1 Predictions Mean                             3.16724\n",
      "trainer/Q1 Predictions Std                              1.78574\n",
      "trainer/Q1 Predictions Max                              9.87102\n",
      "trainer/Q1 Predictions Min                              0.0437747\n",
      "trainer/Q2 Predictions Mean                             3.18917\n",
      "trainer/Q2 Predictions Std                              1.76849\n",
      "trainer/Q2 Predictions Max                              8.87745\n",
      "trainer/Q2 Predictions Min                              0.106869\n",
      "trainer/Q Targets Mean                                  2.95194\n",
      "trainer/Q Targets Std                                   1.95086\n",
      "trainer/Q Targets Max                                  11.0431\n",
      "trainer/Q Targets Min                                  -2.42434\n",
      "trainer/Log Pis Mean                                    5.16773\n",
      "trainer/Log Pis Std                                     4.53451\n",
      "trainer/Log Pis Max                                    18.41\n",
      "trainer/Log Pis Min                                    -5.48783\n",
      "trainer/Policy mu Mean                                 -0.275008\n",
      "trainer/Policy mu Std                                   1.47714\n",
      "trainer/Policy mu Max                                   4.29432\n",
      "trainer/Policy mu Min                                  -4.99152\n",
      "trainer/Policy log std Mean                            -0.689939\n",
      "trainer/Policy log std Std                              0.309739\n",
      "trainer/Policy log std Max                              0.0225417\n",
      "trainer/Policy log std Min                             -1.80488\n",
      "trainer/Alpha                                           0.0143044\n",
      "trainer/Alpha Loss                                     -3.53405\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                            21\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.314385\n",
      "exploration/Rewards Std                                 0.616993\n",
      "exploration/Rewards Max                                 1.43999\n",
      "exploration/Rewards Min                                -2.58777\n",
      "exploration/Returns Mean                             -314.385\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -314.385\n",
      "exploration/Returns Min                              -314.385\n",
      "exploration/Actions Mean                               -0.337234\n",
      "exploration/Actions Std                                 0.711065\n",
      "exploration/Actions Max                                 0.999914\n",
      "exploration/Actions Min                                -0.999938\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -314.385\n",
      "exploration/env_infos/final/reward_run Mean             0.809098\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.809098\n",
      "exploration/env_infos/final/reward_run Min              0.809098\n",
      "exploration/env_infos/initial/reward_run Mean          -0.74699\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.74699\n",
      "exploration/env_infos/initial/reward_run Min           -0.74699\n",
      "exploration/env_infos/reward_run Mean                   0.374859\n",
      "exploration/env_infos/reward_run Std                    0.80844\n",
      "exploration/env_infos/reward_run Max                    2.87008\n",
      "exploration/env_infos/reward_run Min                   -2.01594\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.437967\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.437967\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.437967\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.256979\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.256979\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.256979\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.371604\n",
      "exploration/env_infos/reward_ctrl Std                   0.079348\n",
      "exploration/env_infos/reward_ctrl Max                  -0.112114\n",
      "exploration/env_infos/reward_ctrl Min                  -0.573011\n",
      "exploration/env_infos/final/height Mean                -0.0330249\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0330249\n",
      "exploration/env_infos/final/height Min                 -0.0330249\n",
      "exploration/env_infos/initial/height Mean              -0.0183306\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0183306\n",
      "exploration/env_infos/initial/height Min               -0.0183306\n",
      "exploration/env_infos/height Mean                      -0.0853773\n",
      "exploration/env_infos/height Std                        0.0852069\n",
      "exploration/env_infos/height Max                        0.208033\n",
      "exploration/env_infos/height Min                       -0.234136\n",
      "exploration/env_infos/final/reward_angular Mean        -2.15204\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.15204\n",
      "exploration/env_infos/final/reward_angular Min         -2.15204\n",
      "exploration/env_infos/initial/reward_angular Mean       1.31884\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.31884\n",
      "exploration/env_infos/initial/reward_angular Min        1.31884\n",
      "exploration/env_infos/reward_angular Mean              -0.00653852\n",
      "exploration/env_infos/reward_angular Std                1.27993\n",
      "exploration/env_infos/reward_angular Max                4.77647\n",
      "exploration/env_infos/reward_angular Min               -3.69215\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.225787\n",
      "evaluation/Rewards Std                                  0.993504\n",
      "evaluation/Rewards Max                                  7.57116\n",
      "evaluation/Rewards Min                                 -6.27049\n",
      "evaluation/Returns Mean                              -225.787\n",
      "evaluation/Returns Std                                148.905\n",
      "evaluation/Returns Max                                133.275\n",
      "evaluation/Returns Min                               -461.556\n",
      "evaluation/Actions Mean                                -0.204508\n",
      "evaluation/Actions Std                                  0.742419\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -225.787\n",
      "evaluation/env_infos/final/reward_run Mean             -0.105749\n",
      "evaluation/env_infos/final/reward_run Std               0.773474\n",
      "evaluation/env_infos/final/reward_run Max               1.69139\n",
      "evaluation/env_infos/final/reward_run Min              -1.86432\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.326206\n",
      "evaluation/env_infos/initial/reward_run Std             0.395715\n",
      "evaluation/env_infos/initial/reward_run Max             0.324311\n",
      "evaluation/env_infos/initial/reward_run Min            -0.961769\n",
      "evaluation/env_infos/reward_run Mean                   -0.0487521\n",
      "evaluation/env_infos/reward_run Std                     0.698295\n",
      "evaluation/env_infos/reward_run Max                     3.30944\n",
      "evaluation/env_infos/reward_run Min                    -3.19039\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.343978\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0931413\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.163689\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.501555\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.285424\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.128894\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.100549\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.496805\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.355806\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0888367\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0259089\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.589198\n",
      "evaluation/env_infos/final/height Mean                 -0.295215\n",
      "evaluation/env_infos/final/height Std                   0.128381\n",
      "evaluation/env_infos/final/height Max                  -0.0977398\n",
      "evaluation/env_infos/final/height Min                  -0.559138\n",
      "evaluation/env_infos/initial/height Mean               -0.00015826\n",
      "evaluation/env_infos/initial/height Std                 0.0512008\n",
      "evaluation/env_infos/initial/height Max                 0.0923345\n",
      "evaluation/env_infos/initial/height Min                -0.0955167\n",
      "evaluation/env_infos/height Mean                       -0.269864\n",
      "evaluation/env_infos/height Std                         0.143768\n",
      "evaluation/env_infos/height Max                         0.417607\n",
      "evaluation/env_infos/height Min                        -0.593846\n",
      "evaluation/env_infos/final/reward_angular Mean          0.182684\n",
      "evaluation/env_infos/final/reward_angular Std           1.65173\n",
      "evaluation/env_infos/final/reward_angular Max           4.22628\n",
      "evaluation/env_infos/final/reward_angular Min          -4.53891\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.883173\n",
      "evaluation/env_infos/initial/reward_angular Std         0.887462\n",
      "evaluation/env_infos/initial/reward_angular Max         2.97466\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.05443\n",
      "evaluation/env_infos/reward_angular Mean                0.0512355\n",
      "evaluation/env_infos/reward_angular Std                 1.50808\n",
      "evaluation/env_infos/reward_angular Max                10.0229\n",
      "evaluation/env_infos/reward_angular Min                -6.55893\n",
      "time/data storing (s)                                   0.0201432\n",
      "time/evaluation sampling (s)                           22.6639\n",
      "time/exploration sampling (s)                           1.11404\n",
      "time/logging (s)                                        0.231792\n",
      "time/saving (s)                                         0.0268185\n",
      "time/training (s)                                       4.27408\n",
      "time/epoch (s)                                         28.3308\n",
      "time/total (s)                                        591.435\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:13.081082 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.634145\n",
      "trainer/QF2 Loss                                        0.605469\n",
      "trainer/Policy Loss                                     1.89951\n",
      "trainer/Q1 Predictions Mean                             2.71861\n",
      "trainer/Q1 Predictions Std                              1.87547\n",
      "trainer/Q1 Predictions Max                             12.8364\n",
      "trainer/Q1 Predictions Min                             -0.263986\n",
      "trainer/Q2 Predictions Mean                             2.69255\n",
      "trainer/Q2 Predictions Std                              1.87857\n",
      "trainer/Q2 Predictions Max                             13.6156\n",
      "trainer/Q2 Predictions Min                              0.0653859\n",
      "trainer/Q Targets Mean                                  2.86845\n",
      "trainer/Q Targets Std                                   2.09783\n",
      "trainer/Q Targets Max                                  14.3582\n",
      "trainer/Q Targets Min                                  -1.683\n",
      "trainer/Log Pis Mean                                    4.9616\n",
      "trainer/Log Pis Std                                     5.51326\n",
      "trainer/Log Pis Max                                    31.6073\n",
      "trainer/Log Pis Min                                    -5.43293\n",
      "trainer/Policy mu Mean                                 -0.16079\n",
      "trainer/Policy mu Std                                   1.4283\n",
      "trainer/Policy mu Max                                   5.02383\n",
      "trainer/Policy mu Min                                  -5.5697\n",
      "trainer/Policy log std Mean                            -0.826645\n",
      "trainer/Policy log std Std                              0.332246\n",
      "trainer/Policy log std Max                             -0.049523\n",
      "trainer/Policy log std Min                             -2.13829\n",
      "trainer/Alpha                                           0.0129445\n",
      "trainer/Alpha Loss                                     -4.51315\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                            22\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.248417\n",
      "exploration/Rewards Std                                 1.49839\n",
      "exploration/Rewards Max                                 4.09291\n",
      "exploration/Rewards Min                                -4.3863\n",
      "exploration/Returns Mean                             -248.417\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -248.417\n",
      "exploration/Returns Min                              -248.417\n",
      "exploration/Actions Mean                               -0.0798948\n",
      "exploration/Actions Std                                 0.803315\n",
      "exploration/Actions Max                                 0.999995\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -248.417\n",
      "exploration/env_infos/final/reward_run Mean            -1.49408\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -1.49408\n",
      "exploration/env_infos/final/reward_run Min             -1.49408\n",
      "exploration/env_infos/initial/reward_run Mean           0.214346\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.214346\n",
      "exploration/env_infos/initial/reward_run Min            0.214346\n",
      "exploration/env_infos/reward_run Mean                  -0.0664494\n",
      "exploration/env_infos/reward_run Std                    1.15996\n",
      "exploration/env_infos/reward_run Max                    2.81264\n",
      "exploration/env_infos/reward_run Min                   -2.87308\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.453644\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.453644\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.453644\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.280681\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.280681\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.280681\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.391019\n",
      "exploration/env_infos/reward_ctrl Std                   0.10106\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0823313\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594263\n",
      "exploration/env_infos/final/height Mean                -0.521864\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.521864\n",
      "exploration/env_infos/final/height Min                 -0.521864\n",
      "exploration/env_infos/initial/height Mean              -0.105589\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.105589\n",
      "exploration/env_infos/initial/height Min               -0.105589\n",
      "exploration/env_infos/height Mean                      -0.410014\n",
      "exploration/env_infos/height Std                        0.162591\n",
      "exploration/env_infos/height Max                        0.290279\n",
      "exploration/env_infos/height Min                       -0.592434\n",
      "exploration/env_infos/final/reward_angular Mean         4.652\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          4.652\n",
      "exploration/env_infos/final/reward_angular Min          4.652\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.385962\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.385962\n",
      "exploration/env_infos/initial/reward_angular Min       -0.385962\n",
      "exploration/env_infos/reward_angular Mean               0.0994665\n",
      "exploration/env_infos/reward_angular Std                2.19165\n",
      "exploration/env_infos/reward_angular Max                6.55085\n",
      "exploration/env_infos/reward_angular Min               -8.70948\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.203199\n",
      "evaluation/Rewards Std                                  0.946522\n",
      "evaluation/Rewards Max                                  8.64848\n",
      "evaluation/Rewards Min                                 -6.63627\n",
      "evaluation/Returns Mean                              -203.199\n",
      "evaluation/Returns Std                                119.784\n",
      "evaluation/Returns Max                                 59.4645\n",
      "evaluation/Returns Min                               -462.591\n",
      "evaluation/Actions Mean                                -0.15015\n",
      "evaluation/Actions Std                                  0.766534\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -203.199\n",
      "evaluation/env_infos/final/reward_run Mean              0.111911\n",
      "evaluation/env_infos/final/reward_run Std               0.78649\n",
      "evaluation/env_infos/final/reward_run Max               1.91535\n",
      "evaluation/env_infos/final/reward_run Min              -2.1008\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.226274\n",
      "evaluation/env_infos/initial/reward_run Std             0.430493\n",
      "evaluation/env_infos/initial/reward_run Max             0.89913\n",
      "evaluation/env_infos/initial/reward_run Min            -0.884664\n",
      "evaluation/env_infos/reward_run Mean                   -0.106417\n",
      "evaluation/env_infos/reward_run Std                     0.681963\n",
      "evaluation/env_infos/reward_run Max                     3.71377\n",
      "evaluation/env_infos/reward_run Min                    -3.33406\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.346294\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109614\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.161439\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.585952\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.250735\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0850964\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0919124\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.388976\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.366072\n",
      "evaluation/env_infos/reward_ctrl Std                    0.120126\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0297249\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595645\n",
      "evaluation/env_infos/final/height Mean                 -0.26035\n",
      "evaluation/env_infos/final/height Std                   0.142314\n",
      "evaluation/env_infos/final/height Max                  -0.101829\n",
      "evaluation/env_infos/final/height Min                  -0.57728\n",
      "evaluation/env_infos/initial/height Mean                0.000344733\n",
      "evaluation/env_infos/initial/height Std                 0.0554216\n",
      "evaluation/env_infos/initial/height Max                 0.0901787\n",
      "evaluation/env_infos/initial/height Min                -0.123955\n",
      "evaluation/env_infos/height Mean                       -0.246168\n",
      "evaluation/env_infos/height Std                         0.162007\n",
      "evaluation/env_infos/height Max                         0.439728\n",
      "evaluation/env_infos/height Min                        -0.591784\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0849324\n",
      "evaluation/env_infos/final/reward_angular Std           1.29218\n",
      "evaluation/env_infos/final/reward_angular Max           2.31557\n",
      "evaluation/env_infos/final/reward_angular Min          -3.85638\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.339195\n",
      "evaluation/env_infos/initial/reward_angular Std         0.708591\n",
      "evaluation/env_infos/initial/reward_angular Max         1.47203\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.72042\n",
      "evaluation/env_infos/reward_angular Mean                0.0694575\n",
      "evaluation/env_infos/reward_angular Std                 1.39251\n",
      "evaluation/env_infos/reward_angular Max                 9.70668\n",
      "evaluation/env_infos/reward_angular Min                -8.66186\n",
      "time/data storing (s)                                   0.014869\n",
      "time/evaluation sampling (s)                           22.9834\n",
      "time/exploration sampling (s)                           1.06086\n",
      "time/logging (s)                                        0.230869\n",
      "time/saving (s)                                         0.027772\n",
      "time/training (s)                                       4.34039\n",
      "time/epoch (s)                                         28.6581\n",
      "time/total (s)                                        620.332\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:42.034943 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  23000\n",
      "trainer/QF1 Loss                                        0.871865\n",
      "trainer/QF2 Loss                                        0.779283\n",
      "trainer/Policy Loss                                     1.76257\n",
      "trainer/Q1 Predictions Mean                             3.07085\n",
      "trainer/Q1 Predictions Std                              2.06886\n",
      "trainer/Q1 Predictions Max                             11.2577\n",
      "trainer/Q1 Predictions Min                             -0.210053\n",
      "trainer/Q2 Predictions Mean                             3.05024\n",
      "trainer/Q2 Predictions Std                              2.03418\n",
      "trainer/Q2 Predictions Max                             12.0997\n",
      "trainer/Q2 Predictions Min                             -0.711774\n",
      "trainer/Q Targets Mean                                  3.16988\n",
      "trainer/Q Targets Std                                   2.18844\n",
      "trainer/Q Targets Max                                  13.5239\n",
      "trainer/Q Targets Min                                  -1.6514\n",
      "trainer/Log Pis Mean                                    5.09897\n",
      "trainer/Log Pis Std                                     4.95159\n",
      "trainer/Log Pis Max                                    24.4822\n",
      "trainer/Log Pis Min                                    -4.94922\n",
      "trainer/Policy mu Mean                                 -0.0439693\n",
      "trainer/Policy mu Std                                   1.45288\n",
      "trainer/Policy mu Max                                   5.39476\n",
      "trainer/Policy mu Min                                  -6.46554\n",
      "trainer/Policy log std Mean                            -0.80963\n",
      "trainer/Policy log std Std                              0.324285\n",
      "trainer/Policy log std Max                              0.673803\n",
      "trainer/Policy log std Min                             -2.29215\n",
      "trainer/Alpha                                           0.0118991\n",
      "trainer/Alpha Loss                                     -3.99205\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                            23\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.179487\n",
      "exploration/Rewards Std                                 1.42484\n",
      "exploration/Rewards Max                                 5.59076\n",
      "exploration/Rewards Min                                -5.34803\n",
      "exploration/Returns Mean                             -179.487\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -179.487\n",
      "exploration/Returns Min                              -179.487\n",
      "exploration/Actions Mean                                0.100429\n",
      "exploration/Actions Std                                 0.813895\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -179.487\n",
      "exploration/env_infos/final/reward_run Mean             0.702661\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.702661\n",
      "exploration/env_infos/final/reward_run Min              0.702661\n",
      "exploration/env_infos/initial/reward_run Mean           0.122862\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.122862\n",
      "exploration/env_infos/initial/reward_run Min            0.122862\n",
      "exploration/env_infos/reward_run Mean                   0.12146\n",
      "exploration/env_infos/reward_run Std                    0.840785\n",
      "exploration/env_infos/reward_run Max                    2.73008\n",
      "exploration/env_infos/reward_run Min                   -2.21254\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.332283\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.332283\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.332283\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.493827\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.493827\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.493827\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.403506\n",
      "exploration/env_infos/reward_ctrl Std                   0.0712181\n",
      "exploration/env_infos/reward_ctrl Max                  -0.148854\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593425\n",
      "exploration/env_infos/final/height Mean                -0.544793\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.544793\n",
      "exploration/env_infos/final/height Min                 -0.544793\n",
      "exploration/env_infos/initial/height Mean              -0.0649268\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0649268\n",
      "exploration/env_infos/initial/height Min               -0.0649268\n",
      "exploration/env_infos/height Mean                      -0.487482\n",
      "exploration/env_infos/height Std                        0.143946\n",
      "exploration/env_infos/height Max                        0.343643\n",
      "exploration/env_infos/height Min                       -0.582238\n",
      "exploration/env_infos/final/reward_angular Mean         1.22505\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.22505\n",
      "exploration/env_infos/final/reward_angular Min          1.22505\n",
      "exploration/env_infos/initial/reward_angular Mean       2.08001\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.08001\n",
      "exploration/env_infos/initial/reward_angular Min        2.08001\n",
      "exploration/env_infos/reward_angular Mean               0.11186\n",
      "exploration/env_infos/reward_angular Std                1.57334\n",
      "exploration/env_infos/reward_angular Max                6.62873\n",
      "exploration/env_infos/reward_angular Min               -5.8125\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.186236\n",
      "evaluation/Rewards Std                                  0.891189\n",
      "evaluation/Rewards Max                                  6.00005\n",
      "evaluation/Rewards Min                                 -6.04544\n",
      "evaluation/Returns Mean                              -186.236\n",
      "evaluation/Returns Std                                166.155\n",
      "evaluation/Returns Max                                 66.7584\n",
      "evaluation/Returns Min                               -671.348\n",
      "evaluation/Actions Mean                                -0.0460636\n",
      "evaluation/Actions Std                                  0.715998\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -186.236\n",
      "evaluation/env_infos/final/reward_run Mean              0.0496905\n",
      "evaluation/env_infos/final/reward_run Std               0.647402\n",
      "evaluation/env_infos/final/reward_run Max               1.88576\n",
      "evaluation/env_infos/final/reward_run Min              -1.33241\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.125763\n",
      "evaluation/env_infos/initial/reward_run Std             0.459882\n",
      "evaluation/env_infos/initial/reward_run Max             0.763519\n",
      "evaluation/env_infos/initial/reward_run Min            -0.869177\n",
      "evaluation/env_infos/reward_run Mean                    0.00276778\n",
      "evaluation/env_infos/reward_run Std                     0.73555\n",
      "evaluation/env_infos/reward_run Max                     4.54627\n",
      "evaluation/env_infos/reward_run Min                    -3.03069\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.306335\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0932807\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.121712\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.522581\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.275155\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107288\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.104434\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.495114\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.308865\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106606\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0210776\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598429\n",
      "evaluation/env_infos/final/height Mean                 -0.276757\n",
      "evaluation/env_infos/final/height Std                   0.215827\n",
      "evaluation/env_infos/final/height Max                   0.0659264\n",
      "evaluation/env_infos/final/height Min                  -0.582723\n",
      "evaluation/env_infos/initial/height Mean               -0.00361043\n",
      "evaluation/env_infos/initial/height Std                 0.0536935\n",
      "evaluation/env_infos/initial/height Max                 0.0780978\n",
      "evaluation/env_infos/initial/height Min                -0.0961716\n",
      "evaluation/env_infos/height Mean                       -0.253708\n",
      "evaluation/env_infos/height Std                         0.223447\n",
      "evaluation/env_infos/height Max                         0.434784\n",
      "evaluation/env_infos/height Min                        -0.586933\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0668127\n",
      "evaluation/env_infos/final/reward_angular Std           1.12628\n",
      "evaluation/env_infos/final/reward_angular Max           2.51619\n",
      "evaluation/env_infos/final/reward_angular Min          -3.33321\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.679882\n",
      "evaluation/env_infos/initial/reward_angular Std         0.976411\n",
      "evaluation/env_infos/initial/reward_angular Max         2.88673\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.68213\n",
      "evaluation/env_infos/reward_angular Mean                0.0500092\n",
      "evaluation/env_infos/reward_angular Std                 1.1978\n",
      "evaluation/env_infos/reward_angular Max                 7.26837\n",
      "evaluation/env_infos/reward_angular Min                -7.54489\n",
      "time/data storing (s)                                   0.0183857\n",
      "time/evaluation sampling (s)                           23.4645\n",
      "time/exploration sampling (s)                           1.07573\n",
      "time/logging (s)                                        0.231789\n",
      "time/saving (s)                                         0.0263165\n",
      "time/training (s)                                       3.89262\n",
      "time/epoch (s)                                         28.7093\n",
      "time/total (s)                                        649.286\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:36:10.346291 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  24000\n",
      "trainer/QF1 Loss                                        1.38284\n",
      "trainer/QF2 Loss                                        1.10934\n",
      "trainer/Policy Loss                                     1.49972\n",
      "trainer/Q1 Predictions Mean                             3.14306\n",
      "trainer/Q1 Predictions Std                              2.29263\n",
      "trainer/Q1 Predictions Max                             17.5963\n",
      "trainer/Q1 Predictions Min                             -0.192077\n",
      "trainer/Q2 Predictions Mean                             3.19933\n",
      "trainer/Q2 Predictions Std                              2.31383\n",
      "trainer/Q2 Predictions Max                             16.2103\n",
      "trainer/Q2 Predictions Min                             -0.561851\n",
      "trainer/Q Targets Mean                                  2.99715\n",
      "trainer/Q Targets Std                                   2.28571\n",
      "trainer/Q Targets Max                                  15.7035\n",
      "trainer/Q Targets Min                                  -5.74212\n",
      "trainer/Log Pis Mean                                    4.96908\n",
      "trainer/Log Pis Std                                     5.27823\n",
      "trainer/Log Pis Max                                    29.1736\n",
      "trainer/Log Pis Min                                    -4.04521\n",
      "trainer/Policy mu Mean                                  0.0231211\n",
      "trainer/Policy mu Std                                   1.47376\n",
      "trainer/Policy mu Max                                   7.35077\n",
      "trainer/Policy mu Min                                  -3.75242\n",
      "trainer/Policy log std Mean                            -0.804479\n",
      "trainer/Policy log std Std                              0.386208\n",
      "trainer/Policy log std Max                              0.0610184\n",
      "trainer/Policy log std Min                             -2.20227\n",
      "trainer/Alpha                                           0.0114311\n",
      "trainer/Alpha Loss                                     -4.60948\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                            24\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0842011\n",
      "exploration/Rewards Std                                 0.448779\n",
      "exploration/Rewards Max                                 1.22359\n",
      "exploration/Rewards Min                                -1.24757\n",
      "exploration/Returns Mean                              -84.2011\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -84.2011\n",
      "exploration/Returns Min                               -84.2011\n",
      "exploration/Actions Mean                               -0.218025\n",
      "exploration/Actions Std                                 0.749699\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999949\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -84.2011\n",
      "exploration/env_infos/final/reward_run Mean             0.392789\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.392789\n",
      "exploration/env_infos/final/reward_run Min              0.392789\n",
      "exploration/env_infos/initial/reward_run Mean          -0.632361\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.632361\n",
      "exploration/env_infos/initial/reward_run Min           -0.632361\n",
      "exploration/env_infos/reward_run Mean                  -0.256952\n",
      "exploration/env_infos/reward_run Std                    0.50857\n",
      "exploration/env_infos/reward_run Max                    1.43758\n",
      "exploration/env_infos/reward_run Min                   -1.78678\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.494106\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.494106\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.494106\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.363857\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.363857\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.363857\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.36575\n",
      "exploration/env_infos/reward_ctrl Std                   0.101635\n",
      "exploration/env_infos/reward_ctrl Max                  -0.125834\n",
      "exploration/env_infos/reward_ctrl Min                  -0.587385\n",
      "exploration/env_infos/final/height Mean                -0.54186\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.54186\n",
      "exploration/env_infos/final/height Min                 -0.54186\n",
      "exploration/env_infos/initial/height Mean              -0.00923961\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00923961\n",
      "exploration/env_infos/initial/height Min               -0.00923961\n",
      "exploration/env_infos/height Mean                      -0.208247\n",
      "exploration/env_infos/height Std                        0.153101\n",
      "exploration/env_infos/height Max                        0.186448\n",
      "exploration/env_infos/height Min                       -0.580186\n",
      "exploration/env_infos/final/reward_angular Mean         0.16893\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.16893\n",
      "exploration/env_infos/final/reward_angular Min          0.16893\n",
      "exploration/env_infos/initial/reward_angular Mean       0.864624\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.864624\n",
      "exploration/env_infos/initial/reward_angular Min        0.864624\n",
      "exploration/env_infos/reward_angular Mean              -0.0143659\n",
      "exploration/env_infos/reward_angular Std                0.999568\n",
      "exploration/env_infos/reward_angular Max                2.65792\n",
      "exploration/env_infos/reward_angular Min               -4.09349\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.177308\n",
      "evaluation/Rewards Std                                  0.710945\n",
      "evaluation/Rewards Max                                  4.38395\n",
      "evaluation/Rewards Min                                 -5.47661\n",
      "evaluation/Returns Mean                              -177.308\n",
      "evaluation/Returns Std                                134.246\n",
      "evaluation/Returns Max                                126.265\n",
      "evaluation/Returns Min                               -403.028\n",
      "evaluation/Actions Mean                                -0.08447\n",
      "evaluation/Actions Std                                  0.7307\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -177.308\n",
      "evaluation/env_infos/final/reward_run Mean             -0.169583\n",
      "evaluation/env_infos/final/reward_run Std               0.403661\n",
      "evaluation/env_infos/final/reward_run Max               0.521078\n",
      "evaluation/env_infos/final/reward_run Min              -1.05414\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.103176\n",
      "evaluation/env_infos/initial/reward_run Std             0.417458\n",
      "evaluation/env_infos/initial/reward_run Max             0.772979\n",
      "evaluation/env_infos/initial/reward_run Min            -0.651069\n",
      "evaluation/env_infos/reward_run Mean                   -0.121993\n",
      "evaluation/env_infos/reward_run Std                     0.445128\n",
      "evaluation/env_infos/reward_run Max                     2.20978\n",
      "evaluation/env_infos/reward_run Min                    -2.43653\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.35466\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.138443\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.071472\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.557481\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.271953\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0937756\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0888112\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.4577\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.324635\n",
      "evaluation/env_infos/reward_ctrl Std                    0.123721\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0167702\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.592646\n",
      "evaluation/env_infos/final/height Mean                 -0.180631\n",
      "evaluation/env_infos/final/height Std                   0.181395\n",
      "evaluation/env_infos/final/height Max                   0.168222\n",
      "evaluation/env_infos/final/height Min                  -0.57728\n",
      "evaluation/env_infos/initial/height Mean               -0.0310225\n",
      "evaluation/env_infos/initial/height Std                 0.04647\n",
      "evaluation/env_infos/initial/height Max                 0.0493974\n",
      "evaluation/env_infos/initial/height Min                -0.0984864\n",
      "evaluation/env_infos/height Mean                       -0.170608\n",
      "evaluation/env_infos/height Std                         0.168216\n",
      "evaluation/env_infos/height Max                         0.393943\n",
      "evaluation/env_infos/height Min                        -0.598385\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0703947\n",
      "evaluation/env_infos/final/reward_angular Std           1.08799\n",
      "evaluation/env_infos/final/reward_angular Max           3.75684\n",
      "evaluation/env_infos/final/reward_angular Min          -1.26562\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.039921\n",
      "evaluation/env_infos/initial/reward_angular Std         1.05746\n",
      "evaluation/env_infos/initial/reward_angular Max         2.11842\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.45193\n",
      "evaluation/env_infos/reward_angular Mean                0.0236739\n",
      "evaluation/env_infos/reward_angular Std                 1.10504\n",
      "evaluation/env_infos/reward_angular Max                 6.89122\n",
      "evaluation/env_infos/reward_angular Min                -5.77702\n",
      "time/data storing (s)                                   0.014656\n",
      "time/evaluation sampling (s)                           22.909\n",
      "time/exploration sampling (s)                           0.994389\n",
      "time/logging (s)                                        0.227674\n",
      "time/saving (s)                                         0.0257416\n",
      "time/training (s)                                       3.854\n",
      "time/epoch (s)                                         28.0255\n",
      "time/total (s)                                        677.592\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:36:37.506832 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                  25000\n",
      "trainer/QF1 Loss                                        0.667145\n",
      "trainer/QF2 Loss                                        0.669847\n",
      "trainer/Policy Loss                                     2.23667\n",
      "trainer/Q1 Predictions Mean                             2.92442\n",
      "trainer/Q1 Predictions Std                              2.08085\n",
      "trainer/Q1 Predictions Max                              9.99116\n",
      "trainer/Q1 Predictions Min                             -0.649633\n",
      "trainer/Q2 Predictions Mean                             2.88943\n",
      "trainer/Q2 Predictions Std                              2.07496\n",
      "trainer/Q2 Predictions Max                              9.76935\n",
      "trainer/Q2 Predictions Min                             -1.24152\n",
      "trainer/Q Targets Mean                                  3.01488\n",
      "trainer/Q Targets Std                                   2.36343\n",
      "trainer/Q Targets Max                                  11.8121\n",
      "trainer/Q Targets Min                                  -2.58642\n",
      "trainer/Log Pis Mean                                    5.52213\n",
      "trainer/Log Pis Std                                     5.00103\n",
      "trainer/Log Pis Max                                    23.8922\n",
      "trainer/Log Pis Min                                    -4.62466\n",
      "trainer/Policy mu Mean                                  0.127671\n",
      "trainer/Policy mu Std                                   1.49611\n",
      "trainer/Policy mu Max                                   4.55601\n",
      "trainer/Policy mu Min                                  -4.12533\n",
      "trainer/Policy log std Mean                            -0.776149\n",
      "trainer/Policy log std Std                              0.326186\n",
      "trainer/Policy log std Max                              0.115993\n",
      "trainer/Policy log std Min                             -1.98031\n",
      "trainer/Alpha                                           0.0113723\n",
      "trainer/Alpha Loss                                     -2.13913\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                            25\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.410358\n",
      "exploration/Rewards Std                                 0.761006\n",
      "exploration/Rewards Max                                 3.78623\n",
      "exploration/Rewards Min                                -3.75381\n",
      "exploration/Returns Mean                             -410.358\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -410.358\n",
      "exploration/Returns Min                              -410.358\n",
      "exploration/Actions Mean                                0.347571\n",
      "exploration/Actions Std                                 0.728308\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -0.999904\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -410.358\n",
      "exploration/env_infos/final/reward_run Mean             0.20247\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.20247\n",
      "exploration/env_infos/final/reward_run Min              0.20247\n",
      "exploration/env_infos/initial/reward_run Mean           0.176272\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.176272\n",
      "exploration/env_infos/initial/reward_run Min            0.176272\n",
      "exploration/env_infos/reward_run Mean                  -0.0419815\n",
      "exploration/env_infos/reward_run Std                    0.453554\n",
      "exploration/env_infos/reward_run Max                    2.16963\n",
      "exploration/env_infos/reward_run Min                   -1.81916\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.296731\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.296731\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.296731\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.438874\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.438874\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.438874\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.390743\n",
      "exploration/env_infos/reward_ctrl Std                   0.0882309\n",
      "exploration/env_infos/reward_ctrl Max                  -0.118021\n",
      "exploration/env_infos/reward_ctrl Min                  -0.575175\n",
      "exploration/env_infos/final/height Mean                -0.57739\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.57739\n",
      "exploration/env_infos/final/height Min                 -0.57739\n",
      "exploration/env_infos/initial/height Mean               0.0645774\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0645774\n",
      "exploration/env_infos/initial/height Min                0.0645774\n",
      "exploration/env_infos/height Mean                      -0.45078\n",
      "exploration/env_infos/height Std                        0.221559\n",
      "exploration/env_infos/height Max                        0.280634\n",
      "exploration/env_infos/height Min                       -0.581853\n",
      "exploration/env_infos/final/reward_angular Mean        -0.383386\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.383386\n",
      "exploration/env_infos/final/reward_angular Min         -0.383386\n",
      "exploration/env_infos/initial/reward_angular Mean      -2.00934\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -2.00934\n",
      "exploration/env_infos/initial/reward_angular Min       -2.00934\n",
      "exploration/env_infos/reward_angular Mean               0.0609423\n",
      "exploration/env_infos/reward_angular Std                1.18305\n",
      "exploration/env_infos/reward_angular Max                6.91756\n",
      "exploration/env_infos/reward_angular Min               -5.42664\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.183231\n",
      "evaluation/Rewards Std                                  0.523355\n",
      "evaluation/Rewards Max                                  5.92113\n",
      "evaluation/Rewards Min                                 -4.68755\n",
      "evaluation/Returns Mean                              -183.231\n",
      "evaluation/Returns Std                                144.909\n",
      "evaluation/Returns Max                                159.671\n",
      "evaluation/Returns Min                               -613.878\n",
      "evaluation/Actions Mean                                -0.10516\n",
      "evaluation/Actions Std                                  0.724755\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999962\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -183.231\n",
      "evaluation/env_infos/final/reward_run Mean              0.0707861\n",
      "evaluation/env_infos/final/reward_run Std               0.213717\n",
      "evaluation/env_infos/final/reward_run Max               0.680129\n",
      "evaluation/env_infos/final/reward_run Min              -0.408486\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.146238\n",
      "evaluation/env_infos/initial/reward_run Std             0.497103\n",
      "evaluation/env_infos/initial/reward_run Max             0.844825\n",
      "evaluation/env_infos/initial/reward_run Min            -0.963413\n",
      "evaluation/env_infos/reward_run Mean                   -0.0729102\n",
      "evaluation/env_infos/reward_run Std                     0.336766\n",
      "evaluation/env_infos/reward_run Max                     2.48727\n",
      "evaluation/env_infos/reward_run Min                    -2.61437\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.310288\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.137478\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0795668\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.539329\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.288052\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.106536\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0956842\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.538576\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.321797\n",
      "evaluation/env_infos/reward_ctrl Std                    0.128076\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0309052\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.577424\n",
      "evaluation/env_infos/final/height Mean                 -0.2056\n",
      "evaluation/env_infos/final/height Std                   0.198652\n",
      "evaluation/env_infos/final/height Max                   0.0287216\n",
      "evaluation/env_infos/final/height Min                  -0.577281\n",
      "evaluation/env_infos/initial/height Mean               -0.0234053\n",
      "evaluation/env_infos/initial/height Std                 0.0568327\n",
      "evaluation/env_infos/initial/height Max                 0.0677774\n",
      "evaluation/env_infos/initial/height Min                -0.117935\n",
      "evaluation/env_infos/height Mean                       -0.18411\n",
      "evaluation/env_infos/height Std                         0.189245\n",
      "evaluation/env_infos/height Max                         0.332443\n",
      "evaluation/env_infos/height Min                        -0.592118\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.101051\n",
      "evaluation/env_infos/final/reward_angular Std           0.628587\n",
      "evaluation/env_infos/final/reward_angular Max           1.67753\n",
      "evaluation/env_infos/final/reward_angular Min          -1.86123\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.432742\n",
      "evaluation/env_infos/initial/reward_angular Std         1.12029\n",
      "evaluation/env_infos/initial/reward_angular Max         1.28153\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.68864\n",
      "evaluation/env_infos/reward_angular Mean                0.015615\n",
      "evaluation/env_infos/reward_angular Std                 0.848181\n",
      "evaluation/env_infos/reward_angular Max                 7.16234\n",
      "evaluation/env_infos/reward_angular Min                -5.57783\n",
      "time/data storing (s)                                   0.0152602\n",
      "time/evaluation sampling (s)                           21.5593\n",
      "time/exploration sampling (s)                           1.0464\n",
      "time/logging (s)                                        0.227333\n",
      "time/saving (s)                                         0.0275848\n",
      "time/training (s)                                       4.02778\n",
      "time/epoch (s)                                         26.9037\n",
      "time/total (s)                                        704.751\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:08.628849 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  26000\n",
      "trainer/QF1 Loss                                        0.667388\n",
      "trainer/QF2 Loss                                        0.659748\n",
      "trainer/Policy Loss                                     1.37119\n",
      "trainer/Q1 Predictions Mean                             3.07651\n",
      "trainer/Q1 Predictions Std                              2.11732\n",
      "trainer/Q1 Predictions Max                             12.5292\n",
      "trainer/Q1 Predictions Min                             -1.1954\n",
      "trainer/Q2 Predictions Mean                             3.15597\n",
      "trainer/Q2 Predictions Std                              2.13183\n",
      "trainer/Q2 Predictions Max                             13.0424\n",
      "trainer/Q2 Predictions Min                             -1.53457\n",
      "trainer/Q Targets Mean                                  2.87289\n",
      "trainer/Q Targets Std                                   2.23445\n",
      "trainer/Q Targets Max                                  13.3907\n",
      "trainer/Q Targets Min                                  -3.0896\n",
      "trainer/Log Pis Mean                                    4.82417\n",
      "trainer/Log Pis Std                                     4.68962\n",
      "trainer/Log Pis Max                                    26.361\n",
      "trainer/Log Pis Min                                    -6.05711\n",
      "trainer/Policy mu Mean                                  0.0354728\n",
      "trainer/Policy mu Std                                   1.43835\n",
      "trainer/Policy mu Max                                   4.57124\n",
      "trainer/Policy mu Min                                  -4.1326\n",
      "trainer/Policy log std Mean                            -0.763995\n",
      "trainer/Policy log std Std                              0.296609\n",
      "trainer/Policy log std Max                             -0.0485174\n",
      "trainer/Policy log std Min                             -2.14219\n",
      "trainer/Alpha                                           0.0112665\n",
      "trainer/Alpha Loss                                     -5.27433\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                            26\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.1917\n",
      "exploration/Rewards Std                                 0.169554\n",
      "exploration/Rewards Max                                 0.424621\n",
      "exploration/Rewards Min                                -0.655853\n",
      "exploration/Returns Mean                             -191.7\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -191.7\n",
      "exploration/Returns Min                              -191.7\n",
      "exploration/Actions Mean                               -0.0695239\n",
      "exploration/Actions Std                                 0.658818\n",
      "exploration/Actions Max                                 0.999315\n",
      "exploration/Actions Min                                -0.999938\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -191.7\n",
      "exploration/env_infos/final/reward_run Mean             0.854192\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.854192\n",
      "exploration/env_infos/final/reward_run Min              0.854192\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0206789\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0206789\n",
      "exploration/env_infos/initial/reward_run Min           -0.0206789\n",
      "exploration/env_infos/reward_run Mean                  -0.0759563\n",
      "exploration/env_infos/reward_run Std                    0.618617\n",
      "exploration/env_infos/reward_run Max                    1.87231\n",
      "exploration/env_infos/reward_run Min                   -2.48763\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270255\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270255\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270255\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.252977\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.252977\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.252977\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.263325\n",
      "exploration/env_infos/reward_ctrl Std                   0.0955344\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0250559\n",
      "exploration/env_infos/reward_ctrl Min                  -0.518121\n",
      "exploration/env_infos/final/height Mean                -0.111866\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.111866\n",
      "exploration/env_infos/final/height Min                 -0.111866\n",
      "exploration/env_infos/initial/height Mean               0.05224\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.05224\n",
      "exploration/env_infos/initial/height Min                0.05224\n",
      "exploration/env_infos/height Mean                      -0.0751657\n",
      "exploration/env_infos/height Std                        0.104413\n",
      "exploration/env_infos/height Max                        0.308717\n",
      "exploration/env_infos/height Min                       -0.345736\n",
      "exploration/env_infos/final/reward_angular Mean         3.63533\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.63533\n",
      "exploration/env_infos/final/reward_angular Min          3.63533\n",
      "exploration/env_infos/initial/reward_angular Mean       0.388225\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.388225\n",
      "exploration/env_infos/initial/reward_angular Min        0.388225\n",
      "exploration/env_infos/reward_angular Mean              -0.00947799\n",
      "exploration/env_infos/reward_angular Std                1.62437\n",
      "exploration/env_infos/reward_angular Max                6.05664\n",
      "exploration/env_infos/reward_angular Min               -4.75798\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.159241\n",
      "evaluation/Rewards Std                                  0.750204\n",
      "evaluation/Rewards Max                                  3.39645\n",
      "evaluation/Rewards Min                                 -5.27253\n",
      "evaluation/Returns Mean                              -159.241\n",
      "evaluation/Returns Std                                167.253\n",
      "evaluation/Returns Max                                512.704\n",
      "evaluation/Returns Min                               -382.466\n",
      "evaluation/Actions Mean                                 0.0188393\n",
      "evaluation/Actions Std                                  0.730051\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -0.999993\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -159.241\n",
      "evaluation/env_infos/final/reward_run Mean              0.173904\n",
      "evaluation/env_infos/final/reward_run Std               0.451666\n",
      "evaluation/env_infos/final/reward_run Max               1.64329\n",
      "evaluation/env_infos/final/reward_run Min              -0.431074\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0570084\n",
      "evaluation/env_infos/initial/reward_run Std             0.545171\n",
      "evaluation/env_infos/initial/reward_run Max             0.820841\n",
      "evaluation/env_infos/initial/reward_run Min            -0.853161\n",
      "evaluation/env_infos/reward_run Mean                   -0.0584764\n",
      "evaluation/env_infos/reward_run Std                     0.523517\n",
      "evaluation/env_infos/reward_run Max                     2.92043\n",
      "evaluation/env_infos/reward_run Min                    -2.58494\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.326373\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.142463\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0601384\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.537415\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.248067\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.130282\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0753092\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.482657\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.319998\n",
      "evaluation/env_infos/reward_ctrl Std                    0.130748\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.011196\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.577925\n",
      "evaluation/env_infos/final/height Mean                 -0.188895\n",
      "evaluation/env_infos/final/height Std                   0.158469\n",
      "evaluation/env_infos/final/height Max                   0.00277212\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0213934\n",
      "evaluation/env_infos/initial/height Std                 0.0576807\n",
      "evaluation/env_infos/initial/height Max                 0.0840667\n",
      "evaluation/env_infos/initial/height Min                -0.126353\n",
      "evaluation/env_infos/height Mean                       -0.163004\n",
      "evaluation/env_infos/height Std                         0.158725\n",
      "evaluation/env_infos/height Max                         0.403536\n",
      "evaluation/env_infos/height Min                        -0.582257\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.209196\n",
      "evaluation/env_infos/final/reward_angular Std           1.15804\n",
      "evaluation/env_infos/final/reward_angular Max           1.86836\n",
      "evaluation/env_infos/final/reward_angular Min          -3.22211\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.414373\n",
      "evaluation/env_infos/initial/reward_angular Std         1.26807\n",
      "evaluation/env_infos/initial/reward_angular Max         1.61888\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.99988\n",
      "evaluation/env_infos/reward_angular Mean                0.00608971\n",
      "evaluation/env_infos/reward_angular Std                 1.15841\n",
      "evaluation/env_infos/reward_angular Max                 6.62728\n",
      "evaluation/env_infos/reward_angular Min                -5.74818\n",
      "time/data storing (s)                                   0.015651\n",
      "time/evaluation sampling (s)                           24.7235\n",
      "time/exploration sampling (s)                           1.31801\n",
      "time/logging (s)                                        0.237347\n",
      "time/saving (s)                                         0.02769\n",
      "time/training (s)                                       4.53936\n",
      "time/epoch (s)                                         30.8616\n",
      "time/total (s)                                        735.882\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:37.649835 PDT | [gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  27000\n",
      "trainer/QF1 Loss                                        0.541564\n",
      "trainer/QF2 Loss                                        0.611347\n",
      "trainer/Policy Loss                                     3.70045\n",
      "trainer/Q1 Predictions Mean                             2.86964\n",
      "trainer/Q1 Predictions Std                              2.16974\n",
      "trainer/Q1 Predictions Max                             10.7914\n",
      "trainer/Q1 Predictions Min                             -0.85465\n",
      "trainer/Q2 Predictions Mean                             2.97488\n",
      "trainer/Q2 Predictions Std                              2.23089\n",
      "trainer/Q2 Predictions Max                             10.5417\n",
      "trainer/Q2 Predictions Min                             -0.930123\n",
      "trainer/Q Targets Mean                                  2.94833\n",
      "trainer/Q Targets Std                                   2.31929\n",
      "trainer/Q Targets Max                                  12.4775\n",
      "trainer/Q Targets Min                                  -1.9753\n",
      "trainer/Log Pis Mean                                    6.95403\n",
      "trainer/Log Pis Std                                     7.23675\n",
      "trainer/Log Pis Max                                    30.1576\n",
      "trainer/Log Pis Min                                    -5.39203\n",
      "trainer/Policy mu Mean                                  0.148019\n",
      "trainer/Policy mu Std                                   1.71078\n",
      "trainer/Policy mu Max                                   6.27059\n",
      "trainer/Policy mu Min                                  -5.2572\n",
      "trainer/Policy log std Mean                            -0.77891\n",
      "trainer/Policy log std Std                              0.29958\n",
      "trainer/Policy log std Max                              0.335192\n",
      "trainer/Policy log std Min                             -2.0884\n",
      "trainer/Alpha                                           0.0113557\n",
      "trainer/Alpha Loss                                      4.27231\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                            27\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.248133\n",
      "exploration/Rewards Std                                 0.375803\n",
      "exploration/Rewards Max                                 3.38852\n",
      "exploration/Rewards Min                                -2.31946\n",
      "exploration/Returns Mean                             -248.133\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -248.133\n",
      "exploration/Returns Min                              -248.133\n",
      "exploration/Actions Mean                                0.0628505\n",
      "exploration/Actions Std                                 0.945295\n",
      "exploration/Actions Max                                 0.999991\n",
      "exploration/Actions Min                                -0.999979\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -248.133\n",
      "exploration/env_infos/final/reward_run Mean            -0.0219213\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0219213\n",
      "exploration/env_infos/final/reward_run Min             -0.0219213\n",
      "exploration/env_infos/initial/reward_run Mean           0.240596\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.240596\n",
      "exploration/env_infos/initial/reward_run Min            0.240596\n",
      "exploration/env_infos/reward_run Mean                   0.0207563\n",
      "exploration/env_infos/reward_run Std                    0.145515\n",
      "exploration/env_infos/reward_run Max                    1.45107\n",
      "exploration/env_infos/reward_run Min                   -0.747449\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.571552\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.571552\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.571552\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.368096\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.368096\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.368096\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.53852\n",
      "exploration/env_infos/reward_ctrl Std                   0.0361374\n",
      "exploration/env_infos/reward_ctrl Max                  -0.286713\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590244\n",
      "exploration/env_infos/final/height Mean                -0.577289\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577289\n",
      "exploration/env_infos/final/height Min                 -0.577289\n",
      "exploration/env_infos/initial/height Mean               0.0433356\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0433356\n",
      "exploration/env_infos/initial/height Min                0.0433356\n",
      "exploration/env_infos/height Mean                      -0.551533\n",
      "exploration/env_infos/height Std                        0.119287\n",
      "exploration/env_infos/height Max                        0.27497\n",
      "exploration/env_infos/height Min                       -0.577348\n",
      "exploration/env_infos/final/reward_angular Mean         1.44853e-05\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.44853e-05\n",
      "exploration/env_infos/final/reward_angular Min          1.44853e-05\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.74094\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.74094\n",
      "exploration/env_infos/initial/reward_angular Min       -1.74094\n",
      "exploration/env_infos/reward_angular Mean               0.0678812\n",
      "exploration/env_infos/reward_angular Std                0.479575\n",
      "exploration/env_infos/reward_angular Max                5.07126\n",
      "exploration/env_infos/reward_angular Min               -2.35551\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.278827\n",
      "evaluation/Rewards Std                                  0.780918\n",
      "evaluation/Rewards Max                                  5.74613\n",
      "evaluation/Rewards Min                                 -8.22637\n",
      "evaluation/Returns Mean                              -278.827\n",
      "evaluation/Returns Std                                148.959\n",
      "evaluation/Returns Max                                 -0.155181\n",
      "evaluation/Returns Min                               -612.025\n",
      "evaluation/Actions Mean                                 0.0438096\n",
      "evaluation/Actions Std                                  0.793284\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -278.827\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0310808\n",
      "evaluation/env_infos/final/reward_run Std               0.251846\n",
      "evaluation/env_infos/final/reward_run Max               0.53097\n",
      "evaluation/env_infos/final/reward_run Min              -0.707711\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0614154\n",
      "evaluation/env_infos/initial/reward_run Std             0.601264\n",
      "evaluation/env_infos/initial/reward_run Max             0.736806\n",
      "evaluation/env_infos/initial/reward_run Min            -1.01764\n",
      "evaluation/env_infos/reward_run Mean                   -0.00190716\n",
      "evaluation/env_infos/reward_run Std                     0.47314\n",
      "evaluation/env_infos/reward_run Max                     3.21111\n",
      "evaluation/env_infos/reward_run Min                    -2.98739\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.370643\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.167285\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0602212\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.549305\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.281595\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.13116\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.118705\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.542374\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.378731\n",
      "evaluation/env_infos/reward_ctrl Std                    0.168012\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0139362\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599155\n",
      "evaluation/env_infos/final/height Mean                 -0.431439\n",
      "evaluation/env_infos/final/height Std                   0.203341\n",
      "evaluation/env_infos/final/height Max                   0.037302\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0176284\n",
      "evaluation/env_infos/initial/height Std                 0.043481\n",
      "evaluation/env_infos/initial/height Max                 0.0575225\n",
      "evaluation/env_infos/initial/height Min                -0.0823839\n",
      "evaluation/env_infos/height Mean                       -0.346091\n",
      "evaluation/env_infos/height Std                         0.259375\n",
      "evaluation/env_infos/height Max                         0.372286\n",
      "evaluation/env_infos/height Min                        -0.593013\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0293758\n",
      "evaluation/env_infos/final/reward_angular Std           0.842533\n",
      "evaluation/env_infos/final/reward_angular Max           3.39149\n",
      "evaluation/env_infos/final/reward_angular Min          -1.69465\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.319062\n",
      "evaluation/env_infos/initial/reward_angular Std         1.22378\n",
      "evaluation/env_infos/initial/reward_angular Max         2.35976\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.34532\n",
      "evaluation/env_infos/reward_angular Mean                0.0216261\n",
      "evaluation/env_infos/reward_angular Std                 1.09645\n",
      "evaluation/env_infos/reward_angular Max                 9.01192\n",
      "evaluation/env_infos/reward_angular Min                -5.96804\n",
      "time/data storing (s)                                   0.0159994\n",
      "time/evaluation sampling (s)                           23.1624\n",
      "time/exploration sampling (s)                           1.19498\n",
      "time/logging (s)                                        0.231378\n",
      "time/saving (s)                                         0.0259585\n",
      "time/training (s)                                       4.0894\n",
      "time/epoch (s)                                         28.7201\n",
      "time/total (s)                                        764.896\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f9e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
