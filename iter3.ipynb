{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fba443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15518]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a740). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a768). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa97a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15518]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a22fa9818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a2302a830). One of the two will be used. Which one is undefined.\n",
      "using sparse reward if specified\n",
      "2021-05-25 10:24:59.424982 PDT | Variant:\n",
      "2021-05-25 10:24:59.425577 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"pointreacherobs\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 1000,\n",
      "    \"num_expl_steps_per_train_loop\": 20,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 20,\n",
      "    \"num_train_loops_per_epoch\": 5\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.97,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 2000,\n",
      "    \"latent_dim\": 6,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"sparse_reward\": null,\n",
      "    \"fixed_ratio\": null\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      300,\n",
      "      300,\n",
      "      300\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"horizon\": 20\n",
      "  }\n",
      "}\n",
      "pointreacher\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "sparse reward: None\n",
      "2021-05-25 10:25:34.660135 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 0 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1100\n",
      "trainer/QF1 Loss                                        0.576872\n",
      "trainer/QF2 Loss                                        0.57548\n",
      "trainer/Policy Loss                                    -1.37884\n",
      "trainer/Q1 Predictions Mean                            -0.00126527\n",
      "trainer/Q1 Predictions Std                              0.000605492\n",
      "trainer/Q1 Predictions Max                              0.000269961\n",
      "trainer/Q1 Predictions Min                             -0.00227255\n",
      "trainer/Q2 Predictions Mean                            -4.35855e-06\n",
      "trainer/Q2 Predictions Std                              0.000609121\n",
      "trainer/Q2 Predictions Max                              0.00133012\n",
      "trainer/Q2 Predictions Min                             -0.00163013\n",
      "trainer/Q Targets Mean                                  0.514032\n",
      "trainer/Q Targets Std                                   0.557929\n",
      "trainer/Q Targets Max                                   1.83678\n",
      "trainer/Q Targets Min                                  -1.35869\n",
      "trainer/Log Pis Mean                                   -1.38018\n",
      "trainer/Log Pis Std                                     0.289063\n",
      "trainer/Log Pis Max                                    -0.577882\n",
      "trainer/Log Pis Min                                    -2.60453\n",
      "trainer/Policy mu Mean                                  0.000988936\n",
      "trainer/Policy mu Std                                   0.00058491\n",
      "trainer/Policy mu Max                                   0.00221213\n",
      "trainer/Policy mu Min                                   1.31949e-05\n",
      "trainer/Policy log std Mean                             4.21378e-05\n",
      "trainer/Policy log std Std                              0.000645919\n",
      "trainer/Policy log std Max                              0.000916166\n",
      "trainer/Policy log std Min                             -0.00127692\n",
      "trainer/Alpha                                           0.997005\n",
      "trainer/Alpha Loss                                     -0\n",
      "exploration/num steps total                          1100\n",
      "exploration/num paths total                            55\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.674579\n",
      "exploration/Rewards Std                                 0.290019\n",
      "exploration/Rewards Max                                -0.105313\n",
      "exploration/Rewards Min                                -1.25229\n",
      "exploration/Returns Mean                              -13.4916\n",
      "exploration/Returns Std                                 1.35362\n",
      "exploration/Returns Max                               -11.0651\n",
      "exploration/Returns Min                               -14.8536\n",
      "exploration/Actions Mean                                0.0632062\n",
      "exploration/Actions Std                                 0.578647\n",
      "exploration/Actions Max                                 0.98148\n",
      "exploration/Actions Min                                -0.941187\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -13.4916\n",
      "exploration/env_infos/final/reward_dist Mean            3.94634e-51\n",
      "exploration/env_infos/final/reward_dist Std             7.89266e-51\n",
      "exploration/env_infos/final/reward_dist Max             1.97317e-50\n",
      "exploration/env_infos/final/reward_dist Min             4.19542e-102\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0344088\n",
      "exploration/env_infos/initial/reward_dist Std           0.0428659\n",
      "exploration/env_infos/initial/reward_dist Max           0.106806\n",
      "exploration/env_infos/initial/reward_dist Min           9.56386e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0187599\n",
      "exploration/env_infos/reward_dist Std                   0.0898929\n",
      "exploration/env_infos/reward_dist Max                   0.836662\n",
      "exploration/env_infos/reward_dist Min                   4.19542e-102\n",
      "exploration/env_infos/final/reward_energy Mean         -0.787813\n",
      "exploration/env_infos/final/reward_energy Std           0.299654\n",
      "exploration/env_infos/final/reward_energy Max          -0.19223\n",
      "exploration/env_infos/final/reward_energy Min          -1.00022\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.87809\n",
      "exploration/env_infos/initial/reward_energy Std         0.131384\n",
      "exploration/env_infos/initial/reward_energy Max        -0.719892\n",
      "exploration/env_infos/initial/reward_energy Min        -1.09336\n",
      "exploration/env_infos/reward_energy Mean               -0.775129\n",
      "exploration/env_infos/reward_energy Std                 0.277183\n",
      "exploration/env_infos/reward_energy Max                -0.19223\n",
      "exploration/env_infos/reward_energy Min                -1.25569\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.412974\n",
      "exploration/env_infos/final/end_effector_loc Std        0.646101\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.0216553\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.022725\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0471042\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0352084\n",
      "exploration/env_infos/end_effector_loc Mean             0.273985\n",
      "exploration/env_infos/end_effector_loc Std              0.485792\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           1000\n",
      "evaluation/num paths total                             50\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.0780623\n",
      "evaluation/Rewards Std                                  0.0473164\n",
      "evaluation/Rewards Max                                  0.0425092\n",
      "evaluation/Rewards Min                                 -0.150422\n",
      "evaluation/Returns Mean                                -1.56125\n",
      "evaluation/Returns Std                                  0.945244\n",
      "evaluation/Returns Max                                  0.807234\n",
      "evaluation/Returns Min                                 -2.97967\n",
      "evaluation/Actions Mean                                 0.00098014\n",
      "evaluation/Actions Std                                  0.000587866\n",
      "evaluation/Actions Max                                  0.0020782\n",
      "evaluation/Actions Min                                  0.000173299\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -1.56125\n",
      "evaluation/env_infos/final/reward_dist Mean             0.00593428\n",
      "evaluation/env_infos/final/reward_dist Std              0.00912983\n",
      "evaluation/env_infos/final/reward_dist Max              0.0441983\n",
      "evaluation/env_infos/final/reward_dist Min              5.15938e-07\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00533279\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00750724\n",
      "evaluation/env_infos/initial/reward_dist Max            0.02745\n",
      "evaluation/env_infos/initial/reward_dist Min            9.54696e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.00547818\n",
      "evaluation/env_infos/reward_dist Std                    0.0078623\n",
      "evaluation/env_infos/reward_dist Max                    0.0441983\n",
      "evaluation/env_infos/reward_dist Min                    5.15938e-07\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.00163802\n",
      "evaluation/env_infos/final/reward_energy Std            0.000251158\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00111856\n",
      "evaluation/env_infos/final/reward_energy Min           -0.00210778\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.00156036\n",
      "evaluation/env_infos/initial/reward_energy Std          0.00021699\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00111835\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.00196039\n",
      "evaluation/env_infos/reward_energy Mean                -0.00159921\n",
      "evaluation/env_infos/reward_energy Std                  0.000234643\n",
      "evaluation/env_infos/reward_energy Max                 -0.00111835\n",
      "evaluation/env_infos/reward_energy Min                 -0.00210778\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.0102276\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.00607673\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.0207555\n",
      "evaluation/env_infos/final/end_effector_loc Min         0.00191566\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      4.80703e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Std       2.81336e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Max       9.61769e-05\n",
      "evaluation/env_infos/initial/end_effector_loc Min       8.66496e-06\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00373819\n",
      "evaluation/env_infos/end_effector_loc Std               0.0043006\n",
      "evaluation/env_infos/end_effector_loc Max               0.0207555\n",
      "evaluation/env_infos/end_effector_loc Min               8.66496e-06\n",
      "time/data storing (s)                                   0.00299722\n",
      "time/evaluation sampling (s)                            1.39765\n",
      "time/exploration sampling (s)                           0.122225\n",
      "time/logging (s)                                        0.0222268\n",
      "time/saving (s)                                         0.0734678\n",
      "time/training (s)                                      32.3707\n",
      "time/epoch (s)                                         33.9893\n",
      "time/total (s)                                         38.19\n",
      "Epoch                                                   0\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:17.494277 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 1 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                   1200\n",
      "trainer/QF1 Loss                                        0.0142788\n",
      "trainer/QF2 Loss                                        0.0179015\n",
      "trainer/Policy Loss                                    -0.759393\n",
      "trainer/Q1 Predictions Mean                            -0.46276\n",
      "trainer/Q1 Predictions Std                              0.624373\n",
      "trainer/Q1 Predictions Max                              0.586792\n",
      "trainer/Q1 Predictions Min                             -2.04044\n",
      "trainer/Q2 Predictions Mean                            -0.537618\n",
      "trainer/Q2 Predictions Std                              0.622405\n",
      "trainer/Q2 Predictions Max                              0.567538\n",
      "trainer/Q2 Predictions Min                             -2.17126\n",
      "trainer/Q Targets Mean                                 -0.486009\n",
      "trainer/Q Targets Std                                   0.654017\n",
      "trainer/Q Targets Max                                   0.673842\n",
      "trainer/Q Targets Min                                  -2.31864\n",
      "trainer/Log Pis Mean                                   -1.18884\n",
      "trainer/Log Pis Std                                     0.472486\n",
      "trainer/Log Pis Max                                    -0.544822\n",
      "trainer/Log Pis Min                                    -3.78867\n",
      "trainer/Policy mu Mean                                  0.0285259\n",
      "trainer/Policy mu Std                                   0.0346983\n",
      "trainer/Policy mu Max                                   0.134678\n",
      "trainer/Policy mu Min                                  -0.0757775\n",
      "trainer/Policy log std Mean                            -0.526448\n",
      "trainer/Policy log std Std                              0.0696464\n",
      "trainer/Policy log std Max                             -0.366739\n",
      "trainer/Policy log std Min                             -0.693374\n",
      "trainer/Alpha                                           0.225887\n",
      "trainer/Alpha Loss                                     -4.73488\n",
      "exploration/num steps total                          1200\n",
      "exploration/num paths total                            60\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.517996\n",
      "exploration/Rewards Std                                 0.284947\n",
      "exploration/Rewards Max                                 0.0111953\n",
      "exploration/Rewards Min                                -1.26828\n",
      "exploration/Returns Mean                              -10.3599\n",
      "exploration/Returns Std                                 2.90097\n",
      "exploration/Returns Max                                -6.89275\n",
      "exploration/Returns Min                               -15.7143\n",
      "exploration/Actions Mean                                0.046027\n",
      "exploration/Actions Std                                 0.385387\n",
      "exploration/Actions Max                                 0.915877\n",
      "exploration/Actions Min                                -0.852987\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -10.3599\n",
      "exploration/env_infos/final/reward_dist Mean            1.0713e-53\n",
      "exploration/env_infos/final/reward_dist Std             2.14259e-53\n",
      "exploration/env_infos/final/reward_dist Max             5.35648e-53\n",
      "exploration/env_infos/final/reward_dist Min             1.19818e-151\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0103099\n",
      "exploration/env_infos/initial/reward_dist Std           0.00620404\n",
      "exploration/env_infos/initial/reward_dist Max           0.017284\n",
      "exploration/env_infos/initial/reward_dist Min           2.62818e-07\n",
      "exploration/env_infos/reward_dist Mean                  0.0169753\n",
      "exploration/env_infos/reward_dist Std                   0.049046\n",
      "exploration/env_infos/reward_dist Max                   0.271489\n",
      "exploration/env_infos/reward_dist Min                   1.19818e-151\n",
      "exploration/env_infos/final/reward_energy Mean         -0.484143\n",
      "exploration/env_infos/final/reward_energy Std           0.217856\n",
      "exploration/env_infos/final/reward_energy Max          -0.152269\n",
      "exploration/env_infos/final/reward_energy Min          -0.809446\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.473428\n",
      "exploration/env_infos/initial/reward_energy Std         0.250453\n",
      "exploration/env_infos/initial/reward_energy Max        -0.136648\n",
      "exploration/env_infos/initial/reward_energy Min        -0.769096\n",
      "exploration/env_infos/reward_energy Mean               -0.491715\n",
      "exploration/env_infos/reward_energy Std                 0.243927\n",
      "exploration/env_infos/reward_energy Max                -0.0138491\n",
      "exploration/env_infos/reward_energy Min                -1.14363\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.281292\n",
      "exploration/env_infos/final/end_effector_loc Std        0.753392\n",
      "exploration/env_infos/final/end_effector_loc Max        1\n",
      "exploration/env_infos/final/end_effector_loc Min       -1\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.000124701\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0189357\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0364533\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0353364\n",
      "exploration/env_infos/end_effector_loc Mean             0.131406\n",
      "exploration/env_infos/end_effector_loc Std              0.468006\n",
      "exploration/env_infos/end_effector_loc Max              1\n",
      "exploration/env_infos/end_effector_loc Min             -1\n",
      "evaluation/num steps total                           2000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.129317\n",
      "evaluation/Rewards Std                                  0.0950763\n",
      "evaluation/Rewards Max                                  0.165276\n",
      "evaluation/Rewards Min                                 -0.637119\n",
      "evaluation/Returns Mean                                -2.58635\n",
      "evaluation/Returns Std                                  1.38028\n",
      "evaluation/Returns Max                                  1.35225\n",
      "evaluation/Returns Min                                 -5.58551\n",
      "evaluation/Actions Mean                                 0.0325454\n",
      "evaluation/Actions Std                                  0.0245784\n",
      "evaluation/Actions Max                                  0.123077\n",
      "evaluation/Actions Min                                 -0.057515\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.58635\n",
      "evaluation/env_infos/final/reward_dist Mean             2.69966e-07\n",
      "evaluation/env_infos/final/reward_dist Std              1.09715e-06\n",
      "evaluation/env_infos/final/reward_dist Max              5.76264e-06\n",
      "evaluation/env_infos/final/reward_dist Min              3.98986e-74\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00497559\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00811029\n",
      "evaluation/env_infos/initial/reward_dist Max            0.027829\n",
      "evaluation/env_infos/initial/reward_dist Min            1.12831e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0435771\n",
      "evaluation/env_infos/reward_dist Std                    0.143624\n",
      "evaluation/env_infos/reward_dist Max                    0.954145\n",
      "evaluation/env_infos/reward_dist Min                    3.98986e-74\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.0490002\n",
      "evaluation/env_infos/final/reward_energy Std            0.00727559\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0370384\n",
      "evaluation/env_infos/final/reward_energy Min           -0.0729301\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.0621672\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0211793\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0401044\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.133877\n",
      "evaluation/env_infos/reward_energy Mean                -0.0556139\n",
      "evaluation/env_infos/reward_energy Std                  0.015287\n",
      "evaluation/env_infos/reward_energy Max                 -0.0370384\n",
      "evaluation/env_infos/reward_energy Min                 -0.134243\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.356476\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.266035\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.969306\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.581257\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00178086\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00149003\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.00614425\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.00287575\n",
      "evaluation/env_infos/end_effector_loc Mean              0.133145\n",
      "evaluation/env_infos/end_effector_loc Std               0.172783\n",
      "evaluation/env_infos/end_effector_loc Max               0.969306\n",
      "evaluation/env_infos/end_effector_loc Min              -0.581257\n",
      "time/data storing (s)                                   0.00308579\n",
      "time/evaluation sampling (s)                            1.13693\n",
      "time/exploration sampling (s)                           0.127787\n",
      "time/logging (s)                                        0.0202216\n",
      "time/saving (s)                                         0.0366551\n",
      "time/training (s)                                      41.3416\n",
      "time/epoch (s)                                         42.6663\n",
      "time/total (s)                                         81.0213\n",
      "Epoch                                                   1\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:04.151054 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 2 finished\r\n",
      "---------------------------------------------------  ---------------\r\n",
      "replay_buffer/size                                   1300\r\n",
      "trainer/QF1 Loss                                        0.016673\r\n",
      "trainer/QF2 Loss                                        0.0196761\r\n",
      "trainer/Policy Loss                                     1.37269\r\n",
      "trainer/Q1 Predictions Mean                            -1.49769\r\n",
      "trainer/Q1 Predictions Std                              0.976716\r\n",
      "trainer/Q1 Predictions Max                              0.332782\r\n",
      "trainer/Q1 Predictions Min                             -4.23755\r\n",
      "trainer/Q2 Predictions Mean                            -1.54115\r\n",
      "trainer/Q2 Predictions Std                              0.980129\r\n",
      "trainer/Q2 Predictions Max                              0.280805\r\n",
      "trainer/Q2 Predictions Min                             -4.01748\r\n",
      "trainer/Q Targets Mean                                 -1.5193\r\n",
      "trainer/Q Targets Std                                   1.02966\r\n",
      "trainer/Q Targets Max                                   0.350677\r\n",
      "trainer/Q Targets Min                                  -4.41199\r\n",
      "trainer/Log Pis Mean                                    0.132319\r\n",
      "trainer/Log Pis Std                                     1.14895\r\n",
      "trainer/Log Pis Max                                     3.76002\r\n",
      "trainer/Log Pis Min                                    -6.51631\r\n",
      "trainer/Policy mu Mean                                 -0.0163182\r\n",
      "trainer/Policy mu Std                                   0.341392\r\n",
      "trainer/Policy mu Max                                   1.57785\r\n",
      "trainer/Policy mu Min                                  -1.65983\r\n",
      "trainer/Policy log std Mean                            -1.28305\r\n",
      "trainer/Policy log std Std                              0.384718\r\n",
      "trainer/Policy log std Max                             -0.30159\r\n",
      "trainer/Policy log std Min                             -2.16013\r\n",
      "trainer/Alpha                                           0.0639025\r\n",
      "trainer/Alpha Loss                                     -5.133\r\n",
      "exploration/num steps total                          1300\r\n",
      "exploration/num paths total                            65\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.317213\r\n",
      "exploration/Rewards Std                                 0.206701\r\n",
      "exploration/Rewards Max                                -0.0785534\r\n",
      "exploration/Rewards Min                                -1.08456\r\n",
      "exploration/Returns Mean                               -6.34426\r\n",
      "exploration/Returns Std                                 3.03259\r\n",
      "exploration/Returns Max                                -4.30967\r\n",
      "exploration/Returns Min                               -12.3355\r\n",
      "exploration/Actions Mean                                0.0176146\r\n",
      "exploration/Actions Std                                 0.249469\r\n",
      "exploration/Actions Max                                 0.793571\r\n",
      "exploration/Actions Min                                -0.562318\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -6.34426\r\n",
      "exploration/env_infos/final/reward_dist Mean            1.10525e-05\r\n",
      "exploration/env_infos/final/reward_dist Std             2.2105e-05\r\n",
      "exploration/env_infos/final/reward_dist Max             5.52624e-05\r\n",
      "exploration/env_infos/final/reward_dist Min             8.32703e-100\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00917166\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.0115859\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0301845\r\n",
      "exploration/env_infos/initial/reward_dist Min           1.11096e-05\r\n",
      "exploration/env_infos/reward_dist Mean                  0.0291417\r\n",
      "exploration/env_infos/reward_dist Std                   0.101679\r\n",
      "exploration/env_infos/reward_dist Max                   0.609998\r\n",
      "exploration/env_infos/reward_dist Min                   1.44801e-101\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.447668\r\n",
      "exploration/env_infos/final/reward_energy Std           0.173098\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.170972\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.6067\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.416838\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.144682\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.210439\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.655991\r\n",
      "exploration/env_infos/reward_energy Mean               -0.307407\r\n",
      "exploration/env_infos/reward_energy Std                 0.174901\r\n",
      "exploration/env_infos/reward_energy Max                -0.0300016\r\n",
      "exploration/env_infos/reward_energy Min                -0.815759\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean       0.324347\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.475971\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.857327\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.902187\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00309135\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0152906\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0318791\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0214156\r\n",
      "exploration/env_infos/end_effector_loc Mean             0.143115\r\n",
      "exploration/env_infos/end_effector_loc Std              0.335973\r\n",
      "exploration/env_infos/end_effector_loc Max              0.857327\r\n",
      "exploration/env_infos/end_effector_loc Min             -1\r\n",
      "evaluation/num steps total                           3000\r\n",
      "evaluation/num paths total                            150\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.171235\r\n",
      "evaluation/Rewards Std                                  0.156176\r\n",
      "evaluation/Rewards Max                                  0.0822294\r\n",
      "evaluation/Rewards Min                                 -0.979425\r\n",
      "evaluation/Returns Mean                                -3.4247\r\n",
      "evaluation/Returns Std                                  2.37878\r\n",
      "evaluation/Returns Max                                 -1.03687\r\n",
      "evaluation/Returns Min                                -12.9465\r\n",
      "evaluation/Actions Mean                                -0.00332473\r\n",
      "evaluation/Actions Std                                  0.113027\r\n",
      "evaluation/Actions Max                                  0.655579\r\n",
      "evaluation/Actions Min                                 -0.545786\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -3.4247\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0091053\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.0369815\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.234501\r\n",
      "evaluation/env_infos/final/reward_dist Min              4.91896e-119\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00550878\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0120953\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0659473\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.36827e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0420782\r\n",
      "evaluation/env_infos/reward_dist Std                    0.136436\r\n",
      "evaluation/env_infos/reward_dist Max                    0.985674\r\n",
      "evaluation/env_infos/reward_dist Min                    1.56268e-124\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.189778\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.121088\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00870755\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.55283\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.157609\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.140839\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.00895795\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.66551\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.121254\r\n",
      "evaluation/env_infos/reward_energy Std                  0.104257\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00162313\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.66551\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean        0.123052\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.380354\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         1\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -0.51224\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00333402\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00668802\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.032779\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0145551\r\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0941151\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.253196\r\n",
      "evaluation/env_infos/end_effector_loc Max               1\r\n",
      "evaluation/env_infos/end_effector_loc Min              -0.71139\r\n",
      "time/data storing (s)                                   0.00349065\r\n",
      "time/evaluation sampling (s)                            1.06136\r\n",
      "time/exploration sampling (s)                           0.139693\r\n",
      "time/logging (s)                                        0.0188483\r\n",
      "time/saving (s)                                         0.0263298\r\n",
      "time/training (s)                                      45.341\r\n",
      "time/epoch (s)                                         46.5907\r\n",
      "time/total (s)                                        127.676\r\n",
      "Epoch                                                   2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------  ---------------\n",
      "2021-05-25 10:27:50.188074 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 3 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1400\n",
      "trainer/QF1 Loss                                        0.00992432\n",
      "trainer/QF2 Loss                                        0.0117483\n",
      "trainer/Policy Loss                                     3.07273\n",
      "trainer/Q1 Predictions Mean                            -2.17948\n",
      "trainer/Q1 Predictions Std                              1.31205\n",
      "trainer/Q1 Predictions Max                             -0.0172831\n",
      "trainer/Q1 Predictions Min                             -7.29756\n",
      "trainer/Q2 Predictions Mean                            -2.1591\n",
      "trainer/Q2 Predictions Std                              1.29779\n",
      "trainer/Q2 Predictions Max                             -0.00159588\n",
      "trainer/Q2 Predictions Min                             -7.20492\n",
      "trainer/Q Targets Mean                                 -2.18751\n",
      "trainer/Q Targets Std                                   1.31597\n",
      "trainer/Q Targets Max                                   0.0423553\n",
      "trainer/Q Targets Min                                  -7.1498\n",
      "trainer/Log Pis Mean                                    1.20588\n",
      "trainer/Log Pis Std                                     1.36652\n",
      "trainer/Log Pis Max                                     6.7834\n",
      "trainer/Log Pis Min                                    -3.3873\n",
      "trainer/Policy mu Mean                                 -0.0163686\n",
      "trainer/Policy mu Std                                   0.633887\n",
      "trainer/Policy mu Max                                   2.41305\n",
      "trainer/Policy mu Min                                  -2.83405\n",
      "trainer/Policy log std Mean                            -1.68811\n",
      "trainer/Policy log std Std                              0.555395\n",
      "trainer/Policy log std Max                              0.351206\n",
      "trainer/Policy log std Min                             -2.70888\n",
      "trainer/Alpha                                           0.0278039\n",
      "trainer/Alpha Loss                                     -2.84399\n",
      "exploration/num steps total                          1400\n",
      "exploration/num paths total                            70\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.218513\n",
      "exploration/Rewards Std                                 0.0870036\n",
      "exploration/Rewards Max                                -0.0625078\n",
      "exploration/Rewards Min                                -0.527785\n",
      "exploration/Returns Mean                               -4.37025\n",
      "exploration/Returns Std                                 0.745795\n",
      "exploration/Returns Max                                -3.18479\n",
      "exploration/Returns Min                                -5.5153\n",
      "exploration/Actions Mean                               -0.00675753\n",
      "exploration/Actions Std                                 0.191701\n",
      "exploration/Actions Max                                 0.585023\n",
      "exploration/Actions Min                                -0.698324\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -4.37025\n",
      "exploration/env_infos/final/reward_dist Mean            1.95884e-07\n",
      "exploration/env_infos/final/reward_dist Std             3.91767e-07\n",
      "exploration/env_infos/final/reward_dist Max             9.79418e-07\n",
      "exploration/env_infos/final/reward_dist Min             4.13074e-31\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0065823\n",
      "exploration/env_infos/initial/reward_dist Std           0.00953403\n",
      "exploration/env_infos/initial/reward_dist Max           0.0246377\n",
      "exploration/env_infos/initial/reward_dist Min           5.58389e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0741105\n",
      "exploration/env_infos/reward_dist Std                   0.213269\n",
      "exploration/env_infos/reward_dist Max                   0.996782\n",
      "exploration/env_infos/reward_dist Min                   4.13074e-31\n",
      "exploration/env_infos/final/reward_energy Mean         -0.34174\n",
      "exploration/env_infos/final/reward_energy Std           0.133296\n",
      "exploration/env_infos/final/reward_energy Max          -0.180776\n",
      "exploration/env_infos/final/reward_energy Min          -0.5564\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.194765\n",
      "exploration/env_infos/initial/reward_energy Std         0.0711915\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0959558\n",
      "exploration/env_infos/initial/reward_energy Min        -0.266792\n",
      "exploration/env_infos/reward_energy Mean               -0.227697\n",
      "exploration/env_infos/reward_energy Std                 0.147458\n",
      "exploration/env_infos/reward_energy Max                -0.0210123\n",
      "exploration/env_infos/reward_energy Min                -0.725702\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0748444\n",
      "exploration/env_infos/final/end_effector_loc Std        0.382551\n",
      "exploration/env_infos/final/end_effector_loc Max        0.524773\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.723146\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00169161\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00713376\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0111471\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0120533\n",
      "exploration/env_infos/end_effector_loc Mean             0.00363569\n",
      "exploration/env_infos/end_effector_loc Std              0.223757\n",
      "exploration/env_infos/end_effector_loc Max              0.524773\n",
      "exploration/env_infos/end_effector_loc Min             -0.723146\n",
      "evaluation/num steps total                           4000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.154526\n",
      "evaluation/Rewards Std                                  0.126546\n",
      "evaluation/Rewards Max                                  0.0852592\n",
      "evaluation/Rewards Min                                 -1.04159\n",
      "evaluation/Returns Mean                                -3.09053\n",
      "evaluation/Returns Std                                  1.68053\n",
      "evaluation/Returns Max                                 -0.385755\n",
      "evaluation/Returns Min                                 -9.44947\n",
      "evaluation/Actions Mean                                -0.0106644\n",
      "evaluation/Actions Std                                  0.115393\n",
      "evaluation/Actions Max                                  0.447187\n",
      "evaluation/Actions Min                                 -0.590926\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -3.09053\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0139149\n",
      "evaluation/env_infos/final/reward_dist Std              0.0439649\n",
      "evaluation/env_infos/final/reward_dist Max              0.234655\n",
      "evaluation/env_infos/final/reward_dist Min              1.7802e-77\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00607394\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00989201\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0349882\n",
      "evaluation/env_infos/initial/reward_dist Min            6.16823e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0783719\n",
      "evaluation/env_infos/reward_dist Std                    0.188876\n",
      "evaluation/env_infos/reward_dist Max                    0.983201\n",
      "evaluation/env_infos/reward_dist Min                    1.7802e-77\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194703\n",
      "evaluation/env_infos/final/reward_energy Std            0.119711\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0241036\n",
      "evaluation/env_infos/final/reward_energy Min           -0.594926\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.175934\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0973403\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0299292\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.542958\n",
      "evaluation/env_infos/reward_energy Mean                -0.136909\n",
      "evaluation/env_infos/reward_energy Std                  0.0900816\n",
      "evaluation/env_infos/reward_energy Max                 -0.00539068\n",
      "evaluation/env_infos/reward_energy Min                 -0.594926\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.111028\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.366704\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.688137\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000338744\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00710071\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0223594\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.018419\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0352607\n",
      "evaluation/env_infos/end_effector_loc Std               0.245378\n",
      "evaluation/env_infos/end_effector_loc Max               0.688137\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00297755\n",
      "time/evaluation sampling (s)                            0.949526\n",
      "time/exploration sampling (s)                           0.12669\n",
      "time/logging (s)                                        0.0193505\n",
      "time/saving (s)                                         0.0291837\n",
      "time/training (s)                                      44.8391\n",
      "time/epoch (s)                                         45.9668\n",
      "time/total (s)                                        173.713\n",
      "Epoch                                                   3\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:37.803683 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 4 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1500\n",
      "trainer/QF1 Loss                                        0.00980622\n",
      "trainer/QF2 Loss                                        0.00970217\n",
      "trainer/Policy Loss                                     4.19382\n",
      "trainer/Q1 Predictions Mean                            -2.77101\n",
      "trainer/Q1 Predictions Std                              1.67244\n",
      "trainer/Q1 Predictions Max                             -0.181449\n",
      "trainer/Q1 Predictions Min                             -9.09773\n",
      "trainer/Q2 Predictions Mean                            -2.83605\n",
      "trainer/Q2 Predictions Std                              1.68902\n",
      "trainer/Q2 Predictions Max                             -0.220586\n",
      "trainer/Q2 Predictions Min                             -9.08909\n",
      "trainer/Q Targets Mean                                 -2.80172\n",
      "trainer/Q Targets Std                                   1.67376\n",
      "trainer/Q Targets Max                                  -0.304971\n",
      "trainer/Q Targets Min                                  -9.10335\n",
      "trainer/Log Pis Mean                                    1.73946\n",
      "trainer/Log Pis Std                                     1.47903\n",
      "trainer/Log Pis Max                                     8.6428\n",
      "trainer/Log Pis Min                                    -5.90367\n",
      "trainer/Policy mu Mean                                 -0.0116877\n",
      "trainer/Policy mu Std                                   0.768037\n",
      "trainer/Policy mu Max                                   3.12088\n",
      "trainer/Policy mu Min                                  -2.7154\n",
      "trainer/Policy log std Mean                            -1.85284\n",
      "trainer/Policy log std Std                              0.609531\n",
      "trainer/Policy log std Max                             -0.248283\n",
      "trainer/Policy log std Min                             -2.81579\n",
      "trainer/Alpha                                           0.0179338\n",
      "trainer/Alpha Loss                                     -1.04753\n",
      "exploration/num steps total                          1500\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.185039\n",
      "exploration/Rewards Std                                 0.0864499\n",
      "exploration/Rewards Max                                -0.00210195\n",
      "exploration/Rewards Min                                -0.502919\n",
      "exploration/Returns Mean                               -3.70077\n",
      "exploration/Returns Std                                 0.686651\n",
      "exploration/Returns Max                                -2.5347\n",
      "exploration/Returns Min                                -4.67839\n",
      "exploration/Actions Mean                               -0.0191028\n",
      "exploration/Actions Std                                 0.119402\n",
      "exploration/Actions Max                                 0.34221\n",
      "exploration/Actions Min                                -0.37382\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -3.70077\n",
      "exploration/env_infos/final/reward_dist Mean            2.69802e-06\n",
      "exploration/env_infos/final/reward_dist Std             3.4599e-06\n",
      "exploration/env_infos/final/reward_dist Max             8.42812e-06\n",
      "exploration/env_infos/final/reward_dist Min             4.61265e-28\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120991\n",
      "exploration/env_infos/initial/reward_dist Std           0.0239685\n",
      "exploration/env_infos/initial/reward_dist Max           0.0600351\n",
      "exploration/env_infos/initial/reward_dist Min           9.77122e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0578942\n",
      "exploration/env_infos/reward_dist Std                   0.155278\n",
      "exploration/env_infos/reward_dist Max                   0.794904\n",
      "exploration/env_infos/reward_dist Min                   4.61265e-28\n",
      "exploration/env_infos/final/reward_energy Mean         -0.241985\n",
      "exploration/env_infos/final/reward_energy Std           0.0910221\n",
      "exploration/env_infos/final/reward_energy Max          -0.153321\n",
      "exploration/env_infos/final/reward_energy Min          -0.405928\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.192135\n",
      "exploration/env_infos/initial/reward_energy Std         0.0863032\n",
      "exploration/env_infos/initial/reward_energy Max        -0.124289\n",
      "exploration/env_infos/initial/reward_energy Min        -0.346944\n",
      "exploration/env_infos/reward_energy Mean               -0.150374\n",
      "exploration/env_infos/reward_energy Std                 0.0814307\n",
      "exploration/env_infos/reward_energy Max                -0.0108661\n",
      "exploration/env_infos/reward_energy Min                -0.405928\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0977437\n",
      "exploration/env_infos/final/end_effector_loc Std        0.328272\n",
      "exploration/env_infos/final/end_effector_loc Max        0.412813\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.550653\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00133301\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00732653\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0171105\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0098357\n",
      "exploration/env_infos/end_effector_loc Mean            -0.00177883\n",
      "exploration/env_infos/end_effector_loc Std              0.193416\n",
      "exploration/env_infos/end_effector_loc Max              0.412813\n",
      "exploration/env_infos/end_effector_loc Min             -0.550653\n",
      "evaluation/num steps total                           5000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.138405\n",
      "evaluation/Rewards Std                                  0.127079\n",
      "evaluation/Rewards Max                                  0.175818\n",
      "evaluation/Rewards Min                                 -0.875634\n",
      "evaluation/Returns Mean                                -2.7681\n",
      "evaluation/Returns Std                                  1.6903\n",
      "evaluation/Returns Max                                  0.848121\n",
      "evaluation/Returns Min                                 -7.44648\n",
      "evaluation/Actions Mean                                -0.0132224\n",
      "evaluation/Actions Std                                  0.123843\n",
      "evaluation/Actions Max                                  0.558661\n",
      "evaluation/Actions Min                                 -0.756\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.7681\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0327722\n",
      "evaluation/env_infos/final/reward_dist Std              0.0957473\n",
      "evaluation/env_infos/final/reward_dist Max              0.495844\n",
      "evaluation/env_infos/final/reward_dist Min              1.7421e-111\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.0056543\n",
      "evaluation/env_infos/initial/reward_dist Std            0.009642\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0369354\n",
      "evaluation/env_infos/initial/reward_dist Min            9.7795e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0708348\n",
      "evaluation/env_infos/reward_dist Std                    0.177649\n",
      "evaluation/env_infos/reward_dist Max                    0.998196\n",
      "evaluation/env_infos/reward_dist Min                    1.7421e-111\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.226136\n",
      "evaluation/env_infos/final/reward_energy Std            0.142231\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0337407\n",
      "evaluation/env_infos/final/reward_energy Min           -0.616076\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.24447\n",
      "evaluation/env_infos/initial/reward_energy Std          0.185361\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0357208\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.872488\n",
      "evaluation/env_infos/reward_energy Mean                -0.139616\n",
      "evaluation/env_infos/reward_energy Std                  0.107384\n",
      "evaluation/env_infos/reward_energy Max                 -0.00332395\n",
      "evaluation/env_infos/reward_energy Min                 -0.872488\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.120011\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.356042\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.965892\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.000624074\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.0108289\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.027933\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0378\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0319308\n",
      "evaluation/env_infos/end_effector_loc Std               0.22982\n",
      "evaluation/env_infos/end_effector_loc Max               0.965892\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00349711\n",
      "time/evaluation sampling (s)                            1.34777\n",
      "time/exploration sampling (s)                           0.132789\n",
      "time/logging (s)                                        0.026126\n",
      "time/saving (s)                                         0.0350646\n",
      "time/training (s)                                      45.9903\n",
      "time/epoch (s)                                         47.5356\n",
      "time/total (s)                                        221.335\n",
      "Epoch                                                   4\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:26.335015 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 5 finished\r\n",
      "---------------------------------------------------  --------------\r\n",
      "replay_buffer/size                                   1600\r\n",
      "trainer/QF1 Loss                                        0.00391405\r\n",
      "trainer/QF2 Loss                                        0.00828227\r\n",
      "trainer/Policy Loss                                     4.53958\r\n",
      "trainer/Q1 Predictions Mean                            -2.71977\r\n",
      "trainer/Q1 Predictions Std                              1.61255\r\n",
      "trainer/Q1 Predictions Max                             -0.24123\r\n",
      "trainer/Q1 Predictions Min                            -10.4015\r\n",
      "trainer/Q2 Predictions Mean                            -2.6909\r\n",
      "trainer/Q2 Predictions Std                              1.61907\r\n",
      "trainer/Q2 Predictions Max                             -0.162658\r\n",
      "trainer/Q2 Predictions Min                            -10.2839\r\n",
      "trainer/Q Targets Mean                                 -2.72431\r\n",
      "trainer/Q Targets Std                                   1.61548\r\n",
      "trainer/Q Targets Max                                  -0.134168\r\n",
      "trainer/Q Targets Min                                 -10.3591\r\n",
      "trainer/Log Pis Mean                                    2.15081\r\n",
      "trainer/Log Pis Std                                     1.18245\r\n",
      "trainer/Log Pis Max                                     5.55259\r\n",
      "trainer/Log Pis Min                                    -4.33486\r\n",
      "trainer/Policy mu Mean                                 -0.0691641\r\n",
      "trainer/Policy mu Std                                   0.567645\r\n",
      "trainer/Policy mu Max                                   2.31754\r\n",
      "trainer/Policy mu Min                                  -2.55865\r\n",
      "trainer/Policy log std Mean                            -2.14175\r\n",
      "trainer/Policy log std Std                              0.568775\r\n",
      "trainer/Policy log std Max                             -0.37912\r\n",
      "trainer/Policy log std Min                             -3.00872\r\n",
      "trainer/Alpha                                           0.0152722\r\n",
      "trainer/Alpha Loss                                      0.630623\r\n",
      "exploration/num steps total                          1600\r\n",
      "exploration/num paths total                            80\r\n",
      "exploration/path length Mean                           20\r\n",
      "exploration/path length Std                             0\r\n",
      "exploration/path length Max                            20\r\n",
      "exploration/path length Min                            20\r\n",
      "exploration/Rewards Mean                               -0.159711\r\n",
      "exploration/Rewards Std                                 0.077504\r\n",
      "exploration/Rewards Max                                -0.0124729\r\n",
      "exploration/Rewards Min                                -0.407509\r\n",
      "exploration/Returns Mean                               -3.19422\r\n",
      "exploration/Returns Std                                 1.14011\r\n",
      "exploration/Returns Max                                -1.35609\r\n",
      "exploration/Returns Min                                -4.27307\r\n",
      "exploration/Actions Mean                               -0.0130702\r\n",
      "exploration/Actions Std                                 0.182171\r\n",
      "exploration/Actions Max                                 0.616261\r\n",
      "exploration/Actions Min                                -0.672497\r\n",
      "exploration/Num Paths                                   5\r\n",
      "exploration/Average Returns                            -3.19422\r\n",
      "exploration/env_infos/final/reward_dist Mean            0.129612\r\n",
      "exploration/env_infos/final/reward_dist Std             0.259203\r\n",
      "exploration/env_infos/final/reward_dist Max             0.648019\r\n",
      "exploration/env_infos/final/reward_dist Min             7.32844e-39\r\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00216965\r\n",
      "exploration/env_infos/initial/reward_dist Std           0.00427531\r\n",
      "exploration/env_infos/initial/reward_dist Max           0.0107201\r\n",
      "exploration/env_infos/initial/reward_dist Min           4.88192e-06\r\n",
      "exploration/env_infos/reward_dist Mean                  0.128057\r\n",
      "exploration/env_infos/reward_dist Std                   0.281165\r\n",
      "exploration/env_infos/reward_dist Max                   0.961812\r\n",
      "exploration/env_infos/reward_dist Min                   7.32844e-39\r\n",
      "exploration/env_infos/final/reward_energy Mean         -0.245481\r\n",
      "exploration/env_infos/final/reward_energy Std           0.0895232\r\n",
      "exploration/env_infos/final/reward_energy Max          -0.107677\r\n",
      "exploration/env_infos/final/reward_energy Min          -0.347745\r\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.177843\r\n",
      "exploration/env_infos/initial/reward_energy Std         0.156465\r\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0534861\r\n",
      "exploration/env_infos/initial/reward_energy Min        -0.480436\r\n",
      "exploration/env_infos/reward_energy Mean               -0.202719\r\n",
      "exploration/env_infos/reward_energy Std                 0.16006\r\n",
      "exploration/env_infos/reward_energy Max                -0.00457819\r\n",
      "exploration/env_infos/reward_energy Min                -0.768262\r\n",
      "exploration/env_infos/final/reward_safety Mean          0\r\n",
      "exploration/env_infos/final/reward_safety Std           0\r\n",
      "exploration/env_infos/final/reward_safety Max           0\r\n",
      "exploration/env_infos/final/reward_safety Min           0\r\n",
      "exploration/env_infos/initial/reward_safety Mean        0\r\n",
      "exploration/env_infos/initial/reward_safety Std         0\r\n",
      "exploration/env_infos/initial/reward_safety Max         0\r\n",
      "exploration/env_infos/initial/reward_safety Min         0\r\n",
      "exploration/env_infos/reward_safety Mean                0\r\n",
      "exploration/env_infos/reward_safety Std                 0\r\n",
      "exploration/env_infos/reward_safety Max                 0\r\n",
      "exploration/env_infos/reward_safety Min                 0\r\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0926429\r\n",
      "exploration/env_infos/final/end_effector_loc Std        0.236094\r\n",
      "exploration/env_infos/final/end_effector_loc Max        0.2314\r\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.48896\r\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00360691\r\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00755822\r\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00599825\r\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0230505\r\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0116781\r\n",
      "exploration/env_infos/end_effector_loc Std              0.137859\r\n",
      "exploration/env_infos/end_effector_loc Max              0.266246\r\n",
      "exploration/env_infos/end_effector_loc Min             -0.48896\r\n",
      "evaluation/num steps total                           6000\r\n",
      "evaluation/num paths total                            300\r\n",
      "evaluation/path length Mean                            20\r\n",
      "evaluation/path length Std                              0\r\n",
      "evaluation/path length Max                             20\r\n",
      "evaluation/path length Min                             20\r\n",
      "evaluation/Rewards Mean                                -0.140115\r\n",
      "evaluation/Rewards Std                                  0.0954639\r\n",
      "evaluation/Rewards Max                                  0.122094\r\n",
      "evaluation/Rewards Min                                 -0.676226\r\n",
      "evaluation/Returns Mean                                -2.8023\r\n",
      "evaluation/Returns Std                                  1.18033\r\n",
      "evaluation/Returns Max                                 -0.53575\r\n",
      "evaluation/Returns Min                                 -5.67409\r\n",
      "evaluation/Actions Mean                                -0.0217011\r\n",
      "evaluation/Actions Std                                  0.0957455\r\n",
      "evaluation/Actions Max                                  0.397837\r\n",
      "evaluation/Actions Min                                 -0.409855\r\n",
      "evaluation/Num Paths                                   50\r\n",
      "evaluation/Average Returns                             -2.8023\r\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0370594\r\n",
      "evaluation/env_infos/final/reward_dist Std              0.138308\r\n",
      "evaluation/env_infos/final/reward_dist Max              0.783803\r\n",
      "evaluation/env_infos/final/reward_dist Min              3.64493e-74\r\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00665165\r\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00947028\r\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0324627\r\n",
      "evaluation/env_infos/initial/reward_dist Min            1.2328e-06\r\n",
      "evaluation/env_infos/reward_dist Mean                   0.0515727\r\n",
      "evaluation/env_infos/reward_dist Std                    0.134432\r\n",
      "evaluation/env_infos/reward_dist Max                    0.9591\r\n",
      "evaluation/env_infos/reward_dist Min                    3.64493e-74\r\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.187431\r\n",
      "evaluation/env_infos/final/reward_energy Std            0.118135\r\n",
      "evaluation/env_infos/final/reward_energy Max           -0.043561\r\n",
      "evaluation/env_infos/final/reward_energy Min           -0.528155\r\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.153871\r\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0687537\r\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0442669\r\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.364019\r\n",
      "evaluation/env_infos/reward_energy Mean                -0.114251\r\n",
      "evaluation/env_infos/reward_energy Std                  0.0788854\r\n",
      "evaluation/env_infos/reward_energy Max                 -0.00217171\r\n",
      "evaluation/env_infos/reward_energy Min                 -0.528155\r\n",
      "evaluation/env_infos/final/reward_safety Mean           0\r\n",
      "evaluation/env_infos/final/reward_safety Std            0\r\n",
      "evaluation/env_infos/final/reward_safety Max            0\r\n",
      "evaluation/env_infos/final/reward_safety Min            0\r\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\r\n",
      "evaluation/env_infos/initial/reward_safety Std          0\r\n",
      "evaluation/env_infos/initial/reward_safety Max          0\r\n",
      "evaluation/env_infos/initial/reward_safety Min          0\r\n",
      "evaluation/env_infos/reward_safety Mean                 0\r\n",
      "evaluation/env_infos/reward_safety Std                  0\r\n",
      "evaluation/env_infos/reward_safety Max                  0\r\n",
      "evaluation/env_infos/reward_safety Min                  0\r\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.201088\r\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.29125\r\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.319776\r\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\r\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00126295\r\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00582315\r\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0111058\r\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0180466\r\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0514303\r\n",
      "evaluation/env_infos/end_effector_loc Std               0.188284\r\n",
      "evaluation/env_infos/end_effector_loc Max               0.333243\r\n",
      "evaluation/env_infos/end_effector_loc Min              -1\r\n",
      "time/data storing (s)                                   0.00319928\r\n",
      "time/evaluation sampling (s)                            1.33085\r\n",
      "time/exploration sampling (s)                           0.144429\r\n",
      "time/logging (s)                                        0.020892\r\n",
      "time/saving (s)                                         0.0299084\r\n",
      "time/training (s)                                      46.882\r\n",
      "time/epoch (s)                                         48.4113\r\n",
      "time/total (s)                                        269.86\r\n",
      "Epoch                                                   5\r\n",
      "---------------------------------------------------  --------------\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:11.294383 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 6 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1700\n",
      "trainer/QF1 Loss                                        0.00401628\n",
      "trainer/QF2 Loss                                        0.00301326\n",
      "trainer/Policy Loss                                     4.5679\n",
      "trainer/Q1 Predictions Mean                            -2.94904\n",
      "trainer/Q1 Predictions Std                              1.78239\n",
      "trainer/Q1 Predictions Max                             -0.145669\n",
      "trainer/Q1 Predictions Min                             -9.25414\n",
      "trainer/Q2 Predictions Mean                            -2.95907\n",
      "trainer/Q2 Predictions Std                              1.78011\n",
      "trainer/Q2 Predictions Max                             -0.138948\n",
      "trainer/Q2 Predictions Min                             -9.21313\n",
      "trainer/Q Targets Mean                                 -2.96176\n",
      "trainer/Q Targets Std                                   1.78064\n",
      "trainer/Q Targets Max                                  -0.134168\n",
      "trainer/Q Targets Min                                  -9.25124\n",
      "trainer/Log Pis Mean                                    1.98999\n",
      "trainer/Log Pis Std                                     1.43828\n",
      "trainer/Log Pis Max                                     7.88639\n",
      "trainer/Log Pis Min                                    -3.32601\n",
      "trainer/Policy mu Mean                                 -0.00859346\n",
      "trainer/Policy mu Std                                   0.609074\n",
      "trainer/Policy mu Max                                   2.67305\n",
      "trainer/Policy mu Min                                  -2.60096\n",
      "trainer/Policy log std Mean                            -2.12876\n",
      "trainer/Policy log std Std                              0.567733\n",
      "trainer/Policy log std Max                             -0.163518\n",
      "trainer/Policy log std Min                             -3.13718\n",
      "trainer/Alpha                                           0.0148503\n",
      "trainer/Alpha Loss                                     -0.0421255\n",
      "exploration/num steps total                          1700\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.12011\n",
      "exploration/Rewards Std                                 0.0995193\n",
      "exploration/Rewards Max                                 0.0391944\n",
      "exploration/Rewards Min                                -0.515167\n",
      "exploration/Returns Mean                               -2.40221\n",
      "exploration/Returns Std                                 1.31784\n",
      "exploration/Returns Max                                -0.940018\n",
      "exploration/Returns Min                                -4.38468\n",
      "exploration/Actions Mean                               -0.0201889\n",
      "exploration/Actions Std                                 0.124215\n",
      "exploration/Actions Max                                 0.457947\n",
      "exploration/Actions Min                                -0.424974\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40221\n",
      "exploration/env_infos/final/reward_dist Mean            0.0403122\n",
      "exploration/env_infos/final/reward_dist Std             0.0806241\n",
      "exploration/env_infos/final/reward_dist Max             0.20156\n",
      "exploration/env_infos/final/reward_dist Min             2.46859e-46\n",
      "exploration/env_infos/initial/reward_dist Mean          0.0120158\n",
      "exploration/env_infos/initial/reward_dist Std           0.0158164\n",
      "exploration/env_infos/initial/reward_dist Max           0.0433042\n",
      "exploration/env_infos/initial/reward_dist Min           4.82377e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0555058\n",
      "exploration/env_infos/reward_dist Std                   0.0871416\n",
      "exploration/env_infos/reward_dist Max                   0.48784\n",
      "exploration/env_infos/reward_dist Min                   2.46859e-46\n",
      "exploration/env_infos/final/reward_energy Mean         -0.207155\n",
      "exploration/env_infos/final/reward_energy Std           0.0950714\n",
      "exploration/env_infos/final/reward_energy Max          -0.102552\n",
      "exploration/env_infos/final/reward_energy Min          -0.366865\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.229679\n",
      "exploration/env_infos/initial/reward_energy Std         0.161923\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0247831\n",
      "exploration/env_infos/initial/reward_energy Min        -0.483169\n",
      "exploration/env_infos/reward_energy Mean               -0.153621\n",
      "exploration/env_infos/reward_energy Std                 0.0898589\n",
      "exploration/env_infos/reward_energy Max                -0.0215799\n",
      "exploration/env_infos/reward_energy Min                -0.483169\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.185485\n",
      "exploration/env_infos/final/end_effector_loc Std        0.318867\n",
      "exploration/env_infos/final/end_effector_loc Max        0.246597\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.711116\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.00150753\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00982047\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.012737\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0212487\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0659243\n",
      "exploration/env_infos/end_effector_loc Std              0.195697\n",
      "exploration/env_infos/end_effector_loc Max              0.252024\n",
      "exploration/env_infos/end_effector_loc Min             -0.711116\n",
      "evaluation/num steps total                           7000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.126556\n",
      "evaluation/Rewards Std                                  0.122697\n",
      "evaluation/Rewards Max                                  0.107346\n",
      "evaluation/Rewards Min                                 -0.939904\n",
      "evaluation/Returns Mean                                -2.53112\n",
      "evaluation/Returns Std                                  1.3597\n",
      "evaluation/Returns Max                                 -0.361605\n",
      "evaluation/Returns Min                                 -8.06351\n",
      "evaluation/Actions Mean                                -0.0139378\n",
      "evaluation/Actions Std                                  0.102267\n",
      "evaluation/Actions Max                                  0.43149\n",
      "evaluation/Actions Min                                 -0.801056\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.53112\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0116841\n",
      "evaluation/env_infos/final/reward_dist Std              0.0428651\n",
      "evaluation/env_infos/final/reward_dist Max              0.245459\n",
      "evaluation/env_infos/final/reward_dist Min              1.55703e-63\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00737524\n",
      "evaluation/env_infos/initial/reward_dist Std            0.0124404\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0593836\n",
      "evaluation/env_infos/initial/reward_dist Min            3.99796e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0540835\n",
      "evaluation/env_infos/reward_dist Std                    0.146721\n",
      "evaluation/env_infos/reward_dist Max                    0.960647\n",
      "evaluation/env_infos/reward_dist Min                    2.778e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.217906\n",
      "evaluation/env_infos/final/reward_energy Std            0.184873\n",
      "evaluation/env_infos/final/reward_energy Max           -0.0116104\n",
      "evaluation/env_infos/final/reward_energy Min           -0.878679\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.154152\n",
      "evaluation/env_infos/initial/reward_energy Std          0.0884744\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0229355\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.448848\n",
      "evaluation/env_infos/reward_energy Mean                -0.107257\n",
      "evaluation/env_infos/reward_energy Std                  0.0990024\n",
      "evaluation/env_infos/reward_energy Max                 -0.00113453\n",
      "evaluation/env_infos/reward_energy Min                 -0.878679\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.023272\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.328776\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.518937\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.0025474\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00574448\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0215745\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0149535\n",
      "evaluation/env_infos/end_effector_loc Mean              0.0251593\n",
      "evaluation/env_infos/end_effector_loc Std               0.194369\n",
      "evaluation/env_infos/end_effector_loc Max               0.715969\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00296628\n",
      "time/evaluation sampling (s)                            1.01306\n",
      "time/exploration sampling (s)                           0.118467\n",
      "time/logging (s)                                        0.0265779\n",
      "time/saving (s)                                         0.0293528\n",
      "time/training (s)                                      43.6627\n",
      "time/epoch (s)                                         44.8531\n",
      "time/total (s)                                        314.825\n",
      "Epoch                                                   6\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:58.238699 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 7 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1800\n",
      "trainer/QF1 Loss                                        0.00532094\n",
      "trainer/QF2 Loss                                        0.00913572\n",
      "trainer/Policy Loss                                     4.66646\n",
      "trainer/Q1 Predictions Mean                            -3.1223\n",
      "trainer/Q1 Predictions Std                              1.9577\n",
      "trainer/Q1 Predictions Max                             -0.148908\n",
      "trainer/Q1 Predictions Min                            -10.6049\n",
      "trainer/Q2 Predictions Mean                            -3.04868\n",
      "trainer/Q2 Predictions Std                              1.9379\n",
      "trainer/Q2 Predictions Max                             -0.103933\n",
      "trainer/Q2 Predictions Min                            -10.6075\n",
      "trainer/Q Targets Mean                                 -3.09267\n",
      "trainer/Q Targets Std                                   1.95351\n",
      "trainer/Q Targets Max                                  -0.114265\n",
      "trainer/Q Targets Min                                 -10.6191\n",
      "trainer/Log Pis Mean                                    1.91905\n",
      "trainer/Log Pis Std                                     1.22466\n",
      "trainer/Log Pis Max                                     7.41546\n",
      "trainer/Log Pis Min                                    -3.28219\n",
      "trainer/Policy mu Mean                                 -0.0383501\n",
      "trainer/Policy mu Std                                   0.657596\n",
      "trainer/Policy mu Max                                   2.85599\n",
      "trainer/Policy mu Min                                  -2.87545\n",
      "trainer/Policy log std Mean                            -2.07488\n",
      "trainer/Policy log std Std                              0.596162\n",
      "trainer/Policy log std Max                             -0.0168958\n",
      "trainer/Policy log std Min                             -2.9021\n",
      "trainer/Alpha                                           0.014825\n",
      "trainer/Alpha Loss                                     -0.340906\n",
      "exploration/num steps total                          1800\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.109248\n",
      "exploration/Rewards Std                                 0.078963\n",
      "exploration/Rewards Max                                 0.00457018\n",
      "exploration/Rewards Min                                -0.349884\n",
      "exploration/Returns Mean                               -2.18496\n",
      "exploration/Returns Std                                 1.01039\n",
      "exploration/Returns Max                                -1.00419\n",
      "exploration/Returns Min                                -4.03704\n",
      "exploration/Actions Mean                                0.00175343\n",
      "exploration/Actions Std                                 0.136674\n",
      "exploration/Actions Max                                 0.361814\n",
      "exploration/Actions Min                                -0.312836\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.18496\n",
      "exploration/env_infos/final/reward_dist Mean            0.0328673\n",
      "exploration/env_infos/final/reward_dist Std             0.054723\n",
      "exploration/env_infos/final/reward_dist Max             0.140989\n",
      "exploration/env_infos/final/reward_dist Min             4.04595e-10\n",
      "exploration/env_infos/initial/reward_dist Mean          0.00521036\n",
      "exploration/env_infos/initial/reward_dist Std           0.00504233\n",
      "exploration/env_infos/initial/reward_dist Max           0.0125878\n",
      "exploration/env_infos/initial/reward_dist Min           5.22559e-06\n",
      "exploration/env_infos/reward_dist Mean                  0.0718827\n",
      "exploration/env_infos/reward_dist Std                   0.116213\n",
      "exploration/env_infos/reward_dist Max                   0.51901\n",
      "exploration/env_infos/reward_dist Min                   4.04595e-10\n",
      "exploration/env_infos/final/reward_energy Mean         -0.192104\n",
      "exploration/env_infos/final/reward_energy Std           0.117009\n",
      "exploration/env_infos/final/reward_energy Max          -0.0606609\n",
      "exploration/env_infos/final/reward_energy Min          -0.355326\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.184392\n",
      "exploration/env_infos/initial/reward_energy Std         0.109897\n",
      "exploration/env_infos/initial/reward_energy Max        -0.0346794\n",
      "exploration/env_infos/initial/reward_energy Min        -0.367841\n",
      "exploration/env_infos/reward_energy Mean               -0.165589\n",
      "exploration/env_infos/reward_energy Std                 0.0997282\n",
      "exploration/env_infos/reward_energy Max                -0.0136825\n",
      "exploration/env_infos/reward_energy Min                -0.401459\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.00647823\n",
      "exploration/env_infos/final/end_effector_loc Std        0.250561\n",
      "exploration/env_infos/final/end_effector_loc Max        0.417808\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.369255\n",
      "exploration/env_infos/initial/end_effector_loc Mean    -0.000362005\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.00758064\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.00967479\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0156418\n",
      "exploration/env_infos/end_effector_loc Mean             0.00333751\n",
      "exploration/env_infos/end_effector_loc Std              0.154675\n",
      "exploration/env_infos/end_effector_loc Max              0.417808\n",
      "exploration/env_infos/end_effector_loc Min             -0.369255\n",
      "evaluation/num steps total                           8000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.120225\n",
      "evaluation/Rewards Std                                  0.114706\n",
      "evaluation/Rewards Max                                  0.10838\n",
      "evaluation/Rewards Min                                 -0.824555\n",
      "evaluation/Returns Mean                                -2.40449\n",
      "evaluation/Returns Std                                  1.31461\n",
      "evaluation/Returns Max                                 -0.224426\n",
      "evaluation/Returns Min                                 -5.54946\n",
      "evaluation/Actions Mean                                -0.012673\n",
      "evaluation/Actions Std                                  0.0939277\n",
      "evaluation/Actions Max                                  0.474986\n",
      "evaluation/Actions Min                                 -0.631368\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.40449\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0359109\n",
      "evaluation/env_infos/final/reward_dist Std              0.126213\n",
      "evaluation/env_infos/final/reward_dist Max              0.759091\n",
      "evaluation/env_infos/final/reward_dist Min              4.1122e-70\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00473938\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00780024\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0308037\n",
      "evaluation/env_infos/initial/reward_dist Min            8.80153e-07\n",
      "evaluation/env_infos/reward_dist Mean                   0.0778644\n",
      "evaluation/env_infos/reward_dist Std                    0.183512\n",
      "evaluation/env_infos/reward_dist Max                    0.993649\n",
      "evaluation/env_infos/reward_dist Min                    4.1122e-70\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.194139\n",
      "evaluation/env_infos/final/reward_energy Std            0.166278\n",
      "evaluation/env_infos/final/reward_energy Max           -0.011186\n",
      "evaluation/env_infos/final/reward_energy Min           -0.631476\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.138763\n",
      "evaluation/env_infos/initial/reward_energy Std          0.073323\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0173869\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.401362\n",
      "evaluation/env_infos/reward_energy Mean                -0.100189\n",
      "evaluation/env_infos/reward_energy Std                  0.0890407\n",
      "evaluation/env_infos/reward_energy Max                 -0.0037521\n",
      "evaluation/env_infos/reward_energy Min                 -0.631476\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0413599\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.33914\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.483354\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00209752\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00513709\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0132112\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0181754\n",
      "evaluation/env_infos/end_effector_loc Mean              0.00788367\n",
      "evaluation/env_infos/end_effector_loc Std               0.191619\n",
      "evaluation/env_infos/end_effector_loc Max               0.483354\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00345626\n",
      "time/evaluation sampling (s)                            1.41541\n",
      "time/exploration sampling (s)                           0.128368\n",
      "time/logging (s)                                        0.0210347\n",
      "time/saving (s)                                         0.0292165\n",
      "time/training (s)                                      45.2144\n",
      "time/epoch (s)                                         46.8119\n",
      "time/total (s)                                        361.761\n",
      "Epoch                                                   7\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:44.198167 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 8 finished\n",
      "---------------------------------------------------  --------------\n",
      "replay_buffer/size                                   1900\n",
      "trainer/QF1 Loss                                        0.00315309\n",
      "trainer/QF2 Loss                                        0.0186633\n",
      "trainer/Policy Loss                                     4.69278\n",
      "trainer/Q1 Predictions Mean                            -3.04336\n",
      "trainer/Q1 Predictions Std                              1.76928\n",
      "trainer/Q1 Predictions Max                             -0.101967\n",
      "trainer/Q1 Predictions Min                            -10.13\n",
      "trainer/Q2 Predictions Mean                            -3.15417\n",
      "trainer/Q2 Predictions Std                              1.78896\n",
      "trainer/Q2 Predictions Max                             -0.116019\n",
      "trainer/Q2 Predictions Min                            -10.1045\n",
      "trainer/Q Targets Mean                                 -3.05003\n",
      "trainer/Q Targets Std                                   1.77127\n",
      "trainer/Q Targets Max                                  -0.119584\n",
      "trainer/Q Targets Min                                 -10.2059\n",
      "trainer/Log Pis Mean                                    1.89864\n",
      "trainer/Log Pis Std                                     1.52933\n",
      "trainer/Log Pis Max                                     7.42613\n",
      "trainer/Log Pis Min                                    -2.84766\n",
      "trainer/Policy mu Mean                                  0.00674611\n",
      "trainer/Policy mu Std                                   0.796232\n",
      "trainer/Policy mu Max                                   2.85768\n",
      "trainer/Policy mu Min                                  -2.94379\n",
      "trainer/Policy log std Mean                            -1.96356\n",
      "trainer/Policy log std Std                              0.660782\n",
      "trainer/Policy log std Max                              0.0122131\n",
      "trainer/Policy log std Min                             -3.01445\n",
      "trainer/Alpha                                           0.0155872\n",
      "trainer/Alpha Loss                                     -0.421793\n",
      "exploration/num steps total                          1900\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                           20\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                            20\n",
      "exploration/path length Min                            20\n",
      "exploration/Rewards Mean                               -0.120469\n",
      "exploration/Rewards Std                                 0.0981408\n",
      "exploration/Rewards Max                                 0.028481\n",
      "exploration/Rewards Min                                -0.542977\n",
      "exploration/Returns Mean                               -2.40938\n",
      "exploration/Returns Std                                 0.874631\n",
      "exploration/Returns Max                                -1.33071\n",
      "exploration/Returns Min                                -3.75844\n",
      "exploration/Actions Mean                               -0.00631211\n",
      "exploration/Actions Std                                 0.187973\n",
      "exploration/Actions Max                                 0.727745\n",
      "exploration/Actions Min                                -0.506436\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                            -2.40938\n",
      "exploration/env_infos/final/reward_dist Mean            0.000490911\n",
      "exploration/env_infos/final/reward_dist Std             0.0009812\n",
      "exploration/env_infos/final/reward_dist Max             0.00245331\n",
      "exploration/env_infos/final/reward_dist Min             1.22721e-35\n",
      "exploration/env_infos/initial/reward_dist Mean          0.000109025\n",
      "exploration/env_infos/initial/reward_dist Std           0.000112665\n",
      "exploration/env_infos/initial/reward_dist Max           0.000274696\n",
      "exploration/env_infos/initial/reward_dist Min           1.33551e-05\n",
      "exploration/env_infos/reward_dist Mean                  0.0641289\n",
      "exploration/env_infos/reward_dist Std                   0.181745\n",
      "exploration/env_infos/reward_dist Max                   0.867845\n",
      "exploration/env_infos/reward_dist Min                   1.22721e-35\n",
      "exploration/env_infos/final/reward_energy Mean         -0.244852\n",
      "exploration/env_infos/final/reward_energy Std           0.0872895\n",
      "exploration/env_infos/final/reward_energy Max          -0.0963312\n",
      "exploration/env_infos/final/reward_energy Min          -0.369723\n",
      "exploration/env_infos/initial/reward_energy Mean       -0.454919\n",
      "exploration/env_infos/initial/reward_energy Std         0.176031\n",
      "exploration/env_infos/initial/reward_energy Max        -0.249878\n",
      "exploration/env_infos/initial/reward_energy Min        -0.728648\n",
      "exploration/env_infos/reward_energy Mean               -0.224167\n",
      "exploration/env_infos/reward_energy Std                 0.143166\n",
      "exploration/env_infos/reward_energy Max                -0.0209205\n",
      "exploration/env_infos/reward_energy Min                -0.728648\n",
      "exploration/env_infos/final/reward_safety Mean          0\n",
      "exploration/env_infos/final/reward_safety Std           0\n",
      "exploration/env_infos/final/reward_safety Max           0\n",
      "exploration/env_infos/final/reward_safety Min           0\n",
      "exploration/env_infos/initial/reward_safety Mean        0\n",
      "exploration/env_infos/initial/reward_safety Std         0\n",
      "exploration/env_infos/initial/reward_safety Max         0\n",
      "exploration/env_infos/initial/reward_safety Min         0\n",
      "exploration/env_infos/reward_safety Mean                0\n",
      "exploration/env_infos/reward_safety Std                 0\n",
      "exploration/env_infos/reward_safety Max                 0\n",
      "exploration/env_infos/reward_safety Min                 0\n",
      "exploration/env_infos/final/end_effector_loc Mean      -0.0597196\n",
      "exploration/env_infos/final/end_effector_loc Std        0.401409\n",
      "exploration/env_infos/final/end_effector_loc Max        0.6004\n",
      "exploration/env_infos/final/end_effector_loc Min       -0.768194\n",
      "exploration/env_infos/initial/end_effector_loc Mean     0.00411278\n",
      "exploration/env_infos/initial/end_effector_loc Std      0.0167484\n",
      "exploration/env_infos/initial/end_effector_loc Max      0.0363872\n",
      "exploration/env_infos/initial/end_effector_loc Min     -0.0164456\n",
      "exploration/env_infos/end_effector_loc Mean            -0.0123202\n",
      "exploration/env_infos/end_effector_loc Std              0.265136\n",
      "exploration/env_infos/end_effector_loc Max              0.6004\n",
      "exploration/env_infos/end_effector_loc Min             -0.768194\n",
      "evaluation/num steps total                           9000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                            20\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                             20\n",
      "evaluation/path length Min                             20\n",
      "evaluation/Rewards Mean                                -0.121668\n",
      "evaluation/Rewards Std                                  0.117023\n",
      "evaluation/Rewards Max                                  0.104391\n",
      "evaluation/Rewards Min                                 -0.792242\n",
      "evaluation/Returns Mean                                -2.43336\n",
      "evaluation/Returns Std                                  1.28797\n",
      "evaluation/Returns Max                                 -0.0298396\n",
      "evaluation/Returns Min                                 -5.82313\n",
      "evaluation/Actions Mean                                -0.0164039\n",
      "evaluation/Actions Std                                  0.0997085\n",
      "evaluation/Actions Max                                  0.528124\n",
      "evaluation/Actions Min                                 -0.608067\n",
      "evaluation/Num Paths                                   50\n",
      "evaluation/Average Returns                             -2.43336\n",
      "evaluation/env_infos/final/reward_dist Mean             0.0783531\n",
      "evaluation/env_infos/final/reward_dist Std              0.210993\n",
      "evaluation/env_infos/final/reward_dist Max              0.959972\n",
      "evaluation/env_infos/final/reward_dist Min              5.13075e-81\n",
      "evaluation/env_infos/initial/reward_dist Mean           0.00538389\n",
      "evaluation/env_infos/initial/reward_dist Std            0.00752556\n",
      "evaluation/env_infos/initial/reward_dist Max            0.0352487\n",
      "evaluation/env_infos/initial/reward_dist Min            1.07641e-06\n",
      "evaluation/env_infos/reward_dist Mean                   0.0828767\n",
      "evaluation/env_infos/reward_dist Std                    0.177923\n",
      "evaluation/env_infos/reward_dist Max                    0.98926\n",
      "evaluation/env_infos/reward_dist Min                    5.13075e-81\n",
      "evaluation/env_infos/final/reward_energy Mean          -0.144679\n",
      "evaluation/env_infos/final/reward_energy Std            0.0979144\n",
      "evaluation/env_infos/final/reward_energy Max           -0.00811962\n",
      "evaluation/env_infos/final/reward_energy Min           -0.464694\n",
      "evaluation/env_infos/initial/reward_energy Mean        -0.233305\n",
      "evaluation/env_infos/initial/reward_energy Std          0.141318\n",
      "evaluation/env_infos/initial/reward_energy Max         -0.0240861\n",
      "evaluation/env_infos/initial/reward_energy Min         -0.632523\n",
      "evaluation/env_infos/reward_energy Mean                -0.112431\n",
      "evaluation/env_infos/reward_energy Std                  0.08821\n",
      "evaluation/env_infos/reward_energy Max                 -0.00260803\n",
      "evaluation/env_infos/reward_energy Min                 -0.632523\n",
      "evaluation/env_infos/final/reward_safety Mean           0\n",
      "evaluation/env_infos/final/reward_safety Std            0\n",
      "evaluation/env_infos/final/reward_safety Max            0\n",
      "evaluation/env_infos/final/reward_safety Min            0\n",
      "evaluation/env_infos/initial/reward_safety Mean         0\n",
      "evaluation/env_infos/initial/reward_safety Std          0\n",
      "evaluation/env_infos/initial/reward_safety Max          0\n",
      "evaluation/env_infos/initial/reward_safety Min          0\n",
      "evaluation/env_infos/reward_safety Mean                 0\n",
      "evaluation/env_infos/reward_safety Std                  0\n",
      "evaluation/env_infos/reward_safety Max                  0\n",
      "evaluation/env_infos/reward_safety Min                  0\n",
      "evaluation/env_infos/final/end_effector_loc Mean       -0.0991857\n",
      "evaluation/env_infos/final/end_effector_loc Std         0.393903\n",
      "evaluation/env_infos/final/end_effector_loc Max         0.771833\n",
      "evaluation/env_infos/final/end_effector_loc Min        -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      0.00200743\n",
      "evaluation/env_infos/initial/end_effector_loc Std       0.00943255\n",
      "evaluation/env_infos/initial/end_effector_loc Max       0.0264062\n",
      "evaluation/env_infos/initial/end_effector_loc Min      -0.0304034\n",
      "evaluation/env_infos/end_effector_loc Mean             -0.0129488\n",
      "evaluation/env_infos/end_effector_loc Std               0.236235\n",
      "evaluation/env_infos/end_effector_loc Max               0.771833\n",
      "evaluation/env_infos/end_effector_loc Min              -1\n",
      "time/data storing (s)                                   0.00310569\n",
      "time/evaluation sampling (s)                            0.985649\n",
      "time/exploration sampling (s)                           0.136818\n",
      "time/logging (s)                                        0.0185161\n",
      "time/saving (s)                                         0.0318406\n",
      "time/training (s)                                      44.6417\n",
      "time/epoch (s)                                         45.8176\n",
      "time/total (s)                                        407.717\n",
      "Epoch                                                   8\n",
      "---------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:30.837165 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 9 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00445208\n",
      "trainer/QF2 Loss                                         0.00415612\n",
      "trainer/Policy Loss                                      4.46808\n",
      "trainer/Q1 Predictions Mean                             -2.91392\n",
      "trainer/Q1 Predictions Std                               1.97332\n",
      "trainer/Q1 Predictions Max                              -0.0731378\n",
      "trainer/Q1 Predictions Min                             -11.0758\n",
      "trainer/Q2 Predictions Mean                             -2.88326\n",
      "trainer/Q2 Predictions Std                               1.95605\n",
      "trainer/Q2 Predictions Max                              -0.10773\n",
      "trainer/Q2 Predictions Min                             -10.884\n",
      "trainer/Q Targets Mean                                  -2.88772\n",
      "trainer/Q Targets Std                                    1.95442\n",
      "trainer/Q Targets Max                                   -0.111562\n",
      "trainer/Q Targets Min                                  -11.063\n",
      "trainer/Log Pis Mean                                     1.88253\n",
      "trainer/Log Pis Std                                      1.37722\n",
      "trainer/Log Pis Max                                      7.06818\n",
      "trainer/Log Pis Min                                     -5.68848\n",
      "trainer/Policy mu Mean                                  -0.0142511\n",
      "trainer/Policy mu Std                                    0.634605\n",
      "trainer/Policy mu Max                                    2.53393\n",
      "trainer/Policy mu Min                                   -2.73537\n",
      "trainer/Policy log std Mean                             -2.06168\n",
      "trainer/Policy log std Std                               0.583922\n",
      "trainer/Policy log std Max                              -0.228722\n",
      "trainer/Policy log std Min                              -3.07149\n",
      "trainer/Alpha                                            0.015224\n",
      "trainer/Alpha Loss                                      -0.491583\n",
      "exploration/num steps total                           2000\n",
      "exploration/num paths total                            100\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.117089\n",
      "exploration/Rewards Std                                  0.0956642\n",
      "exploration/Rewards Max                                  0.0709816\n",
      "exploration/Rewards Min                                 -0.41412\n",
      "exploration/Returns Mean                                -2.34178\n",
      "exploration/Returns Std                                  0.93993\n",
      "exploration/Returns Max                                 -1.16824\n",
      "exploration/Returns Min                                 -3.80301\n",
      "exploration/Actions Mean                                -0.000832796\n",
      "exploration/Actions Std                                  0.16664\n",
      "exploration/Actions Max                                  0.473452\n",
      "exploration/Actions Min                                 -0.678519\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.34178\n",
      "exploration/env_infos/final/reward_dist Mean             0.0788667\n",
      "exploration/env_infos/final/reward_dist Std              0.157733\n",
      "exploration/env_infos/final/reward_dist Max              0.394332\n",
      "exploration/env_infos/final/reward_dist Min              3.88658e-19\n",
      "exploration/env_infos/initial/reward_dist Mean           0.0134992\n",
      "exploration/env_infos/initial/reward_dist Std            0.0267896\n",
      "exploration/env_infos/initial/reward_dist Max            0.0670783\n",
      "exploration/env_infos/initial/reward_dist Min            2.26571e-05\n",
      "exploration/env_infos/reward_dist Mean                   0.132425\n",
      "exploration/env_infos/reward_dist Std                    0.205354\n",
      "exploration/env_infos/reward_dist Max                    0.8765\n",
      "exploration/env_infos/reward_dist Min                    9.70504e-22\n",
      "exploration/env_infos/final/reward_energy Mean          -0.299579\n",
      "exploration/env_infos/final/reward_energy Std            0.21723\n",
      "exploration/env_infos/final/reward_energy Max           -0.0449317\n",
      "exploration/env_infos/final/reward_energy Min           -0.682126\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.41637\n",
      "exploration/env_infos/initial/reward_energy Std          0.21694\n",
      "exploration/env_infos/initial/reward_energy Max         -0.141798\n",
      "exploration/env_infos/initial/reward_energy Min         -0.799736\n",
      "exploration/env_infos/reward_energy Mean                -0.196532\n",
      "exploration/env_infos/reward_energy Std                  0.130056\n",
      "exploration/env_infos/reward_energy Max                 -0.0124541\n",
      "exploration/env_infos/reward_energy Min                 -0.799736\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.0561878\n",
      "exploration/env_infos/final/end_effector_loc Std         0.39595\n",
      "exploration/env_infos/final/end_effector_loc Max         0.499235\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.617098\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.00108787\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.0165635\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0228931\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0327848\n",
      "exploration/env_infos/end_effector_loc Mean              0.0335449\n",
      "exploration/env_infos/end_effector_loc Std               0.25367\n",
      "exploration/env_infos/end_effector_loc Max               0.499235\n",
      "exploration/env_infos/end_effector_loc Min              -0.617098\n",
      "evaluation/num steps total                           10000\n",
      "evaluation/num paths total                             500\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.109354\n",
      "evaluation/Rewards Std                                   0.080017\n",
      "evaluation/Rewards Max                                   0.0688032\n",
      "evaluation/Rewards Min                                  -0.717519\n",
      "evaluation/Returns Mean                                 -2.18708\n",
      "evaluation/Returns Std                                   0.944662\n",
      "evaluation/Returns Max                                  -0.131174\n",
      "evaluation/Returns Min                                  -5.55097\n",
      "evaluation/Actions Mean                                 -0.0136973\n",
      "evaluation/Actions Std                                   0.0880017\n",
      "evaluation/Actions Max                                   0.735795\n",
      "evaluation/Actions Min                                  -0.797453\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.18708\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0363625\n",
      "evaluation/env_infos/final/reward_dist Std               0.118915\n",
      "evaluation/env_infos/final/reward_dist Max               0.610875\n",
      "evaluation/env_infos/final/reward_dist Min               4.08377e-65\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00587902\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0188641\n",
      "evaluation/env_infos/initial/reward_dist Max             0.131242\n",
      "evaluation/env_infos/initial/reward_dist Min             1.76679e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0692563\n",
      "evaluation/env_infos/reward_dist Std                     0.171646\n",
      "evaluation/env_infos/reward_dist Max                     0.9802\n",
      "evaluation/env_infos/reward_dist Min                     4.08377e-65\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.139345\n",
      "evaluation/env_infos/final/reward_energy Std             0.122126\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0109747\n",
      "evaluation/env_infos/final/reward_energy Min            -0.575587\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.212727\n",
      "evaluation/env_infos/initial/reward_energy Std           0.185945\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.030087\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.04139\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0912323\n",
      "evaluation/env_infos/reward_energy Std                   0.086836\n",
      "evaluation/env_infos/reward_energy Max                  -0.00263502\n",
      "evaluation/env_infos/reward_energy Min                  -1.04139\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0561189\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.333355\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.740545\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.00146743\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00988088\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0367897\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0398726\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0039207\n",
      "evaluation/env_infos/end_effector_loc Std                0.190851\n",
      "evaluation/env_infos/end_effector_loc Max                0.740545\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00299985\n",
      "time/evaluation sampling (s)                             0.976468\n",
      "time/exploration sampling (s)                            0.118854\n",
      "time/logging (s)                                         0.0203661\n",
      "time/saving (s)                                          0.0399637\n",
      "time/training (s)                                       45.3413\n",
      "time/epoch (s)                                          46.4999\n",
      "time/total (s)                                         454.357\n",
      "Epoch                                                    9\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:16.836145 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 10 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00612566\n",
      "trainer/QF2 Loss                                         0.00450927\n",
      "trainer/Policy Loss                                      4.73966\n",
      "trainer/Q1 Predictions Mean                             -2.95715\n",
      "trainer/Q1 Predictions Std                               1.90926\n",
      "trainer/Q1 Predictions Max                              -0.161132\n",
      "trainer/Q1 Predictions Min                              -9.46225\n",
      "trainer/Q2 Predictions Mean                             -2.95376\n",
      "trainer/Q2 Predictions Std                               1.9247\n",
      "trainer/Q2 Predictions Max                              -0.174487\n",
      "trainer/Q2 Predictions Min                              -9.79856\n",
      "trainer/Q Targets Mean                                  -2.94762\n",
      "trainer/Q Targets Std                                    1.92437\n",
      "trainer/Q Targets Max                                   -0.153345\n",
      "trainer/Q Targets Min                                   -9.82373\n",
      "trainer/Log Pis Mean                                     2.04306\n",
      "trainer/Log Pis Std                                      1.46884\n",
      "trainer/Log Pis Max                                      9.24999\n",
      "trainer/Log Pis Min                                     -2.4602\n",
      "trainer/Policy mu Mean                                  -0.0136412\n",
      "trainer/Policy mu Std                                    0.706992\n",
      "trainer/Policy mu Max                                    2.57877\n",
      "trainer/Policy mu Min                                   -3.37981\n",
      "trainer/Policy log std Mean                             -2.08289\n",
      "trainer/Policy log std Std                               0.592796\n",
      "trainer/Policy log std Max                              -0.181075\n",
      "trainer/Policy log std Min                              -3.24415\n",
      "trainer/Alpha                                            0.0158679\n",
      "trainer/Alpha Loss                                       0.178438\n",
      "exploration/num steps total                           2100\n",
      "exploration/num paths total                            105\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.211701\n",
      "exploration/Rewards Std                                  0.0934085\n",
      "exploration/Rewards Max                                 -0.0372659\n",
      "exploration/Rewards Min                                 -0.556664\n",
      "exploration/Returns Mean                                -4.23401\n",
      "exploration/Returns Std                                  1.21427\n",
      "exploration/Returns Max                                 -2.7763\n",
      "exploration/Returns Min                                 -6.18214\n",
      "exploration/Actions Mean                                 0.00843026\n",
      "exploration/Actions Std                                  0.175229\n",
      "exploration/Actions Max                                  0.747791\n",
      "exploration/Actions Min                                 -0.840261\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -4.23401\n",
      "exploration/env_infos/final/reward_dist Mean             5.10512e-08\n",
      "exploration/env_infos/final/reward_dist Std              1.0208e-07\n",
      "exploration/env_infos/final/reward_dist Max              2.55211e-07\n",
      "exploration/env_infos/final/reward_dist Min              5.44369e-44\n",
      "exploration/env_infos/initial/reward_dist Mean           0.000241264\n",
      "exploration/env_infos/initial/reward_dist Std            0.000357503\n",
      "exploration/env_infos/initial/reward_dist Max            0.000950018\n",
      "exploration/env_infos/initial/reward_dist Min            7.80915e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0147121\n",
      "exploration/env_infos/reward_dist Std                    0.0792127\n",
      "exploration/env_infos/reward_dist Max                    0.642368\n",
      "exploration/env_infos/reward_dist Min                    2.06964e-45\n",
      "exploration/env_infos/final/reward_energy Mean          -0.233899\n",
      "exploration/env_infos/final/reward_energy Std            0.046255\n",
      "exploration/env_infos/final/reward_energy Max           -0.189089\n",
      "exploration/env_infos/final/reward_energy Min           -0.29248\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.180868\n",
      "exploration/env_infos/initial/reward_energy Std          0.0902192\n",
      "exploration/env_infos/initial/reward_energy Max         -0.04204\n",
      "exploration/env_infos/initial/reward_energy Min         -0.322384\n",
      "exploration/env_infos/reward_energy Mean                -0.193262\n",
      "exploration/env_infos/reward_energy Std                  0.15557\n",
      "exploration/env_infos/reward_energy Max                 -0.0153834\n",
      "exploration/env_infos/reward_energy Min                 -0.944226\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean        0.00325038\n",
      "exploration/env_infos/final/end_effector_loc Std         0.420076\n",
      "exploration/env_infos/final/end_effector_loc Max         0.494579\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.991038\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.000785262\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00710275\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.00849682\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0159193\n",
      "exploration/env_infos/end_effector_loc Mean             -0.00370282\n",
      "exploration/env_infos/end_effector_loc Std               0.287445\n",
      "exploration/env_infos/end_effector_loc Max               0.503013\n",
      "exploration/env_infos/end_effector_loc Min              -1\n",
      "evaluation/num steps total                           11000\n",
      "evaluation/num paths total                             550\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.120884\n",
      "evaluation/Rewards Std                                   0.0901638\n",
      "evaluation/Rewards Max                                   0.108132\n",
      "evaluation/Rewards Min                                  -0.718647\n",
      "evaluation/Returns Mean                                 -2.41767\n",
      "evaluation/Returns Std                                   1.2018\n",
      "evaluation/Returns Max                                   0.582752\n",
      "evaluation/Returns Min                                  -5.47631\n",
      "evaluation/Actions Mean                                 -0.00884906\n",
      "evaluation/Actions Std                                   0.0849369\n",
      "evaluation/Actions Max                                   0.60225\n",
      "evaluation/Actions Min                                  -0.58994\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.41767\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0513876\n",
      "evaluation/env_infos/final/reward_dist Std               0.162926\n",
      "evaluation/env_infos/final/reward_dist Max               0.988065\n",
      "evaluation/env_infos/final/reward_dist Min               6.14719e-68\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00744284\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0156932\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0817627\n",
      "evaluation/env_infos/initial/reward_dist Min             1.13791e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557876\n",
      "evaluation/env_infos/reward_dist Std                     0.159761\n",
      "evaluation/env_infos/reward_dist Max                     0.988065\n",
      "evaluation/env_infos/reward_dist Min                     6.14719e-68\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.119466\n",
      "evaluation/env_infos/final/reward_energy Std             0.0836574\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0171798\n",
      "evaluation/env_infos/final/reward_energy Min            -0.433978\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.233227\n",
      "evaluation/env_infos/initial/reward_energy Std           0.165023\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0168539\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.768105\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0881556\n",
      "evaluation/env_infos/reward_energy Std                   0.0825455\n",
      "evaluation/env_infos/reward_energy Max                  -0.000673391\n",
      "evaluation/env_infos/reward_energy Min                  -0.768105\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean        -0.0366146\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.332479\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.641184\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000604364\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0100831\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0301125\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.029497\n",
      "evaluation/env_infos/end_effector_loc Mean              -0.00377817\n",
      "evaluation/env_infos/end_effector_loc Std                0.206309\n",
      "evaluation/env_infos/end_effector_loc Max                0.641184\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00298404\n",
      "time/evaluation sampling (s)                             0.999527\n",
      "time/exploration sampling (s)                            0.120004\n",
      "time/logging (s)                                         0.0196417\n",
      "time/saving (s)                                          0.0301854\n",
      "time/training (s)                                       44.6641\n",
      "time/epoch (s)                                          45.8364\n",
      "time/total (s)                                         500.355\n",
      "Epoch                                                   10\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:05.145095 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 11 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00576653\n",
      "trainer/QF2 Loss                                         0.00529006\n",
      "trainer/Policy Loss                                      4.54779\n",
      "trainer/Q1 Predictions Mean                             -2.77117\n",
      "trainer/Q1 Predictions Std                               1.86587\n",
      "trainer/Q1 Predictions Max                              -0.130918\n",
      "trainer/Q1 Predictions Min                             -11.2886\n",
      "trainer/Q2 Predictions Mean                             -2.74842\n",
      "trainer/Q2 Predictions Std                               1.85531\n",
      "trainer/Q2 Predictions Max                              -0.108681\n",
      "trainer/Q2 Predictions Min                             -11.2334\n",
      "trainer/Q Targets Mean                                  -2.73754\n",
      "trainer/Q Targets Std                                    1.84606\n",
      "trainer/Q Targets Max                                   -0.0946199\n",
      "trainer/Q Targets Min                                  -11.3446\n",
      "trainer/Log Pis Mean                                     2.07449\n",
      "trainer/Log Pis Std                                      1.58767\n",
      "trainer/Log Pis Max                                      9.11468\n",
      "trainer/Log Pis Min                                     -2.71209\n",
      "trainer/Policy mu Mean                                  -0.0443209\n",
      "trainer/Policy mu Std                                    0.793992\n",
      "trainer/Policy mu Max                                    3.44733\n",
      "trainer/Policy mu Min                                   -3.32761\n",
      "trainer/Policy log std Mean                             -2.00017\n",
      "trainer/Policy log std Std                               0.599633\n",
      "trainer/Policy log std Max                              -0.0475062\n",
      "trainer/Policy log std Min                              -2.97489\n",
      "trainer/Alpha                                            0.016235\n",
      "trainer/Alpha Loss                                       0.306914\n",
      "exploration/num steps total                           2200\n",
      "exploration/num paths total                            110\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.183434\n",
      "exploration/Rewards Std                                  0.109815\n",
      "exploration/Rewards Max                                  0.0568666\n",
      "exploration/Rewards Min                                 -0.583449\n",
      "exploration/Returns Mean                                -3.66867\n",
      "exploration/Returns Std                                  0.671211\n",
      "exploration/Returns Max                                 -2.63071\n",
      "exploration/Returns Min                                 -4.27238\n",
      "exploration/Actions Mean                                -0.00890265\n",
      "exploration/Actions Std                                  0.113645\n",
      "exploration/Actions Max                                  0.288116\n",
      "exploration/Actions Min                                 -0.30274\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -3.66867\n",
      "exploration/env_infos/final/reward_dist Mean             0.033112\n",
      "exploration/env_infos/final/reward_dist Std              0.0546735\n",
      "exploration/env_infos/final/reward_dist Max              0.140758\n",
      "exploration/env_infos/final/reward_dist Min              1.42662e-40\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00919615\n",
      "exploration/env_infos/initial/reward_dist Std            0.0110347\n",
      "exploration/env_infos/initial/reward_dist Max            0.027808\n",
      "exploration/env_infos/initial/reward_dist Min            6.24952e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0686175\n",
      "exploration/env_infos/reward_dist Std                    0.114971\n",
      "exploration/env_infos/reward_dist Max                    0.435792\n",
      "exploration/env_infos/reward_dist Min                    1.42662e-40\n",
      "exploration/env_infos/final/reward_energy Mean          -0.178713\n",
      "exploration/env_infos/final/reward_energy Std            0.0543983\n",
      "exploration/env_infos/final/reward_energy Max           -0.0905265\n",
      "exploration/env_infos/final/reward_energy Min           -0.243733\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.215577\n",
      "exploration/env_infos/initial/reward_energy Std          0.0491593\n",
      "exploration/env_infos/initial/reward_energy Max         -0.155156\n",
      "exploration/env_infos/initial/reward_energy Min         -0.30568\n",
      "exploration/env_infos/reward_energy Mean                -0.141997\n",
      "exploration/env_infos/reward_energy Std                  0.0763271\n",
      "exploration/env_infos/reward_energy Max                 -0.0265819\n",
      "exploration/env_infos/reward_energy Min                 -0.360992\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.11665\n",
      "exploration/env_infos/final/end_effector_loc Std         0.385239\n",
      "exploration/env_infos/final/end_effector_loc Max         0.362007\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.842267\n",
      "exploration/env_infos/initial/end_effector_loc Mean     -0.00209792\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00753068\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.0104526\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0149354\n",
      "exploration/env_infos/end_effector_loc Mean             -0.0550128\n",
      "exploration/env_infos/end_effector_loc Std               0.206879\n",
      "exploration/env_infos/end_effector_loc Max               0.362007\n",
      "exploration/env_infos/end_effector_loc Min              -0.842267\n",
      "evaluation/num steps total                           12000\n",
      "evaluation/num paths total                             600\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.107939\n",
      "evaluation/Rewards Std                                   0.0729217\n",
      "evaluation/Rewards Max                                   0.0645711\n",
      "evaluation/Rewards Min                                  -0.62767\n",
      "evaluation/Returns Mean                                 -2.15878\n",
      "evaluation/Returns Std                                   0.967839\n",
      "evaluation/Returns Max                                  -0.598675\n",
      "evaluation/Returns Min                                  -5.50918\n",
      "evaluation/Actions Mean                                  0.000196849\n",
      "evaluation/Actions Std                                   0.0938852\n",
      "evaluation/Actions Max                                   0.474783\n",
      "evaluation/Actions Min                                  -0.975392\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.15878\n",
      "evaluation/env_infos/final/reward_dist Mean              0.0126502\n",
      "evaluation/env_infos/final/reward_dist Std               0.0580807\n",
      "evaluation/env_infos/final/reward_dist Max               0.401314\n",
      "evaluation/env_infos/final/reward_dist Min               9.40453e-47\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.00976908\n",
      "evaluation/env_infos/initial/reward_dist Std             0.0329858\n",
      "evaluation/env_infos/initial/reward_dist Max             0.22407\n",
      "evaluation/env_infos/initial/reward_dist Min             2.34212e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.0557189\n",
      "evaluation/env_infos/reward_dist Std                     0.142974\n",
      "evaluation/env_infos/reward_dist Max                     0.972434\n",
      "evaluation/env_infos/reward_dist Min                     9.40453e-47\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.118539\n",
      "evaluation/env_infos/final/reward_energy Std             0.0847105\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0212853\n",
      "evaluation/env_infos/final/reward_energy Min            -0.412104\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.248573\n",
      "evaluation/env_infos/initial/reward_energy Std           0.255942\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0150159\n",
      "evaluation/env_infos/initial/reward_energy Min          -1.28634\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0874845\n",
      "evaluation/env_infos/reward_energy Std                   0.099877\n",
      "evaluation/env_infos/reward_energy Max                  -0.00165591\n",
      "evaluation/env_infos/reward_energy Min                  -1.28634\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0446976\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.27984\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.662738\n",
      "evaluation/env_infos/final/end_effector_loc Min         -0.964706\n",
      "evaluation/env_infos/initial/end_effector_loc Mean      -0.00106255\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.0125694\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0237391\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0487696\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0179977\n",
      "evaluation/env_infos/end_effector_loc Std                0.175489\n",
      "evaluation/env_infos/end_effector_loc Max                0.662738\n",
      "evaluation/env_infos/end_effector_loc Min               -0.964706\n",
      "time/data storing (s)                                    0.0029602\n",
      "time/evaluation sampling (s)                             0.995818\n",
      "time/exploration sampling (s)                            0.132013\n",
      "time/logging (s)                                         0.0240012\n",
      "time/saving (s)                                          0.0349898\n",
      "time/training (s)                                       46.9489\n",
      "time/epoch (s)                                          48.1387\n",
      "time/total (s)                                         548.668\n",
      "Epoch                                                   11\n",
      "---------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:52.614512 PDT | [gher-pointreacherobs-SAC-100e-20s-disc0.97-horizon20_2021_05_25_10_24_59_0000--s-10] Epoch 12 finished\n",
      "---------------------------------------------------  ---------------\n",
      "replay_buffer/size                                    2000\n",
      "trainer/QF1 Loss                                         0.00317505\n",
      "trainer/QF2 Loss                                         0.00356741\n",
      "trainer/Policy Loss                                      4.224\n",
      "trainer/Q1 Predictions Mean                             -2.44865\n",
      "trainer/Q1 Predictions Std                               1.56015\n",
      "trainer/Q1 Predictions Max                              -0.0263941\n",
      "trainer/Q1 Predictions Min                              -9.22536\n",
      "trainer/Q2 Predictions Mean                             -2.45026\n",
      "trainer/Q2 Predictions Std                               1.55578\n",
      "trainer/Q2 Predictions Max                              -0.0634723\n",
      "trainer/Q2 Predictions Min                              -9.20636\n",
      "trainer/Q Targets Mean                                  -2.46091\n",
      "trainer/Q Targets Std                                    1.56521\n",
      "trainer/Q Targets Max                                   -0.0572312\n",
      "trainer/Q Targets Min                                   -9.26282\n",
      "trainer/Log Pis Mean                                     2.00327\n",
      "trainer/Log Pis Std                                      1.21642\n",
      "trainer/Log Pis Max                                      7.88299\n",
      "trainer/Log Pis Min                                     -2.20284\n",
      "trainer/Policy mu Mean                                  -0.00195495\n",
      "trainer/Policy mu Std                                    0.537883\n",
      "trainer/Policy mu Max                                    2.93236\n",
      "trainer/Policy mu Min                                   -2.59237\n",
      "trainer/Policy log std Mean                             -2.15926\n",
      "trainer/Policy log std Std                               0.529236\n",
      "trainer/Policy log std Max                              -0.392954\n",
      "trainer/Policy log std Min                              -3.06677\n",
      "trainer/Alpha                                            0.0164994\n",
      "trainer/Alpha Loss                                       0.0134078\n",
      "exploration/num steps total                           2300\n",
      "exploration/num paths total                            115\n",
      "exploration/path length Mean                            20\n",
      "exploration/path length Std                              0\n",
      "exploration/path length Max                             20\n",
      "exploration/path length Min                             20\n",
      "exploration/Rewards Mean                                -0.131759\n",
      "exploration/Rewards Std                                  0.0715171\n",
      "exploration/Rewards Max                                  0.0031933\n",
      "exploration/Rewards Min                                 -0.4202\n",
      "exploration/Returns Mean                                -2.63519\n",
      "exploration/Returns Std                                  0.544396\n",
      "exploration/Returns Max                                 -1.78086\n",
      "exploration/Returns Min                                 -3.45244\n",
      "exploration/Actions Mean                                -0.00353127\n",
      "exploration/Actions Std                                  0.122522\n",
      "exploration/Actions Max                                  0.581829\n",
      "exploration/Actions Min                                 -0.349097\n",
      "exploration/Num Paths                                    5\n",
      "exploration/Average Returns                             -2.63519\n",
      "exploration/env_infos/final/reward_dist Mean             0.004765\n",
      "exploration/env_infos/final/reward_dist Std              0.00936162\n",
      "exploration/env_infos/final/reward_dist Max              0.0234869\n",
      "exploration/env_infos/final/reward_dist Min              8.48965e-18\n",
      "exploration/env_infos/initial/reward_dist Mean           0.00470963\n",
      "exploration/env_infos/initial/reward_dist Std            0.00768251\n",
      "exploration/env_infos/initial/reward_dist Max            0.0199204\n",
      "exploration/env_infos/initial/reward_dist Min            6.01152e-06\n",
      "exploration/env_infos/reward_dist Mean                   0.0420192\n",
      "exploration/env_infos/reward_dist Std                    0.113035\n",
      "exploration/env_infos/reward_dist Max                    0.570829\n",
      "exploration/env_infos/reward_dist Min                    8.48965e-18\n",
      "exploration/env_infos/final/reward_energy Mean          -0.129404\n",
      "exploration/env_infos/final/reward_energy Std            0.0663869\n",
      "exploration/env_infos/final/reward_energy Max           -0.0778513\n",
      "exploration/env_infos/final/reward_energy Min           -0.259631\n",
      "exploration/env_infos/initial/reward_energy Mean        -0.214067\n",
      "exploration/env_infos/initial/reward_energy Std          0.120652\n",
      "exploration/env_infos/initial/reward_energy Max         -0.0838295\n",
      "exploration/env_infos/initial/reward_energy Min         -0.411506\n",
      "exploration/env_infos/reward_energy Mean                -0.143218\n",
      "exploration/env_infos/reward_energy Std                  0.097657\n",
      "exploration/env_infos/reward_energy Max                 -0.00381842\n",
      "exploration/env_infos/reward_energy Min                 -0.589576\n",
      "exploration/env_infos/final/reward_safety Mean           0\n",
      "exploration/env_infos/final/reward_safety Std            0\n",
      "exploration/env_infos/final/reward_safety Max            0\n",
      "exploration/env_infos/final/reward_safety Min            0\n",
      "exploration/env_infos/initial/reward_safety Mean         0\n",
      "exploration/env_infos/initial/reward_safety Std          0\n",
      "exploration/env_infos/initial/reward_safety Max          0\n",
      "exploration/env_infos/initial/reward_safety Min          0\n",
      "exploration/env_infos/reward_safety Mean                 0\n",
      "exploration/env_infos/reward_safety Std                  0\n",
      "exploration/env_infos/reward_safety Max                  0\n",
      "exploration/env_infos/reward_safety Min                  0\n",
      "exploration/env_infos/final/end_effector_loc Mean       -0.0781184\n",
      "exploration/env_infos/final/end_effector_loc Std         0.267225\n",
      "exploration/env_infos/final/end_effector_loc Max         0.335246\n",
      "exploration/env_infos/final/end_effector_loc Min        -0.405207\n",
      "exploration/env_infos/initial/end_effector_loc Mean      0.000212404\n",
      "exploration/env_infos/initial/end_effector_loc Std       0.00868514\n",
      "exploration/env_infos/initial/end_effector_loc Max       0.014911\n",
      "exploration/env_infos/initial/end_effector_loc Min      -0.0141776\n",
      "exploration/env_infos/end_effector_loc Mean             -0.047834\n",
      "exploration/env_infos/end_effector_loc Std               0.179718\n",
      "exploration/env_infos/end_effector_loc Max               0.335246\n",
      "exploration/env_infos/end_effector_loc Min              -0.493902\n",
      "evaluation/num steps total                           13000\n",
      "evaluation/num paths total                             650\n",
      "evaluation/path length Mean                             20\n",
      "evaluation/path length Std                               0\n",
      "evaluation/path length Max                              20\n",
      "evaluation/path length Min                              20\n",
      "evaluation/Rewards Mean                                 -0.11331\n",
      "evaluation/Rewards Std                                   0.0805047\n",
      "evaluation/Rewards Max                                   0.089257\n",
      "evaluation/Rewards Min                                  -0.596522\n",
      "evaluation/Returns Mean                                 -2.26621\n",
      "evaluation/Returns Std                                   1.08585\n",
      "evaluation/Returns Max                                   0.051371\n",
      "evaluation/Returns Min                                  -4.6357\n",
      "evaluation/Actions Mean                                 -0.00543072\n",
      "evaluation/Actions Std                                   0.080599\n",
      "evaluation/Actions Max                                   0.58027\n",
      "evaluation/Actions Min                                  -0.531828\n",
      "evaluation/Num Paths                                    50\n",
      "evaluation/Average Returns                              -2.26621\n",
      "evaluation/env_infos/final/reward_dist Mean              0.016623\n",
      "evaluation/env_infos/final/reward_dist Std               0.0590321\n",
      "evaluation/env_infos/final/reward_dist Max               0.394556\n",
      "evaluation/env_infos/final/reward_dist Min               3.77082e-97\n",
      "evaluation/env_infos/initial/reward_dist Mean            0.0056598\n",
      "evaluation/env_infos/initial/reward_dist Std             0.00928809\n",
      "evaluation/env_infos/initial/reward_dist Max             0.0354464\n",
      "evaluation/env_infos/initial/reward_dist Min             1.17022e-06\n",
      "evaluation/env_infos/reward_dist Mean                    0.054205\n",
      "evaluation/env_infos/reward_dist Std                     0.142844\n",
      "evaluation/env_infos/reward_dist Max                     0.994045\n",
      "evaluation/env_infos/reward_dist Min                     3.77082e-97\n",
      "evaluation/env_infos/final/reward_energy Mean           -0.11998\n",
      "evaluation/env_infos/final/reward_energy Std             0.0784046\n",
      "evaluation/env_infos/final/reward_energy Max            -0.0183616\n",
      "evaluation/env_infos/final/reward_energy Min            -0.349532\n",
      "evaluation/env_infos/initial/reward_energy Mean         -0.217207\n",
      "evaluation/env_infos/initial/reward_energy Std           0.132593\n",
      "evaluation/env_infos/initial/reward_energy Max          -0.0133179\n",
      "evaluation/env_infos/initial/reward_energy Min          -0.616938\n",
      "evaluation/env_infos/reward_energy Mean                 -0.0876509\n",
      "evaluation/env_infos/reward_energy Std                   0.0732715\n",
      "evaluation/env_infos/reward_energy Max                  -0.00223285\n",
      "evaluation/env_infos/reward_energy Min                  -0.616938\n",
      "evaluation/env_infos/final/reward_safety Mean            0\n",
      "evaluation/env_infos/final/reward_safety Std             0\n",
      "evaluation/env_infos/final/reward_safety Max             0\n",
      "evaluation/env_infos/final/reward_safety Min             0\n",
      "evaluation/env_infos/initial/reward_safety Mean          0\n",
      "evaluation/env_infos/initial/reward_safety Std           0\n",
      "evaluation/env_infos/initial/reward_safety Max           0\n",
      "evaluation/env_infos/initial/reward_safety Min           0\n",
      "evaluation/env_infos/reward_safety Mean                  0\n",
      "evaluation/env_infos/reward_safety Std                   0\n",
      "evaluation/env_infos/reward_safety Max                   0\n",
      "evaluation/env_infos/reward_safety Min                   0\n",
      "evaluation/env_infos/final/end_effector_loc Mean         0.0213813\n",
      "evaluation/env_infos/final/end_effector_loc Std          0.364162\n",
      "evaluation/env_infos/final/end_effector_loc Max          0.750014\n",
      "evaluation/env_infos/final/end_effector_loc Min         -1\n",
      "evaluation/env_infos/initial/end_effector_loc Mean       0.000851428\n",
      "evaluation/env_infos/initial/end_effector_loc Std        0.00895683\n",
      "evaluation/env_infos/initial/end_effector_loc Max        0.0290135\n",
      "evaluation/env_infos/initial/end_effector_loc Min       -0.0265914\n",
      "evaluation/env_infos/end_effector_loc Mean               0.0184845\n",
      "evaluation/env_infos/end_effector_loc Std                0.211441\n",
      "evaluation/env_infos/end_effector_loc Max                0.750014\n",
      "evaluation/env_infos/end_effector_loc Min               -1\n",
      "time/data storing (s)                                    0.00287573\n",
      "time/evaluation sampling (s)                             1.51933\n",
      "time/exploration sampling (s)                            0.120694\n",
      "time/logging (s)                                         0.0205859\n",
      "time/saving (s)                                          0.0283214\n",
      "time/training (s)                                       45.4317\n",
      "time/epoch (s)                                          47.1235\n",
      "time/total (s)                                         596.132\n",
      "Epoch                                                   12\n",
      "---------------------------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env pointreacherobs --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c881db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
