{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[2456]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2167b778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a216fc740). One of the two will be used. Which one is undefined.\n",
      "objc[2456]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2167b700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a216fc768). One of the two will be used. Which one is undefined.\n",
      "objc[2456]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2167b7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a216fc7b8). One of the two will be used. Which one is undefined.\n",
      "objc[2456]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2167b818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a216fc830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 19:12:30.003807 PDT | Variant:\n",
      "2021-05-25 19:12:30.005983 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"halfcheetahhard\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 300,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 4,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1\n",
      "}\n",
      "halfcheetah\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 19:15:25.124337 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      16.4485\n",
      "trainer/QF2 Loss                                      16.35\n",
      "trainer/Policy Loss                                   -4.03233\n",
      "trainer/Q1 Predictions Mean                           -0.00878209\n",
      "trainer/Q1 Predictions Std                             0.00549112\n",
      "trainer/Q1 Predictions Max                             0.00193813\n",
      "trainer/Q1 Predictions Min                            -0.0297639\n",
      "trainer/Q2 Predictions Mean                            0.00404296\n",
      "trainer/Q2 Predictions Std                             0.00436393\n",
      "trainer/Q2 Predictions Max                             0.0177005\n",
      "trainer/Q2 Predictions Min                            -0.00666305\n",
      "trainer/Q Targets Mean                                 3.85474\n",
      "trainer/Q Targets Std                                  1.23379\n",
      "trainer/Q Targets Max                                  8.65457\n",
      "trainer/Q Targets Min                                  1.00927\n",
      "trainer/Log Pis Mean                                  -4.04118\n",
      "trainer/Log Pis Std                                    0.507311\n",
      "trainer/Log Pis Max                                   -2.36109\n",
      "trainer/Log Pis Min                                   -5.48667\n",
      "trainer/Policy mu Mean                                -0.000942801\n",
      "trainer/Policy mu Std                                  0.00243959\n",
      "trainer/Policy mu Max                                  0.00708468\n",
      "trainer/Policy mu Min                                 -0.00807768\n",
      "trainer/Policy log std Mean                           -0.00133731\n",
      "trainer/Policy log std Std                             0.00174114\n",
      "trainer/Policy log std Max                             0.00438313\n",
      "trainer/Policy log std Min                            -0.00752543\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                            2\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.256938\n",
      "exploration/Rewards Std                                0.483894\n",
      "exploration/Rewards Max                                1.48765\n",
      "exploration/Rewards Min                               -1.93447\n",
      "exploration/Returns Mean                            -256.938\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -256.938\n",
      "exploration/Returns Min                             -256.938\n",
      "exploration/Actions Mean                               0.00112524\n",
      "exploration/Actions Std                                0.627105\n",
      "exploration/Actions Max                                0.999097\n",
      "exploration/Actions Min                               -0.999804\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -256.938\n",
      "exploration/env_infos/final/reward_run Mean           -0.0770525\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.0770525\n",
      "exploration/env_infos/final/reward_run Min            -0.0770525\n",
      "exploration/env_infos/initial/reward_run Mean         -0.337545\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.337545\n",
      "exploration/env_infos/initial/reward_run Min          -0.337545\n",
      "exploration/env_infos/reward_run Mean                  0.0250509\n",
      "exploration/env_infos/reward_run Std                   0.578486\n",
      "exploration/env_infos/reward_run Max                   1.91487\n",
      "exploration/env_infos/reward_run Min                  -2.19651\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112619\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112619\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112619\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.144612\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.144612\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.144612\n",
      "exploration/env_infos/reward_ctrl Mean                -0.235957\n",
      "exploration/env_infos/reward_ctrl Std                  0.0750538\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0182057\n",
      "exploration/env_infos/reward_ctrl Min                 -0.476095\n",
      "exploration/env_infos/final/height Mean               -0.511223\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.511223\n",
      "exploration/env_infos/final/height Min                -0.511223\n",
      "exploration/env_infos/initial/height Mean              0.028933\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.028933\n",
      "exploration/env_infos/initial/height Min               0.028933\n",
      "exploration/env_infos/height Mean                     -0.424873\n",
      "exploration/env_infos/height Std                       0.214461\n",
      "exploration/env_infos/height Max                       0.250689\n",
      "exploration/env_infos/height Min                      -0.585478\n",
      "exploration/env_infos/final/reward_angular Mean        0.023451\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.023451\n",
      "exploration/env_infos/final/reward_angular Min         0.023451\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.088873\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.088873\n",
      "exploration/env_infos/initial/reward_angular Min      -0.088873\n",
      "exploration/env_infos/reward_angular Mean              0.0518222\n",
      "exploration/env_infos/reward_angular Std               1.38249\n",
      "exploration/env_infos/reward_angular Max               5.48365\n",
      "exploration/env_infos/reward_angular Min              -5.68992\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0614772\n",
      "evaluation/Rewards Std                                 0.0458665\n",
      "evaluation/Rewards Max                                 1.27239\n",
      "evaluation/Rewards Min                                -0.830756\n",
      "evaluation/Returns Mean                              -61.4772\n",
      "evaluation/Returns Std                                38.3173\n",
      "evaluation/Returns Max                                -1.73093\n",
      "evaluation/Returns Min                              -127.103\n",
      "evaluation/Actions Mean                               -0.000322945\n",
      "evaluation/Actions Std                                 0.00121828\n",
      "evaluation/Actions Max                                 0.00379123\n",
      "evaluation/Actions Min                                -0.003127\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.4772\n",
      "evaluation/env_infos/final/reward_run Mean             1.38778e-18\n",
      "evaluation/env_infos/final/reward_run Std              1.05909e-16\n",
      "evaluation/env_infos/final/reward_run Max              2.77556e-16\n",
      "evaluation/env_infos/final/reward_run Min             -2.77556e-16\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.00445443\n",
      "evaluation/env_infos/initial/reward_run Std            0.12598\n",
      "evaluation/env_infos/initial/reward_run Max            0.269932\n",
      "evaluation/env_infos/initial/reward_run Min           -0.269817\n",
      "evaluation/env_infos/reward_run Mean                  -0.000415822\n",
      "evaluation/env_infos/reward_run Std                    0.0170344\n",
      "evaluation/env_infos/reward_run Max                    0.375798\n",
      "evaluation/env_infos/reward_run Min                   -0.422828\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -9.52177e-07\n",
      "evaluation/env_infos/final/reward_ctrl Std             9.0322e-08\n",
      "evaluation/env_infos/final/reward_ctrl Max            -8.07404e-07\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.10654e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.02536e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.1561e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -8.74703e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.26525e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -9.53108e-07\n",
      "evaluation/env_infos/reward_ctrl Std                   9.52301e-08\n",
      "evaluation/env_infos/reward_ctrl Max                  -6.20829e-07\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.99266e-06\n",
      "evaluation/env_infos/final/height Mean                -0.132891\n",
      "evaluation/env_infos/final/height Std                  3.5865e-05\n",
      "evaluation/env_infos/final/height Max                 -0.132829\n",
      "evaluation/env_infos/final/height Min                 -0.132952\n",
      "evaluation/env_infos/initial/height Mean               0.0172052\n",
      "evaluation/env_infos/initial/height Std                0.0460871\n",
      "evaluation/env_infos/initial/height Max                0.0745405\n",
      "evaluation/env_infos/initial/height Min               -0.0989749\n",
      "evaluation/env_infos/height Mean                      -0.132372\n",
      "evaluation/env_infos/height Std                        0.00676762\n",
      "evaluation/env_infos/height Max                        0.0745405\n",
      "evaluation/env_infos/height Min                       -0.151261\n",
      "evaluation/env_infos/final/reward_angular Mean         3.59563e-16\n",
      "evaluation/env_infos/final/reward_angular Std          8.55787e-16\n",
      "evaluation/env_infos/final/reward_angular Max          2.20663e-15\n",
      "evaluation/env_infos/final/reward_angular Min         -1.53163e-15\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0767808\n",
      "evaluation/env_infos/initial/reward_angular Std        0.221524\n",
      "evaluation/env_infos/initial/reward_angular Max        0.504993\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.263121\n",
      "evaluation/env_infos/reward_angular Mean               0.00084991\n",
      "evaluation/env_infos/reward_angular Std                0.0430414\n",
      "evaluation/env_infos/reward_angular Max                2.15541\n",
      "evaluation/env_infos/reward_angular Min               -1.35011\n",
      "time/data storing (s)                                  0.0220764\n",
      "time/evaluation sampling (s)                         153.804\n",
      "time/exploration sampling (s)                          1.63047\n",
      "time/logging (s)                                       0.295557\n",
      "time/saving (s)                                        0.0882023\n",
      "time/training (s)                                      4.53722\n",
      "time/epoch (s)                                       160.377\n",
      "time/total (s)                                       202.66\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:15:55.909258 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.759831\n",
      "trainer/QF2 Loss                                       0.756799\n",
      "trainer/Policy Loss                                   -7.1641\n",
      "trainer/Q1 Predictions Mean                            3.15621\n",
      "trainer/Q1 Predictions Std                             0.741393\n",
      "trainer/Q1 Predictions Max                             5.61151\n",
      "trainer/Q1 Predictions Min                             1.47727\n",
      "trainer/Q2 Predictions Mean                            3.1488\n",
      "trainer/Q2 Predictions Std                             0.735498\n",
      "trainer/Q2 Predictions Max                             5.48748\n",
      "trainer/Q2 Predictions Min                             1.48259\n",
      "trainer/Q Targets Mean                                 3.33972\n",
      "trainer/Q Targets Std                                  0.995985\n",
      "trainer/Q Targets Max                                  7.20022\n",
      "trainer/Q Targets Min                                  0.240236\n",
      "trainer/Log Pis Mean                                  -4.05516\n",
      "trainer/Log Pis Std                                    0.468402\n",
      "trainer/Log Pis Max                                   -2.69499\n",
      "trainer/Log Pis Min                                   -6.84255\n",
      "trainer/Policy mu Mean                                -0.105666\n",
      "trainer/Policy mu Std                                  0.0908432\n",
      "trainer/Policy mu Max                                  0.0714079\n",
      "trainer/Policy mu Min                                 -0.365117\n",
      "trainer/Policy log std Mean                           -0.122838\n",
      "trainer/Policy log std Std                             0.0215687\n",
      "trainer/Policy log std Max                            -0.0796456\n",
      "trainer/Policy log std Min                            -0.24552\n",
      "trainer/Alpha                                          0.738853\n",
      "trainer/Alpha Loss                                    -3.01314\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                            3\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.193786\n",
      "exploration/Rewards Std                                0.489026\n",
      "exploration/Rewards Max                                1.27452\n",
      "exploration/Rewards Min                               -1.74409\n",
      "exploration/Returns Mean                            -193.786\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -193.786\n",
      "exploration/Returns Min                             -193.786\n",
      "exploration/Actions Mean                              -0.0734756\n",
      "exploration/Actions Std                                0.592624\n",
      "exploration/Actions Max                                0.995178\n",
      "exploration/Actions Min                               -0.99877\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -193.786\n",
      "exploration/env_infos/final/reward_run Mean           -0.478577\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.478577\n",
      "exploration/env_infos/final/reward_run Min            -0.478577\n",
      "exploration/env_infos/initial/reward_run Mean         -0.310262\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.310262\n",
      "exploration/env_infos/initial/reward_run Min          -0.310262\n",
      "exploration/env_infos/reward_run Mean                 -0.0522605\n",
      "exploration/env_infos/reward_run Std                   0.670857\n",
      "exploration/env_infos/reward_run Max                   2.10458\n",
      "exploration/env_infos/reward_run Min                  -2.4897\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.203889\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.203889\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.203889\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.226613\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.226613\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.226613\n",
      "exploration/env_infos/reward_ctrl Mean                -0.213961\n",
      "exploration/env_infos/reward_ctrl Std                  0.0717174\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0193552\n",
      "exploration/env_infos/reward_ctrl Min                 -0.504585\n",
      "exploration/env_infos/final/height Mean               -0.109812\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.109812\n",
      "exploration/env_infos/final/height Min                -0.109812\n",
      "exploration/env_infos/initial/height Mean              0.0628117\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0628117\n",
      "exploration/env_infos/initial/height Min               0.0628117\n",
      "exploration/env_infos/height Mean                     -0.0741714\n",
      "exploration/env_infos/height Std                       0.0796817\n",
      "exploration/env_infos/height Max                       0.210749\n",
      "exploration/env_infos/height Min                      -0.277917\n",
      "exploration/env_infos/final/reward_angular Mean       -0.466305\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.466305\n",
      "exploration/env_infos/final/reward_angular Min        -0.466305\n",
      "exploration/env_infos/initial/reward_angular Mean      1.11672\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.11672\n",
      "exploration/env_infos/initial/reward_angular Min       1.11672\n",
      "exploration/env_infos/reward_angular Mean             -0.0127937\n",
      "exploration/env_infos/reward_angular Std               1.63903\n",
      "exploration/env_infos/reward_angular Max               5.25217\n",
      "exploration/env_infos/reward_angular Min              -4.86741\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.061646\n",
      "evaluation/Rewards Std                                 0.0453262\n",
      "evaluation/Rewards Max                                 0.817099\n",
      "evaluation/Rewards Min                                -1.16678\n",
      "evaluation/Returns Mean                              -61.646\n",
      "evaluation/Returns Std                                36.2549\n",
      "evaluation/Returns Max                                -2.03329\n",
      "evaluation/Returns Min                              -122.386\n",
      "evaluation/Actions Mean                               -0.0808052\n",
      "evaluation/Actions Std                                 0.0669519\n",
      "evaluation/Actions Max                                 0.031224\n",
      "evaluation/Actions Min                                -0.232126\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -61.646\n",
      "evaluation/env_infos/final/reward_run Mean            -1.17333e-09\n",
      "evaluation/env_infos/final/reward_run Std              6.76128e-09\n",
      "evaluation/env_infos/final/reward_run Max              3.57565e-09\n",
      "evaluation/env_infos/final/reward_run Min             -3.40704e-08\n",
      "evaluation/env_infos/initial/reward_run Mean          -0.0523942\n",
      "evaluation/env_infos/initial/reward_run Std            0.141483\n",
      "evaluation/env_infos/initial/reward_run Max            0.343118\n",
      "evaluation/env_infos/initial/reward_run Min           -0.298618\n",
      "evaluation/env_infos/reward_run Mean                  -0.000199276\n",
      "evaluation/env_infos/reward_run Std                    0.0182051\n",
      "evaluation/env_infos/reward_run Max                    0.393915\n",
      "evaluation/env_infos/reward_run Min                   -0.490515\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00660059\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00117508\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00443816\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0080108\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00666229\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00117311\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0045753\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0082364\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00660722\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00117887\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00417725\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0108785\n",
      "evaluation/env_infos/final/height Mean                -0.127474\n",
      "evaluation/env_infos/final/height Std                  0.000666238\n",
      "evaluation/env_infos/final/height Max                 -0.126775\n",
      "evaluation/env_infos/final/height Min                 -0.128849\n",
      "evaluation/env_infos/initial/height Mean              -0.0235057\n",
      "evaluation/env_infos/initial/height Std                0.0531799\n",
      "evaluation/env_infos/initial/height Max                0.0792834\n",
      "evaluation/env_infos/initial/height Min               -0.100144\n",
      "evaluation/env_infos/height Mean                      -0.127105\n",
      "evaluation/env_infos/height Std                        0.00499391\n",
      "evaluation/env_infos/height Max                        0.0792834\n",
      "evaluation/env_infos/height Min                       -0.150323\n",
      "evaluation/env_infos/final/reward_angular Mean         2.34613e-09\n",
      "evaluation/env_infos/final/reward_angular Std          1.02745e-08\n",
      "evaluation/env_infos/final/reward_angular Max          4.86705e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -1.1566e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.463014\n",
      "evaluation/env_infos/initial/reward_angular Std        0.405404\n",
      "evaluation/env_infos/initial/reward_angular Max        1.29827\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.479825\n",
      "evaluation/env_infos/reward_angular Mean               0.00162165\n",
      "evaluation/env_infos/reward_angular Std                0.0450901\n",
      "evaluation/env_infos/reward_angular Max                1.67448\n",
      "evaluation/env_infos/reward_angular Min               -1.04615\n",
      "time/data storing (s)                                  0.01605\n",
      "time/evaluation sampling (s)                          25.6749\n",
      "time/exploration sampling (s)                          1.0937\n",
      "time/logging (s)                                       0.252259\n",
      "time/saving (s)                                        0.0313565\n",
      "time/training (s)                                      3.4135\n",
      "time/epoch (s)                                        30.4818\n",
      "time/total (s)                                       233.401\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:16:22.378764 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.482409\n",
      "trainer/QF2 Loss                                       0.515073\n",
      "trainer/Policy Loss                                   -6.996\n",
      "trainer/Q1 Predictions Mean                            2.96817\n",
      "trainer/Q1 Predictions Std                             0.561307\n",
      "trainer/Q1 Predictions Max                             5.15647\n",
      "trainer/Q1 Predictions Min                             1.93176\n",
      "trainer/Q2 Predictions Mean                            2.9734\n",
      "trainer/Q2 Predictions Std                             0.551131\n",
      "trainer/Q2 Predictions Max                             5.09151\n",
      "trainer/Q2 Predictions Min                             1.97413\n",
      "trainer/Q Targets Mean                                 3.06144\n",
      "trainer/Q Targets Std                                  0.92545\n",
      "trainer/Q Targets Max                                  6.68418\n",
      "trainer/Q Targets Min                                  0.169637\n",
      "trainer/Log Pis Mean                                  -4.03276\n",
      "trainer/Log Pis Std                                    0.481173\n",
      "trainer/Log Pis Max                                   -2.80205\n",
      "trainer/Log Pis Min                                   -5.52057\n",
      "trainer/Policy mu Mean                                -0.0413206\n",
      "trainer/Policy mu Std                                  0.135977\n",
      "trainer/Policy mu Max                                  0.481553\n",
      "trainer/Policy mu Min                                 -0.410229\n",
      "trainer/Policy log std Mean                           -0.123695\n",
      "trainer/Policy log std Std                             0.0241268\n",
      "trainer/Policy log std Max                            -0.0779001\n",
      "trainer/Policy log std Min                            -0.242472\n",
      "trainer/Alpha                                          0.547324\n",
      "trainer/Alpha Loss                                    -6.01685\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                            4\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.343968\n",
      "exploration/Rewards Std                                0.494135\n",
      "exploration/Rewards Max                                1.25281\n",
      "exploration/Rewards Min                               -2.5068\n",
      "exploration/Returns Mean                            -343.968\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -343.968\n",
      "exploration/Returns Min                             -343.968\n",
      "exploration/Actions Mean                              -0.0514151\n",
      "exploration/Actions Std                                0.593992\n",
      "exploration/Actions Max                                0.995371\n",
      "exploration/Actions Min                               -0.996649\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -343.968\n",
      "exploration/env_infos/final/reward_run Mean            0.220811\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.220811\n",
      "exploration/env_infos/final/reward_run Min             0.220811\n",
      "exploration/env_infos/initial/reward_run Mean          0.2432\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.2432\n",
      "exploration/env_infos/initial/reward_run Min           0.2432\n",
      "exploration/env_infos/reward_run Mean                 -0.0617182\n",
      "exploration/env_infos/reward_run Std                   0.565849\n",
      "exploration/env_infos/reward_run Max                   1.79125\n",
      "exploration/env_infos/reward_run Min                  -2.36721\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.158726\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.158726\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.158726\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.186687\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.186687\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.186687\n",
      "exploration/env_infos/reward_ctrl Mean                -0.213282\n",
      "exploration/env_infos/reward_ctrl Std                  0.0747251\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0212309\n",
      "exploration/env_infos/reward_ctrl Min                 -0.432536\n",
      "exploration/env_infos/final/height Mean               -0.569515\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.569515\n",
      "exploration/env_infos/final/height Min                -0.569515\n",
      "exploration/env_infos/initial/height Mean              0.0274335\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0274335\n",
      "exploration/env_infos/initial/height Min               0.0274335\n",
      "exploration/env_infos/height Mean                     -0.440018\n",
      "exploration/env_infos/height Std                       0.213972\n",
      "exploration/env_infos/height Max                       0.182605\n",
      "exploration/env_infos/height Min                      -0.588792\n",
      "exploration/env_infos/final/reward_angular Mean        0.101318\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.101318\n",
      "exploration/env_infos/final/reward_angular Min         0.101318\n",
      "exploration/env_infos/initial/reward_angular Mean      0.590154\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.590154\n",
      "exploration/env_infos/initial/reward_angular Min       0.590154\n",
      "exploration/env_infos/reward_angular Mean              0.0763354\n",
      "exploration/env_infos/reward_angular Std               1.23674\n",
      "exploration/env_infos/reward_angular Max               5.05028\n",
      "exploration/env_infos/reward_angular Min              -4.10835\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0602008\n",
      "evaluation/Rewards Std                                 0.043909\n",
      "evaluation/Rewards Max                                 1.81843\n",
      "evaluation/Rewards Min                                -1.22773\n",
      "evaluation/Returns Mean                              -60.2008\n",
      "evaluation/Returns Std                                35.8582\n",
      "evaluation/Returns Max                                -1.57134\n",
      "evaluation/Returns Min                              -121.659\n",
      "evaluation/Actions Mean                               -0.0224863\n",
      "evaluation/Actions Std                                 0.0973222\n",
      "evaluation/Actions Max                                 0.214327\n",
      "evaluation/Actions Min                                -0.269973\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -60.2008\n",
      "evaluation/env_infos/final/reward_run Mean            -2.50673e-09\n",
      "evaluation/env_infos/final/reward_run Std              8.38359e-09\n",
      "evaluation/env_infos/final/reward_run Max              1.06644e-08\n",
      "evaluation/env_infos/final/reward_run Min             -3.23104e-08\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0721329\n",
      "evaluation/env_infos/initial/reward_run Std            0.130033\n",
      "evaluation/env_infos/initial/reward_run Max            0.335845\n",
      "evaluation/env_infos/initial/reward_run Min           -0.164762\n",
      "evaluation/env_infos/reward_run Mean                  -0.000307722\n",
      "evaluation/env_infos/reward_run Std                    0.0153106\n",
      "evaluation/env_infos/reward_run Max                    0.335845\n",
      "evaluation/env_infos/reward_run Min                   -0.409997\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00598284\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00290431\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00186405\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0109793\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00749711\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00343742\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00203954\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0141821\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00598634\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00290614\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00169544\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0161255\n",
      "evaluation/env_infos/final/height Mean                -0.12497\n",
      "evaluation/env_infos/final/height Std                  0.00258732\n",
      "evaluation/env_infos/final/height Max                 -0.121117\n",
      "evaluation/env_infos/final/height Min                 -0.129074\n",
      "evaluation/env_infos/initial/height Mean              -0.0229117\n",
      "evaluation/env_infos/initial/height Std                0.0552103\n",
      "evaluation/env_infos/initial/height Max                0.0914091\n",
      "evaluation/env_infos/initial/height Min               -0.0913409\n",
      "evaluation/env_infos/height Mean                      -0.124533\n",
      "evaluation/env_infos/height Std                        0.00567724\n",
      "evaluation/env_infos/height Max                        0.0914091\n",
      "evaluation/env_infos/height Min                       -0.136365\n",
      "evaluation/env_infos/final/reward_angular Mean         5.07299e-11\n",
      "evaluation/env_infos/final/reward_angular Std          2.28459e-08\n",
      "evaluation/env_infos/final/reward_angular Max          9.61492e-08\n",
      "evaluation/env_infos/final/reward_angular Min         -4.61623e-08\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.342994\n",
      "evaluation/env_infos/initial/reward_angular Std        0.445057\n",
      "evaluation/env_infos/initial/reward_angular Max        1.28379\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.454546\n",
      "evaluation/env_infos/reward_angular Mean               0.00141063\n",
      "evaluation/env_infos/reward_angular Std                0.040628\n",
      "evaluation/env_infos/reward_angular Max                2.09566\n",
      "evaluation/env_infos/reward_angular Min               -0.670756\n",
      "time/data storing (s)                                  0.0159882\n",
      "time/evaluation sampling (s)                          21.8197\n",
      "time/exploration sampling (s)                          1.04556\n",
      "time/logging (s)                                       0.238443\n",
      "time/saving (s)                                        0.0286396\n",
      "time/training (s)                                      3.17339\n",
      "time/epoch (s)                                        26.3217\n",
      "time/total (s)                                       259.856\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:16:48.241165 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.557301\n",
      "trainer/QF2 Loss                                        0.520879\n",
      "trainer/Policy Loss                                    -6.86797\n",
      "trainer/Q1 Predictions Mean                             2.80525\n",
      "trainer/Q1 Predictions Std                              0.762896\n",
      "trainer/Q1 Predictions Max                              6.20174\n",
      "trainer/Q1 Predictions Min                              1.03396\n",
      "trainer/Q2 Predictions Mean                             2.9522\n",
      "trainer/Q2 Predictions Std                              0.767162\n",
      "trainer/Q2 Predictions Max                              6.41029\n",
      "trainer/Q2 Predictions Min                              1.11753\n",
      "trainer/Q Targets Mean                                  2.9914\n",
      "trainer/Q Targets Std                                   0.920496\n",
      "trainer/Q Targets Max                                   7.05108\n",
      "trainer/Q Targets Min                                   0.980729\n",
      "trainer/Log Pis Mean                                   -3.94913\n",
      "trainer/Log Pis Std                                     0.714619\n",
      "trainer/Log Pis Max                                    -0.758852\n",
      "trainer/Log Pis Min                                    -5.6518\n",
      "trainer/Policy mu Mean                                 -0.0539171\n",
      "trainer/Policy mu Std                                   0.193199\n",
      "trainer/Policy mu Max                                   0.511531\n",
      "trainer/Policy mu Min                                  -0.995167\n",
      "trainer/Policy log std Mean                            -0.0986545\n",
      "trainer/Policy log std Std                              0.0300217\n",
      "trainer/Policy log std Max                              0.0140674\n",
      "trainer/Policy log std Min                             -0.237736\n",
      "trainer/Alpha                                           0.405991\n",
      "trainer/Alpha Loss                                     -8.93888\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                             5\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.151637\n",
      "exploration/Rewards Std                                 0.557944\n",
      "exploration/Rewards Max                                 1.74894\n",
      "exploration/Rewards Min                                -1.95247\n",
      "exploration/Returns Mean                             -151.637\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -151.637\n",
      "exploration/Returns Min                              -151.637\n",
      "exploration/Actions Mean                               -0.0226448\n",
      "exploration/Actions Std                                 0.600005\n",
      "exploration/Actions Max                                 0.999271\n",
      "exploration/Actions Min                                -0.998251\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -151.637\n",
      "exploration/env_infos/final/reward_run Mean             0.437121\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.437121\n",
      "exploration/env_infos/final/reward_run Min              0.437121\n",
      "exploration/env_infos/initial/reward_run Mean           0.277531\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.277531\n",
      "exploration/env_infos/initial/reward_run Min            0.277531\n",
      "exploration/env_infos/reward_run Mean                  -0.128948\n",
      "exploration/env_infos/reward_run Std                    0.679492\n",
      "exploration/env_infos/reward_run Max                    1.86499\n",
      "exploration/env_infos/reward_run Min                   -2.39707\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.208906\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.208906\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.208906\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.196425\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.196425\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.196425\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.216312\n",
      "exploration/env_infos/reward_ctrl Std                   0.0703085\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00698232\n",
      "exploration/env_infos/reward_ctrl Min                  -0.448976\n",
      "exploration/env_infos/final/height Mean                -0.54242\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.54242\n",
      "exploration/env_infos/final/height Min                 -0.54242\n",
      "exploration/env_infos/initial/height Mean              -0.0996347\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0996347\n",
      "exploration/env_infos/initial/height Min               -0.0996347\n",
      "exploration/env_infos/height Mean                      -0.268666\n",
      "exploration/env_infos/height Std                        0.249135\n",
      "exploration/env_infos/height Max                        0.309918\n",
      "exploration/env_infos/height Min                       -0.581587\n",
      "exploration/env_infos/final/reward_angular Mean        -1.64885\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.64885\n",
      "exploration/env_infos/final/reward_angular Min         -1.64885\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0406069\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0406069\n",
      "exploration/env_infos/initial/reward_angular Min        0.0406069\n",
      "exploration/env_infos/reward_angular Mean               0.0468092\n",
      "exploration/env_infos/reward_angular Std                1.54645\n",
      "exploration/env_infos/reward_angular Max                5.79795\n",
      "exploration/env_infos/reward_angular Min               -5.18096\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0619283\n",
      "evaluation/Rewards Std                                  0.0442326\n",
      "evaluation/Rewards Max                                  1.64246\n",
      "evaluation/Rewards Min                                 -0.909835\n",
      "evaluation/Returns Mean                               -61.9283\n",
      "evaluation/Returns Std                                 32.7853\n",
      "evaluation/Returns Max                                 -3.39957\n",
      "evaluation/Returns Min                               -116.569\n",
      "evaluation/Actions Mean                                -0.023017\n",
      "evaluation/Actions Std                                  0.16756\n",
      "evaluation/Actions Max                                  0.264512\n",
      "evaluation/Actions Min                                 -0.662527\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -61.9283\n",
      "evaluation/env_infos/final/reward_run Mean             -7.62076e-10\n",
      "evaluation/env_infos/final/reward_run Std               3.25559e-08\n",
      "evaluation/env_infos/final/reward_run Max               6.2504e-08\n",
      "evaluation/env_infos/final/reward_run Min              -9.78077e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0722527\n",
      "evaluation/env_infos/initial/reward_run Std             0.13878\n",
      "evaluation/env_infos/initial/reward_run Max             0.41661\n",
      "evaluation/env_infos/initial/reward_run Min            -0.162583\n",
      "evaluation/env_infos/reward_run Mean                   -0.000512736\n",
      "evaluation/env_infos/reward_run Std                     0.0177573\n",
      "evaluation/env_infos/reward_run Max                     0.460333\n",
      "evaluation/env_infos/reward_run Min                    -0.410485\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0171538\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0162101\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00105531\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0511993\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0250472\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0225058\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00136661\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0685814\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0171638\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0162256\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000325173\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0685814\n",
      "evaluation/env_infos/final/height Mean                 -0.121512\n",
      "evaluation/env_infos/final/height Std                   0.00430573\n",
      "evaluation/env_infos/final/height Max                  -0.115257\n",
      "evaluation/env_infos/final/height Min                  -0.128104\n",
      "evaluation/env_infos/initial/height Mean                0.00312027\n",
      "evaluation/env_infos/initial/height Std                 0.0531882\n",
      "evaluation/env_infos/initial/height Max                 0.0850637\n",
      "evaluation/env_infos/initial/height Min                -0.0814433\n",
      "evaluation/env_infos/height Mean                       -0.121054\n",
      "evaluation/env_infos/height Std                         0.0071845\n",
      "evaluation/env_infos/height Max                         0.0850637\n",
      "evaluation/env_infos/height Min                        -0.133377\n",
      "evaluation/env_infos/final/reward_angular Mean          1.36143e-08\n",
      "evaluation/env_infos/final/reward_angular Std           5.51444e-08\n",
      "evaluation/env_infos/final/reward_angular Max           1.39946e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -1.10507e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.21313\n",
      "evaluation/env_infos/initial/reward_angular Std         0.559554\n",
      "evaluation/env_infos/initial/reward_angular Max         1.44383\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.577274\n",
      "evaluation/env_infos/reward_angular Mean                0.00120557\n",
      "evaluation/env_infos/reward_angular Std                 0.0501608\n",
      "evaluation/env_infos/reward_angular Max                 2.5026\n",
      "evaluation/env_infos/reward_angular Min                -1.42582\n",
      "time/data storing (s)                                   0.0148363\n",
      "time/evaluation sampling (s)                           21.2972\n",
      "time/exploration sampling (s)                           0.989067\n",
      "time/logging (s)                                        0.23861\n",
      "time/saving (s)                                         0.0272836\n",
      "time/training (s)                                       3.16581\n",
      "time/epoch (s)                                         25.7328\n",
      "time/total (s)                                        285.718\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:17:14.638382 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.447362\n",
      "trainer/QF2 Loss                                        0.435775\n",
      "trainer/Policy Loss                                    -6.72934\n",
      "trainer/Q1 Predictions Mean                             2.7446\n",
      "trainer/Q1 Predictions Std                              0.647753\n",
      "trainer/Q1 Predictions Max                              5.11695\n",
      "trainer/Q1 Predictions Min                              1.33403\n",
      "trainer/Q2 Predictions Mean                             2.77456\n",
      "trainer/Q2 Predictions Std                              0.630776\n",
      "trainer/Q2 Predictions Max                              5.07814\n",
      "trainer/Q2 Predictions Min                              1.36623\n",
      "trainer/Q Targets Mean                                  2.97705\n",
      "trainer/Q Targets Std                                   0.936496\n",
      "trainer/Q Targets Max                                   6.47634\n",
      "trainer/Q Targets Min                                   0.039161\n",
      "trainer/Log Pis Mean                                   -3.89808\n",
      "trainer/Log Pis Std                                     0.823158\n",
      "trainer/Log Pis Max                                    -0.669906\n",
      "trainer/Log Pis Min                                    -8.78147\n",
      "trainer/Policy mu Mean                                  0.022787\n",
      "trainer/Policy mu Std                                   0.224023\n",
      "trainer/Policy mu Max                                   0.67534\n",
      "trainer/Policy mu Min                                  -0.874064\n",
      "trainer/Policy log std Mean                            -0.112478\n",
      "trainer/Policy log std Std                              0.0303075\n",
      "trainer/Policy log std Max                             -0.0323802\n",
      "trainer/Policy log std Min                             -0.284224\n",
      "trainer/Alpha                                           0.301831\n",
      "trainer/Alpha Loss                                    -11.8275\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                             6\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.203876\n",
      "exploration/Rewards Std                                 0.792677\n",
      "exploration/Rewards Max                                 2.99015\n",
      "exploration/Rewards Min                                -3.17272\n",
      "exploration/Returns Mean                             -203.876\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -203.876\n",
      "exploration/Returns Min                              -203.876\n",
      "exploration/Actions Mean                                0.0167292\n",
      "exploration/Actions Std                                 0.596943\n",
      "exploration/Actions Max                                 0.996153\n",
      "exploration/Actions Min                                -0.995942\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -203.876\n",
      "exploration/env_infos/final/reward_run Mean             0.306342\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.306342\n",
      "exploration/env_infos/final/reward_run Min              0.306342\n",
      "exploration/env_infos/initial/reward_run Mean           0.84575\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.84575\n",
      "exploration/env_infos/initial/reward_run Min            0.84575\n",
      "exploration/env_infos/reward_run Mean                  -0.126171\n",
      "exploration/env_infos/reward_run Std                    0.710293\n",
      "exploration/env_infos/reward_run Max                    2.24454\n",
      "exploration/env_infos/reward_run Min                   -2.46536\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.293732\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.293732\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.293732\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.209067\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.209067\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.209067\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.213973\n",
      "exploration/env_infos/reward_ctrl Std                   0.072987\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0282565\n",
      "exploration/env_infos/reward_ctrl Min                  -0.507166\n",
      "exploration/env_infos/final/height Mean                -0.0992695\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0992695\n",
      "exploration/env_infos/final/height Min                 -0.0992695\n",
      "exploration/env_infos/initial/height Mean              -0.0678538\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0678538\n",
      "exploration/env_infos/initial/height Min               -0.0678538\n",
      "exploration/env_infos/height Mean                      -0.0681359\n",
      "exploration/env_infos/height Std                        0.0897261\n",
      "exploration/env_infos/height Max                        0.230079\n",
      "exploration/env_infos/height Min                       -0.361481\n",
      "exploration/env_infos/final/reward_angular Mean        -1.3882\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.3882\n",
      "exploration/env_infos/final/reward_angular Min         -1.3882\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.94485\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.94485\n",
      "exploration/env_infos/initial/reward_angular Min       -0.94485\n",
      "exploration/env_infos/reward_angular Mean              -0.0228087\n",
      "exploration/env_infos/reward_angular Std                1.73563\n",
      "exploration/env_infos/reward_angular Max                6.54683\n",
      "exploration/env_infos/reward_angular Min               -5.48509\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0703767\n",
      "evaluation/Rewards Std                                  0.0468586\n",
      "evaluation/Rewards Max                                  1.23665\n",
      "evaluation/Rewards Min                                 -1.02046\n",
      "evaluation/Returns Mean                               -70.3767\n",
      "evaluation/Returns Std                                 32.4308\n",
      "evaluation/Returns Max                                 -4.71721\n",
      "evaluation/Returns Min                               -124.503\n",
      "evaluation/Actions Mean                                 0.0507369\n",
      "evaluation/Actions Std                                  0.229434\n",
      "evaluation/Actions Max                                  0.538566\n",
      "evaluation/Actions Min                                 -0.689122\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -70.3767\n",
      "evaluation/env_infos/final/reward_run Mean              3.82048e-09\n",
      "evaluation/env_infos/final/reward_run Std               2.64797e-08\n",
      "evaluation/env_infos/final/reward_run Max               6.77514e-08\n",
      "evaluation/env_infos/final/reward_run Min              -7.26641e-08\n",
      "evaluation/env_infos/initial/reward_run Mean            0.176961\n",
      "evaluation/env_infos/initial/reward_run Std             0.229966\n",
      "evaluation/env_infos/initial/reward_run Max             0.479109\n",
      "evaluation/env_infos/initial/reward_run Min            -0.401974\n",
      "evaluation/env_infos/reward_run Mean                   -0.000323135\n",
      "evaluation/env_infos/reward_run Std                     0.0222168\n",
      "evaluation/env_infos/reward_run Max                     0.654953\n",
      "evaluation/env_infos/reward_run Min                    -0.461159\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0331337\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0180201\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00490091\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0633292\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0500264\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.025175\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00669271\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0943065\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0331285\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0180322\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00393955\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0943065\n",
      "evaluation/env_infos/final/height Mean                 -0.127974\n",
      "evaluation/env_infos/final/height Std                   0.00859101\n",
      "evaluation/env_infos/final/height Max                  -0.116909\n",
      "evaluation/env_infos/final/height Min                  -0.140122\n",
      "evaluation/env_infos/initial/height Mean               -0.0141639\n",
      "evaluation/env_infos/initial/height Std                 0.0572729\n",
      "evaluation/env_infos/initial/height Max                 0.0719891\n",
      "evaluation/env_infos/initial/height Min                -0.113892\n",
      "evaluation/env_infos/height Mean                       -0.12758\n",
      "evaluation/env_infos/height Std                         0.0101331\n",
      "evaluation/env_infos/height Max                         0.0719891\n",
      "evaluation/env_infos/height Min                        -0.160564\n",
      "evaluation/env_infos/final/reward_angular Mean         -9.5566e-09\n",
      "evaluation/env_infos/final/reward_angular Std           7.02038e-08\n",
      "evaluation/env_infos/final/reward_angular Max           2.52799e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -1.50336e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.131445\n",
      "evaluation/env_infos/initial/reward_angular Std         0.927184\n",
      "evaluation/env_infos/initial/reward_angular Max         1.47061\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.59325\n",
      "evaluation/env_infos/reward_angular Mean                0.0001755\n",
      "evaluation/env_infos/reward_angular Std                 0.0542216\n",
      "evaluation/env_infos/reward_angular Max                 2.07569\n",
      "evaluation/env_infos/reward_angular Min                -1.59325\n",
      "time/data storing (s)                                   0.0149209\n",
      "time/evaluation sampling (s)                           21.6638\n",
      "time/exploration sampling (s)                           0.99472\n",
      "time/logging (s)                                        0.231746\n",
      "time/saving (s)                                         0.0274825\n",
      "time/training (s)                                       3.31811\n",
      "time/epoch (s)                                         26.2508\n",
      "time/total (s)                                        312.107\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:17:39.677097 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.522173\n",
      "trainer/QF2 Loss                                        0.506316\n",
      "trainer/Policy Loss                                    -6.8807\n",
      "trainer/Q1 Predictions Mean                             3.0829\n",
      "trainer/Q1 Predictions Std                              0.796732\n",
      "trainer/Q1 Predictions Max                              7.69773\n",
      "trainer/Q1 Predictions Min                              1.31868\n",
      "trainer/Q2 Predictions Mean                             3.05331\n",
      "trainer/Q2 Predictions Std                              0.814356\n",
      "trainer/Q2 Predictions Max                              7.80006\n",
      "trainer/Q2 Predictions Min                              1.31914\n",
      "trainer/Q Targets Mean                                  3.00616\n",
      "trainer/Q Targets Std                                   1.13537\n",
      "trainer/Q Targets Max                                   9.00814\n",
      "trainer/Q Targets Min                                  -0.901056\n",
      "trainer/Log Pis Mean                                   -3.73149\n",
      "trainer/Log Pis Std                                     1.08578\n",
      "trainer/Log Pis Max                                    -0.049711\n",
      "trainer/Log Pis Min                                    -7.76909\n",
      "trainer/Policy mu Mean                                  0.0538834\n",
      "trainer/Policy mu Std                                   0.317568\n",
      "trainer/Policy mu Max                                   1.21093\n",
      "trainer/Policy mu Min                                  -1.14148\n",
      "trainer/Policy log std Mean                            -0.133248\n",
      "trainer/Policy log std Std                              0.035502\n",
      "trainer/Policy log std Max                             -0.0514465\n",
      "trainer/Policy log std Min                             -0.245203\n",
      "trainer/Alpha                                           0.224684\n",
      "trainer/Alpha Loss                                    -14.5011\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                             7\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0995671\n",
      "exploration/Rewards Std                                 0.991383\n",
      "exploration/Rewards Max                                 3.4031\n",
      "exploration/Rewards Min                                -4.36245\n",
      "exploration/Returns Mean                              -99.5671\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -99.5671\n",
      "exploration/Returns Min                               -99.5671\n",
      "exploration/Actions Mean                                0.106898\n",
      "exploration/Actions Std                                 0.585489\n",
      "exploration/Actions Max                                 0.997989\n",
      "exploration/Actions Min                                -0.998488\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -99.5671\n",
      "exploration/env_infos/final/reward_run Mean             1.26971\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.26971\n",
      "exploration/env_infos/final/reward_run Min              1.26971\n",
      "exploration/env_infos/initial/reward_run Mean           0.12809\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.12809\n",
      "exploration/env_infos/initial/reward_run Min            0.12809\n",
      "exploration/env_infos/reward_run Mean                  -0.167685\n",
      "exploration/env_infos/reward_run Std                    0.759556\n",
      "exploration/env_infos/reward_run Max                    2.22335\n",
      "exploration/env_infos/reward_run Min                   -2.89731\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.342016\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.342016\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.342016\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.191465\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.191465\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.191465\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.212535\n",
      "exploration/env_infos/reward_ctrl Std                   0.0719656\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00960072\n",
      "exploration/env_infos/reward_ctrl Min                  -0.465733\n",
      "exploration/env_infos/final/height Mean                -0.123723\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.123723\n",
      "exploration/env_infos/final/height Min                 -0.123723\n",
      "exploration/env_infos/initial/height Mean              -0.0283806\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0283806\n",
      "exploration/env_infos/initial/height Min               -0.0283806\n",
      "exploration/env_infos/height Mean                      -0.0750848\n",
      "exploration/env_infos/height Std                        0.0859603\n",
      "exploration/env_infos/height Max                        0.236318\n",
      "exploration/env_infos/height Min                       -0.308808\n",
      "exploration/env_infos/final/reward_angular Mean        -1.8126\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.8126\n",
      "exploration/env_infos/final/reward_angular Min         -1.8126\n",
      "exploration/env_infos/initial/reward_angular Mean       0.101074\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.101074\n",
      "exploration/env_infos/initial/reward_angular Min        0.101074\n",
      "exploration/env_infos/reward_angular Mean              -0.00498262\n",
      "exploration/env_infos/reward_angular Std                1.59334\n",
      "exploration/env_infos/reward_angular Max                6.46329\n",
      "exploration/env_infos/reward_angular Min               -5.28851\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0732893\n",
      "evaluation/Rewards Std                                  0.0573688\n",
      "evaluation/Rewards Max                                  2.70099\n",
      "evaluation/Rewards Min                                 -1.28956\n",
      "evaluation/Returns Mean                               -73.2893\n",
      "evaluation/Returns Std                                 31.8624\n",
      "evaluation/Returns Max                                 -8.84549\n",
      "evaluation/Returns Min                               -131.758\n",
      "evaluation/Actions Mean                                 0.0654853\n",
      "evaluation/Actions Std                                  0.248863\n",
      "evaluation/Actions Max                                  0.599308\n",
      "evaluation/Actions Min                                 -0.764044\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -73.2893\n",
      "evaluation/env_infos/final/reward_run Mean              9.31717e-09\n",
      "evaluation/env_infos/final/reward_run Std               5.62762e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.60012e-07\n",
      "evaluation/env_infos/final/reward_run Min              -1.09529e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.303276\n",
      "evaluation/env_infos/initial/reward_run Std             0.177329\n",
      "evaluation/env_infos/initial/reward_run Max             0.688697\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0480063\n",
      "evaluation/env_infos/reward_run Mean                   -0.000663679\n",
      "evaluation/env_infos/reward_run Std                     0.0316808\n",
      "evaluation/env_infos/reward_run Max                     0.841664\n",
      "evaluation/env_infos/reward_run Min                    -0.761204\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0397278\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0267438\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00276612\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.100058\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0608311\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0391122\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0102474\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.150705\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0397327\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0267595\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00157277\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.150705\n",
      "evaluation/env_infos/final/height Mean                 -0.12945\n",
      "evaluation/env_infos/final/height Std                   0.014668\n",
      "evaluation/env_infos/final/height Max                  -0.112242\n",
      "evaluation/env_infos/final/height Min                  -0.15431\n",
      "evaluation/env_infos/initial/height Mean               -0.0192605\n",
      "evaluation/env_infos/initial/height Std                 0.049137\n",
      "evaluation/env_infos/initial/height Max                 0.0711766\n",
      "evaluation/env_infos/initial/height Min                -0.0902854\n",
      "evaluation/env_infos/height Mean                       -0.129052\n",
      "evaluation/env_infos/height Std                         0.0155367\n",
      "evaluation/env_infos/height Max                         0.0711766\n",
      "evaluation/env_infos/height Min                        -0.158462\n",
      "evaluation/env_infos/final/reward_angular Mean         -3.50947e-08\n",
      "evaluation/env_infos/final/reward_angular Std           9.80285e-08\n",
      "evaluation/env_infos/final/reward_angular Max           2.03436e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -3.00246e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0751194\n",
      "evaluation/env_infos/initial/reward_angular Std         1.16345\n",
      "evaluation/env_infos/initial/reward_angular Max         3.00526\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.23458\n",
      "evaluation/env_infos/reward_angular Mean                0.000967277\n",
      "evaluation/env_infos/reward_angular Std                 0.0684075\n",
      "evaluation/env_infos/reward_angular Max                 3.00526\n",
      "evaluation/env_infos/reward_angular Min                -1.30873\n",
      "time/data storing (s)                                   0.0141619\n",
      "time/evaluation sampling (s)                           20.653\n",
      "time/exploration sampling (s)                           0.940592\n",
      "time/logging (s)                                        0.227996\n",
      "time/saving (s)                                         0.0266545\n",
      "time/training (s)                                       3.03383\n",
      "time/epoch (s)                                         24.8962\n",
      "time/total (s)                                        337.142\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:18:06.871422 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.342843\n",
      "trainer/QF2 Loss                                        0.397917\n",
      "trainer/Policy Loss                                    -6.4281\n",
      "trainer/Q1 Predictions Mean                             2.95322\n",
      "trainer/Q1 Predictions Std                              0.912051\n",
      "trainer/Q1 Predictions Max                              7.03354\n",
      "trainer/Q1 Predictions Min                              1.35121\n",
      "trainer/Q2 Predictions Mean                             2.91718\n",
      "trainer/Q2 Predictions Std                              0.891917\n",
      "trainer/Q2 Predictions Max                              7.00924\n",
      "trainer/Q2 Predictions Min                              1.10192\n",
      "trainer/Q Targets Mean                                  3.0201\n",
      "trainer/Q Targets Std                                   1.10255\n",
      "trainer/Q Targets Max                                   7.63216\n",
      "trainer/Q Targets Min                                   1.13234\n",
      "trainer/Log Pis Mean                                   -3.38455\n",
      "trainer/Log Pis Std                                     1.25482\n",
      "trainer/Log Pis Max                                     0.677108\n",
      "trainer/Log Pis Min                                    -6.44762\n",
      "trainer/Policy mu Mean                                  0.164773\n",
      "trainer/Policy mu Std                                   0.377097\n",
      "trainer/Policy mu Max                                   1.41608\n",
      "trainer/Policy mu Min                                  -1.29278\n",
      "trainer/Policy log std Mean                            -0.159112\n",
      "trainer/Policy log std Std                              0.057286\n",
      "trainer/Policy log std Max                             -0.0772929\n",
      "trainer/Policy log std Min                             -0.498581\n",
      "trainer/Alpha                                           0.168304\n",
      "trainer/Alpha Loss                                    -16.6964\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                             8\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.187552\n",
      "exploration/Rewards Std                                 0.773314\n",
      "exploration/Rewards Max                                 2.2348\n",
      "exploration/Rewards Min                                -3.07628\n",
      "exploration/Returns Mean                             -187.552\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -187.552\n",
      "exploration/Returns Min                              -187.552\n",
      "exploration/Actions Mean                                0.254149\n",
      "exploration/Actions Std                                 0.572086\n",
      "exploration/Actions Max                                 0.998543\n",
      "exploration/Actions Min                                -0.989297\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -187.552\n",
      "exploration/env_infos/final/reward_run Mean            -0.0296933\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0296933\n",
      "exploration/env_infos/final/reward_run Min             -0.0296933\n",
      "exploration/env_infos/initial/reward_run Mean           0.375775\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.375775\n",
      "exploration/env_infos/initial/reward_run Min            0.375775\n",
      "exploration/env_infos/reward_run Mean                  -0.039463\n",
      "exploration/env_infos/reward_run Std                    0.535309\n",
      "exploration/env_infos/reward_run Max                    1.68612\n",
      "exploration/env_infos/reward_run Min                   -1.86669\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.28704\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.28704\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.28704\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.10021\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.10021\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.10021\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.235124\n",
      "exploration/env_infos/reward_ctrl Std                   0.0728788\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0289083\n",
      "exploration/env_infos/reward_ctrl Min                  -0.456071\n",
      "exploration/env_infos/final/height Mean                -0.172317\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.172317\n",
      "exploration/env_infos/final/height Min                 -0.172317\n",
      "exploration/env_infos/initial/height Mean              -0.0401487\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0401487\n",
      "exploration/env_infos/initial/height Min               -0.0401487\n",
      "exploration/env_infos/height Mean                      -0.123675\n",
      "exploration/env_infos/height Std                        0.0891145\n",
      "exploration/env_infos/height Max                        0.260928\n",
      "exploration/env_infos/height Min                       -0.428226\n",
      "exploration/env_infos/final/reward_angular Mean        -3.03476\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.03476\n",
      "exploration/env_infos/final/reward_angular Min         -3.03476\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.2378\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.2378\n",
      "exploration/env_infos/initial/reward_angular Min       -1.2378\n",
      "exploration/env_infos/reward_angular Mean              -0.0234431\n",
      "exploration/env_infos/reward_angular Std                1.33298\n",
      "exploration/env_infos/reward_angular Max                4.9918\n",
      "exploration/env_infos/reward_angular Min               -4.32244\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.0977848\n",
      "evaluation/Rewards Std                                  0.0648755\n",
      "evaluation/Rewards Max                                  1.80771\n",
      "evaluation/Rewards Min                                 -0.592728\n",
      "evaluation/Returns Mean                               -97.7848\n",
      "evaluation/Returns Std                                 42.6644\n",
      "evaluation/Returns Max                                -16.2407\n",
      "evaluation/Returns Min                               -172.777\n",
      "evaluation/Actions Mean                                 0.160948\n",
      "evaluation/Actions Std                                  0.330511\n",
      "evaluation/Actions Max                                  0.858458\n",
      "evaluation/Actions Min                                 -0.785438\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            -97.7848\n",
      "evaluation/env_infos/final/reward_run Mean             -3.8008e-08\n",
      "evaluation/env_infos/final/reward_run Std               9.19313e-08\n",
      "evaluation/env_infos/final/reward_run Max               1.58422e-07\n",
      "evaluation/env_infos/final/reward_run Min              -2.82958e-07\n",
      "evaluation/env_infos/initial/reward_run Mean            0.245275\n",
      "evaluation/env_infos/initial/reward_run Std             0.213627\n",
      "evaluation/env_infos/initial/reward_run Max             0.731284\n",
      "evaluation/env_infos/initial/reward_run Min            -0.177487\n",
      "evaluation/env_infos/reward_run Mean                   -0.000590917\n",
      "evaluation/env_infos/reward_run Std                     0.0300588\n",
      "evaluation/env_infos/reward_run Max                     0.828923\n",
      "evaluation/env_infos/reward_run Min                    -0.676886\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0811073\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0429223\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0124611\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.152158\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.10121\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0405962\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0359928\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.171882\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0810851\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0429271\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00625302\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.171882\n",
      "evaluation/env_infos/final/height Mean                 -0.148631\n",
      "evaluation/env_infos/final/height Std                   0.0284259\n",
      "evaluation/env_infos/final/height Max                  -0.111803\n",
      "evaluation/env_infos/final/height Min                  -0.198024\n",
      "evaluation/env_infos/initial/height Mean               -0.00903958\n",
      "evaluation/env_infos/initial/height Std                 0.0589706\n",
      "evaluation/env_infos/initial/height Max                 0.0724174\n",
      "evaluation/env_infos/initial/height Min                -0.0994608\n",
      "evaluation/env_infos/height Mean                       -0.14821\n",
      "evaluation/env_infos/height Std                         0.0291022\n",
      "evaluation/env_infos/height Max                         0.0724174\n",
      "evaluation/env_infos/height Min                        -0.198024\n",
      "evaluation/env_infos/final/reward_angular Mean          6.53124e-09\n",
      "evaluation/env_infos/final/reward_angular Std           1.30683e-07\n",
      "evaluation/env_infos/final/reward_angular Max           2.89385e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -2.58457e-07\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.552647\n",
      "evaluation/env_infos/initial/reward_angular Std         1.1249\n",
      "evaluation/env_infos/initial/reward_angular Max         1.9388\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.17776\n",
      "evaluation/env_infos/reward_angular Mean               -0.000697278\n",
      "evaluation/env_infos/reward_angular Std                 0.0658178\n",
      "evaluation/env_infos/reward_angular Max                 2.23983\n",
      "evaluation/env_infos/reward_angular Min                -2.17776\n",
      "time/data storing (s)                                   0.0147106\n",
      "time/evaluation sampling (s)                           22.6049\n",
      "time/exploration sampling (s)                           0.983394\n",
      "time/logging (s)                                        0.230288\n",
      "time/saving (s)                                         0.0275354\n",
      "time/training (s)                                       3.19001\n",
      "time/epoch (s)                                         27.0508\n",
      "time/total (s)                                        364.338\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:18:33.126065 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.451339\n",
      "trainer/QF2 Loss                                        0.503348\n",
      "trainer/Policy Loss                                    -5.98766\n",
      "trainer/Q1 Predictions Mean                             3.09816\n",
      "trainer/Q1 Predictions Std                              1.03829\n",
      "trainer/Q1 Predictions Max                              7.54948\n",
      "trainer/Q1 Predictions Min                              0.955767\n",
      "trainer/Q2 Predictions Mean                             3.06239\n",
      "trainer/Q2 Predictions Std                              1.04906\n",
      "trainer/Q2 Predictions Max                              7.65653\n",
      "trainer/Q2 Predictions Min                              0.779854\n",
      "trainer/Q Targets Mean                                  3.0425\n",
      "trainer/Q Targets Std                                   1.24077\n",
      "trainer/Q Targets Max                                   8.41873\n",
      "trainer/Q Targets Min                                   0.431848\n",
      "trainer/Log Pis Mean                                   -2.78466\n",
      "trainer/Log Pis Std                                     1.83149\n",
      "trainer/Log Pis Max                                     2.03232\n",
      "trainer/Log Pis Min                                    -9.35275\n",
      "trainer/Policy mu Mean                                  0.196138\n",
      "trainer/Policy mu Std                                   0.541507\n",
      "trainer/Policy mu Max                                   1.62383\n",
      "trainer/Policy mu Min                                  -1.55605\n",
      "trainer/Policy log std Mean                            -0.185443\n",
      "trainer/Policy log std Std                              0.0864801\n",
      "trainer/Policy log std Max                             -0.0325914\n",
      "trainer/Policy log std Min                             -0.490725\n",
      "trainer/Alpha                                           0.127455\n",
      "trainer/Alpha Loss                                    -18.0726\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                             9\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.230621\n",
      "exploration/Rewards Std                                 0.341383\n",
      "exploration/Rewards Max                                 0.811662\n",
      "exploration/Rewards Min                                -1.51832\n",
      "exploration/Returns Mean                             -230.621\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -230.621\n",
      "exploration/Returns Min                              -230.621\n",
      "exploration/Actions Mean                                0.327732\n",
      "exploration/Actions Std                                 0.56193\n",
      "exploration/Actions Max                                 0.999013\n",
      "exploration/Actions Min                                -0.992663\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -230.621\n",
      "exploration/env_infos/final/reward_run Mean            -0.370847\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.370847\n",
      "exploration/env_infos/final/reward_run Min             -0.370847\n",
      "exploration/env_infos/initial/reward_run Mean           0.300323\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.300323\n",
      "exploration/env_infos/initial/reward_run Min            0.300323\n",
      "exploration/env_infos/reward_run Mean                   0.0630603\n",
      "exploration/env_infos/reward_run Std                    0.483505\n",
      "exploration/env_infos/reward_run Max                    1.49807\n",
      "exploration/env_infos/reward_run Min                   -2.07949\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.406781\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.406781\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.406781\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.377623\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.377623\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.377623\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.253904\n",
      "exploration/env_infos/reward_ctrl Std                   0.0783239\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0121162\n",
      "exploration/env_infos/reward_ctrl Min                  -0.503543\n",
      "exploration/env_infos/final/height Mean                -0.118415\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.118415\n",
      "exploration/env_infos/final/height Min                 -0.118415\n",
      "exploration/env_infos/initial/height Mean              -0.0624725\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0624725\n",
      "exploration/env_infos/initial/height Min               -0.0624725\n",
      "exploration/env_infos/height Mean                      -0.134301\n",
      "exploration/env_infos/height Std                        0.0951631\n",
      "exploration/env_infos/height Max                        0.157657\n",
      "exploration/env_infos/height Min                       -0.402483\n",
      "exploration/env_infos/final/reward_angular Mean        -1.86588\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.86588\n",
      "exploration/env_infos/final/reward_angular Min         -1.86588\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.76617\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.76617\n",
      "exploration/env_infos/initial/reward_angular Min       -1.76617\n",
      "exploration/env_infos/reward_angular Mean              -0.00443372\n",
      "exploration/env_infos/reward_angular Std                1.41483\n",
      "exploration/env_infos/reward_angular Max                3.89102\n",
      "exploration/env_infos/reward_angular Min               -4.23239\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.124901\n",
      "evaluation/Rewards Std                                  0.156245\n",
      "evaluation/Rewards Max                                  2.06854\n",
      "evaluation/Rewards Min                                 -1.0637\n",
      "evaluation/Returns Mean                              -124.901\n",
      "evaluation/Returns Std                                 54.955\n",
      "evaluation/Returns Max                                -20.9348\n",
      "evaluation/Returns Min                               -219.642\n",
      "evaluation/Actions Mean                                 0.160779\n",
      "evaluation/Actions Std                                  0.445527\n",
      "evaluation/Actions Max                                  0.917659\n",
      "evaluation/Actions Min                                 -0.853058\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -124.901\n",
      "evaluation/env_infos/final/reward_run Mean              0.0583293\n",
      "evaluation/env_infos/final/reward_run Std               0.177625\n",
      "evaluation/env_infos/final/reward_run Max               0.436481\n",
      "evaluation/env_infos/final/reward_run Min              -0.237388\n",
      "evaluation/env_infos/initial/reward_run Mean            0.19463\n",
      "evaluation/env_infos/initial/reward_run Std             0.287087\n",
      "evaluation/env_infos/initial/reward_run Max             0.949636\n",
      "evaluation/env_infos/initial/reward_run Min            -0.47576\n",
      "evaluation/env_infos/reward_run Mean                    0.0193233\n",
      "evaluation/env_infos/reward_run Std                     0.16393\n",
      "evaluation/env_infos/reward_run Max                     1.26967\n",
      "evaluation/env_infos/reward_run Min                    -0.856483\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.134086\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0757737\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0130719\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.254308\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.14601\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0613348\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0266783\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.224195\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.134607\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0747054\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00528936\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.279954\n",
      "evaluation/env_infos/final/height Mean                 -0.167927\n",
      "evaluation/env_infos/final/height Std                   0.0411698\n",
      "evaluation/env_infos/final/height Max                  -0.106391\n",
      "evaluation/env_infos/final/height Min                  -0.306671\n",
      "evaluation/env_infos/initial/height Mean               -0.00734644\n",
      "evaluation/env_infos/initial/height Std                 0.0493513\n",
      "evaluation/env_infos/initial/height Max                 0.0592574\n",
      "evaluation/env_infos/initial/height Min                -0.0992638\n",
      "evaluation/env_infos/height Mean                       -0.167477\n",
      "evaluation/env_infos/height Std                         0.0423271\n",
      "evaluation/env_infos/height Max                         0.0592574\n",
      "evaluation/env_infos/height Min                        -0.437513\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0648711\n",
      "evaluation/env_infos/final/reward_angular Std           0.305895\n",
      "evaluation/env_infos/final/reward_angular Max           0.717203\n",
      "evaluation/env_infos/final/reward_angular Min          -0.933781\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.347665\n",
      "evaluation/env_infos/initial/reward_angular Std         1.48626\n",
      "evaluation/env_infos/initial/reward_angular Max         2.59506\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.16287\n",
      "evaluation/env_infos/reward_angular Mean                0.000283278\n",
      "evaluation/env_infos/reward_angular Std                 0.271575\n",
      "evaluation/env_infos/reward_angular Max                 2.59506\n",
      "evaluation/env_infos/reward_angular Min                -2.27542\n",
      "time/data storing (s)                                   0.0143296\n",
      "time/evaluation sampling (s)                           20.8103\n",
      "time/exploration sampling (s)                           0.970153\n",
      "time/logging (s)                                        0.231922\n",
      "time/saving (s)                                         0.0266295\n",
      "time/training (s)                                       4.05074\n",
      "time/epoch (s)                                         26.1041\n",
      "time/total (s)                                        390.593\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:18:59.834028 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.377065\n",
      "trainer/QF2 Loss                                        0.405138\n",
      "trainer/Policy Loss                                    -5.88476\n",
      "trainer/Q1 Predictions Mean                             2.92187\n",
      "trainer/Q1 Predictions Std                              1.01916\n",
      "trainer/Q1 Predictions Max                              7.56743\n",
      "trainer/Q1 Predictions Min                              1.26406\n",
      "trainer/Q2 Predictions Mean                             2.84127\n",
      "trainer/Q2 Predictions Std                              0.954657\n",
      "trainer/Q2 Predictions Max                              7.46527\n",
      "trainer/Q2 Predictions Min                              1.21644\n",
      "trainer/Q Targets Mean                                  2.92643\n",
      "trainer/Q Targets Std                                   1.15257\n",
      "trainer/Q Targets Max                                   8.60626\n",
      "trainer/Q Targets Min                                   0.261211\n",
      "trainer/Log Pis Mean                                   -2.90367\n",
      "trainer/Log Pis Std                                     1.70258\n",
      "trainer/Log Pis Max                                     3.65764\n",
      "trainer/Log Pis Min                                    -6.62345\n",
      "trainer/Policy mu Mean                                  0.230101\n",
      "trainer/Policy mu Std                                   0.515064\n",
      "trainer/Policy mu Max                                   1.88643\n",
      "trainer/Policy mu Min                                  -1.63196\n",
      "trainer/Policy log std Mean                            -0.184958\n",
      "trainer/Policy log std Std                              0.0846405\n",
      "trainer/Policy log std Max                             -0.0253537\n",
      "trainer/Policy log std Min                             -0.517545\n",
      "trainer/Alpha                                           0.0968658\n",
      "trainer/Alpha Loss                                    -20.7607\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            10\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.285981\n",
      "exploration/Rewards Std                                 0.544896\n",
      "exploration/Rewards Max                                 1.40736\n",
      "exploration/Rewards Min                                -2.42217\n",
      "exploration/Returns Mean                             -285.981\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -285.981\n",
      "exploration/Returns Min                              -285.981\n",
      "exploration/Actions Mean                               -0.037726\n",
      "exploration/Actions Std                                 0.606197\n",
      "exploration/Actions Max                                 0.999234\n",
      "exploration/Actions Min                                -0.995536\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -285.981\n",
      "exploration/env_infos/final/reward_run Mean            -0.666322\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.666322\n",
      "exploration/env_infos/final/reward_run Min             -0.666322\n",
      "exploration/env_infos/initial/reward_run Mean           0.707632\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.707632\n",
      "exploration/env_infos/initial/reward_run Min            0.707632\n",
      "exploration/env_infos/reward_run Mean                  -0.0149979\n",
      "exploration/env_infos/reward_run Std                    0.592942\n",
      "exploration/env_infos/reward_run Max                    2.26828\n",
      "exploration/env_infos/reward_run Min                   -1.89566\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.193219\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.193219\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.193219\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.372655\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.372655\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.372655\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.221339\n",
      "exploration/env_infos/reward_ctrl Std                   0.0751496\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0469962\n",
      "exploration/env_infos/reward_ctrl Min                  -0.474237\n",
      "exploration/env_infos/final/height Mean                -0.567221\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.567221\n",
      "exploration/env_infos/final/height Min                 -0.567221\n",
      "exploration/env_infos/initial/height Mean               0.0238043\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0238043\n",
      "exploration/env_infos/initial/height Min                0.0238043\n",
      "exploration/env_infos/height Mean                      -0.496089\n",
      "exploration/env_infos/height Std                        0.142904\n",
      "exploration/env_infos/height Max                        0.183114\n",
      "exploration/env_infos/height Min                       -0.591288\n",
      "exploration/env_infos/final/reward_angular Mean         0.466207\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.466207\n",
      "exploration/env_infos/final/reward_angular Min          0.466207\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.186274\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.186274\n",
      "exploration/env_infos/initial/reward_angular Min       -0.186274\n",
      "exploration/env_infos/reward_angular Mean               0.0555711\n",
      "exploration/env_infos/reward_angular Std                1.22856\n",
      "exploration/env_infos/reward_angular Max                5.14276\n",
      "exploration/env_infos/reward_angular Min               -5.31121\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.135189\n",
      "evaluation/Rewards Std                                  0.270829\n",
      "evaluation/Rewards Max                                  4.36142\n",
      "evaluation/Rewards Min                                 -3.14853\n",
      "evaluation/Returns Mean                              -135.189\n",
      "evaluation/Returns Std                                 83.1678\n",
      "evaluation/Returns Max                                 26.5906\n",
      "evaluation/Returns Min                               -304.914\n",
      "evaluation/Actions Mean                                 0.154755\n",
      "evaluation/Actions Std                                  0.424748\n",
      "evaluation/Actions Max                                  0.942934\n",
      "evaluation/Actions Min                                 -0.963661\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -135.189\n",
      "evaluation/env_infos/final/reward_run Mean              0.0159784\n",
      "evaluation/env_infos/final/reward_run Std               0.0717751\n",
      "evaluation/env_infos/final/reward_run Max               0.300904\n",
      "evaluation/env_infos/final/reward_run Min              -0.151935\n",
      "evaluation/env_infos/initial/reward_run Mean            0.155816\n",
      "evaluation/env_infos/initial/reward_run Std             0.357583\n",
      "evaluation/env_infos/initial/reward_run Max             0.926164\n",
      "evaluation/env_infos/initial/reward_run Min            -0.611774\n",
      "evaluation/env_infos/reward_run Mean                   -0.00120579\n",
      "evaluation/env_infos/reward_run Std                     0.226064\n",
      "evaluation/env_infos/reward_run Max                     1.90924\n",
      "evaluation/env_infos/reward_run Min                    -2.91761\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.116708\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0830983\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0160409\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.269732\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.126874\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0586308\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0333604\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.226241\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.122616\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0809913\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00960417\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.376958\n",
      "evaluation/env_infos/final/height Mean                 -0.267665\n",
      "evaluation/env_infos/final/height Std                   0.18314\n",
      "evaluation/env_infos/final/height Max                   0.0736083\n",
      "evaluation/env_infos/final/height Min                  -0.57728\n",
      "evaluation/env_infos/initial/height Mean                0.00390358\n",
      "evaluation/env_infos/initial/height Std                 0.0518506\n",
      "evaluation/env_infos/initial/height Max                 0.0907194\n",
      "evaluation/env_infos/initial/height Min                -0.0768416\n",
      "evaluation/env_infos/height Mean                       -0.237218\n",
      "evaluation/env_infos/height Std                         0.172707\n",
      "evaluation/env_infos/height Max                         0.338605\n",
      "evaluation/env_infos/height Min                        -0.587901\n",
      "evaluation/env_infos/final/reward_angular Mean          0.00389942\n",
      "evaluation/env_infos/final/reward_angular Std           0.109402\n",
      "evaluation/env_infos/final/reward_angular Max           0.455544\n",
      "evaluation/env_infos/final/reward_angular Min          -0.266313\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.3328\n",
      "evaluation/env_infos/initial/reward_angular Std         1.203\n",
      "evaluation/env_infos/initial/reward_angular Max         2.0995\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.14708\n",
      "evaluation/env_infos/reward_angular Mean                0.0176629\n",
      "evaluation/env_infos/reward_angular Std                 0.351403\n",
      "evaluation/env_infos/reward_angular Max                 5.96678\n",
      "evaluation/env_infos/reward_angular Min                -3.42324\n",
      "time/data storing (s)                                   0.0148475\n",
      "time/evaluation sampling (s)                           21.3395\n",
      "time/exploration sampling (s)                           0.977487\n",
      "time/logging (s)                                        0.228776\n",
      "time/saving (s)                                         0.0281924\n",
      "time/training (s)                                       3.95404\n",
      "time/epoch (s)                                         26.5428\n",
      "time/total (s)                                        417.297\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:19:28.428970 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.386013\n",
      "trainer/QF2 Loss                                        0.574176\n",
      "trainer/Policy Loss                                    -5.29118\n",
      "trainer/Q1 Predictions Mean                             2.79254\n",
      "trainer/Q1 Predictions Std                              1.26594\n",
      "trainer/Q1 Predictions Max                             13.0006\n",
      "trainer/Q1 Predictions Min                              0.144802\n",
      "trainer/Q2 Predictions Mean                             2.82888\n",
      "trainer/Q2 Predictions Std                              1.18594\n",
      "trainer/Q2 Predictions Max                             11.7294\n",
      "trainer/Q2 Predictions Min                              0.537096\n",
      "trainer/Q Targets Mean                                  2.7079\n",
      "trainer/Q Targets Std                                   1.39686\n",
      "trainer/Q Targets Max                                  12.4416\n",
      "trainer/Q Targets Min                                  -1.84202\n",
      "trainer/Log Pis Mean                                   -2.29666\n",
      "trainer/Log Pis Std                                     2.02507\n",
      "trainer/Log Pis Max                                     5.11311\n",
      "trainer/Log Pis Min                                    -7.4649\n",
      "trainer/Policy mu Mean                                  0.205688\n",
      "trainer/Policy mu Std                                   0.623642\n",
      "trainer/Policy mu Max                                   2.36441\n",
      "trainer/Policy mu Min                                  -2.26218\n",
      "trainer/Policy log std Mean                            -0.233102\n",
      "trainer/Policy log std Std                              0.101188\n",
      "trainer/Policy log std Max                              0.0337113\n",
      "trainer/Policy log std Min                             -0.634542\n",
      "trainer/Alpha                                           0.0739779\n",
      "trainer/Alpha Loss                                    -21.583\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            11\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.157915\n",
      "exploration/Rewards Std                                 0.473722\n",
      "exploration/Rewards Max                                 1.74637\n",
      "exploration/Rewards Min                                -2.20605\n",
      "exploration/Returns Mean                             -157.915\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -157.915\n",
      "exploration/Returns Min                              -157.915\n",
      "exploration/Actions Mean                               -0.0284953\n",
      "exploration/Actions Std                                 0.601534\n",
      "exploration/Actions Max                                 0.997332\n",
      "exploration/Actions Min                                -0.996859\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -157.915\n",
      "exploration/env_infos/final/reward_run Mean            -0.333935\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.333935\n",
      "exploration/env_infos/final/reward_run Min             -0.333935\n",
      "exploration/env_infos/initial/reward_run Mean           0.535454\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.535454\n",
      "exploration/env_infos/initial/reward_run Min            0.535454\n",
      "exploration/env_infos/reward_run Mean                  -0.0709645\n",
      "exploration/env_infos/reward_run Std                    0.608598\n",
      "exploration/env_infos/reward_run Max                    1.87882\n",
      "exploration/env_infos/reward_run Min                   -1.819\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.11666\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.11666\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.11666\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.225454\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.225454\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.225454\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.217593\n",
      "exploration/env_infos/reward_ctrl Std                   0.0709249\n",
      "exploration/env_infos/reward_ctrl Max                  -0.027711\n",
      "exploration/env_infos/reward_ctrl Min                  -0.429757\n",
      "exploration/env_infos/final/height Mean                -0.563895\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.563895\n",
      "exploration/env_infos/final/height Min                 -0.563895\n",
      "exploration/env_infos/initial/height Mean               0.0471402\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0471402\n",
      "exploration/env_infos/initial/height Min                0.0471402\n",
      "exploration/env_infos/height Mean                      -0.436022\n",
      "exploration/env_infos/height Std                        0.204024\n",
      "exploration/env_infos/height Max                        0.393205\n",
      "exploration/env_infos/height Min                       -0.588335\n",
      "exploration/env_infos/final/reward_angular Mean        -0.765539\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.765539\n",
      "exploration/env_infos/final/reward_angular Min         -0.765539\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.60102\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.60102\n",
      "exploration/env_infos/initial/reward_angular Min       -1.60102\n",
      "exploration/env_infos/reward_angular Mean               0.0971609\n",
      "exploration/env_infos/reward_angular Std                1.26086\n",
      "exploration/env_infos/reward_angular Max                5.31938\n",
      "exploration/env_infos/reward_angular Min               -4.65808\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.141124\n",
      "evaluation/Rewards Std                                  0.204256\n",
      "evaluation/Rewards Max                                  2.24656\n",
      "evaluation/Rewards Min                                 -1.92284\n",
      "evaluation/Returns Mean                              -141.124\n",
      "evaluation/Returns Std                                 77.0967\n",
      "evaluation/Returns Max                                  1.89161\n",
      "evaluation/Returns Min                               -265.715\n",
      "evaluation/Actions Mean                                 0.0851132\n",
      "evaluation/Actions Std                                  0.520197\n",
      "evaluation/Actions Max                                  0.96391\n",
      "evaluation/Actions Min                                 -0.987116\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -141.124\n",
      "evaluation/env_infos/final/reward_run Mean              0.0562636\n",
      "evaluation/env_infos/final/reward_run Std               0.205526\n",
      "evaluation/env_infos/final/reward_run Max               0.830593\n",
      "evaluation/env_infos/final/reward_run Min              -0.190894\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.0716112\n",
      "evaluation/env_infos/initial/reward_run Std             0.520331\n",
      "evaluation/env_infos/initial/reward_run Max             0.885982\n",
      "evaluation/env_infos/initial/reward_run Min            -0.785071\n",
      "evaluation/env_infos/reward_run Mean                    0.0755159\n",
      "evaluation/env_infos/reward_run Std                     0.216427\n",
      "evaluation/env_infos/reward_run Max                     1.71205\n",
      "evaluation/env_infos/reward_run Min                    -2.21278\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.167333\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0815668\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0497859\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.315662\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.185574\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0839145\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0485638\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.403689\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.16671\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0841376\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.02007\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.414686\n",
      "evaluation/env_infos/final/height Mean                 -0.201816\n",
      "evaluation/env_infos/final/height Std                   0.035388\n",
      "evaluation/env_infos/final/height Max                  -0.132858\n",
      "evaluation/env_infos/final/height Min                  -0.288886\n",
      "evaluation/env_infos/initial/height Mean               -0.0266087\n",
      "evaluation/env_infos/initial/height Std                 0.0644792\n",
      "evaluation/env_infos/initial/height Max                 0.0862699\n",
      "evaluation/env_infos/initial/height Min                -0.10482\n",
      "evaluation/env_infos/height Mean                       -0.198535\n",
      "evaluation/env_infos/height Std                         0.0362548\n",
      "evaluation/env_infos/height Max                         0.162337\n",
      "evaluation/env_infos/height Min                        -0.365237\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0145761\n",
      "evaluation/env_infos/final/reward_angular Std           0.212693\n",
      "evaluation/env_infos/final/reward_angular Max           0.651028\n",
      "evaluation/env_infos/final/reward_angular Min          -0.596651\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.339671\n",
      "evaluation/env_infos/initial/reward_angular Std         1.2978\n",
      "evaluation/env_infos/initial/reward_angular Max         1.73791\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.40332\n",
      "evaluation/env_infos/reward_angular Mean                0.000681869\n",
      "evaluation/env_infos/reward_angular Std                 0.285318\n",
      "evaluation/env_infos/reward_angular Max                 2.7509\n",
      "evaluation/env_infos/reward_angular Min                -2.40332\n",
      "time/data storing (s)                                   0.0154845\n",
      "time/evaluation sampling (s)                           22.7791\n",
      "time/exploration sampling (s)                           1.06363\n",
      "time/logging (s)                                        0.242135\n",
      "time/saving (s)                                         0.0301212\n",
      "time/training (s)                                       4.31119\n",
      "time/epoch (s)                                         28.4416\n",
      "time/total (s)                                        445.905\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:19:56.256777 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.434224\n",
      "trainer/QF2 Loss                                        0.490606\n",
      "trainer/Policy Loss                                    -4.66643\n",
      "trainer/Q1 Predictions Mean                             2.72614\n",
      "trainer/Q1 Predictions Std                              1.07682\n",
      "trainer/Q1 Predictions Max                              7.05789\n",
      "trainer/Q1 Predictions Min                              0.779706\n",
      "trainer/Q2 Predictions Mean                             2.54704\n",
      "trainer/Q2 Predictions Std                              1.07993\n",
      "trainer/Q2 Predictions Max                              7.72341\n",
      "trainer/Q2 Predictions Min                              0.562059\n",
      "trainer/Q Targets Mean                                  2.61062\n",
      "trainer/Q Targets Std                                   1.26934\n",
      "trainer/Q Targets Max                                   8.01998\n",
      "trainer/Q Targets Min                                  -0.734938\n",
      "trainer/Log Pis Mean                                   -1.95786\n",
      "trainer/Log Pis Std                                     2.20121\n",
      "trainer/Log Pis Max                                     5.25374\n",
      "trainer/Log Pis Min                                    -7.43149\n",
      "trainer/Policy mu Mean                                  0.0931412\n",
      "trainer/Policy mu Std                                   0.704763\n",
      "trainer/Policy mu Max                                   2.19217\n",
      "trainer/Policy mu Min                                  -2.52122\n",
      "trainer/Policy log std Mean                            -0.244154\n",
      "trainer/Policy log std Std                              0.161837\n",
      "trainer/Policy log std Max                              0.0949716\n",
      "trainer/Policy log std Min                             -0.895471\n",
      "trainer/Alpha                                           0.0569751\n",
      "trainer/Alpha Loss                                    -22.7802\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            12\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.262339\n",
      "exploration/Rewards Std                                 0.57635\n",
      "exploration/Rewards Max                                 1.72583\n",
      "exploration/Rewards Min                                -2.32808\n",
      "exploration/Returns Mean                             -262.339\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -262.339\n",
      "exploration/Returns Min                              -262.339\n",
      "exploration/Actions Mean                                0.0493383\n",
      "exploration/Actions Std                                 0.648566\n",
      "exploration/Actions Max                                 0.99833\n",
      "exploration/Actions Min                                -0.998779\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -262.339\n",
      "exploration/env_infos/final/reward_run Mean            -0.227637\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.227637\n",
      "exploration/env_infos/final/reward_run Min             -0.227637\n",
      "exploration/env_infos/initial/reward_run Mean          -0.232861\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.232861\n",
      "exploration/env_infos/initial/reward_run Min           -0.232861\n",
      "exploration/env_infos/reward_run Mean                   0.0836248\n",
      "exploration/env_infos/reward_run Std                    0.562768\n",
      "exploration/env_infos/reward_run Max                    1.89891\n",
      "exploration/env_infos/reward_run Min                   -2.01144\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.384163\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.384163\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.384163\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.314688\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.314688\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.314688\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.253843\n",
      "exploration/env_infos/reward_ctrl Std                   0.071571\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0524361\n",
      "exploration/env_infos/reward_ctrl Min                  -0.480987\n",
      "exploration/env_infos/final/height Mean                -0.25602\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.25602\n",
      "exploration/env_infos/final/height Min                 -0.25602\n",
      "exploration/env_infos/initial/height Mean               0.0551918\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0551918\n",
      "exploration/env_infos/initial/height Min                0.0551918\n",
      "exploration/env_infos/height Mean                      -0.158169\n",
      "exploration/env_infos/height Std                        0.0590847\n",
      "exploration/env_infos/height Max                        0.0686025\n",
      "exploration/env_infos/height Min                       -0.363467\n",
      "exploration/env_infos/final/reward_angular Mean         0.941224\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.941224\n",
      "exploration/env_infos/final/reward_angular Min          0.941224\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.589336\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.589336\n",
      "exploration/env_infos/initial/reward_angular Min       -0.589336\n",
      "exploration/env_infos/reward_angular Mean              -0.0174257\n",
      "exploration/env_infos/reward_angular Std                1.32549\n",
      "exploration/env_infos/reward_angular Max                4.96551\n",
      "exploration/env_infos/reward_angular Min               -4.94909\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.183721\n",
      "evaluation/Rewards Std                                  0.51811\n",
      "evaluation/Rewards Max                                  5.53848\n",
      "evaluation/Rewards Min                                 -4.25655\n",
      "evaluation/Returns Mean                              -183.721\n",
      "evaluation/Returns Std                                 96.1347\n",
      "evaluation/Returns Max                                -39.9172\n",
      "evaluation/Returns Min                               -393.761\n",
      "evaluation/Actions Mean                                -0.0322915\n",
      "evaluation/Actions Std                                  0.536434\n",
      "evaluation/Actions Max                                  0.992664\n",
      "evaluation/Actions Min                                 -0.99962\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -183.721\n",
      "evaluation/env_infos/final/reward_run Mean              0.104478\n",
      "evaluation/env_infos/final/reward_run Std               0.243984\n",
      "evaluation/env_infos/final/reward_run Max               0.763772\n",
      "evaluation/env_infos/final/reward_run Min              -0.246791\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.284265\n",
      "evaluation/env_infos/initial/reward_run Std             0.478345\n",
      "evaluation/env_infos/initial/reward_run Max             0.578983\n",
      "evaluation/env_infos/initial/reward_run Min            -0.940926\n",
      "evaluation/env_infos/reward_run Mean                   -0.0263208\n",
      "evaluation/env_infos/reward_run Std                     0.369145\n",
      "evaluation/env_infos/reward_run Max                     3.10689\n",
      "evaluation/env_infos/reward_run Min                    -4.11139\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.173128\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0733356\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0255043\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.336015\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.192305\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0829217\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0338139\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.403381\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.173282\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0718094\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0155041\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.558365\n",
      "evaluation/env_infos/final/height Mean                 -0.287069\n",
      "evaluation/env_infos/final/height Std                   0.165886\n",
      "evaluation/env_infos/final/height Max                   0.024466\n",
      "evaluation/env_infos/final/height Min                  -0.563103\n",
      "evaluation/env_infos/initial/height Mean               -0.0306006\n",
      "evaluation/env_infos/initial/height Std                 0.0583417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/height Max                 0.0876519\n",
      "evaluation/env_infos/initial/height Min                -0.115929\n",
      "evaluation/env_infos/height Mean                       -0.276849\n",
      "evaluation/env_infos/height Std                         0.161107\n",
      "evaluation/env_infos/height Max                         0.371524\n",
      "evaluation/env_infos/height Min                        -0.586609\n",
      "evaluation/env_infos/final/reward_angular Mean          0.127842\n",
      "evaluation/env_infos/final/reward_angular Std           0.674574\n",
      "evaluation/env_infos/final/reward_angular Max           2.59135\n",
      "evaluation/env_infos/final/reward_angular Min          -1.58084\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.215258\n",
      "evaluation/env_infos/initial/reward_angular Std         1.79047\n",
      "evaluation/env_infos/initial/reward_angular Max         3.13094\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.59308\n",
      "evaluation/env_infos/reward_angular Mean                0.0365537\n",
      "evaluation/env_infos/reward_angular Std                 0.803639\n",
      "evaluation/env_infos/reward_angular Max                 7.11413\n",
      "evaluation/env_infos/reward_angular Min                -4.47933\n",
      "time/data storing (s)                                   0.0157163\n",
      "time/evaluation sampling (s)                           22.5079\n",
      "time/exploration sampling (s)                           1.04947\n",
      "time/logging (s)                                        0.240257\n",
      "time/saving (s)                                         0.028699\n",
      "time/training (s)                                       3.80339\n",
      "time/epoch (s)                                         27.6454\n",
      "time/total (s)                                        473.73\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  --------------\n",
      "2021-05-25 19:20:25.772751 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.540272\n",
      "trainer/QF2 Loss                                        0.570183\n",
      "trainer/Policy Loss                                    -4.11501\n",
      "trainer/Q1 Predictions Mean                             2.55078\n",
      "trainer/Q1 Predictions Std                              1.1888\n",
      "trainer/Q1 Predictions Max                              6.94137\n",
      "trainer/Q1 Predictions Min                              0.496209\n",
      "trainer/Q2 Predictions Mean                             2.49683\n",
      "trainer/Q2 Predictions Std                              1.12898\n",
      "trainer/Q2 Predictions Max                              6.41004\n",
      "trainer/Q2 Predictions Min                              0.703681\n",
      "trainer/Q Targets Mean                                  2.55774\n",
      "trainer/Q Targets Std                                   1.36032\n",
      "trainer/Q Targets Max                                   7.54775\n",
      "trainer/Q Targets Min                                  -0.484931\n",
      "trainer/Log Pis Mean                                   -1.39596\n",
      "trainer/Log Pis Std                                     2.81949\n",
      "trainer/Log Pis Max                                     8.75566\n",
      "trainer/Log Pis Min                                    -6.81495\n",
      "trainer/Policy mu Mean                                  0.145636\n",
      "trainer/Policy mu Std                                   0.782579\n",
      "trainer/Policy mu Max                                   2.48259\n",
      "trainer/Policy mu Min                                  -2.71378\n",
      "trainer/Policy log std Mean                            -0.289593\n",
      "trainer/Policy log std Std                              0.156423\n",
      "trainer/Policy log std Max                              0.0483923\n",
      "trainer/Policy log std Min                             -0.804241\n",
      "trainer/Alpha                                           0.0445151\n",
      "trainer/Alpha Loss                                    -22.9981\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            13\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.201856\n",
      "exploration/Rewards Std                                 0.531948\n",
      "exploration/Rewards Max                                 1.73347\n",
      "exploration/Rewards Min                                -2.14073\n",
      "exploration/Returns Mean                             -201.856\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -201.856\n",
      "exploration/Returns Min                              -201.856\n",
      "exploration/Actions Mean                                0.0333053\n",
      "exploration/Actions Std                                 0.616454\n",
      "exploration/Actions Max                                 0.997825\n",
      "exploration/Actions Min                                -0.997611\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -201.856\n",
      "exploration/env_infos/final/reward_run Mean            -0.785903\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.785903\n",
      "exploration/env_infos/final/reward_run Min             -0.785903\n",
      "exploration/env_infos/initial/reward_run Mean           0.456464\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.456464\n",
      "exploration/env_infos/initial/reward_run Min            0.456464\n",
      "exploration/env_infos/reward_run Mean                  -0.0699646\n",
      "exploration/env_infos/reward_run Std                    0.643457\n",
      "exploration/env_infos/reward_run Max                    1.78564\n",
      "exploration/env_infos/reward_run Min                   -1.91461\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270904\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270904\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270904\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.198007\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.198007\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.198007\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.228675\n",
      "exploration/env_infos/reward_ctrl Std                   0.0772856\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0167956\n",
      "exploration/env_infos/reward_ctrl Min                  -0.480182\n",
      "exploration/env_infos/final/height Mean                -0.51522\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.51522\n",
      "exploration/env_infos/final/height Min                 -0.51522\n",
      "exploration/env_infos/initial/height Mean               0.00810421\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00810421\n",
      "exploration/env_infos/initial/height Min                0.00810421\n",
      "exploration/env_infos/height Mean                      -0.417301\n",
      "exploration/env_infos/height Std                        0.217285\n",
      "exploration/env_infos/height Max                        0.221537\n",
      "exploration/env_infos/height Min                       -0.58544\n",
      "exploration/env_infos/final/reward_angular Mean        -1.37107\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.37107\n",
      "exploration/env_infos/final/reward_angular Min         -1.37107\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.138404\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.138404\n",
      "exploration/env_infos/initial/reward_angular Min       -0.138404\n",
      "exploration/env_infos/reward_angular Mean               0.090957\n",
      "exploration/env_infos/reward_angular Std                1.38638\n",
      "exploration/env_infos/reward_angular Max                5.47576\n",
      "exploration/env_infos/reward_angular Min               -4.5718\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.154011\n",
      "evaluation/Rewards Std                                  0.377915\n",
      "evaluation/Rewards Max                                  3.83682\n",
      "evaluation/Rewards Min                                 -3.28083\n",
      "evaluation/Returns Mean                              -154.011\n",
      "evaluation/Returns Std                                 87.4642\n",
      "evaluation/Returns Max                                 83.8816\n",
      "evaluation/Returns Min                               -276.825\n",
      "evaluation/Actions Mean                                -0.0339071\n",
      "evaluation/Actions Std                                  0.613695\n",
      "evaluation/Actions Max                                  0.984868\n",
      "evaluation/Actions Min                                 -0.998897\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -154.011\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0460162\n",
      "evaluation/env_infos/final/reward_run Std               0.203968\n",
      "evaluation/env_infos/final/reward_run Max               0.229019\n",
      "evaluation/env_infos/final/reward_run Min              -0.825524\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.263051\n",
      "evaluation/env_infos/initial/reward_run Std             0.417334\n",
      "evaluation/env_infos/initial/reward_run Max             0.70053\n",
      "evaluation/env_infos/initial/reward_run Min            -0.920324\n",
      "evaluation/env_infos/reward_run Mean                   -0.00744536\n",
      "evaluation/env_infos/reward_run Std                     0.353127\n",
      "evaluation/env_infos/reward_run Max                     2.12512\n",
      "evaluation/env_infos/reward_run Min                    -2.58241\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.222902\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101371\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0657373\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.394595\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.240578\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0947191\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.043655\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.365473\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.226663\n",
      "evaluation/env_infos/reward_ctrl Std                    0.102711\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00723593\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.418419\n",
      "evaluation/env_infos/final/height Mean                 -0.175234\n",
      "evaluation/env_infos/final/height Std                   0.100329\n",
      "evaluation/env_infos/final/height Max                   0.00515103\n",
      "evaluation/env_infos/final/height Min                  -0.569649\n",
      "evaluation/env_infos/initial/height Mean               -0.0201145\n",
      "evaluation/env_infos/initial/height Std                 0.0520394\n",
      "evaluation/env_infos/initial/height Max                 0.0706987\n",
      "evaluation/env_infos/initial/height Min                -0.116903\n",
      "evaluation/env_infos/height Mean                       -0.170297\n",
      "evaluation/env_infos/height Std                         0.0971242\n",
      "evaluation/env_infos/height Max                         0.306364\n",
      "evaluation/env_infos/height Min                        -0.578836\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.154712\n",
      "evaluation/env_infos/final/reward_angular Std           0.538088\n",
      "evaluation/env_infos/final/reward_angular Max           0.824241\n",
      "evaluation/env_infos/final/reward_angular Min          -1.92861\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.243579\n",
      "evaluation/env_infos/initial/reward_angular Std         1.5316\n",
      "evaluation/env_infos/initial/reward_angular Max         2.38098\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.30728\n",
      "evaluation/env_infos/reward_angular Mean                0.00906995\n",
      "evaluation/env_infos/reward_angular Std                 0.555272\n",
      "evaluation/env_infos/reward_angular Max                 5.62057\n",
      "evaluation/env_infos/reward_angular Min                -3.37209\n",
      "time/data storing (s)                                   0.0154391\n",
      "time/evaluation sampling (s)                           22.4879\n",
      "time/exploration sampling (s)                           1.13088\n",
      "time/logging (s)                                        0.241016\n",
      "time/saving (s)                                         0.0302489\n",
      "time/training (s)                                       5.4259\n",
      "time/epoch (s)                                         29.3314\n",
      "time/total (s)                                        503.246\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:20:56.378608 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.553779\n",
      "trainer/QF2 Loss                                        0.779972\n",
      "trainer/Policy Loss                                    -3.37025\n",
      "trainer/Q1 Predictions Mean                             2.62546\n",
      "trainer/Q1 Predictions Std                              1.3011\n",
      "trainer/Q1 Predictions Max                              9.1001\n",
      "trainer/Q1 Predictions Min                              0.450132\n",
      "trainer/Q2 Predictions Mean                             2.66244\n",
      "trainer/Q2 Predictions Std                              1.36697\n",
      "trainer/Q2 Predictions Max                             10.6998\n",
      "trainer/Q2 Predictions Min                              0.540443\n",
      "trainer/Q Targets Mean                                  2.59799\n",
      "trainer/Q Targets Std                                   1.54328\n",
      "trainer/Q Targets Max                                  10.791\n",
      "trainer/Q Targets Min                                  -0.438106\n",
      "trainer/Log Pis Mean                                   -0.570367\n",
      "trainer/Log Pis Std                                     3.64631\n",
      "trainer/Log Pis Max                                    19.1413\n",
      "trainer/Log Pis Min                                    -7.55234\n",
      "trainer/Policy mu Mean                                 -0.0646952\n",
      "trainer/Policy mu Std                                   0.919327\n",
      "trainer/Policy mu Max                                   3.6711\n",
      "trainer/Policy mu Min                                  -3.39827\n",
      "trainer/Policy log std Mean                            -0.348908\n",
      "trainer/Policy log std Std                              0.191788\n",
      "trainer/Policy log std Max                              0.114611\n",
      "trainer/Policy log std Min                             -1.05239\n",
      "trainer/Alpha                                           0.0353746\n",
      "trainer/Alpha Loss                                    -21.9418\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            14\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0393829\n",
      "exploration/Rewards Std                                 0.660115\n",
      "exploration/Rewards Max                                 2.40069\n",
      "exploration/Rewards Min                                -2.58294\n",
      "exploration/Returns Mean                              -39.3829\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -39.3829\n",
      "exploration/Returns Min                               -39.3829\n",
      "exploration/Actions Mean                                0.013849\n",
      "exploration/Actions Std                                 0.656281\n",
      "exploration/Actions Max                                 0.999159\n",
      "exploration/Actions Min                                -0.998551\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -39.3829\n",
      "exploration/env_infos/final/reward_run Mean             0.297146\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.297146\n",
      "exploration/env_infos/final/reward_run Min              0.297146\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0277567\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0277567\n",
      "exploration/env_infos/initial/reward_run Min           -0.0277567\n",
      "exploration/env_infos/reward_run Mean                   0.038042\n",
      "exploration/env_infos/reward_run Std                    0.404763\n",
      "exploration/env_infos/reward_run Max                    1.23824\n",
      "exploration/env_infos/reward_run Min                   -1.59259\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.148106\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.148106\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.148106\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.225451\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.225451\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.225451\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.258538\n",
      "exploration/env_infos/reward_ctrl Std                   0.0804032\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0335518\n",
      "exploration/env_infos/reward_ctrl Min                  -0.508779\n",
      "exploration/env_infos/final/height Mean                -0.222699\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.222699\n",
      "exploration/env_infos/final/height Min                 -0.222699\n",
      "exploration/env_infos/initial/height Mean              -0.0111804\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0111804\n",
      "exploration/env_infos/initial/height Min               -0.0111804\n",
      "exploration/env_infos/height Mean                      -0.257868\n",
      "exploration/env_infos/height Std                        0.0981592\n",
      "exploration/env_infos/height Max                        0.100598\n",
      "exploration/env_infos/height Min                       -0.446158\n",
      "exploration/env_infos/final/reward_angular Mean        -1.44317\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.44317\n",
      "exploration/env_infos/final/reward_angular Min         -1.44317\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.289236\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.289236\n",
      "exploration/env_infos/initial/reward_angular Min       -0.289236\n",
      "exploration/env_infos/reward_angular Mean              -0.00196767\n",
      "exploration/env_infos/reward_angular Std                1.22846\n",
      "exploration/env_infos/reward_angular Max                4.50561\n",
      "exploration/env_infos/reward_angular Min               -4.49287\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.158728\n",
      "evaluation/Rewards Std                                  0.449805\n",
      "evaluation/Rewards Max                                  5.63258\n",
      "evaluation/Rewards Min                                 -4.53653\n",
      "evaluation/Returns Mean                              -158.728\n",
      "evaluation/Returns Std                                 78.2227\n",
      "evaluation/Returns Max                                -19.3985\n",
      "evaluation/Returns Min                               -269.663\n",
      "evaluation/Actions Mean                                -0.118813\n",
      "evaluation/Actions Std                                  0.638355\n",
      "evaluation/Actions Max                                  0.999871\n",
      "evaluation/Actions Min                                 -0.999756\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -158.728\n",
      "evaluation/env_infos/final/reward_run Mean              0.040772\n",
      "evaluation/env_infos/final/reward_run Std               0.329581\n",
      "evaluation/env_infos/final/reward_run Max               1.21748\n",
      "evaluation/env_infos/final/reward_run Min              -0.877875\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.349431\n",
      "evaluation/env_infos/initial/reward_run Std             0.325045\n",
      "evaluation/env_infos/initial/reward_run Max             0.366501\n",
      "evaluation/env_infos/initial/reward_run Min            -0.918591\n",
      "evaluation/env_infos/reward_run Mean                    0.0350506\n",
      "evaluation/env_infos/reward_run Std                     0.428349\n",
      "evaluation/env_infos/reward_run Max                     2.34627\n",
      "evaluation/env_infos/reward_run Min                    -3.19523\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.235845\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0992298\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.05128\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.386179\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.245881\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0882603\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0507887\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.401328\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.252968\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0969761\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0051843\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.571285\n",
      "evaluation/env_infos/final/height Mean                 -0.21854\n",
      "evaluation/env_infos/final/height Std                   0.156411\n",
      "evaluation/env_infos/final/height Max                   0.072966\n",
      "evaluation/env_infos/final/height Min                  -0.575255\n",
      "evaluation/env_infos/initial/height Mean               -0.018903\n",
      "evaluation/env_infos/initial/height Std                 0.0596961\n",
      "evaluation/env_infos/initial/height Max                 0.0863449\n",
      "evaluation/env_infos/initial/height Min                -0.107283\n",
      "evaluation/env_infos/height Mean                       -0.191135\n",
      "evaluation/env_infos/height Std                         0.139581\n",
      "evaluation/env_infos/height Max                         0.431516\n",
      "evaluation/env_infos/height Min                        -0.58897\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0632623\n",
      "evaluation/env_infos/final/reward_angular Std           0.280626\n",
      "evaluation/env_infos/final/reward_angular Max           0.573378\n",
      "evaluation/env_infos/final/reward_angular Min          -0.842401\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.050254\n",
      "evaluation/env_infos/initial/reward_angular Std         1.19593\n",
      "evaluation/env_infos/initial/reward_angular Max         1.60412\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.31766\n",
      "evaluation/env_infos/reward_angular Mean                0.0234466\n",
      "evaluation/env_infos/reward_angular Std                 0.662398\n",
      "evaluation/env_infos/reward_angular Max                 6.48262\n",
      "evaluation/env_infos/reward_angular Min                -4.94498\n",
      "time/data storing (s)                                   0.0165488\n",
      "time/evaluation sampling (s)                           24.7576\n",
      "time/exploration sampling (s)                           1.20896\n",
      "time/logging (s)                                        0.271231\n",
      "time/saving (s)                                         0.0296682\n",
      "time/training (s)                                       4.14986\n",
      "time/epoch (s)                                         30.4339\n",
      "time/total (s)                                        533.882\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:21:24.819454 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.492712\n",
      "trainer/QF2 Loss                                        0.518173\n",
      "trainer/Policy Loss                                    -1.29521\n",
      "trainer/Q1 Predictions Mean                             2.5417\n",
      "trainer/Q1 Predictions Std                              1.65072\n",
      "trainer/Q1 Predictions Max                             11.124\n",
      "trainer/Q1 Predictions Min                              0.198719\n",
      "trainer/Q2 Predictions Mean                             2.53312\n",
      "trainer/Q2 Predictions Std                              1.65548\n",
      "trainer/Q2 Predictions Max                             11.0357\n",
      "trainer/Q2 Predictions Min                              0.142757\n",
      "trainer/Q Targets Mean                                  2.56297\n",
      "trainer/Q Targets Std                                   1.85494\n",
      "trainer/Q Targets Max                                  13.232\n",
      "trainer/Q Targets Min                                  -0.815886\n",
      "trainer/Log Pis Mean                                    1.55249\n",
      "trainer/Log Pis Std                                     4.33782\n",
      "trainer/Log Pis Max                                    17.9408\n",
      "trainer/Log Pis Min                                    -7.6746\n",
      "trainer/Policy mu Mean                                  0.26288\n",
      "trainer/Policy mu Std                                   1.13775\n",
      "trainer/Policy mu Max                                   3.69563\n",
      "trainer/Policy mu Min                                  -4.30598\n",
      "trainer/Policy log std Mean                            -0.500538\n",
      "trainer/Policy log std Std                              0.234468\n",
      "trainer/Policy log std Max                             -0.00931349\n",
      "trainer/Policy log std Min                             -1.50273\n",
      "trainer/Alpha                                           0.0285075\n",
      "trainer/Alpha Loss                                    -15.8142\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            15\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.086091\n",
      "exploration/Rewards Std                                 0.66824\n",
      "exploration/Rewards Max                                 1.77317\n",
      "exploration/Rewards Min                                -2.86745\n",
      "exploration/Returns Mean                              -86.091\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -86.091\n",
      "exploration/Returns Min                               -86.091\n",
      "exploration/Actions Mean                                0.446519\n",
      "exploration/Actions Std                                 0.523807\n",
      "exploration/Actions Max                                 0.998782\n",
      "exploration/Actions Min                                -0.991795\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -86.091\n",
      "exploration/env_infos/final/reward_run Mean            -0.205186\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.205186\n",
      "exploration/env_infos/final/reward_run Min             -0.205186\n",
      "exploration/env_infos/initial/reward_run Mean           0.0911536\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0911536\n",
      "exploration/env_infos/initial/reward_run Min            0.0911536\n",
      "exploration/env_infos/reward_run Mean                  -0.0460732\n",
      "exploration/env_infos/reward_run Std                    0.448947\n",
      "exploration/env_infos/reward_run Max                    1.35345\n",
      "exploration/env_infos/reward_run Min                   -1.91537\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.2389\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.2389\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.2389\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.285441\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.285441\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.285441\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.284252\n",
      "exploration/env_infos/reward_ctrl Std                   0.0749743\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0737058\n",
      "exploration/env_infos/reward_ctrl Min                  -0.491106\n",
      "exploration/env_infos/final/height Mean                -0.222831\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.222831\n",
      "exploration/env_infos/final/height Min                 -0.222831\n",
      "exploration/env_infos/initial/height Mean               0.0371655\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0371655\n",
      "exploration/env_infos/initial/height Min                0.0371655\n",
      "exploration/env_infos/height Mean                      -0.214366\n",
      "exploration/env_infos/height Std                        0.123223\n",
      "exploration/env_infos/height Max                        0.436901\n",
      "exploration/env_infos/height Min                       -0.451675\n",
      "exploration/env_infos/final/reward_angular Mean         1.29643\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.29643\n",
      "exploration/env_infos/final/reward_angular Min          1.29643\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.71503\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.71503\n",
      "exploration/env_infos/initial/reward_angular Min       -1.71503\n",
      "exploration/env_infos/reward_angular Mean               0.00577378\n",
      "exploration/env_infos/reward_angular Std                1.20237\n",
      "exploration/env_infos/reward_angular Max                3.79527\n",
      "exploration/env_infos/reward_angular Min               -4.31089\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.189629\n",
      "evaluation/Rewards Std                                  0.54949\n",
      "evaluation/Rewards Max                                  3.85735\n",
      "evaluation/Rewards Min                                 -3.69547\n",
      "evaluation/Returns Mean                              -189.629\n",
      "evaluation/Returns Std                                140.236\n",
      "evaluation/Returns Max                                 31.2195\n",
      "evaluation/Returns Min                               -645.574\n",
      "evaluation/Actions Mean                                -0.0273393\n",
      "evaluation/Actions Std                                  0.697791\n",
      "evaluation/Actions Max                                  0.999328\n",
      "evaluation/Actions Min                                 -0.999957\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -189.629\n",
      "evaluation/env_infos/final/reward_run Mean              0.0670224\n",
      "evaluation/env_infos/final/reward_run Std               0.443924\n",
      "evaluation/env_infos/final/reward_run Max               0.991498\n",
      "evaluation/env_infos/final/reward_run Min              -1.00921\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.204572\n",
      "evaluation/env_infos/initial/reward_run Std             0.359289\n",
      "evaluation/env_infos/initial/reward_run Max             0.465512\n",
      "evaluation/env_infos/initial/reward_run Min            -0.808343\n",
      "evaluation/env_infos/reward_run Mean                    0.0893181\n",
      "evaluation/env_infos/reward_run Std                     0.477341\n",
      "evaluation/env_infos/reward_run Max                     2.78401\n",
      "evaluation/env_infos/reward_run Min                    -2.57769\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.304218\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0955753\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0427202\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.496022\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.28296\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0892225\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0943178\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.404252\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.292596\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0926557\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00724446\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.57535\n",
      "evaluation/env_infos/final/height Mean                 -0.228112\n",
      "evaluation/env_infos/final/height Std                   0.125535\n",
      "evaluation/env_infos/final/height Max                  -0.0920331\n",
      "evaluation/env_infos/final/height Min                  -0.548135\n",
      "evaluation/env_infos/initial/height Mean               -0.0127982\n",
      "evaluation/env_infos/initial/height Std                 0.046096\n",
      "evaluation/env_infos/initial/height Max                 0.0681515\n",
      "evaluation/env_infos/initial/height Min                -0.0903375\n",
      "evaluation/env_infos/height Mean                       -0.21761\n",
      "evaluation/env_infos/height Std                         0.123067\n",
      "evaluation/env_infos/height Max                         0.323948\n",
      "evaluation/env_infos/height Min                        -0.590971\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0493096\n",
      "evaluation/env_infos/final/reward_angular Std           0.897814\n",
      "evaluation/env_infos/final/reward_angular Max           2.45862\n",
      "evaluation/env_infos/final/reward_angular Min          -2.22682\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.374941\n",
      "evaluation/env_infos/initial/reward_angular Std         0.936092\n",
      "evaluation/env_infos/initial/reward_angular Max         1.5208\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.86702\n",
      "evaluation/env_infos/reward_angular Mean                0.0142388\n",
      "evaluation/env_infos/reward_angular Std                 0.711617\n",
      "evaluation/env_infos/reward_angular Max                 5.25435\n",
      "evaluation/env_infos/reward_angular Min                -6.44874\n",
      "time/data storing (s)                                   0.0154445\n",
      "time/evaluation sampling (s)                           23.0139\n",
      "time/exploration sampling (s)                           1.03711\n",
      "time/logging (s)                                        0.235279\n",
      "time/saving (s)                                         0.0288099\n",
      "time/training (s)                                       3.82915\n",
      "time/epoch (s)                                         28.1597\n",
      "time/total (s)                                        562.284\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:21:52.559975 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.424245\n",
      "trainer/QF2 Loss                                        0.522077\n",
      "trainer/Policy Loss                                    -1.50925\n",
      "trainer/Q1 Predictions Mean                             2.44441\n",
      "trainer/Q1 Predictions Std                              1.49376\n",
      "trainer/Q1 Predictions Max                             12.7068\n",
      "trainer/Q1 Predictions Min                              0.344019\n",
      "trainer/Q2 Predictions Mean                             2.34774\n",
      "trainer/Q2 Predictions Std                              1.45495\n",
      "trainer/Q2 Predictions Max                             10.9832\n",
      "trainer/Q2 Predictions Min                             -0.472343\n",
      "trainer/Q Targets Mean                                  2.41507\n",
      "trainer/Q Targets Std                                   1.74382\n",
      "trainer/Q Targets Max                                  13.5433\n",
      "trainer/Q Targets Min                                  -2.06646\n",
      "trainer/Log Pis Mean                                    1.10527\n",
      "trainer/Log Pis Std                                     4.0084\n",
      "trainer/Log Pis Max                                    18.7857\n",
      "trainer/Log Pis Min                                    -8.33223\n",
      "trainer/Policy mu Mean                                  0.26534\n",
      "trainer/Policy mu Std                                   1.04773\n",
      "trainer/Policy mu Max                                   3.04357\n",
      "trainer/Policy mu Min                                  -3.39131\n",
      "trainer/Policy log std Mean                            -0.545387\n",
      "trainer/Policy log std Std                              0.241147\n",
      "trainer/Policy log std Max                             -0.0279653\n",
      "trainer/Policy log std Min                             -1.51528\n",
      "trainer/Alpha                                           0.0237327\n",
      "trainer/Alpha Loss                                    -18.3016\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            16\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.369496\n",
      "exploration/Rewards Std                                 0.545345\n",
      "exploration/Rewards Max                                 1.55089\n",
      "exploration/Rewards Min                                -3.03665\n",
      "exploration/Returns Mean                             -369.496\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -369.496\n",
      "exploration/Returns Min                              -369.496\n",
      "exploration/Actions Mean                                0.288296\n",
      "exploration/Actions Std                                 0.571388\n",
      "exploration/Actions Max                                 0.999413\n",
      "exploration/Actions Min                                -0.991507\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -369.496\n",
      "exploration/env_infos/final/reward_run Mean            -0.170123\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.170123\n",
      "exploration/env_infos/final/reward_run Min             -0.170123\n",
      "exploration/env_infos/initial/reward_run Mean           0.357865\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.357865\n",
      "exploration/env_infos/initial/reward_run Min            0.357865\n",
      "exploration/env_infos/reward_run Mean                  -0.0187698\n",
      "exploration/env_infos/reward_run Std                    0.493349\n",
      "exploration/env_infos/reward_run Max                    1.62027\n",
      "exploration/env_infos/reward_run Min                   -2.2242\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.236672\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.236672\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.236672\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.260934\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.260934\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.260934\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.245759\n",
      "exploration/env_infos/reward_ctrl Std                   0.079129\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0256649\n",
      "exploration/env_infos/reward_ctrl Min                  -0.504614\n",
      "exploration/env_infos/final/height Mean                -0.560254\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.560254\n",
      "exploration/env_infos/final/height Min                 -0.560254\n",
      "exploration/env_infos/initial/height Mean              -0.0283637\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0283637\n",
      "exploration/env_infos/initial/height Min               -0.0283637\n",
      "exploration/env_infos/height Mean                      -0.501561\n",
      "exploration/env_infos/height Std                        0.120574\n",
      "exploration/env_infos/height Max                        0.278527\n",
      "exploration/env_infos/height Min                       -0.585865\n",
      "exploration/env_infos/final/reward_angular Mean        -1.33821\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.33821\n",
      "exploration/env_infos/final/reward_angular Min         -1.33821\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.73689\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.73689\n",
      "exploration/env_infos/initial/reward_angular Min       -1.73689\n",
      "exploration/env_infos/reward_angular Mean               0.0714783\n",
      "exploration/env_infos/reward_angular Std                1.0427\n",
      "exploration/env_infos/reward_angular Max                7.54731\n",
      "exploration/env_infos/reward_angular Min               -3.02554\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.185303\n",
      "evaluation/Rewards Std                                  0.372014\n",
      "evaluation/Rewards Max                                  3.55685\n",
      "evaluation/Rewards Min                                 -3.9235\n",
      "evaluation/Returns Mean                              -185.303\n",
      "evaluation/Returns Std                                 92.311\n",
      "evaluation/Returns Max                                -18.0663\n",
      "evaluation/Returns Min                               -336.56\n",
      "evaluation/Actions Mean                                -0.083692\n",
      "evaluation/Actions Std                                  0.697867\n",
      "evaluation/Actions Max                                  0.997484\n",
      "evaluation/Actions Min                                 -0.99961\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -185.303\n",
      "evaluation/env_infos/final/reward_run Mean             -0.077325\n",
      "evaluation/env_infos/final/reward_run Std               0.224247\n",
      "evaluation/env_infos/final/reward_run Max               0.151065\n",
      "evaluation/env_infos/final/reward_run Min              -0.758384\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.308585\n",
      "evaluation/env_infos/initial/reward_run Std             0.426851\n",
      "evaluation/env_infos/initial/reward_run Max             0.637318\n",
      "evaluation/env_infos/initial/reward_run Min            -1.0064\n",
      "evaluation/env_infos/reward_run Mean                    0.0114502\n",
      "evaluation/env_infos/reward_run Std                     0.299659\n",
      "evaluation/env_infos/reward_run Max                     2.72543\n",
      "evaluation/env_infos/reward_run Min                    -1.9924\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.29408\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0817494\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.092503\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.435814\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.322218\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0973645\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.125723\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.485615\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.296414\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0869985\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0328437\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.559065\n",
      "evaluation/env_infos/final/height Mean                 -0.187746\n",
      "evaluation/env_infos/final/height Std                   0.109235\n",
      "evaluation/env_infos/final/height Max                   0.013113\n",
      "evaluation/env_infos/final/height Min                  -0.412689\n",
      "evaluation/env_infos/initial/height Mean               -0.0154526\n",
      "evaluation/env_infos/initial/height Std                 0.055268\n",
      "evaluation/env_infos/initial/height Max                 0.0881297\n",
      "evaluation/env_infos/initial/height Min                -0.107307\n",
      "evaluation/env_infos/height Mean                       -0.192227\n",
      "evaluation/env_infos/height Std                         0.103832\n",
      "evaluation/env_infos/height Max                         0.263794\n",
      "evaluation/env_infos/height Min                        -0.429618\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00930724\n",
      "evaluation/env_infos/final/reward_angular Std           0.320648\n",
      "evaluation/env_infos/final/reward_angular Max           0.993644\n",
      "evaluation/env_infos/final/reward_angular Min          -0.947979\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.337483\n",
      "evaluation/env_infos/initial/reward_angular Std         1.73676\n",
      "evaluation/env_infos/initial/reward_angular Max         3.25627\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.13182\n",
      "evaluation/env_infos/reward_angular Mean                0.00730366\n",
      "evaluation/env_infos/reward_angular Std                 0.542367\n",
      "evaluation/env_infos/reward_angular Max                 4.00693\n",
      "evaluation/env_infos/reward_angular Min                -5.15638\n",
      "time/data storing (s)                                   0.0151592\n",
      "time/evaluation sampling (s)                           22.252\n",
      "time/exploration sampling (s)                           1.06849\n",
      "time/logging (s)                                        0.23901\n",
      "time/saving (s)                                         0.0277711\n",
      "time/training (s)                                       3.93723\n",
      "time/epoch (s)                                         27.5397\n",
      "time/total (s)                                        590.027\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:22:21.709515 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  17000\n",
      "trainer/QF1 Loss                                        0.534364\n",
      "trainer/QF2 Loss                                        0.695021\n",
      "trainer/Policy Loss                                    -0.639606\n",
      "trainer/Q1 Predictions Mean                             2.60215\n",
      "trainer/Q1 Predictions Std                              1.80977\n",
      "trainer/Q1 Predictions Max                             11.5092\n",
      "trainer/Q1 Predictions Min                              0.0553266\n",
      "trainer/Q2 Predictions Mean                             2.42478\n",
      "trainer/Q2 Predictions Std                              1.77636\n",
      "trainer/Q2 Predictions Max                             10.1861\n",
      "trainer/Q2 Predictions Min                             -1.36886\n",
      "trainer/Q Targets Mean                                  2.50979\n",
      "trainer/Q Targets Std                                   1.84138\n",
      "trainer/Q Targets Max                                  10.6378\n",
      "trainer/Q Targets Min                                  -2.14059\n",
      "trainer/Log Pis Mean                                    2.06909\n",
      "trainer/Log Pis Std                                     4.3026\n",
      "trainer/Log Pis Max                                    15.1532\n",
      "trainer/Log Pis Min                                    -6.27559\n",
      "trainer/Policy mu Mean                                  0.106719\n",
      "trainer/Policy mu Std                                   1.17775\n",
      "trainer/Policy mu Max                                   3.39527\n",
      "trainer/Policy mu Min                                  -4.88661\n",
      "trainer/Policy log std Mean                            -0.523706\n",
      "trainer/Policy log std Std                              0.261519\n",
      "trainer/Policy log std Max                              0.0330231\n",
      "trainer/Policy log std Min                             -1.58028\n",
      "trainer/Alpha                                           0.0198855\n",
      "trainer/Alpha Loss                                    -15.3935\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            17\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.177667\n",
      "exploration/Rewards Std                                 0.115596\n",
      "exploration/Rewards Max                                 0.571966\n",
      "exploration/Rewards Min                                -0.690096\n",
      "exploration/Returns Mean                             -177.667\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -177.667\n",
      "exploration/Returns Min                              -177.667\n",
      "exploration/Actions Mean                               -0.471399\n",
      "exploration/Actions Std                                 0.650719\n",
      "exploration/Actions Max                                 0.991422\n",
      "exploration/Actions Min                                -0.99946\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -177.667\n",
      "exploration/env_infos/final/reward_run Mean            -0.151692\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.151692\n",
      "exploration/env_infos/final/reward_run Min             -0.151692\n",
      "exploration/env_infos/initial/reward_run Mean          -0.76353\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.76353\n",
      "exploration/env_infos/initial/reward_run Min           -0.76353\n",
      "exploration/env_infos/reward_run Mean                  -0.0738142\n",
      "exploration/env_infos/reward_run Std                    0.178167\n",
      "exploration/env_infos/reward_run Max                    0.543955\n",
      "exploration/env_infos/reward_run Min                   -0.975101\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.216086\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.216086\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.216086\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.506678\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.506678\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.506678\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.387391\n",
      "exploration/env_infos/reward_ctrl Std                   0.0580083\n",
      "exploration/env_infos/reward_ctrl Max                  -0.162881\n",
      "exploration/env_infos/reward_ctrl Min                  -0.520452\n",
      "exploration/env_infos/final/height Mean                -0.228332\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.228332\n",
      "exploration/env_infos/final/height Min                 -0.228332\n",
      "exploration/env_infos/initial/height Mean              -0.000476062\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.000476062\n",
      "exploration/env_infos/initial/height Min               -0.000476062\n",
      "exploration/env_infos/height Mean                      -0.211321\n",
      "exploration/env_infos/height Std                        0.0184834\n",
      "exploration/env_infos/height Max                       -0.000476062\n",
      "exploration/env_infos/height Min                       -0.259095\n",
      "exploration/env_infos/final/reward_angular Mean         0.367132\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.367132\n",
      "exploration/env_infos/final/reward_angular Min          0.367132\n",
      "exploration/env_infos/initial/reward_angular Mean       1.20952\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.20952\n",
      "exploration/env_infos/initial/reward_angular Min        1.20952\n",
      "exploration/env_infos/reward_angular Mean              -0.00319405\n",
      "exploration/env_infos/reward_angular Std                0.663624\n",
      "exploration/env_infos/reward_angular Max                1.88481\n",
      "exploration/env_infos/reward_angular Min               -2.11344\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.221242\n",
      "evaluation/Rewards Std                                  0.512257\n",
      "evaluation/Rewards Max                                  4.99098\n",
      "evaluation/Rewards Min                                 -5.10759\n",
      "evaluation/Returns Mean                              -221.242\n",
      "evaluation/Returns Std                                 93.303\n",
      "evaluation/Returns Max                                -57.8063\n",
      "evaluation/Returns Min                               -491.346\n",
      "evaluation/Actions Mean                                -0.132278\n",
      "evaluation/Actions Std                                  0.693427\n",
      "evaluation/Actions Max                                  0.998139\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -221.242\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0203322\n",
      "evaluation/env_infos/final/reward_run Std               0.462478\n",
      "evaluation/env_infos/final/reward_run Max               1.20953\n",
      "evaluation/env_infos/final/reward_run Min              -1.10574\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.478238\n",
      "evaluation/env_infos/initial/reward_run Std             0.310612\n",
      "evaluation/env_infos/initial/reward_run Max             0.223523\n",
      "evaluation/env_infos/initial/reward_run Min            -1.15497\n",
      "evaluation/env_infos/reward_run Mean                   -0.0909216\n",
      "evaluation/env_infos/reward_run Std                     0.465719\n",
      "evaluation/env_infos/reward_run Max                     2.34711\n",
      "evaluation/env_infos/reward_run Min                    -3.19859\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.290333\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0756285\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.155498\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.392266\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.324126\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0910694\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0942353\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.419662\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.299003\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0755804\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0276134\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.561138\n",
      "evaluation/env_infos/final/height Mean                 -0.232294\n",
      "evaluation/env_infos/final/height Std                   0.119571\n",
      "evaluation/env_infos/final/height Max                  -0.0386124\n",
      "evaluation/env_infos/final/height Min                  -0.548047\n",
      "evaluation/env_infos/initial/height Mean               -0.00357504\n",
      "evaluation/env_infos/initial/height Std                 0.0511432\n",
      "evaluation/env_infos/initial/height Max                 0.0924233\n",
      "evaluation/env_infos/initial/height Min                -0.0969029\n",
      "evaluation/env_infos/height Mean                       -0.19844\n",
      "evaluation/env_infos/height Std                         0.119563\n",
      "evaluation/env_infos/height Max                         0.465302\n",
      "evaluation/env_infos/height Min                        -0.584953\n",
      "evaluation/env_infos/final/reward_angular Mean          0.123671\n",
      "evaluation/env_infos/final/reward_angular Std           0.537762\n",
      "evaluation/env_infos/final/reward_angular Max           1.6468\n",
      "evaluation/env_infos/final/reward_angular Min          -1.09037\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.146629\n",
      "evaluation/env_infos/initial/reward_angular Std         1.42094\n",
      "evaluation/env_infos/initial/reward_angular Max         2.25789\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.78295\n",
      "evaluation/env_infos/reward_angular Mean                0.0120244\n",
      "evaluation/env_infos/reward_angular Std                 0.750183\n",
      "evaluation/env_infos/reward_angular Max                 6.48154\n",
      "evaluation/env_infos/reward_angular Min                -5.49141\n",
      "time/data storing (s)                                   0.0169235\n",
      "time/evaluation sampling (s)                           23.2937\n",
      "time/exploration sampling (s)                           1.13629\n",
      "time/logging (s)                                        0.248774\n",
      "time/saving (s)                                         0.0320045\n",
      "time/training (s)                                       4.21857\n",
      "time/epoch (s)                                         28.9463\n",
      "time/total (s)                                        619.185\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:22:49.987065 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.832311\n",
      "trainer/QF2 Loss                                        0.844802\n",
      "trainer/Policy Loss                                    -1.20159\n",
      "trainer/Q1 Predictions Mean                             2.62496\n",
      "trainer/Q1 Predictions Std                              1.93041\n",
      "trainer/Q1 Predictions Max                             13.1632\n",
      "trainer/Q1 Predictions Min                             -0.202837\n",
      "trainer/Q2 Predictions Mean                             2.58156\n",
      "trainer/Q2 Predictions Std                              1.91825\n",
      "trainer/Q2 Predictions Max                             12.3657\n",
      "trainer/Q2 Predictions Min                             -0.121581\n",
      "trainer/Q Targets Mean                                  2.38605\n",
      "trainer/Q Targets Std                                   2.02475\n",
      "trainer/Q Targets Max                                  12.6767\n",
      "trainer/Q Targets Min                                  -2.34927\n",
      "trainer/Log Pis Mean                                    1.66527\n",
      "trainer/Log Pis Std                                     4.35591\n",
      "trainer/Log Pis Max                                    18.3783\n",
      "trainer/Log Pis Min                                    -7.325\n",
      "trainer/Policy mu Mean                                  0.0657768\n",
      "trainer/Policy mu Std                                   1.15412\n",
      "trainer/Policy mu Max                                   4.30939\n",
      "trainer/Policy mu Min                                  -4.47478\n",
      "trainer/Policy log std Mean                            -0.573199\n",
      "trainer/Policy log std Std                              0.309442\n",
      "trainer/Policy log std Max                              0.00991506\n",
      "trainer/Policy log std Min                             -1.91259\n",
      "trainer/Alpha                                           0.0169439\n",
      "trainer/Alpha Loss                                    -17.669\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            18\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.434067\n",
      "exploration/Rewards Std                                 1.17816\n",
      "exploration/Rewards Max                                 3.44846\n",
      "exploration/Rewards Min                                -3.88527\n",
      "exploration/Returns Mean                             -434.067\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -434.067\n",
      "exploration/Returns Min                              -434.067\n",
      "exploration/Actions Mean                                0.0583659\n",
      "exploration/Actions Std                                 0.72538\n",
      "exploration/Actions Max                                 0.999955\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -434.067\n",
      "exploration/env_infos/final/reward_run Mean            -0.637285\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.637285\n",
      "exploration/env_infos/final/reward_run Min             -0.637285\n",
      "exploration/env_infos/initial/reward_run Mean           0.524354\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.524354\n",
      "exploration/env_infos/initial/reward_run Min            0.524354\n",
      "exploration/env_infos/reward_run Mean                  -0.364665\n",
      "exploration/env_infos/reward_run Std                    0.947666\n",
      "exploration/env_infos/reward_run Max                    2.26552\n",
      "exploration/env_infos/reward_run Min                   -2.6265\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.183968\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.183968\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.183968\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.277137\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.277137\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.277137\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.31775\n",
      "exploration/env_infos/reward_ctrl Std                   0.08743\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0428542\n",
      "exploration/env_infos/reward_ctrl Min                  -0.522963\n",
      "exploration/env_infos/final/height Mean                -0.560724\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.560724\n",
      "exploration/env_infos/final/height Min                 -0.560724\n",
      "exploration/env_infos/initial/height Mean              -0.053542\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.053542\n",
      "exploration/env_infos/initial/height Min               -0.053542\n",
      "exploration/env_infos/height Mean                      -0.425813\n",
      "exploration/env_infos/height Std                        0.172419\n",
      "exploration/env_infos/height Max                        0.308064\n",
      "exploration/env_infos/height Min                       -0.592718\n",
      "exploration/env_infos/final/reward_angular Mean         0.337732\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.337732\n",
      "exploration/env_infos/final/reward_angular Min          0.337732\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.560444\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.560444\n",
      "exploration/env_infos/initial/reward_angular Min       -0.560444\n",
      "exploration/env_infos/reward_angular Mean               0.088459\n",
      "exploration/env_infos/reward_angular Std                1.75113\n",
      "exploration/env_infos/reward_angular Max                5.92127\n",
      "exploration/env_infos/reward_angular Min               -5.08424\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.236057\n",
      "evaluation/Rewards Std                                  0.761503\n",
      "evaluation/Rewards Max                                  7.08241\n",
      "evaluation/Rewards Min                                 -4.66589\n",
      "evaluation/Returns Mean                              -236.057\n",
      "evaluation/Returns Std                                197.723\n",
      "evaluation/Returns Max                                116.077\n",
      "evaluation/Returns Min                               -773.797\n",
      "evaluation/Actions Mean                                -0.0914077\n",
      "evaluation/Actions Std                                  0.715703\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -236.057\n",
      "evaluation/env_infos/final/reward_run Mean             -0.275327\n",
      "evaluation/env_infos/final/reward_run Std               0.705818\n",
      "evaluation/env_infos/final/reward_run Max               0.912155\n",
      "evaluation/env_infos/final/reward_run Min              -2.13651\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.372105\n",
      "evaluation/env_infos/initial/reward_run Std             0.259207\n",
      "evaluation/env_infos/initial/reward_run Max             0.2209\n",
      "evaluation/env_infos/initial/reward_run Min            -0.91398\n",
      "evaluation/env_infos/reward_run Mean                   -0.242305\n",
      "evaluation/env_infos/reward_run Std                     0.634497\n",
      "evaluation/env_infos/reward_run Max                     2.97057\n",
      "evaluation/env_infos/reward_run Min                    -3.04313\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.308897\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0728851\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.193456\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.464937\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.250276\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.091724\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0364424\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.378631\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.312352\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0867243\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.020907\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.579402\n",
      "evaluation/env_infos/final/height Mean                 -0.31361\n",
      "evaluation/env_infos/final/height Std                   0.174558\n",
      "evaluation/env_infos/final/height Max                  -0.121302\n",
      "evaluation/env_infos/final/height Min                  -0.577212\n",
      "evaluation/env_infos/initial/height Mean               -0.0151353\n",
      "evaluation/env_infos/initial/height Std                 0.0558012\n",
      "evaluation/env_infos/initial/height Max                 0.0847473\n",
      "evaluation/env_infos/initial/height Min                -0.113272\n",
      "evaluation/env_infos/height Mean                       -0.273861\n",
      "evaluation/env_infos/height Std                         0.180838\n",
      "evaluation/env_infos/height Max                         0.542094\n",
      "evaluation/env_infos/height Min                        -0.594574\n",
      "evaluation/env_infos/final/reward_angular Mean          0.128999\n",
      "evaluation/env_infos/final/reward_angular Std           0.787117\n",
      "evaluation/env_infos/final/reward_angular Max           1.78394\n",
      "evaluation/env_infos/final/reward_angular Min          -1.37669\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.123418\n",
      "evaluation/env_infos/initial/reward_angular Std         1.17654\n",
      "evaluation/env_infos/initial/reward_angular Max         2.96341\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.67757\n",
      "evaluation/env_infos/reward_angular Mean                0.0477787\n",
      "evaluation/env_infos/reward_angular Std                 1.05435\n",
      "evaluation/env_infos/reward_angular Max                 8.36351\n",
      "evaluation/env_infos/reward_angular Min                -6.08507\n",
      "time/data storing (s)                                   0.0160721\n",
      "time/evaluation sampling (s)                           22.8642\n",
      "time/exploration sampling (s)                           1.04146\n",
      "time/logging (s)                                        0.246417\n",
      "time/saving (s)                                         0.0289259\n",
      "time/training (s)                                       3.84738\n",
      "time/epoch (s)                                         28.0445\n",
      "time/total (s)                                        647.46\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:23:17.948541 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  19000\n",
      "trainer/QF1 Loss                                        0.445194\n",
      "trainer/QF2 Loss                                        0.592752\n",
      "trainer/Policy Loss                                     0.147125\n",
      "trainer/Q1 Predictions Mean                             2.30743\n",
      "trainer/Q1 Predictions Std                              1.60694\n",
      "trainer/Q1 Predictions Max                              7.73909\n",
      "trainer/Q1 Predictions Min                             -0.574613\n",
      "trainer/Q2 Predictions Mean                             2.51903\n",
      "trainer/Q2 Predictions Std                              1.64531\n",
      "trainer/Q2 Predictions Max                              8.21799\n",
      "trainer/Q2 Predictions Min                             -0.645269\n",
      "trainer/Q Targets Mean                                  2.26694\n",
      "trainer/Q Targets Std                                   1.75127\n",
      "trainer/Q Targets Max                                   8.04516\n",
      "trainer/Q Targets Min                                  -2.22116\n",
      "trainer/Log Pis Mean                                    2.82213\n",
      "trainer/Log Pis Std                                     4.38354\n",
      "trainer/Log Pis Max                                    17.4491\n",
      "trainer/Log Pis Min                                    -5.74066\n",
      "trainer/Policy mu Mean                                  0.174229\n",
      "trainer/Policy mu Std                                   1.23059\n",
      "trainer/Policy mu Max                                   5.22548\n",
      "trainer/Policy mu Min                                  -4.98599\n",
      "trainer/Policy log std Mean                            -0.641488\n",
      "trainer/Policy log std Std                              0.320993\n",
      "trainer/Policy log std Max                             -0.00625432\n",
      "trainer/Policy log std Min                             -1.86325\n",
      "trainer/Alpha                                           0.0148688\n",
      "trainer/Alpha Loss                                    -13.3697\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            19\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.29287\n",
      "exploration/Rewards Std                                 0.290083\n",
      "exploration/Rewards Max                                 0.528716\n",
      "exploration/Rewards Min                                -1.09145\n",
      "exploration/Returns Mean                             -292.87\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -292.87\n",
      "exploration/Returns Min                              -292.87\n",
      "exploration/Actions Mean                               -0.326324\n",
      "exploration/Actions Std                                 0.66059\n",
      "exploration/Actions Max                                 0.998941\n",
      "exploration/Actions Min                                -0.999948\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -292.87\n",
      "exploration/env_infos/final/reward_run Mean            -0.388758\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.388758\n",
      "exploration/env_infos/final/reward_run Min             -0.388758\n",
      "exploration/env_infos/initial/reward_run Mean          -0.318764\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.318764\n",
      "exploration/env_infos/initial/reward_run Min           -0.318764\n",
      "exploration/env_infos/reward_run Mean                  -0.3032\n",
      "exploration/env_infos/reward_run Std                    0.321417\n",
      "exploration/env_infos/reward_run Max                    0.702536\n",
      "exploration/env_infos/reward_run Min                   -1.48911\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.274262\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.274262\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.274262\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.117768\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.117768\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.117768\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.32572\n",
      "exploration/env_infos/reward_ctrl Std                   0.0742243\n",
      "exploration/env_infos/reward_ctrl Max                  -0.10073\n",
      "exploration/env_infos/reward_ctrl Min                  -0.521054\n",
      "exploration/env_infos/final/height Mean                -0.170986\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.170986\n",
      "exploration/env_infos/final/height Min                 -0.170986\n",
      "exploration/env_infos/initial/height Mean              -0.0626521\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0626521\n",
      "exploration/env_infos/initial/height Min               -0.0626521\n",
      "exploration/env_infos/height Mean                      -0.155042\n",
      "exploration/env_infos/height Std                        0.0305396\n",
      "exploration/env_infos/height Max                        0.0184717\n",
      "exploration/env_infos/height Min                       -0.211528\n",
      "exploration/env_infos/final/reward_angular Mean        -1.09977\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.09977\n",
      "exploration/env_infos/final/reward_angular Min         -1.09977\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.210172\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.210172\n",
      "exploration/env_infos/initial/reward_angular Min       -0.210172\n",
      "exploration/env_infos/reward_angular Mean               0.020828\n",
      "exploration/env_infos/reward_angular Std                1.26316\n",
      "exploration/env_infos/reward_angular Max                3.64676\n",
      "exploration/env_infos/reward_angular Min               -3.80955\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.166013\n",
      "evaluation/Rewards Std                                  0.800177\n",
      "evaluation/Rewards Max                                  6.81967\n",
      "evaluation/Rewards Min                                 -4.49596\n",
      "evaluation/Returns Mean                              -166.013\n",
      "evaluation/Returns Std                                119.569\n",
      "evaluation/Returns Max                                124.275\n",
      "evaluation/Returns Min                               -379.379\n",
      "evaluation/Actions Mean                                -0.105541\n",
      "evaluation/Actions Std                                  0.753849\n",
      "evaluation/Actions Max                                  0.999987\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -166.013\n",
      "evaluation/env_infos/final/reward_run Mean             -0.15456\n",
      "evaluation/env_infos/final/reward_run Std               0.732919\n",
      "evaluation/env_infos/final/reward_run Max               2.20225\n",
      "evaluation/env_infos/final/reward_run Min              -1.71556\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.146718\n",
      "evaluation/env_infos/initial/reward_run Std             0.400476\n",
      "evaluation/env_infos/initial/reward_run Max             0.795517\n",
      "evaluation/env_infos/initial/reward_run Min            -0.874027\n",
      "evaluation/env_infos/reward_run Mean                   -0.103571\n",
      "evaluation/env_infos/reward_run Std                     0.577703\n",
      "evaluation/env_infos/reward_run Max                     3.03376\n",
      "evaluation/env_infos/reward_run Min                    -2.29047\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.35783\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.102098\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.113746\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.518017\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.259398\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.072101\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0902075\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.386962\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.347657\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0933188\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0818496\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.575827\n",
      "evaluation/env_infos/final/height Mean                 -0.276259\n",
      "evaluation/env_infos/final/height Std                   0.170391\n",
      "evaluation/env_infos/final/height Max                   0.018679\n",
      "evaluation/env_infos/final/height Min                  -0.566806\n",
      "evaluation/env_infos/initial/height Mean               -0.00400029\n",
      "evaluation/env_infos/initial/height Std                 0.0579625\n",
      "evaluation/env_infos/initial/height Max                 0.0942366\n",
      "evaluation/env_infos/initial/height Min                -0.106802\n",
      "evaluation/env_infos/height Mean                       -0.250853\n",
      "evaluation/env_infos/height Std                         0.174758\n",
      "evaluation/env_infos/height Max                         0.550927\n",
      "evaluation/env_infos/height Min                        -0.591995\n",
      "evaluation/env_infos/final/reward_angular Mean          0.172297\n",
      "evaluation/env_infos/final/reward_angular Std           0.884538\n",
      "evaluation/env_infos/final/reward_angular Max           1.8462\n",
      "evaluation/env_infos/final/reward_angular Min          -2.16556\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0637675\n",
      "evaluation/env_infos/initial/reward_angular Std         0.867655\n",
      "evaluation/env_infos/initial/reward_angular Max         1.31847\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.46337\n",
      "evaluation/env_infos/reward_angular Mean                0.0428082\n",
      "evaluation/env_infos/reward_angular Std                 1.07509\n",
      "evaluation/env_infos/reward_angular Max                 8.36719\n",
      "evaluation/env_infos/reward_angular Min                -5.07336\n",
      "time/data storing (s)                                   0.0152716\n",
      "time/evaluation sampling (s)                           22.6059\n",
      "time/exploration sampling (s)                           1.05616\n",
      "time/logging (s)                                        0.239246\n",
      "time/saving (s)                                         0.0283546\n",
      "time/training (s)                                       3.77529\n",
      "time/epoch (s)                                         27.7202\n",
      "time/total (s)                                        675.413\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:23:46.197267 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.621959\n",
      "trainer/QF2 Loss                                        0.596132\n",
      "trainer/Policy Loss                                     0.0672324\n",
      "trainer/Q1 Predictions Mean                             2.61239\n",
      "trainer/Q1 Predictions Std                              1.96823\n",
      "trainer/Q1 Predictions Max                             10.5715\n",
      "trainer/Q1 Predictions Min                             -0.183131\n",
      "trainer/Q2 Predictions Mean                             2.56585\n",
      "trainer/Q2 Predictions Std                              1.94355\n",
      "trainer/Q2 Predictions Max                             10.4316\n",
      "trainer/Q2 Predictions Min                             -0.560785\n",
      "trainer/Q Targets Mean                                  2.58163\n",
      "trainer/Q Targets Std                                   2.01367\n",
      "trainer/Q Targets Max                                  10.1256\n",
      "trainer/Q Targets Min                                  -0.7847\n",
      "trainer/Log Pis Mean                                    2.94721\n",
      "trainer/Log Pis Std                                     4.48388\n",
      "trainer/Log Pis Max                                    24.2286\n",
      "trainer/Log Pis Min                                    -6.35822\n",
      "trainer/Policy mu Mean                                 -0.0499626\n",
      "trainer/Policy mu Std                                   1.28373\n",
      "trainer/Policy mu Max                                   3.77374\n",
      "trainer/Policy mu Min                                  -7.03758\n",
      "trainer/Policy log std Mean                            -0.638612\n",
      "trainer/Policy log std Std                              0.346039\n",
      "trainer/Policy log std Max                              0.108322\n",
      "trainer/Policy log std Min                             -1.94852\n",
      "trainer/Alpha                                           0.0132999\n",
      "trainer/Alpha Loss                                    -13.1844\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                            20\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0893935\n",
      "exploration/Rewards Std                                 0.480167\n",
      "exploration/Rewards Max                                 1.46946\n",
      "exploration/Rewards Min                                -1.72992\n",
      "exploration/Returns Mean                              -89.3935\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -89.3935\n",
      "exploration/Returns Min                               -89.3935\n",
      "exploration/Actions Mean                               -0.439643\n",
      "exploration/Actions Std                                 0.577994\n",
      "exploration/Actions Max                                 0.998974\n",
      "exploration/Actions Min                                -0.999971\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -89.3935\n",
      "exploration/env_infos/final/reward_run Mean            -0.681561\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.681561\n",
      "exploration/env_infos/final/reward_run Min             -0.681561\n",
      "exploration/env_infos/initial/reward_run Mean          -0.528978\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.528978\n",
      "exploration/env_infos/initial/reward_run Min           -0.528978\n",
      "exploration/env_infos/reward_run Mean                  -0.172812\n",
      "exploration/env_infos/reward_run Std                    0.43366\n",
      "exploration/env_infos/reward_run Max                    0.999391\n",
      "exploration/env_infos/reward_run Min                   -1.47832\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.494487\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.494487\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.494487\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.246617\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.246617\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.246617\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.316418\n",
      "exploration/env_infos/reward_ctrl Std                   0.0752573\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0979362\n",
      "exploration/env_infos/reward_ctrl Min                  -0.526892\n",
      "exploration/env_infos/final/height Mean                -0.122873\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.122873\n",
      "exploration/env_infos/final/height Min                 -0.122873\n",
      "exploration/env_infos/initial/height Mean              -0.0438475\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0438475\n",
      "exploration/env_infos/initial/height Min               -0.0438475\n",
      "exploration/env_infos/height Mean                      -0.11995\n",
      "exploration/env_infos/height Std                        0.0562508\n",
      "exploration/env_infos/height Max                        0.104307\n",
      "exploration/env_infos/height Min                       -0.252792\n",
      "exploration/env_infos/final/reward_angular Mean        -0.180683\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.180683\n",
      "exploration/env_infos/final/reward_angular Min         -0.180683\n",
      "exploration/env_infos/initial/reward_angular Mean       1.56866\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.56866\n",
      "exploration/env_infos/initial/reward_angular Min        1.56866\n",
      "exploration/env_infos/reward_angular Mean               0.00536039\n",
      "exploration/env_infos/reward_angular Std                0.979817\n",
      "exploration/env_infos/reward_angular Max                3.57142\n",
      "exploration/env_infos/reward_angular Min               -3.2889\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.231325\n",
      "evaluation/Rewards Std                                  0.810949\n",
      "evaluation/Rewards Max                                  4.79461\n",
      "evaluation/Rewards Min                                 -5.49963\n",
      "evaluation/Returns Mean                              -231.325\n",
      "evaluation/Returns Std                                110.894\n",
      "evaluation/Returns Max                                  1.89699\n",
      "evaluation/Returns Min                               -431.176\n",
      "evaluation/Actions Mean                                -0.205304\n",
      "evaluation/Actions Std                                  0.756464\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -231.325\n",
      "evaluation/env_infos/final/reward_run Mean              0.119335\n",
      "evaluation/env_infos/final/reward_run Std               0.463202\n",
      "evaluation/env_infos/final/reward_run Max               1.64033\n",
      "evaluation/env_infos/final/reward_run Min              -0.554526\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.278742\n",
      "evaluation/env_infos/initial/reward_run Std             0.419446\n",
      "evaluation/env_infos/initial/reward_run Max             0.505995\n",
      "evaluation/env_infos/initial/reward_run Min            -0.814067\n",
      "evaluation/env_infos/reward_run Mean                   -0.138816\n",
      "evaluation/env_infos/reward_run Std                     0.561442\n",
      "evaluation/env_infos/reward_run Max                     2.65928\n",
      "evaluation/env_infos/reward_run Min                    -2.81032\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.367735\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0917364\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.183975\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.498221\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277982\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.101132\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0979714\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.463211\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.368632\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0909425\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.063856\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.589893\n",
      "evaluation/env_infos/final/height Mean                 -0.268253\n",
      "evaluation/env_infos/final/height Std                   0.161413\n",
      "evaluation/env_infos/final/height Max                   0.00206512\n",
      "evaluation/env_infos/final/height Min                  -0.576714\n",
      "evaluation/env_infos/initial/height Mean                0.000446859\n",
      "evaluation/env_infos/initial/height Std                 0.0607306\n",
      "evaluation/env_infos/initial/height Max                 0.0943862\n",
      "evaluation/env_infos/initial/height Min                -0.100167\n",
      "evaluation/env_infos/height Mean                       -0.247523\n",
      "evaluation/env_infos/height Std                         0.1617\n",
      "evaluation/env_infos/height Max                         0.38936\n",
      "evaluation/env_infos/height Min                        -0.59132\n",
      "evaluation/env_infos/final/reward_angular Mean          0.442644\n",
      "evaluation/env_infos/final/reward_angular Std           1.04162\n",
      "evaluation/env_infos/final/reward_angular Max           3.69511\n",
      "evaluation/env_infos/final/reward_angular Min          -1.09491\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.584376\n",
      "evaluation/env_infos/initial/reward_angular Std         1.05961\n",
      "evaluation/env_infos/initial/reward_angular Max         3.59759\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.30896\n",
      "evaluation/env_infos/reward_angular Mean                0.033407\n",
      "evaluation/env_infos/reward_angular Std                 1.08981\n",
      "evaluation/env_infos/reward_angular Max                 6.55845\n",
      "evaluation/env_infos/reward_angular Min                -5.83311\n",
      "time/data storing (s)                                   0.0167119\n",
      "time/evaluation sampling (s)                           22.5523\n",
      "time/exploration sampling (s)                           1.17512\n",
      "time/logging (s)                                        0.247423\n",
      "time/saving (s)                                         0.0298928\n",
      "time/training (s)                                       3.9981\n",
      "time/epoch (s)                                         28.0196\n",
      "time/total (s)                                        703.669\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:24:13.935397 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  21000\n",
      "trainer/QF1 Loss                                        0.436807\n",
      "trainer/QF2 Loss                                        0.521447\n",
      "trainer/Policy Loss                                     1.77432\n",
      "trainer/Q1 Predictions Mean                             2.3294\n",
      "trainer/Q1 Predictions Std                              1.87462\n",
      "trainer/Q1 Predictions Max                             14.3833\n",
      "trainer/Q1 Predictions Min                             -1.09265\n",
      "trainer/Q2 Predictions Mean                             2.45771\n",
      "trainer/Q2 Predictions Std                              1.92708\n",
      "trainer/Q2 Predictions Max                             15.2759\n",
      "trainer/Q2 Predictions Min                             -1.18769\n",
      "trainer/Q Targets Mean                                  2.34869\n",
      "trainer/Q Targets Std                                   2.02359\n",
      "trainer/Q Targets Max                                  15.8565\n",
      "trainer/Q Targets Min                                  -2.28565\n",
      "trainer/Log Pis Mean                                    4.46473\n",
      "trainer/Log Pis Std                                     5.88842\n",
      "trainer/Log Pis Max                                    24.3851\n",
      "trainer/Log Pis Min                                    -7.14936\n",
      "trainer/Policy mu Mean                                  0.0849247\n",
      "trainer/Policy mu Std                                   1.45607\n",
      "trainer/Policy mu Max                                   4.87764\n",
      "trainer/Policy mu Min                                  -4.89633\n",
      "trainer/Policy log std Mean                            -0.673432\n",
      "trainer/Policy log std Std                              0.347765\n",
      "trainer/Policy log std Max                              0.229307\n",
      "trainer/Policy log std Min                             -2.22811\n",
      "trainer/Alpha                                           0.0121151\n",
      "trainer/Alpha Loss                                     -6.7746\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                            21\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.257542\n",
      "exploration/Rewards Std                                 0.670777\n",
      "exploration/Rewards Max                                 1.63582\n",
      "exploration/Rewards Min                                -2.51882\n",
      "exploration/Returns Mean                             -257.542\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -257.542\n",
      "exploration/Returns Min                              -257.542\n",
      "exploration/Actions Mean                               -0.208541\n",
      "exploration/Actions Std                                 0.646249\n",
      "exploration/Actions Max                                 0.998313\n",
      "exploration/Actions Min                                -0.99999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -257.542\n",
      "exploration/env_infos/final/reward_run Mean            -0.574606\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.574606\n",
      "exploration/env_infos/final/reward_run Min             -0.574606\n",
      "exploration/env_infos/initial/reward_run Mean          -0.334637\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.334637\n",
      "exploration/env_infos/initial/reward_run Min           -0.334637\n",
      "exploration/env_infos/reward_run Mean                  -0.153489\n",
      "exploration/env_infos/reward_run Std                    0.307659\n",
      "exploration/env_infos/reward_run Max                    1.05778\n",
      "exploration/env_infos/reward_run Min                   -1.00856\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.194569\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.194569\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.194569\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.229371\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.229371\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.229371\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.276676\n",
      "exploration/env_infos/reward_ctrl Std                   0.0844834\n",
      "exploration/env_infos/reward_ctrl Max                  -0.078586\n",
      "exploration/env_infos/reward_ctrl Min                  -0.494714\n",
      "exploration/env_infos/final/height Mean                -0.114591\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.114591\n",
      "exploration/env_infos/final/height Min                 -0.114591\n",
      "exploration/env_infos/initial/height Mean               0.0414111\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0414111\n",
      "exploration/env_infos/initial/height Min                0.0414111\n",
      "exploration/env_infos/height Mean                      -0.100016\n",
      "exploration/env_infos/height Std                        0.0463254\n",
      "exploration/env_infos/height Max                        0.0802287\n",
      "exploration/env_infos/height Min                       -0.228638\n",
      "exploration/env_infos/final/reward_angular Mean        -1.65429\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.65429\n",
      "exploration/env_infos/final/reward_angular Min         -1.65429\n",
      "exploration/env_infos/initial/reward_angular Mean       1.21556\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.21556\n",
      "exploration/env_infos/initial/reward_angular Min        1.21556\n",
      "exploration/env_infos/reward_angular Mean               0.000655349\n",
      "exploration/env_infos/reward_angular Std                1.32734\n",
      "exploration/env_infos/reward_angular Max                4.5981\n",
      "exploration/env_infos/reward_angular Min               -3.79753\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.21541\n",
      "evaluation/Rewards Std                                  0.527013\n",
      "evaluation/Rewards Max                                  4.47509\n",
      "evaluation/Rewards Min                                 -4.35837\n",
      "evaluation/Returns Mean                              -215.41\n",
      "evaluation/Returns Std                                112.887\n",
      "evaluation/Returns Max                                -31.4412\n",
      "evaluation/Returns Min                               -523.614\n",
      "evaluation/Actions Mean                                -0.0401253\n",
      "evaluation/Actions Std                                  0.755727\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -215.41\n",
      "evaluation/env_infos/final/reward_run Mean              0.0185559\n",
      "evaluation/env_infos/final/reward_run Std               0.150942\n",
      "evaluation/env_infos/final/reward_run Max               0.300454\n",
      "evaluation/env_infos/final/reward_run Min              -0.303363\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.154648\n",
      "evaluation/env_infos/initial/reward_run Std             0.406419\n",
      "evaluation/env_infos/initial/reward_run Max             0.731329\n",
      "evaluation/env_infos/initial/reward_run Min            -0.709882\n",
      "evaluation/env_infos/reward_run Mean                   -0.00963917\n",
      "evaluation/env_infos/reward_run Std                     0.279684\n",
      "evaluation/env_infos/reward_run Max                     2.67391\n",
      "evaluation/env_infos/reward_run Min                    -1.42309\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.34687\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.111714\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.100479\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.523741\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.248931\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.147688\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.100519\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.554421\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.34364\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115302\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0473678\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.570344\n",
      "evaluation/env_infos/final/height Mean                 -0.209184\n",
      "evaluation/env_infos/final/height Std                   0.124518\n",
      "evaluation/env_infos/final/height Max                  -0.000306302\n",
      "evaluation/env_infos/final/height Min                  -0.576882\n",
      "evaluation/env_infos/initial/height Mean               -0.0114634\n",
      "evaluation/env_infos/initial/height Std                 0.0669072\n",
      "evaluation/env_infos/initial/height Max                 0.0888989\n",
      "evaluation/env_infos/initial/height Min                -0.109504\n",
      "evaluation/env_infos/height Mean                       -0.210141\n",
      "evaluation/env_infos/height Std                         0.119037\n",
      "evaluation/env_infos/height Max                         0.398591\n",
      "evaluation/env_infos/height Min                        -0.579896\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.143832\n",
      "evaluation/env_infos/final/reward_angular Std           0.568293\n",
      "evaluation/env_infos/final/reward_angular Max           0.799204\n",
      "evaluation/env_infos/final/reward_angular Min          -1.60787\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.656172\n",
      "evaluation/env_infos/initial/reward_angular Std         0.795771\n",
      "evaluation/env_infos/initial/reward_angular Max         2.4068\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.05452\n",
      "evaluation/env_infos/reward_angular Mean               -0.00766491\n",
      "evaluation/env_infos/reward_angular Std                 0.71004\n",
      "evaluation/env_infos/reward_angular Max                 6.06751\n",
      "evaluation/env_infos/reward_angular Min                -4.50053\n",
      "time/data storing (s)                                   0.0152914\n",
      "time/evaluation sampling (s)                           22.3627\n",
      "time/exploration sampling (s)                           1.04875\n",
      "time/logging (s)                                        0.241905\n",
      "time/saving (s)                                         0.0287614\n",
      "time/training (s)                                       3.75925\n",
      "time/epoch (s)                                         27.4566\n",
      "time/total (s)                                        731.401\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:24:42.123328 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.621351\n",
      "trainer/QF2 Loss                                        0.726154\n",
      "trainer/Policy Loss                                     1.75518\n",
      "trainer/Q1 Predictions Mean                             2.35087\n",
      "trainer/Q1 Predictions Std                              1.98237\n",
      "trainer/Q1 Predictions Max                             16.4239\n",
      "trainer/Q1 Predictions Min                             -1.34234\n",
      "trainer/Q2 Predictions Mean                             2.4841\n",
      "trainer/Q2 Predictions Std                              1.95332\n",
      "trainer/Q2 Predictions Max                             15.67\n",
      "trainer/Q2 Predictions Min                             -0.781142\n",
      "trainer/Q Targets Mean                                  2.24452\n",
      "trainer/Q Targets Std                                   2.22232\n",
      "trainer/Q Targets Max                                  18.105\n",
      "trainer/Q Targets Min                                  -1.94037\n",
      "trainer/Log Pis Mean                                    4.53351\n",
      "trainer/Log Pis Std                                     4.68863\n",
      "trainer/Log Pis Max                                    22.2281\n",
      "trainer/Log Pis Min                                    -5.94018\n",
      "trainer/Policy mu Mean                                 -0.173528\n",
      "trainer/Policy mu Std                                   1.39042\n",
      "trainer/Policy mu Max                                   4.34086\n",
      "trainer/Policy mu Min                                  -4.83349\n",
      "trainer/Policy log std Mean                            -0.779082\n",
      "trainer/Policy log std Std                              0.382034\n",
      "trainer/Policy log std Max                              0.201679\n",
      "trainer/Policy log std Min                             -2.04343\n",
      "trainer/Alpha                                           0.0112981\n",
      "trainer/Alpha Loss                                     -6.57333\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                            22\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.399756\n",
      "exploration/Rewards Std                                 1.22416\n",
      "exploration/Rewards Max                                 2.47695\n",
      "exploration/Rewards Min                                -4.48399\n",
      "exploration/Returns Mean                             -399.756\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -399.756\n",
      "exploration/Returns Min                              -399.756\n",
      "exploration/Actions Mean                                0.164089\n",
      "exploration/Actions Std                                 0.75272\n",
      "exploration/Actions Max                                 0.999977\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -399.756\n",
      "exploration/env_infos/final/reward_run Mean            -0.98011\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.98011\n",
      "exploration/env_infos/final/reward_run Min             -0.98011\n",
      "exploration/env_infos/initial/reward_run Mean           0.0404402\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0404402\n",
      "exploration/env_infos/initial/reward_run Min            0.0404402\n",
      "exploration/env_infos/reward_run Mean                  -0.287796\n",
      "exploration/env_infos/reward_run Std                    1.06244\n",
      "exploration/env_infos/reward_run Max                    2.85875\n",
      "exploration/env_infos/reward_run Min                   -3.36871\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.442979\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.442979\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.442979\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.244955\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.244955\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.244955\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.356108\n",
      "exploration/env_infos/reward_ctrl Std                   0.110678\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0527161\n",
      "exploration/env_infos/reward_ctrl Min                  -0.586091\n",
      "exploration/env_infos/final/height Mean                -0.207615\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.207615\n",
      "exploration/env_infos/final/height Min                 -0.207615\n",
      "exploration/env_infos/initial/height Mean               0.0503674\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0503674\n",
      "exploration/env_infos/initial/height Min                0.0503674\n",
      "exploration/env_infos/height Mean                      -0.395871\n",
      "exploration/env_infos/height Std                        0.17304\n",
      "exploration/env_infos/height Max                        0.42409\n",
      "exploration/env_infos/height Min                       -0.585399\n",
      "exploration/env_infos/final/reward_angular Mean        -1.01369\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.01369\n",
      "exploration/env_infos/final/reward_angular Min         -1.01369\n",
      "exploration/env_infos/initial/reward_angular Mean       0.899193\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.899193\n",
      "exploration/env_infos/initial/reward_angular Min        0.899193\n",
      "exploration/env_infos/reward_angular Mean               0.117676\n",
      "exploration/env_infos/reward_angular Std                2.12067\n",
      "exploration/env_infos/reward_angular Max                5.92263\n",
      "exploration/env_infos/reward_angular Min               -8.01585\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.203755\n",
      "evaluation/Rewards Std                                  0.808489\n",
      "evaluation/Rewards Max                                  5.15279\n",
      "evaluation/Rewards Min                                 -4.4219\n",
      "evaluation/Returns Mean                              -203.755\n",
      "evaluation/Returns Std                                128.746\n",
      "evaluation/Returns Max                                 92.5509\n",
      "evaluation/Returns Min                               -497.388\n",
      "evaluation/Actions Mean                                -0.119613\n",
      "evaluation/Actions Std                                  0.7551\n",
      "evaluation/Actions Max                                  0.999993\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -203.755\n",
      "evaluation/env_infos/final/reward_run Mean             -0.306353\n",
      "evaluation/env_infos/final/reward_run Std               0.615144\n",
      "evaluation/env_infos/final/reward_run Max               0.746593\n",
      "evaluation/env_infos/final/reward_run Min              -2.09254\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.347741\n",
      "evaluation/env_infos/initial/reward_run Std             0.335377\n",
      "evaluation/env_infos/initial/reward_run Max             0.199925\n",
      "evaluation/env_infos/initial/reward_run Min            -0.932611\n",
      "evaluation/env_infos/reward_run Mean                   -0.129599\n",
      "evaluation/env_infos/reward_run Std                     0.630081\n",
      "evaluation/env_infos/reward_run Max                     3.34671\n",
      "evaluation/env_infos/reward_run Min                    -3.27541\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.351775\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.116253\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.121461\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.58303\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.261792\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0864769\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0742258\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.456734\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.35069\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106588\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0491232\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593903\n",
      "evaluation/env_infos/final/height Mean                 -0.282584\n",
      "evaluation/env_infos/final/height Std                   0.15847\n",
      "evaluation/env_infos/final/height Max                  -0.119693\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.014225\n",
      "evaluation/env_infos/initial/height Std                 0.047496\n",
      "evaluation/env_infos/initial/height Max                 0.0722216\n",
      "evaluation/env_infos/initial/height Min                -0.105331\n",
      "evaluation/env_infos/height Mean                       -0.27187\n",
      "evaluation/env_infos/height Std                         0.173361\n",
      "evaluation/env_infos/height Max                         0.624854\n",
      "evaluation/env_infos/height Min                        -0.588707\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0350695\n",
      "evaluation/env_infos/final/reward_angular Std           1.31797\n",
      "evaluation/env_infos/final/reward_angular Max           4.55767\n",
      "evaluation/env_infos/final/reward_angular Min          -2.09274\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.898558\n",
      "evaluation/env_infos/initial/reward_angular Std         1.25982\n",
      "evaluation/env_infos/initial/reward_angular Max         2.9525\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.43026\n",
      "evaluation/env_infos/reward_angular Mean                0.0547172\n",
      "evaluation/env_infos/reward_angular Std                 1.20176\n",
      "evaluation/env_infos/reward_angular Max                 9.3661\n",
      "evaluation/env_infos/reward_angular Min                -5.9011\n",
      "time/data storing (s)                                   0.0160534\n",
      "time/evaluation sampling (s)                           22.3561\n",
      "time/exploration sampling (s)                           1.14801\n",
      "time/logging (s)                                        0.248072\n",
      "time/saving (s)                                         0.0300918\n",
      "time/training (s)                                       4.14867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/epoch (s)                                         27.947\n",
      "time/total (s)                                        759.594\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  --------------\n",
      "2021-05-25 19:25:10.282663 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  23000\n",
      "trainer/QF1 Loss                                        1.07991\n",
      "trainer/QF2 Loss                                        1.02806\n",
      "trainer/Policy Loss                                     1.91839\n",
      "trainer/Q1 Predictions Mean                             2.66694\n",
      "trainer/Q1 Predictions Std                              2.36408\n",
      "trainer/Q1 Predictions Max                             14.2191\n",
      "trainer/Q1 Predictions Min                             -1.30877\n",
      "trainer/Q2 Predictions Mean                             2.85456\n",
      "trainer/Q2 Predictions Std                              2.47422\n",
      "trainer/Q2 Predictions Max                             18.1707\n",
      "trainer/Q2 Predictions Min                             -1.02698\n",
      "trainer/Q Targets Mean                                  2.76361\n",
      "trainer/Q Targets Std                                   2.71307\n",
      "trainer/Q Targets Max                                  23.025\n",
      "trainer/Q Targets Min                                  -1.4779\n",
      "trainer/Log Pis Mean                                    4.96021\n",
      "trainer/Log Pis Std                                     4.80594\n",
      "trainer/Log Pis Max                                    19.5297\n",
      "trainer/Log Pis Min                                    -5.6464\n",
      "trainer/Policy mu Mean                                 -0.118715\n",
      "trainer/Policy mu Std                                   1.42802\n",
      "trainer/Policy mu Max                                   3.93202\n",
      "trainer/Policy mu Min                                  -5.17388\n",
      "trainer/Policy log std Mean                            -0.812576\n",
      "trainer/Policy log std Std                              0.397634\n",
      "trainer/Policy log std Max                              0.136171\n",
      "trainer/Policy log std Min                             -2.44781\n",
      "trainer/Alpha                                           0.0107261\n",
      "trainer/Alpha Loss                                     -4.71502\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                            23\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.157957\n",
      "exploration/Rewards Std                                 1.64341\n",
      "exploration/Rewards Max                                 7.52006\n",
      "exploration/Rewards Min                                -4.88119\n",
      "exploration/Returns Mean                             -157.957\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -157.957\n",
      "exploration/Returns Min                              -157.957\n",
      "exploration/Actions Mean                                0.293619\n",
      "exploration/Actions Std                                 0.736523\n",
      "exploration/Actions Max                                 0.999876\n",
      "exploration/Actions Min                                -0.999998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -157.957\n",
      "exploration/env_infos/final/reward_run Mean            -0.165167\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.165167\n",
      "exploration/env_infos/final/reward_run Min             -0.165167\n",
      "exploration/env_infos/initial/reward_run Mean          -0.18204\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.18204\n",
      "exploration/env_infos/initial/reward_run Min           -0.18204\n",
      "exploration/env_infos/reward_run Mean                   0.00187554\n",
      "exploration/env_infos/reward_run Std                    0.643791\n",
      "exploration/env_infos/reward_run Max                    1.93889\n",
      "exploration/env_infos/reward_run Min                   -1.86057\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.447765\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.447765\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.447765\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.367306\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.367306\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.367306\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.377207\n",
      "exploration/env_infos/reward_ctrl Std                   0.0655322\n",
      "exploration/env_infos/reward_ctrl Max                  -0.183661\n",
      "exploration/env_infos/reward_ctrl Min                  -0.566934\n",
      "exploration/env_infos/final/height Mean                -0.447499\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.447499\n",
      "exploration/env_infos/final/height Min                 -0.447499\n",
      "exploration/env_infos/initial/height Mean              -0.019461\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.019461\n",
      "exploration/env_infos/initial/height Min               -0.019461\n",
      "exploration/env_infos/height Mean                      -0.487508\n",
      "exploration/env_infos/height Std                        0.109345\n",
      "exploration/env_infos/height Max                        0.257868\n",
      "exploration/env_infos/height Min                       -0.588849\n",
      "exploration/env_infos/final/reward_angular Mean        -1.1689\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.1689\n",
      "exploration/env_infos/final/reward_angular Min         -1.1689\n",
      "exploration/env_infos/initial/reward_angular Mean       1.17359\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.17359\n",
      "exploration/env_infos/initial/reward_angular Min        1.17359\n",
      "exploration/env_infos/reward_angular Mean               0.100123\n",
      "exploration/env_infos/reward_angular Std                1.82362\n",
      "exploration/env_infos/reward_angular Max                8.42722\n",
      "exploration/env_infos/reward_angular Min               -5.1543\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.295078\n",
      "evaluation/Rewards Std                                  0.99594\n",
      "evaluation/Rewards Max                                  9.00817\n",
      "evaluation/Rewards Min                                 -5.45378\n",
      "evaluation/Returns Mean                              -295.078\n",
      "evaluation/Returns Std                                143.011\n",
      "evaluation/Returns Max                                -79.5771\n",
      "evaluation/Returns Min                               -618.677\n",
      "evaluation/Actions Mean                                 0.0667254\n",
      "evaluation/Actions Std                                  0.821235\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -295.078\n",
      "evaluation/env_infos/final/reward_run Mean              0.0388541\n",
      "evaluation/env_infos/final/reward_run Std               0.395896\n",
      "evaluation/env_infos/final/reward_run Max               1.16256\n",
      "evaluation/env_infos/final/reward_run Min              -0.987896\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.30444\n",
      "evaluation/env_infos/initial/reward_run Std             0.322108\n",
      "evaluation/env_infos/initial/reward_run Max             0.431989\n",
      "evaluation/env_infos/initial/reward_run Min            -0.714365\n",
      "evaluation/env_infos/reward_run Mean                   -0.0573505\n",
      "evaluation/env_infos/reward_run Std                     0.494668\n",
      "evaluation/env_infos/reward_run Max                     3.08661\n",
      "evaluation/env_infos/reward_run Min                    -2.17687\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.427017\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.107568\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.244385\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.595931\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.236528\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.083548\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0867929\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.393396\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.407327\n",
      "evaluation/env_infos/reward_ctrl Std                    0.116363\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0209924\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598165\n",
      "evaluation/env_infos/final/height Mean                 -0.437381\n",
      "evaluation/env_infos/final/height Std                   0.167973\n",
      "evaluation/env_infos/final/height Max                  -0.0224501\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00814096\n",
      "evaluation/env_infos/initial/height Std                 0.0637073\n",
      "evaluation/env_infos/initial/height Max                 0.0968719\n",
      "evaluation/env_infos/initial/height Min                -0.0853796\n",
      "evaluation/env_infos/height Mean                       -0.390841\n",
      "evaluation/env_infos/height Std                         0.196805\n",
      "evaluation/env_infos/height Max                         0.370754\n",
      "evaluation/env_infos/height Min                        -0.597025\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.118992\n",
      "evaluation/env_infos/final/reward_angular Std           1.86339\n",
      "evaluation/env_infos/final/reward_angular Max           5.2115\n",
      "evaluation/env_infos/final/reward_angular Min          -3.26916\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.285574\n",
      "evaluation/env_infos/initial/reward_angular Std         0.92475\n",
      "evaluation/env_infos/initial/reward_angular Max         1.70338\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.49364\n",
      "evaluation/env_infos/reward_angular Mean                0.0243505\n",
      "evaluation/env_infos/reward_angular Std                 1.47424\n",
      "evaluation/env_infos/reward_angular Max                10.8937\n",
      "evaluation/env_infos/reward_angular Min                -5.87882\n",
      "time/data storing (s)                                   0.0154511\n",
      "time/evaluation sampling (s)                           22.9596\n",
      "time/exploration sampling (s)                           0.98055\n",
      "time/logging (s)                                        0.237036\n",
      "time/saving (s)                                         0.0281131\n",
      "time/training (s)                                       3.65687\n",
      "time/epoch (s)                                         27.8776\n",
      "time/total (s)                                        787.741\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:25:36.611372 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                  24000\n",
      "trainer/QF1 Loss                                        0.797659\n",
      "trainer/QF2 Loss                                        0.620605\n",
      "trainer/Policy Loss                                     2.98827\n",
      "trainer/Q1 Predictions Mean                             2.70381\n",
      "trainer/Q1 Predictions Std                              2.14255\n",
      "trainer/Q1 Predictions Max                             12.5675\n",
      "trainer/Q1 Predictions Min                             -1.41773\n",
      "trainer/Q2 Predictions Mean                             2.43163\n",
      "trainer/Q2 Predictions Std                              2.06937\n",
      "trainer/Q2 Predictions Max                             11.0782\n",
      "trainer/Q2 Predictions Min                             -1.80411\n",
      "trainer/Q Targets Mean                                  2.42676\n",
      "trainer/Q Targets Std                                   2.15264\n",
      "trainer/Q Targets Max                                  12.3704\n",
      "trainer/Q Targets Min                                  -1.86118\n",
      "trainer/Log Pis Mean                                    5.88798\n",
      "trainer/Log Pis Std                                     4.70967\n",
      "trainer/Log Pis Max                                    19.3487\n",
      "trainer/Log Pis Min                                    -5.74792\n",
      "trainer/Policy mu Mean                                 -0.0552299\n",
      "trainer/Policy mu Std                                   1.52414\n",
      "trainer/Policy mu Max                                   3.78991\n",
      "trainer/Policy mu Min                                  -5.69643\n",
      "trainer/Policy log std Mean                            -0.794799\n",
      "trainer/Policy log std Std                              0.437647\n",
      "trainer/Policy log std Max                              0.56031\n",
      "trainer/Policy log std Min                             -2.25573\n",
      "trainer/Alpha                                           0.0103891\n",
      "trainer/Alpha Loss                                     -0.511577\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                            24\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.10084\n",
      "exploration/Rewards Std                                 0.520763\n",
      "exploration/Rewards Max                                 1.4556\n",
      "exploration/Rewards Min                                -1.59373\n",
      "exploration/Returns Mean                             -100.84\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -100.84\n",
      "exploration/Returns Min                              -100.84\n",
      "exploration/Actions Mean                               -0.264117\n",
      "exploration/Actions Std                                 0.736264\n",
      "exploration/Actions Max                                 0.999351\n",
      "exploration/Actions Min                                -0.999998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -100.84\n",
      "exploration/env_infos/final/reward_run Mean            -0.496968\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.496968\n",
      "exploration/env_infos/final/reward_run Min             -0.496968\n",
      "exploration/env_infos/initial/reward_run Mean          -0.261013\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.261013\n",
      "exploration/env_infos/initial/reward_run Min           -0.261013\n",
      "exploration/env_infos/reward_run Mean                  -0.187104\n",
      "exploration/env_infos/reward_run Std                    0.631207\n",
      "exploration/env_infos/reward_run Max                    1.54841\n",
      "exploration/env_infos/reward_run Min                   -2.02974\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.348511\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.348511\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.348511\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.331234\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.331234\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.331234\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.367105\n",
      "exploration/env_infos/reward_ctrl Std                   0.0594124\n",
      "exploration/env_infos/reward_ctrl Max                  -0.109547\n",
      "exploration/env_infos/reward_ctrl Min                  -0.53544\n",
      "exploration/env_infos/final/height Mean                -0.176826\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.176826\n",
      "exploration/env_infos/final/height Min                 -0.176826\n",
      "exploration/env_infos/initial/height Mean              -0.013719\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.013719\n",
      "exploration/env_infos/initial/height Min               -0.013719\n",
      "exploration/env_infos/height Mean                      -0.159471\n",
      "exploration/env_infos/height Std                        0.0407201\n",
      "exploration/env_infos/height Max                       -0.013719\n",
      "exploration/env_infos/height Min                       -0.25412\n",
      "exploration/env_infos/final/reward_angular Mean         1.42522\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.42522\n",
      "exploration/env_infos/final/reward_angular Min          1.42522\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0221698\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0221698\n",
      "exploration/env_infos/initial/reward_angular Min        0.0221698\n",
      "exploration/env_infos/reward_angular Mean               0.046803\n",
      "exploration/env_infos/reward_angular Std                1.15831\n",
      "exploration/env_infos/reward_angular Max                3.35391\n",
      "exploration/env_infos/reward_angular Min               -3.3946\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.209498\n",
      "evaluation/Rewards Std                                  1.01018\n",
      "evaluation/Rewards Max                                  7.89375\n",
      "evaluation/Rewards Min                                 -5.49726\n",
      "evaluation/Returns Mean                              -209.498\n",
      "evaluation/Returns Std                                144.27\n",
      "evaluation/Returns Max                                137.257\n",
      "evaluation/Returns Min                               -478.611\n",
      "evaluation/Actions Mean                                 0.0412971\n",
      "evaluation/Actions Std                                  0.756095\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -209.498\n",
      "evaluation/env_infos/final/reward_run Mean             -0.142813\n",
      "evaluation/env_infos/final/reward_run Std               0.701537\n",
      "evaluation/env_infos/final/reward_run Max               1.63511\n",
      "evaluation/env_infos/final/reward_run Min              -1.73139\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.200751\n",
      "evaluation/env_infos/initial/reward_run Std             0.396649\n",
      "evaluation/env_infos/initial/reward_run Max             0.67933\n",
      "evaluation/env_infos/initial/reward_run Min            -0.80121\n",
      "evaluation/env_infos/reward_run Mean                   -0.106696\n",
      "evaluation/env_infos/reward_run Std                     0.521394\n",
      "evaluation/env_infos/reward_run Max                     3.70901\n",
      "evaluation/env_infos/reward_run Min                    -2.56693\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.352965\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0941101\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.103169\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.508736\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.246376\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0788899\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.081489\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.367301\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.344031\n",
      "evaluation/env_infos/reward_ctrl Std                    0.101708\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0316711\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.581283\n",
      "evaluation/env_infos/final/height Mean                 -0.299614\n",
      "evaluation/env_infos/final/height Std                   0.153777\n",
      "evaluation/env_infos/final/height Max                  -0.149681\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.022619\n",
      "evaluation/env_infos/initial/height Std                 0.0616648\n",
      "evaluation/env_infos/initial/height Max                 0.0830661\n",
      "evaluation/env_infos/initial/height Min                -0.104355\n",
      "evaluation/env_infos/height Mean                       -0.287126\n",
      "evaluation/env_infos/height Std                         0.159215\n",
      "evaluation/env_infos/height Max                         0.3696\n",
      "evaluation/env_infos/height Min                        -0.596426\n",
      "evaluation/env_infos/final/reward_angular Mean          0.613363\n",
      "evaluation/env_infos/final/reward_angular Std           1.63803\n",
      "evaluation/env_infos/final/reward_angular Max           6.59268\n",
      "evaluation/env_infos/final/reward_angular Min          -1.59923\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.576954\n",
      "evaluation/env_infos/initial/reward_angular Std         1.33123\n",
      "evaluation/env_infos/initial/reward_angular Max         3.50664\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.52498\n",
      "evaluation/env_infos/reward_angular Mean                0.0435709\n",
      "evaluation/env_infos/reward_angular Std                 1.37161\n",
      "evaluation/env_infos/reward_angular Max                 9.29049\n",
      "evaluation/env_infos/reward_angular Min                -6.76016\n",
      "time/data storing (s)                                   0.0146752\n",
      "time/evaluation sampling (s)                           21.1086\n",
      "time/exploration sampling (s)                           1.02071\n",
      "time/logging (s)                                        0.237349\n",
      "time/saving (s)                                         0.0271484\n",
      "time/training (s)                                       3.66541\n",
      "time/epoch (s)                                         26.0739\n",
      "time/total (s)                                        814.069\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:03.363816 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  25000\n",
      "trainer/QF1 Loss                                        0.939769\n",
      "trainer/QF2 Loss                                        0.934274\n",
      "trainer/Policy Loss                                     3.64541\n",
      "trainer/Q1 Predictions Mean                             2.31769\n",
      "trainer/Q1 Predictions Std                              2.03781\n",
      "trainer/Q1 Predictions Max                             11.7167\n",
      "trainer/Q1 Predictions Min                             -2.13757\n",
      "trainer/Q2 Predictions Mean                             2.40847\n",
      "trainer/Q2 Predictions Std                              2.01818\n",
      "trainer/Q2 Predictions Max                             10.5292\n",
      "trainer/Q2 Predictions Min                             -1.86639\n",
      "trainer/Q Targets Mean                                  2.39926\n",
      "trainer/Q Targets Std                                   2.29388\n",
      "trainer/Q Targets Max                                  11.7312\n",
      "trainer/Q Targets Min                                  -3.31038\n",
      "trainer/Log Pis Mean                                    6.37368\n",
      "trainer/Log Pis Std                                     4.9612\n",
      "trainer/Log Pis Max                                    24.2193\n",
      "trainer/Log Pis Min                                    -4.78643\n",
      "trainer/Policy mu Mean                                 -0.308172\n",
      "trainer/Policy mu Std                                   1.57196\n",
      "trainer/Policy mu Max                                   4.23337\n",
      "trainer/Policy mu Min                                  -5.18734\n",
      "trainer/Policy log std Mean                            -0.788598\n",
      "trainer/Policy log std Std                              0.372653\n",
      "trainer/Policy log std Max                              0.0567256\n",
      "trainer/Policy log std Min                             -2.04477\n",
      "trainer/Alpha                                           0.010254\n",
      "trainer/Alpha Loss                                      1.71147\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                            25\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.333424\n",
      "exploration/Rewards Std                                 0.995973\n",
      "exploration/Rewards Max                                 2.74121\n",
      "exploration/Rewards Min                                -3.72388\n",
      "exploration/Returns Mean                             -333.424\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -333.424\n",
      "exploration/Returns Min                              -333.424\n",
      "exploration/Actions Mean                               -0.0167449\n",
      "exploration/Actions Std                                 0.747642\n",
      "exploration/Actions Max                                 0.999811\n",
      "exploration/Actions Min                                -0.99999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -333.424\n",
      "exploration/env_infos/final/reward_run Mean            -0.850283\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.850283\n",
      "exploration/env_infos/final/reward_run Min             -0.850283\n",
      "exploration/env_infos/initial/reward_run Mean          -0.20378\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.20378\n",
      "exploration/env_infos/initial/reward_run Min           -0.20378\n",
      "exploration/env_infos/reward_run Mean                  -0.0739785\n",
      "exploration/env_infos/reward_run Std                    0.699963\n",
      "exploration/env_infos/reward_run Max                    1.76101\n",
      "exploration/env_infos/reward_run Min                   -2.07895\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.374624\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.374624\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.374624\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.24568\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.24568\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.24568\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.335549\n",
      "exploration/env_infos/reward_ctrl Std                   0.0888045\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0805516\n",
      "exploration/env_infos/reward_ctrl Min                  -0.579373\n",
      "exploration/env_infos/final/height Mean                -0.523067\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.523067\n",
      "exploration/env_infos/final/height Min                 -0.523067\n",
      "exploration/env_infos/initial/height Mean              -0.0602985\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0602985\n",
      "exploration/env_infos/initial/height Min               -0.0602985\n",
      "exploration/env_infos/height Mean                      -0.427976\n",
      "exploration/env_infos/height Std                        0.189319\n",
      "exploration/env_infos/height Max                        0.246731\n",
      "exploration/env_infos/height Min                       -0.588627\n",
      "exploration/env_infos/final/reward_angular Mean        -1.17892\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.17892\n",
      "exploration/env_infos/final/reward_angular Min         -1.17892\n",
      "exploration/env_infos/initial/reward_angular Mean       1.3135\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.3135\n",
      "exploration/env_infos/initial/reward_angular Min        1.3135\n",
      "exploration/env_infos/reward_angular Mean               0.120895\n",
      "exploration/env_infos/reward_angular Std                1.6214\n",
      "exploration/env_infos/reward_angular Max                5.13598\n",
      "exploration/env_infos/reward_angular Min               -5.56142\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.207735\n",
      "evaluation/Rewards Std                                  0.60668\n",
      "evaluation/Rewards Max                                  4.74117\n",
      "evaluation/Rewards Min                                 -4.80661\n",
      "evaluation/Returns Mean                              -207.735\n",
      "evaluation/Returns Std                                102.995\n",
      "evaluation/Returns Max                                 60.958\n",
      "evaluation/Returns Min                               -506.078\n",
      "evaluation/Actions Mean                                -0.223516\n",
      "evaluation/Actions Std                                  0.721145\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -207.735\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0133568\n",
      "evaluation/env_infos/final/reward_run Std               0.323729\n",
      "evaluation/env_infos/final/reward_run Max               0.80093\n",
      "evaluation/env_infos/final/reward_run Min              -0.856349\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.18354\n",
      "evaluation/env_infos/initial/reward_run Std             0.370572\n",
      "evaluation/env_infos/initial/reward_run Max             0.809094\n",
      "evaluation/env_infos/initial/reward_run Min            -0.81057\n",
      "evaluation/env_infos/reward_run Mean                   -0.0841138\n",
      "evaluation/env_infos/reward_run Std                     0.431945\n",
      "evaluation/env_infos/reward_run Max                     3.20957\n",
      "evaluation/env_infos/reward_run Min                    -2.08994\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.356135\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.117933\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.114102\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.582359\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.26891\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.101577\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0699867\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.512474\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.342006\n",
      "evaluation/env_infos/reward_ctrl Std                    0.120945\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00385128\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.592074\n",
      "evaluation/env_infos/final/height Mean                 -0.290735\n",
      "evaluation/env_infos/final/height Std                   0.142218\n",
      "evaluation/env_infos/final/height Max                  -0.135394\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0105059\n",
      "evaluation/env_infos/initial/height Std                 0.0586363\n",
      "evaluation/env_infos/initial/height Max                 0.0793539\n",
      "evaluation/env_infos/initial/height Min                -0.112474\n",
      "evaluation/env_infos/height Mean                       -0.272633\n",
      "evaluation/env_infos/height Std                         0.140472\n",
      "evaluation/env_infos/height Max                         0.36669\n",
      "evaluation/env_infos/height Min                        -0.590801\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00990603\n",
      "evaluation/env_infos/final/reward_angular Std           0.717161\n",
      "evaluation/env_infos/final/reward_angular Max           1.56301\n",
      "evaluation/env_infos/final/reward_angular Min          -2.05111\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.339495\n",
      "evaluation/env_infos/initial/reward_angular Std         1.04306\n",
      "evaluation/env_infos/initial/reward_angular Max         1.78941\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.95326\n",
      "evaluation/env_infos/reward_angular Mean                0.00945844\n",
      "evaluation/env_infos/reward_angular Std                 0.893958\n",
      "evaluation/env_infos/reward_angular Max                 7.41313\n",
      "evaluation/env_infos/reward_angular Min                -6.7758\n",
      "time/data storing (s)                                   0.0152295\n",
      "time/evaluation sampling (s)                           21.5415\n",
      "time/exploration sampling (s)                           0.984642\n",
      "time/logging (s)                                        0.240375\n",
      "time/saving (s)                                         0.028734\n",
      "time/training (s)                                       3.68051\n",
      "time/epoch (s)                                         26.4909\n",
      "time/total (s)                                        840.823\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:29.879998 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  26000\n",
      "trainer/QF1 Loss                                        0.678115\n",
      "trainer/QF2 Loss                                        0.784148\n",
      "trainer/Policy Loss                                     3.98198\n",
      "trainer/Q1 Predictions Mean                             2.68367\n",
      "trainer/Q1 Predictions Std                              2.63576\n",
      "trainer/Q1 Predictions Max                             19.8014\n",
      "trainer/Q1 Predictions Min                             -2.68074\n",
      "trainer/Q2 Predictions Mean                             2.52786\n",
      "trainer/Q2 Predictions Std                              2.38208\n",
      "trainer/Q2 Predictions Max                             16.4925\n",
      "trainer/Q2 Predictions Min                             -3.07981\n",
      "trainer/Q Targets Mean                                  2.43982\n",
      "trainer/Q Targets Std                                   2.6369\n",
      "trainer/Q Targets Max                                  17.4867\n",
      "trainer/Q Targets Min                                  -4.4081\n",
      "trainer/Log Pis Mean                                    6.90017\n",
      "trainer/Log Pis Std                                     6.13367\n",
      "trainer/Log Pis Max                                    32.8361\n",
      "trainer/Log Pis Min                                    -4.07986\n",
      "trainer/Policy mu Mean                                 -0.400632\n",
      "trainer/Policy mu Std                                   1.60454\n",
      "trainer/Policy mu Max                                   4.25504\n",
      "trainer/Policy mu Min                                  -9.56204\n",
      "trainer/Policy log std Mean                            -0.893936\n",
      "trainer/Policy log std Std                              0.404312\n",
      "trainer/Policy log std Max                             -0.0950342\n",
      "trainer/Policy log std Min                             -2.61237\n",
      "trainer/Alpha                                           0.0102709\n",
      "trainer/Alpha Loss                                      4.12141\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                            26\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.377296\n",
      "exploration/Rewards Std                                 0.106694\n",
      "exploration/Rewards Max                                 0.0971999\n",
      "exploration/Rewards Min                                -0.643904\n",
      "exploration/Returns Mean                             -377.296\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -377.296\n",
      "exploration/Returns Min                              -377.296\n",
      "exploration/Actions Mean                               -0.37785\n",
      "exploration/Actions Std                                 0.752152\n",
      "exploration/Actions Max                                 0.999335\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -377.296\n",
      "exploration/env_infos/final/reward_run Mean             0.0429511\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0429511\n",
      "exploration/env_infos/final/reward_run Min              0.0429511\n",
      "exploration/env_infos/initial/reward_run Mean          -0.250351\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.250351\n",
      "exploration/env_infos/initial/reward_run Min           -0.250351\n",
      "exploration/env_infos/reward_run Mean                  -0.0862586\n",
      "exploration/env_infos/reward_run Std                    0.323698\n",
      "exploration/env_infos/reward_run Max                    1.10695\n",
      "exploration/env_infos/reward_run Min                   -1.58279\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.459995\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.459995\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.459995\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.317716\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.317716\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.317716\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.425102\n",
      "exploration/env_infos/reward_ctrl Std                   0.0638729\n",
      "exploration/env_infos/reward_ctrl Max                  -0.184465\n",
      "exploration/env_infos/reward_ctrl Min                  -0.542743\n",
      "exploration/env_infos/final/height Mean                -0.210003\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.210003\n",
      "exploration/env_infos/final/height Min                 -0.210003\n",
      "exploration/env_infos/initial/height Mean              -0.0369159\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0369159\n",
      "exploration/env_infos/initial/height Min               -0.0369159\n",
      "exploration/env_infos/height Mean                      -0.196997\n",
      "exploration/env_infos/height Std                        0.0242364\n",
      "exploration/env_infos/height Max                       -0.0369159\n",
      "exploration/env_infos/height Min                       -0.234061\n",
      "exploration/env_infos/final/reward_angular Mean        -0.342903\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.342903\n",
      "exploration/env_infos/final/reward_angular Min         -0.342903\n",
      "exploration/env_infos/initial/reward_angular Mean       1.42511\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.42511\n",
      "exploration/env_infos/initial/reward_angular Min        1.42511\n",
      "exploration/env_infos/reward_angular Mean              -0.00995015\n",
      "exploration/env_infos/reward_angular Std                0.759034\n",
      "exploration/env_infos/reward_angular Max                2.66683\n",
      "exploration/env_infos/reward_angular Min               -2.97334\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.239121\n",
      "evaluation/Rewards Std                                  0.727603\n",
      "evaluation/Rewards Max                                  6.69912\n",
      "evaluation/Rewards Min                                 -5.17465\n",
      "evaluation/Returns Mean                              -239.121\n",
      "evaluation/Returns Std                                104.991\n",
      "evaluation/Returns Max                                -31.5788\n",
      "evaluation/Returns Min                               -378.332\n",
      "evaluation/Actions Mean                                -0.171192\n",
      "evaluation/Actions Std                                  0.783327\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -239.121\n",
      "evaluation/env_infos/final/reward_run Mean              0.0817363\n",
      "evaluation/env_infos/final/reward_run Std               0.440377\n",
      "evaluation/env_infos/final/reward_run Max               1.72573\n",
      "evaluation/env_infos/final/reward_run Min              -0.701396\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.411346\n",
      "evaluation/env_infos/initial/reward_run Std             0.318734\n",
      "evaluation/env_infos/initial/reward_run Max             0.614259\n",
      "evaluation/env_infos/initial/reward_run Min            -0.999811\n",
      "evaluation/env_infos/reward_run Mean                   -0.0733212\n",
      "evaluation/env_infos/reward_run Std                     0.401311\n",
      "evaluation/env_infos/reward_run Max                     2.78066\n",
      "evaluation/env_infos/reward_run Min                    -2.1553\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.401849\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.105181\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.164317\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.598138\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.289544\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.109456\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0522531\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.461966\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.385745\n",
      "evaluation/env_infos/reward_ctrl Std                    0.107999\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0165389\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599375\n",
      "evaluation/env_infos/final/height Mean                 -0.311121\n",
      "evaluation/env_infos/final/height Std                   0.138084\n",
      "evaluation/env_infos/final/height Max                  -0.171934\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00711116\n",
      "evaluation/env_infos/initial/height Std                 0.0535485\n",
      "evaluation/env_infos/initial/height Max                 0.077792\n",
      "evaluation/env_infos/initial/height Min                -0.110929\n",
      "evaluation/env_infos/height Mean                       -0.287194\n",
      "evaluation/env_infos/height Std                         0.138496\n",
      "evaluation/env_infos/height Max                         0.37813\n",
      "evaluation/env_infos/height Min                        -0.592971\n",
      "evaluation/env_infos/final/reward_angular Mean          0.311213\n",
      "evaluation/env_infos/final/reward_angular Std           1.19834\n",
      "evaluation/env_infos/final/reward_angular Max           5.21896\n",
      "evaluation/env_infos/final/reward_angular Min          -1.70217\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.883424\n",
      "evaluation/env_infos/initial/reward_angular Std         0.857867\n",
      "evaluation/env_infos/initial/reward_angular Max         2.24474\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.97729\n",
      "evaluation/env_infos/reward_angular Mean                0.00778162\n",
      "evaluation/env_infos/reward_angular Std                 1.10387\n",
      "evaluation/env_infos/reward_angular Max                 8.87256\n",
      "evaluation/env_infos/reward_angular Min                -6.45566\n",
      "time/data storing (s)                                   0.0138368\n",
      "time/evaluation sampling (s)                           21.2712\n",
      "time/exploration sampling (s)                           1.04027\n",
      "time/logging (s)                                        0.235921\n",
      "time/saving (s)                                         0.0270128\n",
      "time/training (s)                                       3.67446\n",
      "time/epoch (s)                                         26.2627\n",
      "time/total (s)                                        867.335\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:26:56.418828 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  27000\n",
      "trainer/QF1 Loss                                        0.718295\n",
      "trainer/QF2 Loss                                        0.73697\n",
      "trainer/Policy Loss                                     4.04073\n",
      "trainer/Q1 Predictions Mean                             2.57968\n",
      "trainer/Q1 Predictions Std                              2.25348\n",
      "trainer/Q1 Predictions Max                             10.9301\n",
      "trainer/Q1 Predictions Min                             -2.96758\n",
      "trainer/Q2 Predictions Mean                             2.58155\n",
      "trainer/Q2 Predictions Std                              2.27562\n",
      "trainer/Q2 Predictions Max                             10.7304\n",
      "trainer/Q2 Predictions Min                             -3.00815\n",
      "trainer/Q Targets Mean                                  2.44929\n",
      "trainer/Q Targets Std                                   2.26606\n",
      "trainer/Q Targets Max                                   9.46502\n",
      "trainer/Q Targets Min                                  -3.37346\n",
      "trainer/Log Pis Mean                                    6.96483\n",
      "trainer/Log Pis Std                                     6.12208\n",
      "trainer/Log Pis Max                                    46.5582\n",
      "trainer/Log Pis Min                                    -3.92221\n",
      "trainer/Policy mu Mean                                  0.288242\n",
      "trainer/Policy mu Std                                   1.67031\n",
      "trainer/Policy mu Max                                   8.85311\n",
      "trainer/Policy mu Min                                  -5.27753\n",
      "trainer/Policy log std Mean                            -0.779638\n",
      "trainer/Policy log std Std                              0.379747\n",
      "trainer/Policy log std Max                              0.0667573\n",
      "trainer/Policy log std Min                             -2.3039\n",
      "trainer/Alpha                                           0.0104663\n",
      "trainer/Alpha Loss                                      4.39942\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                            27\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0591115\n",
      "exploration/Rewards Std                                 1.07169\n",
      "exploration/Rewards Max                                 3.66096\n",
      "exploration/Rewards Min                                -3.4943\n",
      "exploration/Returns Mean                              -59.1115\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -59.1115\n",
      "exploration/Returns Min                               -59.1115\n",
      "exploration/Actions Mean                                0.0502161\n",
      "exploration/Actions Std                                 0.7782\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999994\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -59.1115\n",
      "exploration/env_infos/final/reward_run Mean            -0.367034\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.367034\n",
      "exploration/env_infos/final/reward_run Min             -0.367034\n",
      "exploration/env_infos/initial/reward_run Mean          -0.101489\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.101489\n",
      "exploration/env_infos/initial/reward_run Min           -0.101489\n",
      "exploration/env_infos/reward_run Mean                  -0.15139\n",
      "exploration/env_infos/reward_run Std                    0.587013\n",
      "exploration/env_infos/reward_run Max                    1.96819\n",
      "exploration/env_infos/reward_run Min                   -2.62814\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.333451\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.333451\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.333451\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.371975\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.371975\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.371975\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.36487\n",
      "exploration/env_infos/reward_ctrl Std                   0.0778619\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0960354\n",
      "exploration/env_infos/reward_ctrl Min                  -0.599029\n",
      "exploration/env_infos/final/height Mean                -0.547884\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.547884\n",
      "exploration/env_infos/final/height Min                 -0.547884\n",
      "exploration/env_infos/initial/height Mean               0.0981548\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0981548\n",
      "exploration/env_infos/initial/height Min                0.0981548\n",
      "exploration/env_infos/height Mean                      -0.456792\n",
      "exploration/env_infos/height Std                        0.192966\n",
      "exploration/env_infos/height Max                        0.423661\n",
      "exploration/env_infos/height Min                       -0.59329\n",
      "exploration/env_infos/final/reward_angular Mean        -1.99466\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.99466\n",
      "exploration/env_infos/final/reward_angular Min         -1.99466\n",
      "exploration/env_infos/initial/reward_angular Mean       0.957357\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.957357\n",
      "exploration/env_infos/initial/reward_angular Min        0.957357\n",
      "exploration/env_infos/reward_angular Mean               0.119199\n",
      "exploration/env_infos/reward_angular Std                1.38823\n",
      "exploration/env_infos/reward_angular Max                5.14453\n",
      "exploration/env_infos/reward_angular Min               -4.38844\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.298999\n",
      "evaluation/Rewards Std                                  0.723562\n",
      "evaluation/Rewards Max                                  4.82661\n",
      "evaluation/Rewards Min                                 -6.02668\n",
      "evaluation/Returns Mean                              -298.999\n",
      "evaluation/Returns Std                                172.859\n",
      "evaluation/Returns Max                                 81.9159\n",
      "evaluation/Returns Min                               -693.776\n",
      "evaluation/Actions Mean                                 0.098728\n",
      "evaluation/Actions Std                                  0.808606\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -298.999\n",
      "evaluation/env_infos/final/reward_run Mean              0.0207477\n",
      "evaluation/env_infos/final/reward_run Std               0.14022\n",
      "evaluation/env_infos/final/reward_run Max               0.51611\n",
      "evaluation/env_infos/final/reward_run Min              -0.272947\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.00892762\n",
      "evaluation/env_infos/initial/reward_run Std             0.556821\n",
      "evaluation/env_infos/initial/reward_run Max             1.10213\n",
      "evaluation/env_infos/initial/reward_run Min            -0.758548\n",
      "evaluation/env_infos/reward_run Mean                    0.0023685\n",
      "evaluation/env_infos/reward_run Std                     0.400542\n",
      "evaluation/env_infos/reward_run Max                     2.98999\n",
      "evaluation/env_infos/reward_run Min                    -3.73972\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.411013\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.111632\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.278335\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.599493\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.2952\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.085771\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0526175\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.423577\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.398155\n",
      "evaluation/env_infos/reward_ctrl Std                    0.1142\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0212053\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599862\n",
      "evaluation/env_infos/final/height Mean                 -0.410173\n",
      "evaluation/env_infos/final/height Std                   0.187984\n",
      "evaluation/env_infos/final/height Max                   0.06491\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.0067511\n",
      "evaluation/env_infos/initial/height Std                 0.0442264\n",
      "evaluation/env_infos/initial/height Max                 0.106552\n",
      "evaluation/env_infos/initial/height Min                -0.0665638\n",
      "evaluation/env_infos/height Mean                       -0.353927\n",
      "evaluation/env_infos/height Std                         0.209424\n",
      "evaluation/env_infos/height Max                         0.520312\n",
      "evaluation/env_infos/height Min                        -0.591736\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.366946\n",
      "evaluation/env_infos/final/reward_angular Std           0.829829\n",
      "evaluation/env_infos/final/reward_angular Max           2.20493e-07\n",
      "evaluation/env_infos/final/reward_angular Min          -3.34709\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.10639\n",
      "evaluation/env_infos/initial/reward_angular Std         0.880963\n",
      "evaluation/env_infos/initial/reward_angular Max         1.26637\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.17293\n",
      "evaluation/env_infos/reward_angular Mean                0.0193695\n",
      "evaluation/env_infos/reward_angular Std                 1.0301\n",
      "evaluation/env_infos/reward_angular Max                 6.42843\n",
      "evaluation/env_infos/reward_angular Min                -6.1594\n",
      "time/data storing (s)                                   0.015241\n",
      "time/evaluation sampling (s)                           21.2683\n",
      "time/exploration sampling (s)                           0.98009\n",
      "time/logging (s)                                        0.239001\n",
      "time/saving (s)                                         0.0310508\n",
      "time/training (s)                                       3.73251\n",
      "time/epoch (s)                                         26.2662\n",
      "time/total (s)                                        893.876\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:27:23.279686 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 26 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  28000\n",
      "trainer/QF1 Loss                                        5.42219\n",
      "trainer/QF2 Loss                                        0.966503\n",
      "trainer/Policy Loss                                     5.91064\n",
      "trainer/Q1 Predictions Mean                             3.26457\n",
      "trainer/Q1 Predictions Std                              4.69526\n",
      "trainer/Q1 Predictions Max                             22.8951\n",
      "trainer/Q1 Predictions Min                             -3.21606\n",
      "trainer/Q2 Predictions Mean                             2.79352\n",
      "trainer/Q2 Predictions Std                              3.50124\n",
      "trainer/Q2 Predictions Max                             17.8547\n",
      "trainer/Q2 Predictions Min                             -2.55606\n",
      "trainer/Q Targets Mean                                  2.74739\n",
      "trainer/Q Targets Std                                   3.12303\n",
      "trainer/Q Targets Max                                  17.6209\n",
      "trainer/Q Targets Min                                  -2.47192\n",
      "trainer/Log Pis Mean                                    9.12745\n",
      "trainer/Log Pis Std                                    12.4988\n",
      "trainer/Log Pis Max                                    64.3034\n",
      "trainer/Log Pis Min                                    -2.95164\n",
      "trainer/Policy mu Mean                                  0.438465\n",
      "trainer/Policy mu Std                                   2.13786\n",
      "trainer/Policy mu Max                                   9.30474\n",
      "trainer/Policy mu Min                                  -8.67411\n",
      "trainer/Policy log std Mean                            -0.826836\n",
      "trainer/Policy log std Std                              0.421751\n",
      "trainer/Policy log std Max                              0.659108\n",
      "trainer/Policy log std Min                             -2.56207\n",
      "trainer/Alpha                                           0.0106986\n",
      "trainer/Alpha Loss                                     14.1931\n",
      "exploration/num steps total                         28000\n",
      "exploration/num paths total                            28\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.780453\n",
      "exploration/Rewards Std                                 0.114908\n",
      "exploration/Rewards Max                                 0.0896367\n",
      "exploration/Rewards Min                                -1.76745\n",
      "exploration/Returns Mean                             -780.453\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -780.453\n",
      "exploration/Returns Min                              -780.453\n",
      "exploration/Actions Mean                                0.627503\n",
      "exploration/Actions Std                                 0.757758\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -780.453\n",
      "exploration/env_infos/final/reward_run Mean             4.4253e-05\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              4.4253e-05\n",
      "exploration/env_infos/final/reward_run Min              4.4253e-05\n",
      "exploration/env_infos/initial/reward_run Mean          -0.593696\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.593696\n",
      "exploration/env_infos/initial/reward_run Min           -0.593696\n",
      "exploration/env_infos/reward_run Mean                  -0.0223491\n",
      "exploration/env_infos/reward_run Std                    0.158362\n",
      "exploration/env_infos/reward_run Max                    0.544393\n",
      "exploration/env_infos/reward_run Min                   -1.35045\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.59669\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.59669\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.59669\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.267515\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.267515\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.267515\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.580774\n",
      "exploration/env_infos/reward_ctrl Std                   0.0641616\n",
      "exploration/env_infos/reward_ctrl Max                  -0.119903\n",
      "exploration/env_infos/reward_ctrl Min                  -0.599139\n",
      "exploration/env_infos/final/height Mean                -0.577283\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577283\n",
      "exploration/env_infos/final/height Min                 -0.577283\n",
      "exploration/env_infos/initial/height Mean               0.0548694\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0548694\n",
      "exploration/env_infos/initial/height Min                0.0548694\n",
      "exploration/env_infos/height Mean                      -0.537702\n",
      "exploration/env_infos/height Std                        0.151941\n",
      "exploration/env_infos/height Max                        0.391418\n",
      "exploration/env_infos/height Min                       -0.577821\n",
      "exploration/env_infos/final/reward_angular Mean         9.39727e-06\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          9.39727e-06\n",
      "exploration/env_infos/final/reward_angular Min          9.39727e-06\n",
      "exploration/env_infos/initial/reward_angular Mean       0.365269\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.365269\n",
      "exploration/env_infos/initial/reward_angular Min        0.365269\n",
      "exploration/env_infos/reward_angular Mean              -0.0602485\n",
      "exploration/env_infos/reward_angular Std                0.405365\n",
      "exploration/env_infos/reward_angular Max                2.33905\n",
      "exploration/env_infos/reward_angular Min               -4.76634\n",
      "evaluation/num steps total                         675000\n",
      "evaluation/num paths total                            675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.351966\n",
      "evaluation/Rewards Std                                  0.910442\n",
      "evaluation/Rewards Max                                  6.44223\n",
      "evaluation/Rewards Min                                 -5.01603\n",
      "evaluation/Returns Mean                              -351.966\n",
      "evaluation/Returns Std                                213.952\n",
      "evaluation/Returns Max                                 22.3846\n",
      "evaluation/Returns Min                               -791.74\n",
      "evaluation/Actions Mean                                 0.347494\n",
      "evaluation/Actions Std                                  0.777212\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -351.966\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0483808\n",
      "evaluation/env_infos/final/reward_run Std               0.332528\n",
      "evaluation/env_infos/final/reward_run Max               1.07951\n",
      "evaluation/env_infos/final/reward_run Min              -1.04161\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.237157\n",
      "evaluation/env_infos/initial/reward_run Std             0.47154\n",
      "evaluation/env_infos/initial/reward_run Max             0.662762\n",
      "evaluation/env_infos/initial/reward_run Min            -0.949873\n",
      "evaluation/env_infos/reward_run Mean                   -0.0523291\n",
      "evaluation/env_infos/reward_run Std                     0.397076\n",
      "evaluation/env_infos/reward_run Max                     2.25931\n",
      "evaluation/env_infos/reward_run Min                    -2.44363\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.449114\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.108243\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.253285\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.590928\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277859\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.095705\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0835878\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.449146\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.434887\n",
      "evaluation/env_infos/reward_ctrl Std                    0.121534\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.046142\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599762\n",
      "evaluation/env_infos/final/height Mean                 -0.510155\n",
      "evaluation/env_infos/final/height Std                   0.0991129\n",
      "evaluation/env_infos/final/height Max                  -0.239897\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00147944\n",
      "evaluation/env_infos/initial/height Std                 0.0528322\n",
      "evaluation/env_infos/initial/height Max                 0.0755732\n",
      "evaluation/env_infos/initial/height Min                -0.0951252\n",
      "evaluation/env_infos/height Mean                       -0.488231\n",
      "evaluation/env_infos/height Std                         0.146813\n",
      "evaluation/env_infos/height Max                         0.549624\n",
      "evaluation/env_infos/height Min                        -0.594519\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0548772\n",
      "evaluation/env_infos/final/reward_angular Std           0.595033\n",
      "evaluation/env_infos/final/reward_angular Max           1.44478\n",
      "evaluation/env_infos/final/reward_angular Min          -2.0175\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.283678\n",
      "evaluation/env_infos/initial/reward_angular Std         0.907942\n",
      "evaluation/env_infos/initial/reward_angular Max         2.02689\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.64014\n",
      "evaluation/env_infos/reward_angular Mean                0.0056221\n",
      "evaluation/env_infos/reward_angular Std                 1.12926\n",
      "evaluation/env_infos/reward_angular Max                 7.69822\n",
      "evaluation/env_infos/reward_angular Min                -5.97065\n",
      "time/data storing (s)                                   0.0155612\n",
      "time/evaluation sampling (s)                           21.2725\n",
      "time/exploration sampling (s)                           1.05128\n",
      "time/logging (s)                                        0.254135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/saving (s)                                         0.0346453\n",
      "time/training (s)                                       3.96088\n",
      "time/epoch (s)                                         26.5891\n",
      "time/total (s)                                        920.751\n",
      "Epoch                                                  26\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 19:27:50.750031 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 27 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  29000\n",
      "trainer/QF1 Loss                                        1.35812\n",
      "trainer/QF2 Loss                                        1.24746\n",
      "trainer/Policy Loss                                     3.9171\n",
      "trainer/Q1 Predictions Mean                             2.89949\n",
      "trainer/Q1 Predictions Std                              3.27278\n",
      "trainer/Q1 Predictions Max                             24.3024\n",
      "trainer/Q1 Predictions Min                             -4.12285\n",
      "trainer/Q2 Predictions Mean                             2.80029\n",
      "trainer/Q2 Predictions Std                              3.2348\n",
      "trainer/Q2 Predictions Max                             24.706\n",
      "trainer/Q2 Predictions Min                             -4.35291\n",
      "trainer/Q Targets Mean                                  2.93654\n",
      "trainer/Q Targets Std                                   3.40428\n",
      "trainer/Q Targets Max                                  26.5542\n",
      "trainer/Q Targets Min                                  -4.23278\n",
      "trainer/Log Pis Mean                                    7.09044\n",
      "trainer/Log Pis Std                                     6.33315\n",
      "trainer/Log Pis Max                                    36.119\n",
      "trainer/Log Pis Min                                    -4.11025\n",
      "trainer/Policy mu Mean                                  0.257764\n",
      "trainer/Policy mu Std                                   1.78235\n",
      "trainer/Policy mu Max                                   9.00217\n",
      "trainer/Policy mu Min                                 -10.0492\n",
      "trainer/Policy log std Mean                            -0.730954\n",
      "trainer/Policy log std Std                              0.513592\n",
      "trainer/Policy log std Max                              2\n",
      "trainer/Policy log std Min                             -2.41448\n",
      "trainer/Alpha                                           0.011412\n",
      "trainer/Alpha Loss                                      4.87848\n",
      "exploration/num steps total                         29000\n",
      "exploration/num paths total                            29\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.270516\n",
      "exploration/Rewards Std                                 0.400154\n",
      "exploration/Rewards Max                                 1.23018\n",
      "exploration/Rewards Min                                -1.45113\n",
      "exploration/Returns Mean                             -270.516\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -270.516\n",
      "exploration/Returns Min                              -270.516\n",
      "exploration/Actions Mean                                0.00313056\n",
      "exploration/Actions Std                                 0.802902\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -270.516\n",
      "exploration/env_infos/final/reward_run Mean             0.435666\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.435666\n",
      "exploration/env_infos/final/reward_run Min              0.435666\n",
      "exploration/env_infos/initial/reward_run Mean          -0.366364\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.366364\n",
      "exploration/env_infos/initial/reward_run Min           -0.366364\n",
      "exploration/env_infos/reward_run Mean                  -0.152981\n",
      "exploration/env_infos/reward_run Std                    0.47611\n",
      "exploration/env_infos/reward_run Max                    1.27604\n",
      "exploration/env_infos/reward_run Min                   -1.67859\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.361598\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.361598\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.361598\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.183219\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.183219\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.183219\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.386797\n",
      "exploration/env_infos/reward_ctrl Std                   0.0796747\n",
      "exploration/env_infos/reward_ctrl Max                  -0.135094\n",
      "exploration/env_infos/reward_ctrl Min                  -0.572349\n",
      "exploration/env_infos/final/height Mean                -0.568144\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.568144\n",
      "exploration/env_infos/final/height Min                 -0.568144\n",
      "exploration/env_infos/initial/height Mean              -0.0338764\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0338764\n",
      "exploration/env_infos/initial/height Min               -0.0338764\n",
      "exploration/env_infos/height Mean                      -0.536003\n",
      "exploration/env_infos/height Std                        0.096828\n",
      "exploration/env_infos/height Max                       -0.0338764\n",
      "exploration/env_infos/height Min                       -0.582361\n",
      "exploration/env_infos/final/reward_angular Mean         0.252808\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.252808\n",
      "exploration/env_infos/final/reward_angular Min          0.252808\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0153571\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0153571\n",
      "exploration/env_infos/initial/reward_angular Min        0.0153571\n",
      "exploration/env_infos/reward_angular Mean              -0.0749949\n",
      "exploration/env_infos/reward_angular Std                0.839471\n",
      "exploration/env_infos/reward_angular Max                4.03801\n",
      "exploration/env_infos/reward_angular Min               -4.40641\n",
      "evaluation/num steps total                         700000\n",
      "evaluation/num paths total                            700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.312045\n",
      "evaluation/Rewards Std                                  0.686887\n",
      "evaluation/Rewards Max                                  4.64593\n",
      "evaluation/Rewards Min                                 -4.25383\n",
      "evaluation/Returns Mean                              -312.045\n",
      "evaluation/Returns Std                                178.413\n",
      "evaluation/Returns Max                                -36.4711\n",
      "evaluation/Returns Min                               -641.397\n",
      "evaluation/Actions Mean                                 0.0498993\n",
      "evaluation/Actions Std                                  0.77074\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -312.045\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00391267\n",
      "evaluation/env_infos/final/reward_run Std               0.289801\n",
      "evaluation/env_infos/final/reward_run Max               0.811142\n",
      "evaluation/env_infos/final/reward_run Min              -1.03517\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0888436\n",
      "evaluation/env_infos/initial/reward_run Std             0.62837\n",
      "evaluation/env_infos/initial/reward_run Max             1.00944\n",
      "evaluation/env_infos/initial/reward_run Min            -0.84222\n",
      "evaluation/env_infos/reward_run Mean                   -0.00979584\n",
      "evaluation/env_infos/reward_run Std                     0.376599\n",
      "evaluation/env_infos/reward_run Max                     3.14221\n",
      "evaluation/env_infos/reward_run Min                    -2.13363\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.357961\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0695704\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.239504\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.474504\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.326718\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.116133\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0924532\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.491695\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.357918\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0781203\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0373086\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593169\n",
      "evaluation/env_infos/final/height Mean                 -0.485032\n",
      "evaluation/env_infos/final/height Std                   0.140761\n",
      "evaluation/env_infos/final/height Max                   0.00675842\n",
      "evaluation/env_infos/final/height Min                  -0.578995\n",
      "evaluation/env_infos/initial/height Mean                0.0143979\n",
      "evaluation/env_infos/initial/height Std                 0.0473521\n",
      "evaluation/env_infos/initial/height Max                 0.0943819\n",
      "evaluation/env_infos/initial/height Min                -0.0663337\n",
      "evaluation/env_infos/height Mean                       -0.452592\n",
      "evaluation/env_infos/height Std                         0.172612\n",
      "evaluation/env_infos/height Max                         0.302037\n",
      "evaluation/env_infos/height Min                        -0.587632\n",
      "evaluation/env_infos/final/reward_angular Mean          0.253916\n",
      "evaluation/env_infos/final/reward_angular Std           0.904197\n",
      "evaluation/env_infos/final/reward_angular Max           3.86833\n",
      "evaluation/env_infos/final/reward_angular Min          -1.80045\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0689153\n",
      "evaluation/env_infos/initial/reward_angular Std         0.640773\n",
      "evaluation/env_infos/initial/reward_angular Max         1.20542\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.48618\n",
      "evaluation/env_infos/reward_angular Mean                0.0112666\n",
      "evaluation/env_infos/reward_angular Std                 0.907624\n",
      "evaluation/env_infos/reward_angular Max                 5.96296\n",
      "evaluation/env_infos/reward_angular Min                -4.76792\n",
      "time/data storing (s)                                   0.0148151\n",
      "time/evaluation sampling (s)                           22.1899\n",
      "time/exploration sampling (s)                           0.989287\n",
      "time/logging (s)                                        0.238244\n",
      "time/saving (s)                                         0.0281957\n",
      "time/training (s)                                       3.68833\n",
      "time/epoch (s)                                         27.1488\n",
      "time/total (s)                                        948.204\n",
      "Epoch                                                  27\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:28:17.374679 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 28 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  30000\n",
      "trainer/QF1 Loss                                        0.68938\n",
      "trainer/QF2 Loss                                        0.663128\n",
      "trainer/Policy Loss                                     3.49424\n",
      "trainer/Q1 Predictions Mean                             3.09761\n",
      "trainer/Q1 Predictions Std                              3.45355\n",
      "trainer/Q1 Predictions Max                             13.8847\n",
      "trainer/Q1 Predictions Min                             -2.94213\n",
      "trainer/Q2 Predictions Mean                             3.07625\n",
      "trainer/Q2 Predictions Std                              3.34856\n",
      "trainer/Q2 Predictions Max                             14.3922\n",
      "trainer/Q2 Predictions Min                             -2.83754\n",
      "trainer/Q Targets Mean                                  3.09131\n",
      "trainer/Q Targets Std                                   3.52214\n",
      "trainer/Q Targets Max                                  14.0534\n",
      "trainer/Q Targets Min                                  -3.60155\n",
      "trainer/Log Pis Mean                                    7.01893\n",
      "trainer/Log Pis Std                                     5.40225\n",
      "trainer/Log Pis Max                                    26.7242\n",
      "trainer/Log Pis Min                                    -3.99733\n",
      "trainer/Policy mu Mean                                  0.3118\n",
      "trainer/Policy mu Std                                   1.58622\n",
      "trainer/Policy mu Max                                   4.84424\n",
      "trainer/Policy mu Min                                  -5.19438\n",
      "trainer/Policy log std Mean                            -0.807998\n",
      "trainer/Policy log std Std                              0.371102\n",
      "trainer/Policy log std Max                             -0.0432604\n",
      "trainer/Policy log std Min                             -2.63052\n",
      "trainer/Alpha                                           0.0121919\n",
      "trainer/Alpha Loss                                      4.49092\n",
      "exploration/num steps total                         30000\n",
      "exploration/num paths total                            30\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0853409\n",
      "exploration/Rewards Std                                 1.69036\n",
      "exploration/Rewards Max                                 5.20133\n",
      "exploration/Rewards Min                                -5.00255\n",
      "exploration/Returns Mean                              -85.3409\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -85.3409\n",
      "exploration/Returns Min                               -85.3409\n",
      "exploration/Actions Mean                                0.181781\n",
      "exploration/Actions Std                                 0.759768\n",
      "exploration/Actions Max                                 0.999976\n",
      "exploration/Actions Min                                -0.99996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -85.3409\n",
      "exploration/env_infos/final/reward_run Mean             0.0298865\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0298865\n",
      "exploration/env_infos/final/reward_run Min              0.0298865\n",
      "exploration/env_infos/initial/reward_run Mean           0.781101\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.781101\n",
      "exploration/env_infos/initial/reward_run Min            0.781101\n",
      "exploration/env_infos/reward_run Mean                  -0.0467785\n",
      "exploration/env_infos/reward_run Std                    0.594053\n",
      "exploration/env_infos/reward_run Max                    1.72175\n",
      "exploration/env_infos/reward_run Min                   -1.72911\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.354529\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.354529\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.354529\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.33832\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.33832\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.33832\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.366175\n",
      "exploration/env_infos/reward_ctrl Std                   0.0854411\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0760745\n",
      "exploration/env_infos/reward_ctrl Min                  -0.582015\n",
      "exploration/env_infos/final/height Mean                -0.469957\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.469957\n",
      "exploration/env_infos/final/height Min                 -0.469957\n",
      "exploration/env_infos/initial/height Mean              -0.0635285\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0635285\n",
      "exploration/env_infos/initial/height Min               -0.0635285\n",
      "exploration/env_infos/height Mean                      -0.487406\n",
      "exploration/env_infos/height Std                        0.119185\n",
      "exploration/env_infos/height Max                        0.270884\n",
      "exploration/env_infos/height Min                       -0.585496\n",
      "exploration/env_infos/final/reward_angular Mean        -2.24122\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.24122\n",
      "exploration/env_infos/final/reward_angular Min         -2.24122\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.44141\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.44141\n",
      "exploration/env_infos/initial/reward_angular Min       -1.44141\n",
      "exploration/env_infos/reward_angular Mean               0.0824514\n",
      "exploration/env_infos/reward_angular Std                1.80745\n",
      "exploration/env_infos/reward_angular Max                5.6141\n",
      "exploration/env_infos/reward_angular Min               -5.14136\n",
      "evaluation/num steps total                         725000\n",
      "evaluation/num paths total                            725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.251144\n",
      "evaluation/Rewards Std                                  0.750217\n",
      "evaluation/Rewards Max                                  5.31629\n",
      "evaluation/Rewards Min                                 -4.60799\n",
      "evaluation/Returns Mean                              -251.144\n",
      "evaluation/Returns Std                                144.05\n",
      "evaluation/Returns Max                                -34.0902\n",
      "evaluation/Returns Min                               -583.963\n",
      "evaluation/Actions Mean                                 0.157948\n",
      "evaluation/Actions Std                                  0.691005\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -0.999965\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -251.144\n",
      "evaluation/env_infos/final/reward_run Mean             -0.107644\n",
      "evaluation/env_infos/final/reward_run Std               0.3051\n",
      "evaluation/env_infos/final/reward_run Max               0.339355\n",
      "evaluation/env_infos/final/reward_run Min              -0.846153\n",
      "evaluation/env_infos/initial/reward_run Mean            0.209097\n",
      "evaluation/env_infos/initial/reward_run Std             0.653342\n",
      "evaluation/env_infos/initial/reward_run Max             1.02617\n",
      "evaluation/env_infos/initial/reward_run Min            -1.04579\n",
      "evaluation/env_infos/reward_run Mean                   -0.0102929\n",
      "evaluation/env_infos/reward_run Std                     0.377777\n",
      "evaluation/env_infos/reward_run Max                     2.31085\n",
      "evaluation/env_infos/reward_run Min                    -1.81129\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.328848\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0816092\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.14511\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.491193\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.326225\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0990549\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0965992\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.474218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.301461\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0829663\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0172048\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.564075\n",
      "evaluation/env_infos/final/height Mean                 -0.416086\n",
      "evaluation/env_infos/final/height Std                   0.161244\n",
      "evaluation/env_infos/final/height Max                  -0.138893\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00459539\n",
      "evaluation/env_infos/initial/height Std                 0.052026\n",
      "evaluation/env_infos/initial/height Max                 0.0896254\n",
      "evaluation/env_infos/initial/height Min                -0.0820844\n",
      "evaluation/env_infos/height Mean                       -0.363608\n",
      "evaluation/env_infos/height Std                         0.191017\n",
      "evaluation/env_infos/height Max                         0.35636\n",
      "evaluation/env_infos/height Min                        -0.597983\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.290605\n",
      "evaluation/env_infos/final/reward_angular Std           1.16172\n",
      "evaluation/env_infos/final/reward_angular Max           0.939648\n",
      "evaluation/env_infos/final/reward_angular Min          -3.8364\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.108244\n",
      "evaluation/env_infos/initial/reward_angular Std         0.921572\n",
      "evaluation/env_infos/initial/reward_angular Max         1.50473\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.32011\n",
      "evaluation/env_infos/reward_angular Mean                0.0147468\n",
      "evaluation/env_infos/reward_angular Std                 0.983666\n",
      "evaluation/env_infos/reward_angular Max                 6.88576\n",
      "evaluation/env_infos/reward_angular Min                -5.69762\n",
      "time/data storing (s)                                   0.0152856\n",
      "time/evaluation sampling (s)                           21.1746\n",
      "time/exploration sampling (s)                           1.00656\n",
      "time/logging (s)                                        0.246021\n",
      "time/saving (s)                                         0.0279525\n",
      "time/training (s)                                       3.86676\n",
      "time/epoch (s)                                         26.3371\n",
      "time/total (s)                                        974.835\n",
      "Epoch                                                  28\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:28:44.000493 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 29 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  31000\n",
      "trainer/QF1 Loss                                        1.11675\n",
      "trainer/QF2 Loss                                        0.900294\n",
      "trainer/Policy Loss                                     3.41042\n",
      "trainer/Q1 Predictions Mean                             2.94946\n",
      "trainer/Q1 Predictions Std                              3.56674\n",
      "trainer/Q1 Predictions Max                             16.4681\n",
      "trainer/Q1 Predictions Min                             -3.34351\n",
      "trainer/Q2 Predictions Mean                             3.14044\n",
      "trainer/Q2 Predictions Std                              3.62507\n",
      "trainer/Q2 Predictions Max                             17.2236\n",
      "trainer/Q2 Predictions Min                             -3.37861\n",
      "trainer/Q Targets Mean                                  3.05351\n",
      "trainer/Q Targets Std                                   3.74477\n",
      "trainer/Q Targets Max                                  16.6699\n",
      "trainer/Q Targets Min                                  -3.37283\n",
      "trainer/Log Pis Mean                                    6.84269\n",
      "trainer/Log Pis Std                                     5.07369\n",
      "trainer/Log Pis Max                                    20.8092\n",
      "trainer/Log Pis Min                                    -4.30415\n",
      "trainer/Policy mu Mean                                  0.178116\n",
      "trainer/Policy mu Std                                   1.61278\n",
      "trainer/Policy mu Max                                   4.30962\n",
      "trainer/Policy mu Min                                  -3.71232\n",
      "trainer/Policy log std Mean                            -0.768218\n",
      "trainer/Policy log std Std                              0.36068\n",
      "trainer/Policy log std Max                              0.0593136\n",
      "trainer/Policy log std Min                             -2.32001\n",
      "trainer/Alpha                                           0.0122995\n",
      "trainer/Alpha Loss                                      3.70665\n",
      "exploration/num steps total                         31000\n",
      "exploration/num paths total                            31\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.24419\n",
      "exploration/Rewards Std                                 0.768792\n",
      "exploration/Rewards Max                                 2.20661\n",
      "exploration/Rewards Min                                -3.36614\n",
      "exploration/Returns Mean                             -244.19\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -244.19\n",
      "exploration/Returns Min                              -244.19\n",
      "exploration/Actions Mean                                0.0730361\n",
      "exploration/Actions Std                                 0.726803\n",
      "exploration/Actions Max                                 0.999478\n",
      "exploration/Actions Min                                -0.999946\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -244.19\n",
      "exploration/env_infos/final/reward_run Mean             0.211064\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.211064\n",
      "exploration/env_infos/final/reward_run Min              0.211064\n",
      "exploration/env_infos/initial/reward_run Mean           0.729886\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.729886\n",
      "exploration/env_infos/initial/reward_run Min            0.729886\n",
      "exploration/env_infos/reward_run Mean                   0.317662\n",
      "exploration/env_infos/reward_run Std                    0.780364\n",
      "exploration/env_infos/reward_run Max                    2.59808\n",
      "exploration/env_infos/reward_run Min                   -2.00596\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.402378\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.402378\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.402378\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.273843\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.273843\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.273843\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.320146\n",
      "exploration/env_infos/reward_ctrl Std                   0.0859374\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0949047\n",
      "exploration/env_infos/reward_ctrl Min                  -0.556273\n",
      "exploration/env_infos/final/height Mean                -0.157004\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.157004\n",
      "exploration/env_infos/final/height Min                 -0.157004\n",
      "exploration/env_infos/initial/height Mean              -0.071583\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.071583\n",
      "exploration/env_infos/initial/height Min               -0.071583\n",
      "exploration/env_infos/height Mean                      -0.050105\n",
      "exploration/env_infos/height Std                        0.104172\n",
      "exploration/env_infos/height Max                        0.313401\n",
      "exploration/env_infos/height Min                       -0.271827\n",
      "exploration/env_infos/final/reward_angular Mean        -0.291653\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.291653\n",
      "exploration/env_infos/final/reward_angular Min         -0.291653\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.732665\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.732665\n",
      "exploration/env_infos/initial/reward_angular Min       -0.732665\n",
      "exploration/env_infos/reward_angular Mean              -0.0133\n",
      "exploration/env_infos/reward_angular Std                1.91877\n",
      "exploration/env_infos/reward_angular Max                8.16413\n",
      "exploration/env_infos/reward_angular Min               -6.11597\n",
      "evaluation/num steps total                         750000\n",
      "evaluation/num paths total                            750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.265882\n",
      "evaluation/Rewards Std                                  0.676265\n",
      "evaluation/Rewards Max                                  6.7871\n",
      "evaluation/Rewards Min                                 -4.97881\n",
      "evaluation/Returns Mean                              -265.882\n",
      "evaluation/Returns Std                                182.615\n",
      "evaluation/Returns Max                                -21.779\n",
      "evaluation/Returns Min                               -678.62\n",
      "evaluation/Actions Mean                                 0.173522\n",
      "evaluation/Actions Std                                  0.725655\n",
      "evaluation/Actions Max                                  0.99997\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -265.882\n",
      "evaluation/env_infos/final/reward_run Mean              0.00667049\n",
      "evaluation/env_infos/final/reward_run Std               0.438441\n",
      "evaluation/env_infos/final/reward_run Max               1.27289\n",
      "evaluation/env_infos/final/reward_run Min              -1.45645\n",
      "evaluation/env_infos/initial/reward_run Mean            0.23683\n",
      "evaluation/env_infos/initial/reward_run Std             0.66678\n",
      "evaluation/env_infos/initial/reward_run Max             1.1444\n",
      "evaluation/env_infos/initial/reward_run Min            -1.2601\n",
      "evaluation/env_infos/reward_run Mean                   -0.0162079\n",
      "evaluation/env_infos/reward_run Std                     0.349311\n",
      "evaluation/env_infos/reward_run Max                     2.66827\n",
      "evaluation/env_infos/reward_run Min                    -2.28202\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.345536\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0815446\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.247061\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.540457\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.325615\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0810883\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.124891\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.465329\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.334011\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0851202\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0394118\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.568759\n",
      "evaluation/env_infos/final/height Mean                 -0.394357\n",
      "evaluation/env_infos/final/height Std                   0.220773\n",
      "evaluation/env_infos/final/height Max                   0.102648\n",
      "evaluation/env_infos/final/height Min                  -0.577303\n",
      "evaluation/env_infos/initial/height Mean               -0.00299738\n",
      "evaluation/env_infos/initial/height Std                 0.0508711\n",
      "evaluation/env_infos/initial/height Max                 0.0857157\n",
      "evaluation/env_infos/initial/height Min                -0.0929811\n",
      "evaluation/env_infos/height Mean                       -0.37125\n",
      "evaluation/env_infos/height Std                         0.224761\n",
      "evaluation/env_infos/height Max                         0.340671\n",
      "evaluation/env_infos/height Min                        -0.59861\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0356939\n",
      "evaluation/env_infos/final/reward_angular Std           0.626063\n",
      "evaluation/env_infos/final/reward_angular Max           1.50593\n",
      "evaluation/env_infos/final/reward_angular Min          -2.27214\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.345616\n",
      "evaluation/env_infos/initial/reward_angular Std         0.775159\n",
      "evaluation/env_infos/initial/reward_angular Max         1.00278\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.93296\n",
      "evaluation/env_infos/reward_angular Mean                0.00990085\n",
      "evaluation/env_infos/reward_angular Std                 0.878899\n",
      "evaluation/env_infos/reward_angular Max                 7.85748\n",
      "evaluation/env_infos/reward_angular Min                -6.57034\n",
      "time/data storing (s)                                   0.0153927\n",
      "time/evaluation sampling (s)                           21.2947\n",
      "time/exploration sampling (s)                           0.970466\n",
      "time/logging (s)                                        0.228707\n",
      "time/saving (s)                                         0.0265933\n",
      "time/training (s)                                       3.75889\n",
      "time/epoch (s)                                         26.2948\n",
      "time/total (s)                                       1001.44\n",
      "Epoch                                                  29\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:29:10.541320 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 30 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  32000\n",
      "trainer/QF1 Loss                                        0.688495\n",
      "trainer/QF2 Loss                                        0.906426\n",
      "trainer/Policy Loss                                     2.10492\n",
      "trainer/Q1 Predictions Mean                             3.31106\n",
      "trainer/Q1 Predictions Std                              4.04928\n",
      "trainer/Q1 Predictions Max                             25.539\n",
      "trainer/Q1 Predictions Min                             -2.94868\n",
      "trainer/Q2 Predictions Mean                             3.31502\n",
      "trainer/Q2 Predictions Std                              4.08944\n",
      "trainer/Q2 Predictions Max                             25.3658\n",
      "trainer/Q2 Predictions Min                             -2.92564\n",
      "trainer/Q Targets Mean                                  3.17538\n",
      "trainer/Q Targets Std                                   4.1803\n",
      "trainer/Q Targets Max                                  25.3811\n",
      "trainer/Q Targets Min                                  -3.38541\n",
      "trainer/Log Pis Mean                                    5.83895\n",
      "trainer/Log Pis Std                                     5.87482\n",
      "trainer/Log Pis Max                                    28.0339\n",
      "trainer/Log Pis Min                                    -5.44176\n",
      "trainer/Policy mu Mean                                  0.568858\n",
      "trainer/Policy mu Std                                   1.48485\n",
      "trainer/Policy mu Max                                   6.50485\n",
      "trainer/Policy mu Min                                  -6.89441\n",
      "trainer/Policy log std Mean                            -0.768645\n",
      "trainer/Policy log std Std                              0.349468\n",
      "trainer/Policy log std Max                              0.205242\n",
      "trainer/Policy log std Min                             -3.0374\n",
      "trainer/Alpha                                           0.0120727\n",
      "trainer/Alpha Loss                                     -0.711295\n",
      "exploration/num steps total                         32000\n",
      "exploration/num paths total                            32\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.47015\n",
      "exploration/Rewards Std                                 0.26125\n",
      "exploration/Rewards Max                                 1.77199\n",
      "exploration/Rewards Min                                -1.15907\n",
      "exploration/Returns Mean                             -470.15\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -470.15\n",
      "exploration/Returns Min                              -470.15\n",
      "exploration/Actions Mean                                0.531683\n",
      "exploration/Actions Std                                 0.542457\n",
      "exploration/Actions Max                                 0.999797\n",
      "exploration/Actions Min                                -0.99824\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -470.15\n",
      "exploration/env_infos/final/reward_run Mean             0.316783\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.316783\n",
      "exploration/env_infos/final/reward_run Min              0.316783\n",
      "exploration/env_infos/initial/reward_run Mean           0.724977\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.724977\n",
      "exploration/env_infos/initial/reward_run Min            0.724977\n",
      "exploration/env_infos/reward_run Mean                   0.051544\n",
      "exploration/env_infos/reward_run Std                    0.311054\n",
      "exploration/env_infos/reward_run Max                    1.42968\n",
      "exploration/env_infos/reward_run Min                   -0.834229\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.356659\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.356659\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.356659\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.293722\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.293722\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.293722\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.346168\n",
      "exploration/env_infos/reward_ctrl Std                   0.0643545\n",
      "exploration/env_infos/reward_ctrl Max                  -0.110011\n",
      "exploration/env_infos/reward_ctrl Min                  -0.526649\n",
      "exploration/env_infos/final/height Mean                -0.564198\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.564198\n",
      "exploration/env_infos/final/height Min                 -0.564198\n",
      "exploration/env_infos/initial/height Mean               0.0658951\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0658951\n",
      "exploration/env_infos/initial/height Min                0.0658951\n",
      "exploration/env_infos/height Mean                      -0.551529\n",
      "exploration/env_infos/height Std                        0.0926877\n",
      "exploration/env_infos/height Max                        0.112129\n",
      "exploration/env_infos/height Min                       -0.580606\n",
      "exploration/env_infos/final/reward_angular Mean        -0.349758\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.349758\n",
      "exploration/env_infos/final/reward_angular Min         -0.349758\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.468698\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.468698\n",
      "exploration/env_infos/initial/reward_angular Min       -0.468698\n",
      "exploration/env_infos/reward_angular Mean               0.0683432\n",
      "exploration/env_infos/reward_angular Std                0.687795\n",
      "exploration/env_infos/reward_angular Max                6.66304\n",
      "exploration/env_infos/reward_angular Min               -2.86553\n",
      "evaluation/num steps total                         775000\n",
      "evaluation/num paths total                            775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.272804\n",
      "evaluation/Rewards Std                                  0.691883\n",
      "evaluation/Rewards Max                                  7.03991\n",
      "evaluation/Rewards Min                                 -6.14031\n",
      "evaluation/Returns Mean                              -272.804\n",
      "evaluation/Returns Std                                177.653\n",
      "evaluation/Returns Max                                -48.1196\n",
      "evaluation/Returns Min                               -635.648\n",
      "evaluation/Actions Mean                                 0.437416\n",
      "evaluation/Actions Std                                  0.619267\n",
      "evaluation/Actions Max                                  0.999972\n",
      "evaluation/Actions Min                                 -0.999993\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -272.804\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0320437\n",
      "evaluation/env_infos/final/reward_run Std               0.346335\n",
      "evaluation/env_infos/final/reward_run Max               0.969705\n",
      "evaluation/env_infos/final/reward_run Min              -1.37875\n",
      "evaluation/env_infos/initial/reward_run Mean            0.335433\n",
      "evaluation/env_infos/initial/reward_run Std             0.660636\n",
      "evaluation/env_infos/initial/reward_run Max             1.03923\n",
      "evaluation/env_infos/initial/reward_run Min            -0.923886\n",
      "evaluation/env_infos/reward_run Mean                   -0.0144721\n",
      "evaluation/env_infos/reward_run Std                     0.337625\n",
      "evaluation/env_infos/reward_run Max                     2.30413\n",
      "evaluation/env_infos/reward_run Min                    -2.55622\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.359395\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0962119\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.113867\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.531638\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.324842\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.126002\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.044173\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.49477\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.344895\n",
      "evaluation/env_infos/reward_ctrl Std                    0.088283\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.044173\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.565507\n",
      "evaluation/env_infos/final/height Mean                 -0.440697\n",
      "evaluation/env_infos/final/height Std                   0.186355\n",
      "evaluation/env_infos/final/height Max                   0.02184\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0156582\n",
      "evaluation/env_infos/initial/height Std                 0.0489891\n",
      "evaluation/env_infos/initial/height Max                 0.0801055\n",
      "evaluation/env_infos/initial/height Min                -0.112044\n",
      "evaluation/env_infos/height Mean                       -0.390413\n",
      "evaluation/env_infos/height Std                         0.217999\n",
      "evaluation/env_infos/height Max                         0.410743\n",
      "evaluation/env_infos/height Min                        -0.592707\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.118482\n",
      "evaluation/env_infos/final/reward_angular Std           0.448232\n",
      "evaluation/env_infos/final/reward_angular Max           0.752775\n",
      "evaluation/env_infos/final/reward_angular Min          -1.55092\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.379912\n",
      "evaluation/env_infos/initial/reward_angular Std         1.05012\n",
      "evaluation/env_infos/initial/reward_angular Max         1.17053\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.60782\n",
      "evaluation/env_infos/reward_angular Mean                0.00913202\n",
      "evaluation/env_infos/reward_angular Std                 0.870571\n",
      "evaluation/env_infos/reward_angular Max                 8.55581\n",
      "evaluation/env_infos/reward_angular Min                -6.28521\n",
      "time/data storing (s)                                   0.0149378\n",
      "time/evaluation sampling (s)                           21.1316\n",
      "time/exploration sampling (s)                           1.02662\n",
      "time/logging (s)                                        0.235904\n",
      "time/saving (s)                                         0.0270891\n",
      "time/training (s)                                       3.80943\n",
      "time/epoch (s)                                         26.2456\n",
      "time/total (s)                                       1027.99\n",
      "Epoch                                                  30\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:29:36.717672 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 31 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  33000\n",
      "trainer/QF1 Loss                                        0.746581\n",
      "trainer/QF2 Loss                                        0.739944\n",
      "trainer/Policy Loss                                     2.43134\n",
      "trainer/Q1 Predictions Mean                             2.97128\n",
      "trainer/Q1 Predictions Std                              3.61332\n",
      "trainer/Q1 Predictions Max                             19.2576\n",
      "trainer/Q1 Predictions Min                             -4.44581\n",
      "trainer/Q2 Predictions Mean                             3.06491\n",
      "trainer/Q2 Predictions Std                              3.5641\n",
      "trainer/Q2 Predictions Max                             17.7189\n",
      "trainer/Q2 Predictions Min                             -4.19015\n",
      "trainer/Q Targets Mean                                  2.96325\n",
      "trainer/Q Targets Std                                   3.57325\n",
      "trainer/Q Targets Max                                  18.6903\n",
      "trainer/Q Targets Min                                  -4.85502\n",
      "trainer/Log Pis Mean                                    5.87413\n",
      "trainer/Log Pis Std                                     5.00258\n",
      "trainer/Log Pis Max                                    22.6688\n",
      "trainer/Log Pis Min                                    -6.22536\n",
      "trainer/Policy mu Mean                                  0.537464\n",
      "trainer/Policy mu Std                                   1.45554\n",
      "trainer/Policy mu Max                                   5.29263\n",
      "trainer/Policy mu Min                                  -3.75138\n",
      "trainer/Policy log std Mean                            -0.735702\n",
      "trainer/Policy log std Std                              0.32735\n",
      "trainer/Policy log std Max                              0.0519331\n",
      "trainer/Policy log std Min                             -2.22317\n",
      "trainer/Alpha                                           0.0121224\n",
      "trainer/Alpha Loss                                     -0.555452\n",
      "exploration/num steps total                         33000\n",
      "exploration/num paths total                            33\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.22474\n",
      "exploration/Rewards Std                                 0.275106\n",
      "exploration/Rewards Max                                 0.642253\n",
      "exploration/Rewards Min                                -1.25242\n",
      "exploration/Returns Mean                             -224.74\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -224.74\n",
      "exploration/Returns Min                              -224.74\n",
      "exploration/Actions Mean                               -0.103117\n",
      "exploration/Actions Std                                 0.636002\n",
      "exploration/Actions Max                                 0.997087\n",
      "exploration/Actions Min                                -0.998485\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -224.74\n",
      "exploration/env_infos/final/reward_run Mean             0.746528\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.746528\n",
      "exploration/env_infos/final/reward_run Min              0.746528\n",
      "exploration/env_infos/initial/reward_run Mean           0.698796\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.698796\n",
      "exploration/env_infos/initial/reward_run Min            0.698796\n",
      "exploration/env_infos/reward_run Mean                   0.113406\n",
      "exploration/env_infos/reward_run Std                    0.351419\n",
      "exploration/env_infos/reward_run Max                    1.24877\n",
      "exploration/env_infos/reward_run Min                   -1.12269\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.228453\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.228453\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.228453\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.263393\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.263393\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.263393\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.249079\n",
      "exploration/env_infos/reward_ctrl Std                   0.0661946\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0562553\n",
      "exploration/env_infos/reward_ctrl Min                  -0.460654\n",
      "exploration/env_infos/final/height Mean                -0.0312656\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0312656\n",
      "exploration/env_infos/final/height Min                 -0.0312656\n",
      "exploration/env_infos/initial/height Mean              -0.00367983\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00367983\n",
      "exploration/env_infos/initial/height Min               -0.00367983\n",
      "exploration/env_infos/height Mean                      -0.243483\n",
      "exploration/env_infos/height Std                        0.114808\n",
      "exploration/env_infos/height Max                        0.0799595\n",
      "exploration/env_infos/height Min                       -0.447524\n",
      "exploration/env_infos/final/reward_angular Mean         1.39742\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.39742\n",
      "exploration/env_infos/final/reward_angular Min          1.39742\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.943502\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.943502\n",
      "exploration/env_infos/initial/reward_angular Min       -0.943502\n",
      "exploration/env_infos/reward_angular Mean               0.023319\n",
      "exploration/env_infos/reward_angular Std                1.4149\n",
      "exploration/env_infos/reward_angular Max                3.78394\n",
      "exploration/env_infos/reward_angular Min               -5.30571\n",
      "evaluation/num steps total                         800000\n",
      "evaluation/num paths total                            800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.235441\n",
      "evaluation/Rewards Std                                  0.604889\n",
      "evaluation/Rewards Max                                  4.46498\n",
      "evaluation/Rewards Min                                 -5.11593\n",
      "evaluation/Returns Mean                              -235.441\n",
      "evaluation/Returns Std                                140.917\n",
      "evaluation/Returns Max                                -25.6659\n",
      "evaluation/Returns Min                               -461.241\n",
      "evaluation/Actions Mean                                 0.234255\n",
      "evaluation/Actions Std                                  0.670168\n",
      "evaluation/Actions Max                                  0.999917\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -235.441\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0533843\n",
      "evaluation/env_infos/final/reward_run Std               0.322139\n",
      "evaluation/env_infos/final/reward_run Max               0.891676\n",
      "evaluation/env_infos/final/reward_run Min              -1.07227\n",
      "evaluation/env_infos/initial/reward_run Mean            0.385859\n",
      "evaluation/env_infos/initial/reward_run Std             0.621532\n",
      "evaluation/env_infos/initial/reward_run Max             1.1773\n",
      "evaluation/env_infos/initial/reward_run Min            -0.874976\n",
      "evaluation/env_infos/reward_run Mean                   -0.0261039\n",
      "evaluation/env_infos/reward_run Std                     0.441314\n",
      "evaluation/env_infos/reward_run Max                     2.22746\n",
      "evaluation/env_infos/reward_run Min                    -2.18722\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.310617\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.086737\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.14497\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.493349\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.303391\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.116962\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0298003\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.463641\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.3024\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0891809\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00623075\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.57458\n",
      "evaluation/env_infos/final/height Mean                 -0.338579\n",
      "evaluation/env_infos/final/height Std                   0.221053\n",
      "evaluation/env_infos/final/height Max                   0.0903239\n",
      "evaluation/env_infos/final/height Min                  -0.577307\n",
      "evaluation/env_infos/initial/height Mean               -0.0348567\n",
      "evaluation/env_infos/initial/height Std                 0.0542543\n",
      "evaluation/env_infos/initial/height Max                 0.0732446\n",
      "evaluation/env_infos/initial/height Min                -0.10627\n",
      "evaluation/env_infos/height Mean                       -0.309958\n",
      "evaluation/env_infos/height Std                         0.228612\n",
      "evaluation/env_infos/height Max                         0.468811\n",
      "evaluation/env_infos/height Min                        -0.589417\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.157104\n",
      "evaluation/env_infos/final/reward_angular Std           0.537753\n",
      "evaluation/env_infos/final/reward_angular Max           1.43712\n",
      "evaluation/env_infos/final/reward_angular Min          -1.89227\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.700132\n",
      "evaluation/env_infos/initial/reward_angular Std         0.889587\n",
      "evaluation/env_infos/initial/reward_angular Max         1.10271\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.0527\n",
      "evaluation/env_infos/reward_angular Mean                0.0270089\n",
      "evaluation/env_infos/reward_angular Std                 0.961447\n",
      "evaluation/env_infos/reward_angular Max                 6.03686\n",
      "evaluation/env_infos/reward_angular Min                -5.65747\n",
      "time/data storing (s)                                   0.0156237\n",
      "time/evaluation sampling (s)                           20.8665\n",
      "time/exploration sampling (s)                           1.00987\n",
      "time/logging (s)                                        0.237586\n",
      "time/saving (s)                                         0.0277842\n",
      "time/training (s)                                       3.70401\n",
      "time/epoch (s)                                         25.8614\n",
      "time/total (s)                                       1054.17\n",
      "Epoch                                                  31\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:30:02.990529 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 32 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  34000\n",
      "trainer/QF1 Loss                                        0.750699\n",
      "trainer/QF2 Loss                                        0.691137\n",
      "trainer/Policy Loss                                     3.15596\n",
      "trainer/Q1 Predictions Mean                             2.78048\n",
      "trainer/Q1 Predictions Std                              3.67409\n",
      "trainer/Q1 Predictions Max                             23.7655\n",
      "trainer/Q1 Predictions Min                             -3.83463\n",
      "trainer/Q2 Predictions Mean                             2.74195\n",
      "trainer/Q2 Predictions Std                              3.75428\n",
      "trainer/Q2 Predictions Max                             24.236\n",
      "trainer/Q2 Predictions Min                             -3.71465\n",
      "trainer/Q Targets Mean                                  3.0444\n",
      "trainer/Q Targets Std                                   3.9833\n",
      "trainer/Q Targets Max                                  25.4237\n",
      "trainer/Q Targets Min                                  -4.59852\n",
      "trainer/Log Pis Mean                                    6.3635\n",
      "trainer/Log Pis Std                                     5.50831\n",
      "trainer/Log Pis Max                                    28.3009\n",
      "trainer/Log Pis Min                                    -4.159\n",
      "trainer/Policy mu Mean                                  0.577\n",
      "trainer/Policy mu Std                                   1.48848\n",
      "trainer/Policy mu Max                                   4.34495\n",
      "trainer/Policy mu Min                                  -4.58253\n",
      "trainer/Policy log std Mean                            -0.755587\n",
      "trainer/Policy log std Std                              0.364595\n",
      "trainer/Policy log std Max                              0.206997\n",
      "trainer/Policy log std Min                             -2.74193\n",
      "trainer/Alpha                                           0.0122813\n",
      "trainer/Alpha Loss                                      1.59938\n",
      "exploration/num steps total                         34000\n",
      "exploration/num paths total                            34\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.170591\n",
      "exploration/Rewards Std                                 0.42438\n",
      "exploration/Rewards Max                                 1.09813\n",
      "exploration/Rewards Min                                -1.42131\n",
      "exploration/Returns Mean                             -170.591\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -170.591\n",
      "exploration/Returns Min                              -170.591\n",
      "exploration/Actions Mean                                0.225827\n",
      "exploration/Actions Std                                 0.611224\n",
      "exploration/Actions Max                                 0.999957\n",
      "exploration/Actions Min                                -0.999841\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -170.591\n",
      "exploration/env_infos/final/reward_run Mean             0.953409\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.953409\n",
      "exploration/env_infos/final/reward_run Min              0.953409\n",
      "exploration/env_infos/initial/reward_run Mean           0.519146\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.519146\n",
      "exploration/env_infos/initial/reward_run Min            0.519146\n",
      "exploration/env_infos/reward_run Mean                   0.217676\n",
      "exploration/env_infos/reward_run Std                    0.723953\n",
      "exploration/env_infos/reward_run Max                    2.28645\n",
      "exploration/env_infos/reward_run Min                   -2.04146\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.409184\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.409184\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.409184\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.257678\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.257678\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.257678\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.254756\n",
      "exploration/env_infos/reward_ctrl Std                   0.0837913\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0220612\n",
      "exploration/env_infos/reward_ctrl Min                  -0.532591\n",
      "exploration/env_infos/final/height Mean                 0.114566\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.114566\n",
      "exploration/env_infos/final/height Min                  0.114566\n",
      "exploration/env_infos/initial/height Mean              -0.0790322\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0790322\n",
      "exploration/env_infos/initial/height Min               -0.0790322\n",
      "exploration/env_infos/height Mean                      -0.0514706\n",
      "exploration/env_infos/height Std                        0.089445\n",
      "exploration/env_infos/height Max                        0.260027\n",
      "exploration/env_infos/height Min                       -0.352803\n",
      "exploration/env_infos/final/reward_angular Mean        -1.15549\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.15549\n",
      "exploration/env_infos/final/reward_angular Min         -1.15549\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.64063\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.64063\n",
      "exploration/env_infos/initial/reward_angular Min       -1.64063\n",
      "exploration/env_infos/reward_angular Mean              -0.00645204\n",
      "exploration/env_infos/reward_angular Std                1.98797\n",
      "exploration/env_infos/reward_angular Max                7.37915\n",
      "exploration/env_infos/reward_angular Min               -6.00859\n",
      "evaluation/num steps total                         825000\n",
      "evaluation/num paths total                            825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.196331\n",
      "evaluation/Rewards Std                                  0.827499\n",
      "evaluation/Rewards Max                                  4.91669\n",
      "evaluation/Rewards Min                                 -5.26604\n",
      "evaluation/Returns Mean                              -196.331\n",
      "evaluation/Returns Std                                213.509\n",
      "evaluation/Returns Max                                300.071\n",
      "evaluation/Returns Min                               -625.204\n",
      "evaluation/Actions Mean                                 0.232118\n",
      "evaluation/Actions Std                                  0.700831\n",
      "evaluation/Actions Max                                  0.999993\n",
      "evaluation/Actions Min                                 -0.999982\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -196.331\n",
      "evaluation/env_infos/final/reward_run Mean             -0.168934\n",
      "evaluation/env_infos/final/reward_run Std               0.479627\n",
      "evaluation/env_infos/final/reward_run Max               0.361484\n",
      "evaluation/env_infos/final/reward_run Min              -1.62895\n",
      "evaluation/env_infos/initial/reward_run Mean            0.451178\n",
      "evaluation/env_infos/initial/reward_run Std             0.551873\n",
      "evaluation/env_infos/initial/reward_run Max             1.06182\n",
      "evaluation/env_infos/initial/reward_run Min            -1.11723\n",
      "evaluation/env_infos/reward_run Mean                   -0.166172\n",
      "evaluation/env_infos/reward_run Std                     0.631701\n",
      "evaluation/env_infos/reward_run Max                     2.81463\n",
      "evaluation/env_infos/reward_run Min                    -3.26695\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.347931\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.116979\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0415647\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.518779\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.287474\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.109578\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0202194\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.417218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.327026\n",
      "evaluation/env_infos/reward_ctrl Std                    0.123062\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0121193\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.567971\n",
      "evaluation/env_infos/final/height Mean                 -0.337669\n",
      "evaluation/env_infos/final/height Std                   0.258048\n",
      "evaluation/env_infos/final/height Max                   0.107269\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.010021\n",
      "evaluation/env_infos/initial/height Std                 0.0525376\n",
      "evaluation/env_infos/initial/height Max                 0.0858874\n",
      "evaluation/env_infos/initial/height Min                -0.100578\n",
      "evaluation/env_infos/height Mean                       -0.290563\n",
      "evaluation/env_infos/height Std                         0.258967\n",
      "evaluation/env_infos/height Max                         0.469682\n",
      "evaluation/env_infos/height Min                        -0.590267\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.162135\n",
      "evaluation/env_infos/final/reward_angular Std           0.88187\n",
      "evaluation/env_infos/final/reward_angular Max           1.48751\n",
      "evaluation/env_infos/final/reward_angular Min          -2.80976\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.564178\n",
      "evaluation/env_infos/initial/reward_angular Std         0.705336\n",
      "evaluation/env_infos/initial/reward_angular Max         1.2611\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.76953\n",
      "evaluation/env_infos/reward_angular Mean                0.0286197\n",
      "evaluation/env_infos/reward_angular Std                 1.25623\n",
      "evaluation/env_infos/reward_angular Max                 7.91518\n",
      "evaluation/env_infos/reward_angular Min                -5.8742\n",
      "time/data storing (s)                                   0.0148481\n",
      "time/evaluation sampling (s)                           20.7277\n",
      "time/exploration sampling (s)                           1.00738\n",
      "time/logging (s)                                        0.236792\n",
      "time/saving (s)                                         0.0283089\n",
      "time/training (s)                                       3.93186\n",
      "time/epoch (s)                                         25.9469\n",
      "time/total (s)                                       1080.44\n",
      "Epoch                                                  32\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:30:30.073805 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 33 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  35000\n",
      "trainer/QF1 Loss                                        0.411599\n",
      "trainer/QF2 Loss                                        0.470959\n",
      "trainer/Policy Loss                                     1.59745\n",
      "trainer/Q1 Predictions Mean                             2.82584\n",
      "trainer/Q1 Predictions Std                              3.35818\n",
      "trainer/Q1 Predictions Max                             13.1523\n",
      "trainer/Q1 Predictions Min                             -4.46446\n",
      "trainer/Q2 Predictions Mean                             2.84068\n",
      "trainer/Q2 Predictions Std                              3.36537\n",
      "trainer/Q2 Predictions Max                             13.44\n",
      "trainer/Q2 Predictions Min                             -4.54654\n",
      "trainer/Q Targets Mean                                  2.75985\n",
      "trainer/Q Targets Std                                   3.34125\n",
      "trainer/Q Targets Max                                  12.4622\n",
      "trainer/Q Targets Min                                  -4.42336\n",
      "trainer/Log Pis Mean                                    4.68259\n",
      "trainer/Log Pis Std                                     4.81196\n",
      "trainer/Log Pis Max                                    21.6179\n",
      "trainer/Log Pis Min                                    -4.72939\n",
      "trainer/Policy mu Mean                                  0.290438\n",
      "trainer/Policy mu Std                                   1.41317\n",
      "trainer/Policy mu Max                                   4.86485\n",
      "trainer/Policy mu Min                                  -6.18238\n",
      "trainer/Policy log std Mean                            -0.757952\n",
      "trainer/Policy log std Std                              0.359606\n",
      "trainer/Policy log std Max                              0.133518\n",
      "trainer/Policy log std Min                             -2.33597\n",
      "trainer/Alpha                                           0.0119095\n",
      "trainer/Alpha Loss                                     -5.83529\n",
      "exploration/num steps total                         35000\n",
      "exploration/num paths total                            35\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.189009\n",
      "exploration/Rewards Std                                 0.572963\n",
      "exploration/Rewards Max                                 2.49677\n",
      "exploration/Rewards Min                                -2.24136\n",
      "exploration/Returns Mean                             -189.009\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -189.009\n",
      "exploration/Returns Min                              -189.009\n",
      "exploration/Actions Mean                               -0.0758698\n",
      "exploration/Actions Std                                 0.727629\n",
      "exploration/Actions Max                                 0.99959\n",
      "exploration/Actions Min                                -0.999924\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -189.009\n",
      "exploration/env_infos/final/reward_run Mean             0.606179\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.606179\n",
      "exploration/env_infos/final/reward_run Min              0.606179\n",
      "exploration/env_infos/initial/reward_run Mean           0.495823\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.495823\n",
      "exploration/env_infos/initial/reward_run Min            0.495823\n",
      "exploration/env_infos/reward_run Mean                   0.322992\n",
      "exploration/env_infos/reward_run Std                    0.536572\n",
      "exploration/env_infos/reward_run Max                    2.55254\n",
      "exploration/env_infos/reward_run Min                   -1.09005\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.271845\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.271845\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.271845\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.257479\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.257479\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.257479\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.32112\n",
      "exploration/env_infos/reward_ctrl Std                   0.0809508\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0802671\n",
      "exploration/env_infos/reward_ctrl Min                  -0.539755\n",
      "exploration/env_infos/final/height Mean                -0.545706\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.545706\n",
      "exploration/env_infos/final/height Min                 -0.545706\n",
      "exploration/env_infos/initial/height Mean              -0.0332184\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0332184\n",
      "exploration/env_infos/initial/height Min               -0.0332184\n",
      "exploration/env_infos/height Mean                      -0.471187\n",
      "exploration/env_infos/height Std                        0.153414\n",
      "exploration/env_infos/height Max                        0.249209\n",
      "exploration/env_infos/height Min                       -0.588564\n",
      "exploration/env_infos/final/reward_angular Mean         0.67481\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.67481\n",
      "exploration/env_infos/final/reward_angular Min          0.67481\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.671136\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.671136\n",
      "exploration/env_infos/initial/reward_angular Min       -0.671136\n",
      "exploration/env_infos/reward_angular Mean               0.0756296\n",
      "exploration/env_infos/reward_angular Std                1.48233\n",
      "exploration/env_infos/reward_angular Max                7.49172\n",
      "exploration/env_infos/reward_angular Min               -5.88001\n",
      "evaluation/num steps total                         850000\n",
      "evaluation/num paths total                            850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.20515\n",
      "evaluation/Rewards Std                                  0.596538\n",
      "evaluation/Rewards Max                                  4.13974\n",
      "evaluation/Rewards Min                                 -5.42692\n",
      "evaluation/Returns Mean                              -205.15\n",
      "evaluation/Returns Std                                100.848\n",
      "evaluation/Returns Max                                  3.3498\n",
      "evaluation/Returns Min                               -371.056\n",
      "evaluation/Actions Mean                                 0.0427343\n",
      "evaluation/Actions Std                                  0.714851\n",
      "evaluation/Actions Max                                  0.999956\n",
      "evaluation/Actions Min                                 -0.999941\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -205.15\n",
      "evaluation/env_infos/final/reward_run Mean              0.0194208\n",
      "evaluation/env_infos/final/reward_run Std               0.2234\n",
      "evaluation/env_infos/final/reward_run Max               0.491489\n",
      "evaluation/env_infos/final/reward_run Min              -0.614294\n",
      "evaluation/env_infos/initial/reward_run Mean            0.343573\n",
      "evaluation/env_infos/initial/reward_run Std             0.540505\n",
      "evaluation/env_infos/initial/reward_run Max             1.13872\n",
      "evaluation/env_infos/initial/reward_run Min            -0.741406\n",
      "evaluation/env_infos/reward_run Mean                    0.0526455\n",
      "evaluation/env_infos/reward_run Std                     0.41162\n",
      "evaluation/env_infos/reward_run Max                     2.54795\n",
      "evaluation/env_infos/reward_run Min                    -2.24971\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.326837\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.106647\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0768717\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.526258\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.224907\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0974305\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0381057\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.428394\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.307703\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106344\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0210096\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.568493\n",
      "evaluation/env_infos/final/height Mean                 -0.385925\n",
      "evaluation/env_infos/final/height Std                   0.183175\n",
      "evaluation/env_infos/final/height Max                   0.0782794\n",
      "evaluation/env_infos/final/height Min                  -0.577296\n",
      "evaluation/env_infos/initial/height Mean                0.00597964\n",
      "evaluation/env_infos/initial/height Std                 0.0651104\n",
      "evaluation/env_infos/initial/height Max                 0.0966302\n",
      "evaluation/env_infos/initial/height Min                -0.0970264\n",
      "evaluation/env_infos/height Mean                       -0.32885\n",
      "evaluation/env_infos/height Std                         0.2112\n",
      "evaluation/env_infos/height Max                         0.528113\n",
      "evaluation/env_infos/height Min                        -0.59426\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.150543\n",
      "evaluation/env_infos/final/reward_angular Std           0.602136\n",
      "evaluation/env_infos/final/reward_angular Max           0.980352\n",
      "evaluation/env_infos/final/reward_angular Min          -2.26638\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.387816\n",
      "evaluation/env_infos/initial/reward_angular Std         0.897669\n",
      "evaluation/env_infos/initial/reward_angular Max         1.02819\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.78739\n",
      "evaluation/env_infos/reward_angular Mean                0.0231477\n",
      "evaluation/env_infos/reward_angular Std                 0.962716\n",
      "evaluation/env_infos/reward_angular Max                 6.89032\n",
      "evaluation/env_infos/reward_angular Min                -5.2632\n",
      "time/data storing (s)                                   0.0163603\n",
      "time/evaluation sampling (s)                           21.651\n",
      "time/exploration sampling (s)                           1.0726\n",
      "time/logging (s)                                        0.238067\n",
      "time/saving (s)                                         0.0293072\n",
      "time/training (s)                                       3.74948\n",
      "time/epoch (s)                                         26.7568\n",
      "time/total (s)                                       1107.52\n",
      "Epoch                                                  33\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:30:56.329748 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 34 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  36000\n",
      "trainer/QF1 Loss                                        0.739084\n",
      "trainer/QF2 Loss                                        0.477763\n",
      "trainer/Policy Loss                                     2.53663\n",
      "trainer/Q1 Predictions Mean                             3.00147\n",
      "trainer/Q1 Predictions Std                              4.12874\n",
      "trainer/Q1 Predictions Max                             19.1464\n",
      "trainer/Q1 Predictions Min                             -4.88576\n",
      "trainer/Q2 Predictions Mean                             3.06462\n",
      "trainer/Q2 Predictions Std                              4.2369\n",
      "trainer/Q2 Predictions Max                             17.2135\n",
      "trainer/Q2 Predictions Min                             -5.51413\n",
      "trainer/Q Targets Mean                                  3.16131\n",
      "trainer/Q Targets Std                                   4.3195\n",
      "trainer/Q Targets Max                                  17.0883\n",
      "trainer/Q Targets Min                                  -5.53317\n",
      "trainer/Log Pis Mean                                    6.02309\n",
      "trainer/Log Pis Std                                     5.29588\n",
      "trainer/Log Pis Max                                    30.4205\n",
      "trainer/Log Pis Min                                    -6.86159\n",
      "trainer/Policy mu Mean                                  0.514986\n",
      "trainer/Policy mu Std                                   1.50292\n",
      "trainer/Policy mu Max                                   4.78285\n",
      "trainer/Policy mu Min                                  -4.38916\n",
      "trainer/Policy log std Mean                            -0.763386\n",
      "trainer/Policy log std Std                              0.365789\n",
      "trainer/Policy log std Max                              0.0882012\n",
      "trainer/Policy log std Min                             -2.14862\n",
      "trainer/Alpha                                           0.0113308\n",
      "trainer/Alpha Loss                                      0.103436\n",
      "exploration/num steps total                         36000\n",
      "exploration/num paths total                            36\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.590367\n",
      "exploration/Rewards Std                                 0.231747\n",
      "exploration/Rewards Max                                 1.09488\n",
      "exploration/Rewards Min                                -1.60521\n",
      "exploration/Returns Mean                             -590.367\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -590.367\n",
      "exploration/Returns Min                              -590.367\n",
      "exploration/Actions Mean                                0.549863\n",
      "exploration/Actions Std                                 0.557655\n",
      "exploration/Actions Max                                 0.999801\n",
      "exploration/Actions Min                                -0.992918\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -590.367\n",
      "exploration/env_infos/final/reward_run Mean             0.112057\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.112057\n",
      "exploration/env_infos/final/reward_run Min              0.112057\n",
      "exploration/env_infos/initial/reward_run Mean           0.336736\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.336736\n",
      "exploration/env_infos/initial/reward_run Min            0.336736\n",
      "exploration/env_infos/reward_run Mean                  -0.00623655\n",
      "exploration/env_infos/reward_run Std                    0.240401\n",
      "exploration/env_infos/reward_run Max                    0.863924\n",
      "exploration/env_infos/reward_run Min                   -1.58434\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.390452\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.390452\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.390452\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.223163\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.223163\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.223163\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.367997\n",
      "exploration/env_infos/reward_ctrl Std                   0.0647248\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0841041\n",
      "exploration/env_infos/reward_ctrl Min                  -0.515326\n",
      "exploration/env_infos/final/height Mean                -0.576118\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576118\n",
      "exploration/env_infos/final/height Min                 -0.576118\n",
      "exploration/env_infos/initial/height Mean               0.064829\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.064829\n",
      "exploration/env_infos/initial/height Min                0.064829\n",
      "exploration/env_infos/height Mean                      -0.49613\n",
      "exploration/env_infos/height Std                        0.194519\n",
      "exploration/env_infos/height Max                        0.338583\n",
      "exploration/env_infos/height Min                       -0.578866\n",
      "exploration/env_infos/final/reward_angular Mean         0.0225651\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0225651\n",
      "exploration/env_infos/final/reward_angular Min          0.0225651\n",
      "exploration/env_infos/initial/reward_angular Mean       0.030544\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.030544\n",
      "exploration/env_infos/initial/reward_angular Min        0.030544\n",
      "exploration/env_infos/reward_angular Mean              -0.0689387\n",
      "exploration/env_infos/reward_angular Std                0.677517\n",
      "exploration/env_infos/reward_angular Max                4.70608\n",
      "exploration/env_infos/reward_angular Min               -5.32407\n",
      "evaluation/num steps total                         875000\n",
      "evaluation/num paths total                            875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.218344\n",
      "evaluation/Rewards Std                                  0.881698\n",
      "evaluation/Rewards Max                                  8.0812\n",
      "evaluation/Rewards Min                                 -6.07589\n",
      "evaluation/Returns Mean                              -218.344\n",
      "evaluation/Returns Std                                165.176\n",
      "evaluation/Returns Max                                 74.3664\n",
      "evaluation/Returns Min                               -564.438\n",
      "evaluation/Actions Mean                                 0.272028\n",
      "evaluation/Actions Std                                  0.653889\n",
      "evaluation/Actions Max                                  0.99998\n",
      "evaluation/Actions Min                                 -0.999933\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -218.344\n",
      "evaluation/env_infos/final/reward_run Mean              0.0995847\n",
      "evaluation/env_infos/final/reward_run Std               0.506273\n",
      "evaluation/env_infos/final/reward_run Max               1.80333\n",
      "evaluation/env_infos/final/reward_run Min              -0.887214\n",
      "evaluation/env_infos/initial/reward_run Mean            0.439658\n",
      "evaluation/env_infos/initial/reward_run Std             0.605794\n",
      "evaluation/env_infos/initial/reward_run Max             1.14348\n",
      "evaluation/env_infos/initial/reward_run Min            -0.838566\n",
      "evaluation/env_infos/reward_run Mean                    0.0147556\n",
      "evaluation/env_infos/reward_run Std                     0.59063\n",
      "evaluation/env_infos/reward_run Max                     2.77827\n",
      "evaluation/env_infos/reward_run Min                    -2.65752\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.310636\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.120168\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0283348\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.502487\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.322431\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.114447\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0996221\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.455308\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.300942\n",
      "evaluation/env_infos/reward_ctrl Std                    0.121484\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00335311\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.582022\n",
      "evaluation/env_infos/final/height Mean                 -0.448829\n",
      "evaluation/env_infos/final/height Std                   0.170822\n",
      "evaluation/env_infos/final/height Max                  -0.0070313\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0213528\n",
      "evaluation/env_infos/initial/height Std                 0.0420923\n",
      "evaluation/env_infos/initial/height Max                 0.0786177\n",
      "evaluation/env_infos/initial/height Min                -0.0775398\n",
      "evaluation/env_infos/height Mean                       -0.395546\n",
      "evaluation/env_infos/height Std                         0.216977\n",
      "evaluation/env_infos/height Max                         0.383124\n",
      "evaluation/env_infos/height Min                        -0.593482\n",
      "evaluation/env_infos/final/reward_angular Mean          0.348074\n",
      "evaluation/env_infos/final/reward_angular Std           0.849111\n",
      "evaluation/env_infos/final/reward_angular Max           1.71532\n",
      "evaluation/env_infos/final/reward_angular Min          -1.88356\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.243395\n",
      "evaluation/env_infos/initial/reward_angular Std         0.943616\n",
      "evaluation/env_infos/initial/reward_angular Max         1.66958\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.94651\n",
      "evaluation/env_infos/reward_angular Mean                0.0273228\n",
      "evaluation/env_infos/reward_angular Std                 1.32822\n",
      "evaluation/env_infos/reward_angular Max                 9.35888\n",
      "evaluation/env_infos/reward_angular Min                -5.5627\n",
      "time/data storing (s)                                   0.0157807\n",
      "time/evaluation sampling (s)                           20.7007\n",
      "time/exploration sampling (s)                           1.20046\n",
      "time/logging (s)                                        0.229522\n",
      "time/saving (s)                                         0.0266568\n",
      "time/training (s)                                       3.73107\n",
      "time/epoch (s)                                         25.9042\n",
      "time/total (s)                                       1133.77\n",
      "Epoch                                                  34\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:31:22.463273 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 35 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  37000\n",
      "trainer/QF1 Loss                                        0.713988\n",
      "trainer/QF2 Loss                                        0.703249\n",
      "trainer/Policy Loss                                     3.21062\n",
      "trainer/Q1 Predictions Mean                             2.5552\n",
      "trainer/Q1 Predictions Std                              4.18342\n",
      "trainer/Q1 Predictions Max                             23.4803\n",
      "trainer/Q1 Predictions Min                             -4.20538\n",
      "trainer/Q2 Predictions Mean                             2.42083\n",
      "trainer/Q2 Predictions Std                              4.0304\n",
      "trainer/Q2 Predictions Max                             21.4826\n",
      "trainer/Q2 Predictions Min                             -4.49329\n",
      "trainer/Q Targets Mean                                  2.55945\n",
      "trainer/Q Targets Std                                   4.16974\n",
      "trainer/Q Targets Max                                  20.5576\n",
      "trainer/Q Targets Min                                  -4.87475\n",
      "trainer/Log Pis Mean                                    6.0143\n",
      "trainer/Log Pis Std                                     5.73483\n",
      "trainer/Log Pis Max                                    38.9502\n",
      "trainer/Log Pis Min                                    -8.58289\n",
      "trainer/Policy mu Mean                                  0.323908\n",
      "trainer/Policy mu Std                                   1.56748\n",
      "trainer/Policy mu Max                                   4.65844\n",
      "trainer/Policy mu Min                                  -6.84305\n",
      "trainer/Policy log std Mean                            -0.807255\n",
      "trainer/Policy log std Std                              0.391146\n",
      "trainer/Policy log std Max                              0.117681\n",
      "trainer/Policy log std Min                             -2.4362\n",
      "trainer/Alpha                                           0.01106\n",
      "trainer/Alpha Loss                                      0.0644148\n",
      "exploration/num steps total                         37000\n",
      "exploration/num paths total                            37\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0167836\n",
      "exploration/Rewards Std                                 0.57416\n",
      "exploration/Rewards Max                                 1.83135\n",
      "exploration/Rewards Min                                -1.60832\n",
      "exploration/Returns Mean                              -16.7836\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -16.7836\n",
      "exploration/Returns Min                               -16.7836\n",
      "exploration/Actions Mean                                0.166547\n",
      "exploration/Actions Std                                 0.772294\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -16.7836\n",
      "exploration/env_infos/final/reward_run Mean             0.243443\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.243443\n",
      "exploration/env_infos/final/reward_run Min              0.243443\n",
      "exploration/env_infos/initial/reward_run Mean           1.0336\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.0336\n",
      "exploration/env_infos/initial/reward_run Min            1.0336\n",
      "exploration/env_infos/reward_run Mean                   0.235936\n",
      "exploration/env_infos/reward_run Std                    0.553479\n",
      "exploration/env_infos/reward_run Max                    1.96449\n",
      "exploration/env_infos/reward_run Min                   -1.17362\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.262192\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.262192\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.262192\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.453489\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.453489\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.453489\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.374505\n",
      "exploration/env_infos/reward_ctrl Std                   0.0895864\n",
      "exploration/env_infos/reward_ctrl Max                  -0.101536\n",
      "exploration/env_infos/reward_ctrl Min                  -0.583289\n",
      "exploration/env_infos/final/height Mean                -0.523333\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.523333\n",
      "exploration/env_infos/final/height Min                 -0.523333\n",
      "exploration/env_infos/initial/height Mean              -0.0622384\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0622384\n",
      "exploration/env_infos/initial/height Min               -0.0622384\n",
      "exploration/env_infos/height Mean                      -0.487252\n",
      "exploration/env_infos/height Std                        0.0969541\n",
      "exploration/env_infos/height Max                        0.103984\n",
      "exploration/env_infos/height Min                       -0.588821\n",
      "exploration/env_infos/final/reward_angular Mean         1.17934\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.17934\n",
      "exploration/env_infos/final/reward_angular Min          1.17934\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0579559\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0579559\n",
      "exploration/env_infos/initial/reward_angular Min        0.0579559\n",
      "exploration/env_infos/reward_angular Mean               0.0328477\n",
      "exploration/env_infos/reward_angular Std                1.75029\n",
      "exploration/env_infos/reward_angular Max                7.9028\n",
      "exploration/env_infos/reward_angular Min               -6.71372\n",
      "evaluation/num steps total                         900000\n",
      "evaluation/num paths total                            900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.194975\n",
      "evaluation/Rewards Std                                  0.726301\n",
      "evaluation/Rewards Max                                  5.26454\n",
      "evaluation/Rewards Min                                 -5.1609\n",
      "evaluation/Returns Mean                              -194.975\n",
      "evaluation/Returns Std                                168.839\n",
      "evaluation/Returns Max                                101.542\n",
      "evaluation/Returns Min                               -572.308\n",
      "evaluation/Actions Mean                                 0.22142\n",
      "evaluation/Actions Std                                  0.657106\n",
      "evaluation/Actions Max                                  0.999986\n",
      "evaluation/Actions Min                                 -0.999977\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -194.975\n",
      "evaluation/env_infos/final/reward_run Mean             -0.00781766\n",
      "evaluation/env_infos/final/reward_run Std               0.456966\n",
      "evaluation/env_infos/final/reward_run Max               1.22557\n",
      "evaluation/env_infos/final/reward_run Min              -1.29755\n",
      "evaluation/env_infos/initial/reward_run Mean            0.407709\n",
      "evaluation/env_infos/initial/reward_run Std             0.501644\n",
      "evaluation/env_infos/initial/reward_run Max             0.983246\n",
      "evaluation/env_infos/initial/reward_run Min            -0.540041\n",
      "evaluation/env_infos/reward_run Mean                   -0.0246302\n",
      "evaluation/env_infos/reward_run Std                     0.518653\n",
      "evaluation/env_infos/reward_run Max                     2.4173\n",
      "evaluation/env_infos/reward_run Min                    -2.41677\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.308678\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.123503\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0679059\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.522686\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.275199\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.125452\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.012178\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.462108\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.288489\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109708\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.012178\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.587623\n",
      "evaluation/env_infos/final/height Mean                 -0.357403\n",
      "evaluation/env_infos/final/height Std                   0.236791\n",
      "evaluation/env_infos/final/height Max                   0.0820575\n",
      "evaluation/env_infos/final/height Min                  -0.577315\n",
      "evaluation/env_infos/initial/height Mean                0.00228998\n",
      "evaluation/env_infos/initial/height Std                 0.0529247\n",
      "evaluation/env_infos/initial/height Max                 0.0946285\n",
      "evaluation/env_infos/initial/height Min                -0.092065\n",
      "evaluation/env_infos/height Mean                       -0.313295\n",
      "evaluation/env_infos/height Std                         0.249988\n",
      "evaluation/env_infos/height Max                         0.490307\n",
      "evaluation/env_infos/height Min                        -0.594441\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.117288\n",
      "evaluation/env_infos/final/reward_angular Std           0.945686\n",
      "evaluation/env_infos/final/reward_angular Max           2.35847\n",
      "evaluation/env_infos/final/reward_angular Min          -2.68757\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0356791\n",
      "evaluation/env_infos/initial/reward_angular Std         0.822304\n",
      "evaluation/env_infos/initial/reward_angular Max         2.37877\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.3145\n",
      "evaluation/env_infos/reward_angular Mean                0.00419862\n",
      "evaluation/env_infos/reward_angular Std                 1.14352\n",
      "evaluation/env_infos/reward_angular Max                 7.6525\n",
      "evaluation/env_infos/reward_angular Min                -6.09605\n",
      "time/data storing (s)                                   0.014914\n",
      "time/evaluation sampling (s)                           20.7989\n",
      "time/exploration sampling (s)                           1.01439\n",
      "time/logging (s)                                        0.236134\n",
      "time/saving (s)                                         0.0278581\n",
      "time/training (s)                                       3.71307\n",
      "time/epoch (s)                                         25.8053\n",
      "time/total (s)                                       1159.91\n",
      "Epoch                                                  35\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:31:48.632524 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 36 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                  38000\n",
      "trainer/QF1 Loss                                        0.623865\n",
      "trainer/QF2 Loss                                        0.601903\n",
      "trainer/Policy Loss                                     2.67889\n",
      "trainer/Q1 Predictions Mean                             2.83185\n",
      "trainer/Q1 Predictions Std                              4.09926\n",
      "trainer/Q1 Predictions Max                             22.809\n",
      "trainer/Q1 Predictions Min                             -4.95463\n",
      "trainer/Q2 Predictions Mean                             3.00519\n",
      "trainer/Q2 Predictions Std                              4.1864\n",
      "trainer/Q2 Predictions Max                             24.2945\n",
      "trainer/Q2 Predictions Min                             -4.94418\n",
      "trainer/Q Targets Mean                                  2.92676\n",
      "trainer/Q Targets Std                                   4.11462\n",
      "trainer/Q Targets Max                                  22.1145\n",
      "trainer/Q Targets Min                                  -5.61312\n",
      "trainer/Log Pis Mean                                    5.99417\n",
      "trainer/Log Pis Std                                     5.45472\n",
      "trainer/Log Pis Max                                    25.3189\n",
      "trainer/Log Pis Min                                    -6.63482\n",
      "trainer/Policy mu Mean                                  0.378565\n",
      "trainer/Policy mu Std                                   1.51716\n",
      "trainer/Policy mu Max                                   5.10014\n",
      "trainer/Policy mu Min                                  -4.9328\n",
      "trainer/Policy log std Mean                            -0.7534\n",
      "trainer/Policy log std Std                              0.35804\n",
      "trainer/Policy log std Max                              0.120065\n",
      "trainer/Policy log std Min                             -2.51498\n",
      "trainer/Alpha                                           0.0110061\n",
      "trainer/Alpha Loss                                     -0.0262962\n",
      "exploration/num steps total                         38000\n",
      "exploration/num paths total                            38\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0240852\n",
      "exploration/Rewards Std                                 0.50645\n",
      "exploration/Rewards Max                                 1.27512\n",
      "exploration/Rewards Min                                -1.64872\n",
      "exploration/Returns Mean                               24.0852\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                24.0852\n",
      "exploration/Returns Min                                24.0852\n",
      "exploration/Actions Mean                                0.116551\n",
      "exploration/Actions Std                                 0.671809\n",
      "exploration/Actions Max                                 0.998827\n",
      "exploration/Actions Min                                -0.999896\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            24.0852\n",
      "exploration/env_infos/final/reward_run Mean            -0.366742\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.366742\n",
      "exploration/env_infos/final/reward_run Min             -0.366742\n",
      "exploration/env_infos/initial/reward_run Mean           0.294587\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.294587\n",
      "exploration/env_infos/initial/reward_run Min            0.294587\n",
      "exploration/env_infos/reward_run Mean                   0.643096\n",
      "exploration/env_infos/reward_run Std                    0.549361\n",
      "exploration/env_infos/reward_run Max                    2.11588\n",
      "exploration/env_infos/reward_run Min                   -0.687951\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.299764\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.299764\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.299764\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.239476\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.239476\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.239476\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.278947\n",
      "exploration/env_infos/reward_ctrl Std                   0.0859722\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0432886\n",
      "exploration/env_infos/reward_ctrl Min                  -0.528303\n",
      "exploration/env_infos/final/height Mean                -0.333012\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.333012\n",
      "exploration/env_infos/final/height Min                 -0.333012\n",
      "exploration/env_infos/initial/height Mean              -0.0670326\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0670326\n",
      "exploration/env_infos/initial/height Min               -0.0670326\n",
      "exploration/env_infos/height Mean                      -0.247267\n",
      "exploration/env_infos/height Std                        0.0614629\n",
      "exploration/env_infos/height Max                       -0.0423816\n",
      "exploration/env_infos/height Min                       -0.390033\n",
      "exploration/env_infos/final/reward_angular Mean        -0.4966\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.4966\n",
      "exploration/env_infos/final/reward_angular Min         -0.4966\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.293791\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.293791\n",
      "exploration/env_infos/initial/reward_angular Min       -0.293791\n",
      "exploration/env_infos/reward_angular Mean               0.0346014\n",
      "exploration/env_infos/reward_angular Std                1.30177\n",
      "exploration/env_infos/reward_angular Max                4.16291\n",
      "exploration/env_infos/reward_angular Min               -3.28286\n",
      "evaluation/num steps total                         925000\n",
      "evaluation/num paths total                            925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.140844\n",
      "evaluation/Rewards Std                                  0.733076\n",
      "evaluation/Rewards Max                                  5.01158\n",
      "evaluation/Rewards Min                                 -3.49464\n",
      "evaluation/Returns Mean                              -140.844\n",
      "evaluation/Returns Std                                256.241\n",
      "evaluation/Returns Max                                558.339\n",
      "evaluation/Returns Min                               -577.241\n",
      "evaluation/Actions Mean                                 0.178969\n",
      "evaluation/Actions Std                                  0.710631\n",
      "evaluation/Actions Max                                  0.99994\n",
      "evaluation/Actions Min                                 -0.99998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -140.844\n",
      "evaluation/env_infos/final/reward_run Mean             -0.325462\n",
      "evaluation/env_infos/final/reward_run Std               0.621356\n",
      "evaluation/env_infos/final/reward_run Max               0.414621\n",
      "evaluation/env_infos/final/reward_run Min              -1.73556\n",
      "evaluation/env_infos/initial/reward_run Mean            0.350026\n",
      "evaluation/env_infos/initial/reward_run Std             0.455776\n",
      "evaluation/env_infos/initial/reward_run Max             1.01809\n",
      "evaluation/env_infos/initial/reward_run Min            -0.349299\n",
      "evaluation/env_infos/reward_run Mean                   -0.251905\n",
      "evaluation/env_infos/reward_run Std                     0.767111\n",
      "evaluation/env_infos/reward_run Max                     2.35772\n",
      "evaluation/env_infos/reward_run Min                    -2.89462\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.318303\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11335\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0740954\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.519279\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.271136\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.117637\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0828026\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.453098\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.322216\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109036\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0155322\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.558863\n",
      "evaluation/env_infos/final/height Mean                 -0.304814\n",
      "evaluation/env_infos/final/height Std                   0.230749\n",
      "evaluation/env_infos/final/height Max                   0.081361\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0140721\n",
      "evaluation/env_infos/initial/height Std                 0.0434282\n",
      "evaluation/env_infos/initial/height Max                 0.0840747\n",
      "evaluation/env_infos/initial/height Min                -0.0830435\n",
      "evaluation/env_infos/height Mean                       -0.28516\n",
      "evaluation/env_infos/height Std                         0.242799\n",
      "evaluation/env_infos/height Max                         0.400651\n",
      "evaluation/env_infos/height Min                        -0.587767\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.200962\n",
      "evaluation/env_infos/final/reward_angular Std           0.872812\n",
      "evaluation/env_infos/final/reward_angular Max           1.27981\n",
      "evaluation/env_infos/final/reward_angular Min          -2.66939\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0866285\n",
      "evaluation/env_infos/initial/reward_angular Std         0.943662\n",
      "evaluation/env_infos/initial/reward_angular Max         1.73536\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.68534\n",
      "evaluation/env_infos/reward_angular Mean                0.0312936\n",
      "evaluation/env_infos/reward_angular Std                 1.20274\n",
      "evaluation/env_infos/reward_angular Max                 7.02121\n",
      "evaluation/env_infos/reward_angular Min                -6.40085\n",
      "time/data storing (s)                                   0.0152523\n",
      "time/evaluation sampling (s)                           20.8092\n",
      "time/exploration sampling (s)                           1.02169\n",
      "time/logging (s)                                        0.236133\n",
      "time/saving (s)                                         0.0285928\n",
      "time/training (s)                                       3.71302\n",
      "time/epoch (s)                                         25.8239\n",
      "time/total (s)                                       1186.07\n",
      "Epoch                                                  36\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:32:14.687092 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 37 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  39000\n",
      "trainer/QF1 Loss                                        0.524877\n",
      "trainer/QF2 Loss                                        0.581953\n",
      "trainer/Policy Loss                                     3.48527\n",
      "trainer/Q1 Predictions Mean                             2.56823\n",
      "trainer/Q1 Predictions Std                              4.00486\n",
      "trainer/Q1 Predictions Max                             19.0049\n",
      "trainer/Q1 Predictions Min                             -4.60681\n",
      "trainer/Q2 Predictions Mean                             2.34805\n",
      "trainer/Q2 Predictions Std                              4.02229\n",
      "trainer/Q2 Predictions Max                             18.4317\n",
      "trainer/Q2 Predictions Min                             -5.11085\n",
      "trainer/Q Targets Mean                                  2.5361\n",
      "trainer/Q Targets Std                                   4.02856\n",
      "trainer/Q Targets Max                                  19.2491\n",
      "trainer/Q Targets Min                                  -6.02828\n",
      "trainer/Log Pis Mean                                    6.26477\n",
      "trainer/Log Pis Std                                     5.45445\n",
      "trainer/Log Pis Max                                    27.5762\n",
      "trainer/Log Pis Min                                    -6.7242\n",
      "trainer/Policy mu Mean                                  0.187729\n",
      "trainer/Policy mu Std                                   1.61211\n",
      "trainer/Policy mu Max                                   5.62507\n",
      "trainer/Policy mu Min                                  -5.93125\n",
      "trainer/Policy log std Mean                            -0.746204\n",
      "trainer/Policy log std Std                              0.350887\n",
      "trainer/Policy log std Max                              0.102852\n",
      "trainer/Policy log std Min                             -2.51522\n",
      "trainer/Alpha                                           0.0111767\n",
      "trainer/Alpha Loss                                      1.18997\n",
      "exploration/num steps total                         39000\n",
      "exploration/num paths total                            39\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0602377\n",
      "exploration/Rewards Std                                 1.10057\n",
      "exploration/Rewards Max                                 4.13878\n",
      "exploration/Rewards Min                                -4.19396\n",
      "exploration/Returns Mean                               60.2377\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                60.2377\n",
      "exploration/Returns Min                                60.2377\n",
      "exploration/Actions Mean                                0.0413339\n",
      "exploration/Actions Std                                 0.646271\n",
      "exploration/Actions Max                                 0.999811\n",
      "exploration/Actions Min                                -0.999955\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            60.2377\n",
      "exploration/env_infos/final/reward_run Mean            -0.239679\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.239679\n",
      "exploration/env_infos/final/reward_run Min             -0.239679\n",
      "exploration/env_infos/initial/reward_run Mean          -0.87929\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.87929\n",
      "exploration/env_infos/initial/reward_run Min           -0.87929\n",
      "exploration/env_infos/reward_run Mean                  -0.242129\n",
      "exploration/env_infos/reward_run Std                    0.653451\n",
      "exploration/env_infos/reward_run Max                    1.51346\n",
      "exploration/env_infos/reward_run Min                   -2.83611\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.200934\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.200934\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.200934\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.232854\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.232854\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.232854\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.251625\n",
      "exploration/env_infos/reward_ctrl Std                   0.0953927\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0304226\n",
      "exploration/env_infos/reward_ctrl Min                  -0.538578\n",
      "exploration/env_infos/final/height Mean                -0.472543\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.472543\n",
      "exploration/env_infos/final/height Min                 -0.472543\n",
      "exploration/env_infos/initial/height Mean               0.0732144\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0732144\n",
      "exploration/env_infos/initial/height Min                0.0732144\n",
      "exploration/env_infos/height Mean                      -0.457586\n",
      "exploration/env_infos/height Std                        0.153604\n",
      "exploration/env_infos/height Max                        0.17436\n",
      "exploration/env_infos/height Min                       -0.586084\n",
      "exploration/env_infos/final/reward_angular Mean         0.345096\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.345096\n",
      "exploration/env_infos/final/reward_angular Min          0.345096\n",
      "exploration/env_infos/initial/reward_angular Mean       0.999853\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.999853\n",
      "exploration/env_infos/initial/reward_angular Min        0.999853\n",
      "exploration/env_infos/reward_angular Mean              -0.0617211\n",
      "exploration/env_infos/reward_angular Std                1.56095\n",
      "exploration/env_infos/reward_angular Max                5.09879\n",
      "exploration/env_infos/reward_angular Min               -6.89734\n",
      "evaluation/num steps total                         950000\n",
      "evaluation/num paths total                            950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.19156\n",
      "evaluation/Rewards Std                                  0.732934\n",
      "evaluation/Rewards Max                                  4.6143\n",
      "evaluation/Rewards Min                                 -4.92864\n",
      "evaluation/Returns Mean                              -191.56\n",
      "evaluation/Returns Std                                166.671\n",
      "evaluation/Returns Max                                173.131\n",
      "evaluation/Returns Min                               -539.746\n",
      "evaluation/Actions Mean                                 0.0250622\n",
      "evaluation/Actions Std                                  0.712008\n",
      "evaluation/Actions Max                                  0.999969\n",
      "evaluation/Actions Min                                 -0.999992\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -191.56\n",
      "evaluation/env_infos/final/reward_run Mean             -0.157724\n",
      "evaluation/env_infos/final/reward_run Std               0.432403\n",
      "evaluation/env_infos/final/reward_run Max               0.396272\n",
      "evaluation/env_infos/final/reward_run Min              -1.52369\n",
      "evaluation/env_infos/initial/reward_run Mean            0.230202\n",
      "evaluation/env_infos/initial/reward_run Std             0.528051\n",
      "evaluation/env_infos/initial/reward_run Max             0.919516\n",
      "evaluation/env_infos/initial/reward_run Min            -0.720524\n",
      "evaluation/env_infos/reward_run Mean                   -0.13708\n",
      "evaluation/env_infos/reward_run Std                     0.674092\n",
      "evaluation/env_infos/reward_run Max                     2.90742\n",
      "evaluation/env_infos/reward_run Min                    -3.71506\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.313798\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.120318\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.153356\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.538456\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.292742\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.104581\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0720087\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.449987\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.30455\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109518\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0102723\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.58091\n",
      "evaluation/env_infos/final/height Mean                 -0.320725\n",
      "evaluation/env_infos/final/height Std                   0.268278\n",
      "evaluation/env_infos/final/height Max                   0.083371\n",
      "evaluation/env_infos/final/height Min                  -0.580158\n",
      "evaluation/env_infos/initial/height Mean                0.00885639\n",
      "evaluation/env_infos/initial/height Std                 0.0511494\n",
      "evaluation/env_infos/initial/height Max                 0.0888428\n",
      "evaluation/env_infos/initial/height Min                -0.0801881\n",
      "evaluation/env_infos/height Mean                       -0.271598\n",
      "evaluation/env_infos/height Std                         0.268595\n",
      "evaluation/env_infos/height Max                         0.433996\n",
      "evaluation/env_infos/height Min                        -0.600315\n",
      "evaluation/env_infos/final/reward_angular Mean          0.205371\n",
      "evaluation/env_infos/final/reward_angular Std           1.2451\n",
      "evaluation/env_infos/final/reward_angular Max           2.3671\n",
      "evaluation/env_infos/final/reward_angular Min          -2.53629\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.265081\n",
      "evaluation/env_infos/initial/reward_angular Std         0.941711\n",
      "evaluation/env_infos/initial/reward_angular Max         2.16738\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.39733\n",
      "evaluation/env_infos/reward_angular Mean                0.0222861\n",
      "evaluation/env_infos/reward_angular Std                 1.32417\n",
      "evaluation/env_infos/reward_angular Max                 8.90874\n",
      "evaluation/env_infos/reward_angular Min               -11.5206\n",
      "time/data storing (s)                                   0.0154704\n",
      "time/evaluation sampling (s)                           20.739\n",
      "time/exploration sampling (s)                           0.969743\n",
      "time/logging (s)                                        0.237102\n",
      "time/saving (s)                                         0.0289316\n",
      "time/training (s)                                       3.70971\n",
      "time/epoch (s)                                         25.7\n",
      "time/total (s)                                       1212.13\n",
      "Epoch                                                  37\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:32:41.730535 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 38 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  40000\n",
      "trainer/QF1 Loss                                        0.8106\n",
      "trainer/QF2 Loss                                        1.22936\n",
      "trainer/Policy Loss                                     2.00792\n",
      "trainer/Q1 Predictions Mean                             3.4429\n",
      "trainer/Q1 Predictions Std                              4.52847\n",
      "trainer/Q1 Predictions Max                             23.1785\n",
      "trainer/Q1 Predictions Min                             -5.39741\n",
      "trainer/Q2 Predictions Mean                             3.33254\n",
      "trainer/Q2 Predictions Std                              4.57105\n",
      "trainer/Q2 Predictions Max                             23.2646\n",
      "trainer/Q2 Predictions Min                             -5.8099\n",
      "trainer/Q Targets Mean                                  3.31301\n",
      "trainer/Q Targets Std                                   4.46047\n",
      "trainer/Q Targets Max                                  23.4745\n",
      "trainer/Q Targets Min                                  -5.57535\n",
      "trainer/Log Pis Mean                                    5.75754\n",
      "trainer/Log Pis Std                                     5.66612\n",
      "trainer/Log Pis Max                                    38.9558\n",
      "trainer/Log Pis Min                                    -5.13138\n",
      "trainer/Policy mu Mean                                  0.0667199\n",
      "trainer/Policy mu Std                                   1.57503\n",
      "trainer/Policy mu Max                                   4.87286\n",
      "trainer/Policy mu Min                                  -7.18252\n",
      "trainer/Policy log std Mean                            -0.751898\n",
      "trainer/Policy log std Std                              0.353687\n",
      "trainer/Policy log std Max                              0.224454\n",
      "trainer/Policy log std Min                             -2.54125\n",
      "trainer/Alpha                                           0.0110479\n",
      "trainer/Alpha Loss                                     -1.09234\n",
      "exploration/num steps total                         40000\n",
      "exploration/num paths total                            40\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.619949\n",
      "exploration/Rewards Std                                 0.139508\n",
      "exploration/Rewards Max                                 0.229701\n",
      "exploration/Rewards Min                                -0.797203\n",
      "exploration/Returns Mean                             -619.949\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -619.949\n",
      "exploration/Returns Min                              -619.949\n",
      "exploration/Actions Mean                                0.144158\n",
      "exploration/Actions Std                                 0.750637\n",
      "exploration/Actions Max                                 0.999543\n",
      "exploration/Actions Min                                -0.996533\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -619.949\n",
      "exploration/env_infos/final/reward_run Mean            -0.120837\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.120837\n",
      "exploration/env_infos/final/reward_run Min             -0.120837\n",
      "exploration/env_infos/initial/reward_run Mean          -0.454246\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.454246\n",
      "exploration/env_infos/initial/reward_run Min           -0.454246\n",
      "exploration/env_infos/reward_run Mean                  -0.0646122\n",
      "exploration/env_infos/reward_run Std                    0.250206\n",
      "exploration/env_infos/reward_run Max                    0.679387\n",
      "exploration/env_infos/reward_run Min                   -1.72027\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.348011\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.348011\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.348011\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.178897\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.178897\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.178897\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.350542\n",
      "exploration/env_infos/reward_ctrl Std                   0.0686993\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0799758\n",
      "exploration/env_infos/reward_ctrl Min                  -0.530919\n",
      "exploration/env_infos/final/height Mean                -0.577131\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577131\n",
      "exploration/env_infos/final/height Min                 -0.577131\n",
      "exploration/env_infos/initial/height Mean               0.00841045\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00841045\n",
      "exploration/env_infos/initial/height Min                0.00841045\n",
      "exploration/env_infos/height Mean                      -0.548966\n",
      "exploration/env_infos/height Std                        0.115907\n",
      "exploration/env_infos/height Max                        0.29866\n",
      "exploration/env_infos/height Min                       -0.580699\n",
      "exploration/env_infos/final/reward_angular Mean         0.00166884\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.00166884\n",
      "exploration/env_infos/final/reward_angular Min          0.00166884\n",
      "exploration/env_infos/initial/reward_angular Mean       1.0581\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.0581\n",
      "exploration/env_infos/initial/reward_angular Min        1.0581\n",
      "exploration/env_infos/reward_angular Mean              -0.0689946\n",
      "exploration/env_infos/reward_angular Std                0.54697\n",
      "exploration/env_infos/reward_angular Max                3.38609\n",
      "exploration/env_infos/reward_angular Min               -5.83698\n",
      "evaluation/num steps total                         975000\n",
      "evaluation/num paths total                            975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                -0.223692\n",
      "evaluation/Rewards Std                                  0.655605\n",
      "evaluation/Rewards Max                                  5.36001\n",
      "evaluation/Rewards Min                                 -5.77479\n",
      "evaluation/Returns Mean                              -223.692\n",
      "evaluation/Returns Std                                140.035\n",
      "evaluation/Returns Max                                -50.1174\n",
      "evaluation/Returns Min                               -605.701\n",
      "evaluation/Actions Mean                                 0.138457\n",
      "evaluation/Actions Std                                  0.699831\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           -223.692\n",
      "evaluation/env_infos/final/reward_run Mean             -0.020358\n",
      "evaluation/env_infos/final/reward_run Std               0.511099\n",
      "evaluation/env_infos/final/reward_run Max               1.24452\n",
      "evaluation/env_infos/final/reward_run Min              -1.24951\n",
      "evaluation/env_infos/initial/reward_run Mean            0.190203\n",
      "evaluation/env_infos/initial/reward_run Std             0.445195\n",
      "evaluation/env_infos/initial/reward_run Max             0.981952\n",
      "evaluation/env_infos/initial/reward_run Min            -0.610308\n",
      "evaluation/env_infos/reward_run Mean                   -0.0340547\n",
      "evaluation/env_infos/reward_run Std                     0.469504\n",
      "evaluation/env_infos/reward_run Max                     2.80486\n",
      "evaluation/env_infos/reward_run Min                    -2.58466\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.305962\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10845\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0328587\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.462191\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266313\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0947453\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0286591\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.404356\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.305361\n",
      "evaluation/env_infos/reward_ctrl Std                    0.114495\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00759007\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598126\n",
      "evaluation/env_infos/final/height Mean                 -0.404869\n",
      "evaluation/env_infos/final/height Std                   0.193548\n",
      "evaluation/env_infos/final/height Max                  -0.0560764\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.000456455\n",
      "evaluation/env_infos/initial/height Std                 0.0636526\n",
      "evaluation/env_infos/initial/height Max                 0.081502\n",
      "evaluation/env_infos/initial/height Min                -0.103793\n",
      "evaluation/env_infos/height Mean                       -0.341249\n",
      "evaluation/env_infos/height Std                         0.218021\n",
      "evaluation/env_infos/height Max                         0.435359\n",
      "evaluation/env_infos/height Min                        -0.595288\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0150628\n",
      "evaluation/env_infos/final/reward_angular Std           0.832332\n",
      "evaluation/env_infos/final/reward_angular Max           2.24177\n",
      "evaluation/env_infos/final/reward_angular Min          -2.71718\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0971324\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07311\n",
      "evaluation/env_infos/initial/reward_angular Max         1.41356\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.21725\n",
      "evaluation/env_infos/reward_angular Mean                0.00379725\n",
      "evaluation/env_infos/reward_angular Std                 1.11109\n",
      "evaluation/env_infos/reward_angular Max                10.1034\n",
      "evaluation/env_infos/reward_angular Min                -7.38532\n",
      "time/data storing (s)                                   0.0149401\n",
      "time/evaluation sampling (s)                           21.4307\n",
      "time/exploration sampling (s)                           1.08325\n",
      "time/logging (s)                                        0.238613\n",
      "time/saving (s)                                         0.028653\n",
      "time/training (s)                                       3.88254\n",
      "time/epoch (s)                                         26.6787\n",
      "time/total (s)                                       1239.17\n",
      "Epoch                                                  38\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:33:09.400864 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 39 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 41000\n",
      "trainer/QF1 Loss                                       0.485766\n",
      "trainer/QF2 Loss                                       0.579268\n",
      "trainer/Policy Loss                                    1.46178\n",
      "trainer/Q1 Predictions Mean                            3.24353\n",
      "trainer/Q1 Predictions Std                             5.23609\n",
      "trainer/Q1 Predictions Max                            31.9568\n",
      "trainer/Q1 Predictions Min                            -4.38134\n",
      "trainer/Q2 Predictions Mean                            3.12365\n",
      "trainer/Q2 Predictions Std                             5.17845\n",
      "trainer/Q2 Predictions Max                            29.8212\n",
      "trainer/Q2 Predictions Min                            -4.69683\n",
      "trainer/Q Targets Mean                                 3.14012\n",
      "trainer/Q Targets Std                                  5.43814\n",
      "trainer/Q Targets Max                                 33.8768\n",
      "trainer/Q Targets Min                                 -4.4937\n",
      "trainer/Log Pis Mean                                   4.86214\n",
      "trainer/Log Pis Std                                    4.62129\n",
      "trainer/Log Pis Max                                   25.187\n",
      "trainer/Log Pis Min                                   -6.59539\n",
      "trainer/Policy mu Mean                                 0.158578\n",
      "trainer/Policy mu Std                                  1.43727\n",
      "trainer/Policy mu Max                                  4.24319\n",
      "trainer/Policy mu Min                                 -4.20187\n",
      "trainer/Policy log std Mean                           -0.754281\n",
      "trainer/Policy log std Std                             0.372876\n",
      "trainer/Policy log std Max                             0.128839\n",
      "trainer/Policy log std Min                            -3.32602\n",
      "trainer/Alpha                                          0.0109098\n",
      "trainer/Alpha Loss                                    -5.1403\n",
      "exploration/num steps total                        41000\n",
      "exploration/num paths total                           41\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.282701\n",
      "exploration/Rewards Std                                0.642583\n",
      "exploration/Rewards Max                                2.0609\n",
      "exploration/Rewards Min                               -2.20802\n",
      "exploration/Returns Mean                            -282.701\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -282.701\n",
      "exploration/Returns Min                             -282.701\n",
      "exploration/Actions Mean                              -0.00506996\n",
      "exploration/Actions Std                                0.736015\n",
      "exploration/Actions Max                                0.999901\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -282.701\n",
      "exploration/env_infos/final/reward_run Mean           -1.05214\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.05214\n",
      "exploration/env_infos/final/reward_run Min            -1.05214\n",
      "exploration/env_infos/initial/reward_run Mean          0.603527\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.603527\n",
      "exploration/env_infos/initial/reward_run Min           0.603527\n",
      "exploration/env_infos/reward_run Mean                  0.307836\n",
      "exploration/env_infos/reward_run Std                   0.739413\n",
      "exploration/env_infos/reward_run Max                   2.66775\n",
      "exploration/env_infos/reward_run Min                  -1.39621\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.145762\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.145762\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.145762\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.378679\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.378679\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.378679\n",
      "exploration/env_infos/reward_ctrl Mean                -0.325046\n",
      "exploration/env_infos/reward_ctrl Std                  0.0850838\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0890812\n",
      "exploration/env_infos/reward_ctrl Min                 -0.560729\n",
      "exploration/env_infos/final/height Mean               -0.569185\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.569185\n",
      "exploration/env_infos/final/height Min                -0.569185\n",
      "exploration/env_infos/initial/height Mean              0.0235545\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0235545\n",
      "exploration/env_infos/initial/height Min               0.0235545\n",
      "exploration/env_infos/height Mean                     -0.417853\n",
      "exploration/env_infos/height Std                       0.207557\n",
      "exploration/env_infos/height Max                       0.301193\n",
      "exploration/env_infos/height Min                      -0.582328\n",
      "exploration/env_infos/final/reward_angular Mean       -1.24522\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.24522\n",
      "exploration/env_infos/final/reward_angular Min        -1.24522\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.571309\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.571309\n",
      "exploration/env_infos/initial/reward_angular Min      -0.571309\n",
      "exploration/env_infos/reward_angular Mean              0.10727\n",
      "exploration/env_infos/reward_angular Std               1.68848\n",
      "exploration/env_infos/reward_angular Max               6.17093\n",
      "exploration/env_infos/reward_angular Min              -5.56205\n",
      "evaluation/num steps total                             1e+06\n",
      "evaluation/num paths total                          1000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.102807\n",
      "evaluation/Rewards Std                                 0.797657\n",
      "evaluation/Rewards Max                                 7.09766\n",
      "evaluation/Rewards Min                                -5.60751\n",
      "evaluation/Returns Mean                             -102.807\n",
      "evaluation/Returns Std                               299.174\n",
      "evaluation/Returns Max                               956.718\n",
      "evaluation/Returns Min                              -508.607\n",
      "evaluation/Actions Mean                                0.178128\n",
      "evaluation/Actions Std                                 0.67291\n",
      "evaluation/Actions Max                                 0.999963\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -102.807\n",
      "evaluation/env_infos/final/reward_run Mean            -0.150203\n",
      "evaluation/env_infos/final/reward_run Std              0.66396\n",
      "evaluation/env_infos/final/reward_run Max              1.20662\n",
      "evaluation/env_infos/final/reward_run Min             -1.45603\n",
      "evaluation/env_infos/initial/reward_run Mean           0.187444\n",
      "evaluation/env_infos/initial/reward_run Std            0.453216\n",
      "evaluation/env_infos/initial/reward_run Max            1.14108\n",
      "evaluation/env_infos/initial/reward_run Min           -0.630106\n",
      "evaluation/env_infos/reward_run Mean                  -0.257888\n",
      "evaluation/env_infos/reward_run Std                    0.750833\n",
      "evaluation/env_infos/reward_run Max                    2.70369\n",
      "evaluation/env_infos/reward_run Min                   -3.03532\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.282627\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.102166\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0932852\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.41926\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.269462\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.107845\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0742606\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.440565\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.290722\n",
      "evaluation/env_infos/reward_ctrl Std                   0.100125\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0118515\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.576814\n",
      "evaluation/env_infos/final/height Mean                -0.350102\n",
      "evaluation/env_infos/final/height Std                  0.211338\n",
      "evaluation/env_infos/final/height Max                  0.0116251\n",
      "evaluation/env_infos/final/height Min                 -0.577339\n",
      "evaluation/env_infos/initial/height Mean               0.0023441\n",
      "evaluation/env_infos/initial/height Std                0.053034\n",
      "evaluation/env_infos/initial/height Max                0.08793\n",
      "evaluation/env_infos/initial/height Min               -0.0864264\n",
      "evaluation/env_infos/height Mean                      -0.307224\n",
      "evaluation/env_infos/height Std                        0.221171\n",
      "evaluation/env_infos/height Max                        0.416679\n",
      "evaluation/env_infos/height Min                       -0.596394\n",
      "evaluation/env_infos/final/reward_angular Mean         0.127239\n",
      "evaluation/env_infos/final/reward_angular Std          0.737675\n",
      "evaluation/env_infos/final/reward_angular Max          1.59922\n",
      "evaluation/env_infos/final/reward_angular Min         -2.24529\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.132386\n",
      "evaluation/env_infos/initial/reward_angular Std        0.734181\n",
      "evaluation/env_infos/initial/reward_angular Max        1.63965\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.29247\n",
      "evaluation/env_infos/reward_angular Mean               0.0273255\n",
      "evaluation/env_infos/reward_angular Std                1.2189\n",
      "evaluation/env_infos/reward_angular Max                8.07767\n",
      "evaluation/env_infos/reward_angular Min               -6.02367\n",
      "time/data storing (s)                                  0.0141929\n",
      "time/evaluation sampling (s)                          21.7856\n",
      "time/exploration sampling (s)                          1.10792\n",
      "time/logging (s)                                       0.241123\n",
      "time/saving (s)                                        0.0269634\n",
      "time/training (s)                                      4.12347\n",
      "time/epoch (s)                                        27.2992\n",
      "time/total (s)                                      1266.84\n",
      "Epoch                                                 39\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:33:36.031585 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 40 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 42000\n",
      "trainer/QF1 Loss                                       0.716366\n",
      "trainer/QF2 Loss                                       0.692567\n",
      "trainer/Policy Loss                                    3.41947\n",
      "trainer/Q1 Predictions Mean                            2.51724\n",
      "trainer/Q1 Predictions Std                             3.9945\n",
      "trainer/Q1 Predictions Max                            29.1674\n",
      "trainer/Q1 Predictions Min                            -3.93652\n",
      "trainer/Q2 Predictions Mean                            2.55274\n",
      "trainer/Q2 Predictions Std                             4.07277\n",
      "trainer/Q2 Predictions Max                            29.6271\n",
      "trainer/Q2 Predictions Min                            -4.8064\n",
      "trainer/Q Targets Mean                                 2.61998\n",
      "trainer/Q Targets Std                                  4.06887\n",
      "trainer/Q Targets Max                                 28.5215\n",
      "trainer/Q Targets Min                                 -5.34348\n",
      "trainer/Log Pis Mean                                   6.22602\n",
      "trainer/Log Pis Std                                    6.29374\n",
      "trainer/Log Pis Max                                   30.3851\n",
      "trainer/Log Pis Min                                   -5.98025\n",
      "trainer/Policy mu Mean                                 0.145859\n",
      "trainer/Policy mu Std                                  1.6273\n",
      "trainer/Policy mu Max                                  4.51451\n",
      "trainer/Policy mu Min                                 -7.13673\n",
      "trainer/Policy log std Mean                           -0.76722\n",
      "trainer/Policy log std Std                             0.334728\n",
      "trainer/Policy log std Max                             0.107961\n",
      "trainer/Policy log std Min                            -2.50341\n",
      "trainer/Alpha                                          0.0104837\n",
      "trainer/Alpha Loss                                     1.03014\n",
      "exploration/num steps total                        42000\n",
      "exploration/num paths total                           42\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.199006\n",
      "exploration/Rewards Std                                0.742909\n",
      "exploration/Rewards Max                                1.63014\n",
      "exploration/Rewards Min                               -2.75275\n",
      "exploration/Returns Mean                            -199.006\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -199.006\n",
      "exploration/Returns Min                             -199.006\n",
      "exploration/Actions Mean                               0.0966933\n",
      "exploration/Actions Std                                0.635484\n",
      "exploration/Actions Max                                0.999066\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -199.006\n",
      "exploration/env_infos/final/reward_run Mean            0.0596931\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.0596931\n",
      "exploration/env_infos/final/reward_run Min             0.0596931\n",
      "exploration/env_infos/initial/reward_run Mean          0.579403\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.579403\n",
      "exploration/env_infos/initial/reward_run Min           0.579403\n",
      "exploration/env_infos/reward_run Mean                  0.00484749\n",
      "exploration/env_infos/reward_run Std                   0.560395\n",
      "exploration/env_infos/reward_run Max                   1.72252\n",
      "exploration/env_infos/reward_run Min                  -1.68729\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.23551\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.23551\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.23551\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.287463\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.287463\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.287463\n",
      "exploration/env_infos/reward_ctrl Mean                -0.247914\n",
      "exploration/env_infos/reward_ctrl Std                  0.0815276\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0281219\n",
      "exploration/env_infos/reward_ctrl Min                 -0.554018\n",
      "exploration/env_infos/final/height Mean               -0.0674056\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0674056\n",
      "exploration/env_infos/final/height Min                -0.0674056\n",
      "exploration/env_infos/initial/height Mean             -0.0826541\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0826541\n",
      "exploration/env_infos/initial/height Min              -0.0826541\n",
      "exploration/env_infos/height Mean                     -0.0473693\n",
      "exploration/env_infos/height Std                       0.0618708\n",
      "exploration/env_infos/height Max                       0.228367\n",
      "exploration/env_infos/height Min                      -0.224828\n",
      "exploration/env_infos/final/reward_angular Mean       -1.00404\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.00404\n",
      "exploration/env_infos/final/reward_angular Min        -1.00404\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.984765\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.984765\n",
      "exploration/env_infos/initial/reward_angular Min      -0.984765\n",
      "exploration/env_infos/reward_angular Mean             -0.00353098\n",
      "exploration/env_infos/reward_angular Std               1.25366\n",
      "exploration/env_infos/reward_angular Max               4.46145\n",
      "exploration/env_infos/reward_angular Min              -3.09543\n",
      "evaluation/num steps total                             1.025e+06\n",
      "evaluation/num paths total                          1025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.193044\n",
      "evaluation/Rewards Std                                 0.784554\n",
      "evaluation/Rewards Max                                 4.92057\n",
      "evaluation/Rewards Min                                -4.78153\n",
      "evaluation/Returns Mean                             -193.044\n",
      "evaluation/Returns Std                               166.015\n",
      "evaluation/Returns Max                                88.6717\n",
      "evaluation/Returns Min                              -563.607\n",
      "evaluation/Actions Mean                                0.111308\n",
      "evaluation/Actions Std                                 0.713103\n",
      "evaluation/Actions Max                                 0.999969\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -193.044\n",
      "evaluation/env_infos/final/reward_run Mean             0.0053768\n",
      "evaluation/env_infos/final/reward_run Std              0.507948\n",
      "evaluation/env_infos/final/reward_run Max              1.27114\n",
      "evaluation/env_infos/final/reward_run Min             -1.11104\n",
      "evaluation/env_infos/initial/reward_run Mean           0.456142\n",
      "evaluation/env_infos/initial/reward_run Std            0.440636\n",
      "evaluation/env_infos/initial/reward_run Max            1.13268\n",
      "evaluation/env_infos/initial/reward_run Min           -0.33675\n",
      "evaluation/env_infos/reward_run Mean                   0.056278\n",
      "evaluation/env_infos/reward_run Std                    0.579457\n",
      "evaluation/env_infos/reward_run Max                    2.6272\n",
      "evaluation/env_infos/reward_run Min                   -2.04571\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.325233\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0982643\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.148467\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.485635\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.321539\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.129414\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0622806\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.492619\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.312543\n",
      "evaluation/env_infos/reward_ctrl Std                   0.111365\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00654889\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.573882\n",
      "evaluation/env_infos/final/height Mean                -0.260351\n",
      "evaluation/env_infos/final/height Std                  0.230329\n",
      "evaluation/env_infos/final/height Max                  0.0713168\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean               0.000188121\n",
      "evaluation/env_infos/initial/height Std                0.0478512\n",
      "evaluation/env_infos/initial/height Max                0.0936035\n",
      "evaluation/env_infos/initial/height Min               -0.0661052\n",
      "evaluation/env_infos/height Mean                      -0.239904\n",
      "evaluation/env_infos/height Std                        0.242414\n",
      "evaluation/env_infos/height Max                        0.330115\n",
      "evaluation/env_infos/height Min                       -0.589238\n",
      "evaluation/env_infos/final/reward_angular Mean         0.0919646\n",
      "evaluation/env_infos/final/reward_angular Std          1.07528\n",
      "evaluation/env_infos/final/reward_angular Max          3.76198\n",
      "evaluation/env_infos/final/reward_angular Min         -1.98192\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.536759\n",
      "evaluation/env_infos/initial/reward_angular Std        1.18051\n",
      "evaluation/env_infos/initial/reward_angular Max        3.43837\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.14966\n",
      "evaluation/env_infos/reward_angular Mean               0.0393666\n",
      "evaluation/env_infos/reward_angular Std                1.29579\n",
      "evaluation/env_infos/reward_angular Max                7.5986\n",
      "evaluation/env_infos/reward_angular Min               -5.92513\n",
      "time/data storing (s)                                  0.0153063\n",
      "time/evaluation sampling (s)                          21.2348\n",
      "time/exploration sampling (s)                          1.00884\n",
      "time/logging (s)                                       0.234962\n",
      "time/saving (s)                                        0.0270671\n",
      "time/training (s)                                      3.70969\n",
      "time/epoch (s)                                        26.2307\n",
      "time/total (s)                                      1293.47\n",
      "Epoch                                                 40\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:34:02.442755 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 41 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 43000\n",
      "trainer/QF1 Loss                                       0.649755\n",
      "trainer/QF2 Loss                                       0.582254\n",
      "trainer/Policy Loss                                    3.24202\n",
      "trainer/Q1 Predictions Mean                            3.44093\n",
      "trainer/Q1 Predictions Std                             4.08982\n",
      "trainer/Q1 Predictions Max                            24.6016\n",
      "trainer/Q1 Predictions Min                            -4.01439\n",
      "trainer/Q2 Predictions Mean                            3.35785\n",
      "trainer/Q2 Predictions Std                             4.07661\n",
      "trainer/Q2 Predictions Max                            21.8855\n",
      "trainer/Q2 Predictions Min                            -5.58193\n",
      "trainer/Q Targets Mean                                 3.4661\n",
      "trainer/Q Targets Std                                  4.23908\n",
      "trainer/Q Targets Max                                 24.7331\n",
      "trainer/Q Targets Min                                 -5.72902\n",
      "trainer/Log Pis Mean                                   7.11503\n",
      "trainer/Log Pis Std                                    5.89792\n",
      "trainer/Log Pis Max                                   28.6959\n",
      "trainer/Log Pis Min                                   -6.82903\n",
      "trainer/Policy mu Mean                                 0.229478\n",
      "trainer/Policy mu Std                                  1.66767\n",
      "trainer/Policy mu Max                                  5.52534\n",
      "trainer/Policy mu Min                                 -5.48059\n",
      "trainer/Policy log std Mean                           -0.790785\n",
      "trainer/Policy log std Std                             0.329922\n",
      "trainer/Policy log std Max                             0.118057\n",
      "trainer/Policy log std Min                            -2.3687\n",
      "trainer/Alpha                                          0.0109752\n",
      "trainer/Alpha Loss                                     5.03147\n",
      "exploration/num steps total                        43000\n",
      "exploration/num paths total                           43\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.0116193\n",
      "exploration/Rewards Std                                0.623159\n",
      "exploration/Rewards Max                                2.27731\n",
      "exploration/Rewards Min                               -2.00075\n",
      "exploration/Returns Mean                              11.6193\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                               11.6193\n",
      "exploration/Returns Min                               11.6193\n",
      "exploration/Actions Mean                               0.306596\n",
      "exploration/Actions Std                                0.660228\n",
      "exploration/Actions Max                                0.999878\n",
      "exploration/Actions Min                               -0.99997\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                           11.6193\n",
      "exploration/env_infos/final/reward_run Mean            1.42065\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.42065\n",
      "exploration/env_infos/final/reward_run Min             1.42065\n",
      "exploration/env_infos/initial/reward_run Mean          0.709386\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.709386\n",
      "exploration/env_infos/initial/reward_run Min           0.709386\n",
      "exploration/env_infos/reward_run Mean                  0.461269\n",
      "exploration/env_infos/reward_run Std                   0.612519\n",
      "exploration/env_infos/reward_run Max                   2.61958\n",
      "exploration/env_infos/reward_run Min                  -2.0143\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.398653\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.398653\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.398653\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.208726\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.208726\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.208726\n",
      "exploration/env_infos/reward_ctrl Mean                -0.317941\n",
      "exploration/env_infos/reward_ctrl Std                  0.0748728\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0518367\n",
      "exploration/env_infos/reward_ctrl Min                 -0.507334\n",
      "exploration/env_infos/final/height Mean               -0.227783\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.227783\n",
      "exploration/env_infos/final/height Min                -0.227783\n",
      "exploration/env_infos/initial/height Mean             -0.0923839\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0923839\n",
      "exploration/env_infos/initial/height Min              -0.0923839\n",
      "exploration/env_infos/height Mean                     -0.204393\n",
      "exploration/env_infos/height Std                       0.0537543\n",
      "exploration/env_infos/height Max                      -0.0113103\n",
      "exploration/env_infos/height Min                      -0.375381\n",
      "exploration/env_infos/final/reward_angular Mean       -1.82733\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.82733\n",
      "exploration/env_infos/final/reward_angular Min        -1.82733\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.36419\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.36419\n",
      "exploration/env_infos/initial/reward_angular Min      -1.36419\n",
      "exploration/env_infos/reward_angular Mean              0.00159495\n",
      "exploration/env_infos/reward_angular Std               1.19764\n",
      "exploration/env_infos/reward_angular Max               4.02821\n",
      "exploration/env_infos/reward_angular Min              -4.40722\n",
      "evaluation/num steps total                             1.05e+06\n",
      "evaluation/num paths total                          1050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.124465\n",
      "evaluation/Rewards Std                                 0.712703\n",
      "evaluation/Rewards Max                                 4.41005\n",
      "evaluation/Rewards Min                                -4.05835\n",
      "evaluation/Returns Mean                             -124.465\n",
      "evaluation/Returns Std                               189.206\n",
      "evaluation/Returns Max                               237.959\n",
      "evaluation/Returns Min                              -543.475\n",
      "evaluation/Actions Mean                                0.245269\n",
      "evaluation/Actions Std                                 0.70225\n",
      "evaluation/Actions Max                                 0.99997\n",
      "evaluation/Actions Min                                -0.999989\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -124.465\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0320693\n",
      "evaluation/env_infos/final/reward_run Std              0.575057\n",
      "evaluation/env_infos/final/reward_run Max              2.09976\n",
      "evaluation/env_infos/final/reward_run Min             -1.1449\n",
      "evaluation/env_infos/initial/reward_run Mean           0.373793\n",
      "evaluation/env_infos/initial/reward_run Std            0.583777\n",
      "evaluation/env_infos/initial/reward_run Max            1.12182\n",
      "evaluation/env_infos/initial/reward_run Min           -0.870793\n",
      "evaluation/env_infos/reward_run Mean                  -0.0572157\n",
      "evaluation/env_infos/reward_run Std                    0.634914\n",
      "evaluation/env_infos/reward_run Max                    2.91902\n",
      "evaluation/env_infos/reward_run Min                   -2.22646\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.338522\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.118288\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0723595\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.523856\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.311147\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.132762\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0863793\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.483933\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.331987\n",
      "evaluation/env_infos/reward_ctrl Std                   0.113121\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0330354\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.58608\n",
      "evaluation/env_infos/final/height Mean                -0.32421\n",
      "evaluation/env_infos/final/height Std                  0.223831\n",
      "evaluation/env_infos/final/height Max                  0.00893244\n",
      "evaluation/env_infos/final/height Min                 -0.583257\n",
      "evaluation/env_infos/initial/height Mean               0.00544094\n",
      "evaluation/env_infos/initial/height Std                0.0525433\n",
      "evaluation/env_infos/initial/height Max                0.0819496\n",
      "evaluation/env_infos/initial/height Min               -0.0782682\n",
      "evaluation/env_infos/height Mean                      -0.310217\n",
      "evaluation/env_infos/height Std                        0.215307\n",
      "evaluation/env_infos/height Max                        0.385371\n",
      "evaluation/env_infos/height Min                       -0.591494\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.177583\n",
      "evaluation/env_infos/final/reward_angular Std          1.14206\n",
      "evaluation/env_infos/final/reward_angular Max          1.98002\n",
      "evaluation/env_infos/final/reward_angular Min         -3.46763\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.139538\n",
      "evaluation/env_infos/initial/reward_angular Std        0.980271\n",
      "evaluation/env_infos/initial/reward_angular Max        2.02045\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.88224\n",
      "evaluation/env_infos/reward_angular Mean               0.0448032\n",
      "evaluation/env_infos/reward_angular Std                1.16497\n",
      "evaluation/env_infos/reward_angular Max                8.70694\n",
      "evaluation/env_infos/reward_angular Min               -6.82093\n",
      "time/data storing (s)                                  0.0146935\n",
      "time/evaluation sampling (s)                          20.8226\n",
      "time/exploration sampling (s)                          0.996375\n",
      "time/logging (s)                                       0.237748\n",
      "time/saving (s)                                        0.0279334\n",
      "time/training (s)                                      3.93016\n",
      "time/epoch (s)                                        26.0295\n",
      "time/total (s)                                      1319.88\n",
      "Epoch                                                 41\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:34:28.784366 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 42 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 44000\n",
      "trainer/QF1 Loss                                       0.781145\n",
      "trainer/QF2 Loss                                       0.699807\n",
      "trainer/Policy Loss                                    2.61163\n",
      "trainer/Q1 Predictions Mean                            3.15784\n",
      "trainer/Q1 Predictions Std                             4.44935\n",
      "trainer/Q1 Predictions Max                            25.8818\n",
      "trainer/Q1 Predictions Min                            -5.8342\n",
      "trainer/Q2 Predictions Mean                            3.19546\n",
      "trainer/Q2 Predictions Std                             4.48447\n",
      "trainer/Q2 Predictions Max                            23.7455\n",
      "trainer/Q2 Predictions Min                            -5.78446\n",
      "trainer/Q Targets Mean                                 3.07787\n",
      "trainer/Q Targets Std                                  4.44583\n",
      "trainer/Q Targets Max                                 23.2307\n",
      "trainer/Q Targets Min                                 -6.8866\n",
      "trainer/Log Pis Mean                                   6.20813\n",
      "trainer/Log Pis Std                                    5.71464\n",
      "trainer/Log Pis Max                                   29.9785\n",
      "trainer/Log Pis Min                                   -5.51093\n",
      "trainer/Policy mu Mean                                 0.0160076\n",
      "trainer/Policy mu Std                                  1.6044\n",
      "trainer/Policy mu Max                                  4.69545\n",
      "trainer/Policy mu Min                                 -6.69121\n",
      "trainer/Policy log std Mean                           -0.809922\n",
      "trainer/Policy log std Std                             0.30654\n",
      "trainer/Policy log std Max                            -0.0538103\n",
      "trainer/Policy log std Min                            -2.15055\n",
      "trainer/Alpha                                          0.0108917\n",
      "trainer/Alpha Loss                                     0.940756\n",
      "exploration/num steps total                        44000\n",
      "exploration/num paths total                           44\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.322358\n",
      "exploration/Rewards Std                                0.579628\n",
      "exploration/Rewards Max                                2.45611\n",
      "exploration/Rewards Min                               -1.45232\n",
      "exploration/Returns Mean                             322.358\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              322.358\n",
      "exploration/Returns Min                              322.358\n",
      "exploration/Actions Mean                               0.115495\n",
      "exploration/Actions Std                                0.631514\n",
      "exploration/Actions Max                                0.999392\n",
      "exploration/Actions Min                               -0.999473\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          322.358\n",
      "exploration/env_infos/final/reward_run Mean            1.16571\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.16571\n",
      "exploration/env_infos/final/reward_run Min             1.16571\n",
      "exploration/env_infos/initial/reward_run Mean          0.707493\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.707493\n",
      "exploration/env_infos/initial/reward_run Min           0.707493\n",
      "exploration/env_infos/reward_run Mean                  0.423284\n",
      "exploration/env_infos/reward_run Std                   0.406728\n",
      "exploration/env_infos/reward_run Max                   1.58544\n",
      "exploration/env_infos/reward_run Min                  -0.933824\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.189116\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.189116\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.189116\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.188867\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.188867\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.188867\n",
      "exploration/env_infos/reward_ctrl Mean                -0.247289\n",
      "exploration/env_infos/reward_ctrl Std                  0.0777114\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0267827\n",
      "exploration/env_infos/reward_ctrl Min                 -0.476697\n",
      "exploration/env_infos/final/height Mean               -0.313401\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.313401\n",
      "exploration/env_infos/final/height Min                -0.313401\n",
      "exploration/env_infos/initial/height Mean              0.0203122\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0203122\n",
      "exploration/env_infos/initial/height Min               0.0203122\n",
      "exploration/env_infos/height Mean                     -0.251563\n",
      "exploration/env_infos/height Std                       0.0598281\n",
      "exploration/env_infos/height Max                       0.0203122\n",
      "exploration/env_infos/height Min                      -0.405172\n",
      "exploration/env_infos/final/reward_angular Mean        1.27696\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.27696\n",
      "exploration/env_infos/final/reward_angular Min         1.27696\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.30408\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.30408\n",
      "exploration/env_infos/initial/reward_angular Min      -1.30408\n",
      "exploration/env_infos/reward_angular Mean             -0.00451626\n",
      "exploration/env_infos/reward_angular Std               1.0995\n",
      "exploration/env_infos/reward_angular Max               3.69775\n",
      "exploration/env_infos/reward_angular Min              -4.53578\n",
      "evaluation/num steps total                             1.075e+06\n",
      "evaluation/num paths total                          1075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.137699\n",
      "evaluation/Rewards Std                                 0.994962\n",
      "evaluation/Rewards Max                                 5.66963\n",
      "evaluation/Rewards Min                                -5.55387\n",
      "evaluation/Returns Mean                             -137.699\n",
      "evaluation/Returns Std                               178.445\n",
      "evaluation/Returns Max                               197.582\n",
      "evaluation/Returns Min                              -483.893\n",
      "evaluation/Actions Mean                                0.106224\n",
      "evaluation/Actions Std                                 0.701996\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -137.699\n",
      "evaluation/env_infos/final/reward_run Mean            -0.041742\n",
      "evaluation/env_infos/final/reward_run Std              0.575123\n",
      "evaluation/env_infos/final/reward_run Max              1.28895\n",
      "evaluation/env_infos/final/reward_run Min             -1.51761\n",
      "evaluation/env_infos/initial/reward_run Mean           0.152426\n",
      "evaluation/env_infos/initial/reward_run Std            0.615876\n",
      "evaluation/env_infos/initial/reward_run Max            1.03249\n",
      "evaluation/env_infos/initial/reward_run Min           -0.913017\n",
      "evaluation/env_infos/reward_run Mean                  -0.28321\n",
      "evaluation/env_infos/reward_run Std                    0.811427\n",
      "evaluation/env_infos/reward_run Max                    2.702\n",
      "evaluation/env_infos/reward_run Min                   -3.13863\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.298551\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.124977\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.093906\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.536952\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.324072\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.124379\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0690576\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.492208\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.302449\n",
      "evaluation/env_infos/reward_ctrl Std                   0.121239\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0182798\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.585278\n",
      "evaluation/env_infos/final/height Mean                -0.268713\n",
      "evaluation/env_infos/final/height Std                  0.243886\n",
      "evaluation/env_infos/final/height Max                  0.166319\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean               0.00825211\n",
      "evaluation/env_infos/initial/height Std                0.0383245\n",
      "evaluation/env_infos/initial/height Max                0.0780348\n",
      "evaluation/env_infos/initial/height Min               -0.0710065\n",
      "evaluation/env_infos/height Mean                      -0.213006\n",
      "evaluation/env_infos/height Std                        0.231123\n",
      "evaluation/env_infos/height Max                        0.349066\n",
      "evaluation/env_infos/height Min                       -0.591797\n",
      "evaluation/env_infos/final/reward_angular Mean         0.750541\n",
      "evaluation/env_infos/final/reward_angular Std          1.76377\n",
      "evaluation/env_infos/final/reward_angular Max          4.83775\n",
      "evaluation/env_infos/final/reward_angular Min         -2.83652\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.721347\n",
      "evaluation/env_infos/initial/reward_angular Std        0.926742\n",
      "evaluation/env_infos/initial/reward_angular Max        2.24108\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.03002\n",
      "evaluation/env_infos/reward_angular Mean               0.0547759\n",
      "evaluation/env_infos/reward_angular Std                1.3787\n",
      "evaluation/env_infos/reward_angular Max                8.24366\n",
      "evaluation/env_infos/reward_angular Min               -5.98416\n",
      "time/data storing (s)                                  0.0153277\n",
      "time/evaluation sampling (s)                          20.8512\n",
      "time/exploration sampling (s)                          0.996252\n",
      "time/logging (s)                                       0.224971\n",
      "time/saving (s)                                        0.0277208\n",
      "time/training (s)                                      3.82821\n",
      "time/epoch (s)                                        25.9437\n",
      "time/total (s)                                      1346.21\n",
      "Epoch                                                 42\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:34:54.884812 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 43 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 45000\n",
      "trainer/QF1 Loss                                       0.753977\n",
      "trainer/QF2 Loss                                       0.546045\n",
      "trainer/Policy Loss                                    3.14197\n",
      "trainer/Q1 Predictions Mean                            2.73766\n",
      "trainer/Q1 Predictions Std                             4.82373\n",
      "trainer/Q1 Predictions Max                            29.7803\n",
      "trainer/Q1 Predictions Min                            -4.33212\n",
      "trainer/Q2 Predictions Mean                            2.51832\n",
      "trainer/Q2 Predictions Std                             4.78006\n",
      "trainer/Q2 Predictions Max                            29.0484\n",
      "trainer/Q2 Predictions Min                            -4.71861\n",
      "trainer/Q Targets Mean                                 2.66336\n",
      "trainer/Q Targets Std                                  4.74957\n",
      "trainer/Q Targets Max                                 30.3912\n",
      "trainer/Q Targets Min                                 -4.48678\n",
      "trainer/Log Pis Mean                                   6.14559\n",
      "trainer/Log Pis Std                                    5.79488\n",
      "trainer/Log Pis Max                                   27.1718\n",
      "trainer/Log Pis Min                                   -5.38317\n",
      "trainer/Policy mu Mean                                 0.196513\n",
      "trainer/Policy mu Std                                  1.58347\n",
      "trainer/Policy mu Max                                  5.24695\n",
      "trainer/Policy mu Min                                 -4.32154\n",
      "trainer/Policy log std Mean                           -0.764771\n",
      "trainer/Policy log std Std                             0.328524\n",
      "trainer/Policy log std Max                             0.148645\n",
      "trainer/Policy log std Min                            -2.50834\n",
      "trainer/Alpha                                          0.0116317\n",
      "trainer/Alpha Loss                                     0.648522\n",
      "exploration/num steps total                        45000\n",
      "exploration/num paths total                           45\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.182706\n",
      "exploration/Rewards Std                                0.991517\n",
      "exploration/Rewards Max                                3.40363\n",
      "exploration/Rewards Min                               -3.00329\n",
      "exploration/Returns Mean                            -182.706\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -182.706\n",
      "exploration/Returns Min                             -182.706\n",
      "exploration/Actions Mean                               0.194573\n",
      "exploration/Actions Std                                0.797133\n",
      "exploration/Actions Max                                0.999933\n",
      "exploration/Actions Min                               -0.999955\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -182.706\n",
      "exploration/env_infos/final/reward_run Mean            0.546565\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.546565\n",
      "exploration/env_infos/final/reward_run Min             0.546565\n",
      "exploration/env_infos/initial/reward_run Mean          0.0988977\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0988977\n",
      "exploration/env_infos/initial/reward_run Min           0.0988977\n",
      "exploration/env_infos/reward_run Mean                 -0.0720001\n",
      "exploration/env_infos/reward_run Std                   0.708706\n",
      "exploration/env_infos/reward_run Max                   1.88174\n",
      "exploration/env_infos/reward_run Min                  -1.92318\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.307615\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.307615\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.307615\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.463384\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.463384\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.463384\n",
      "exploration/env_infos/reward_ctrl Mean                -0.403968\n",
      "exploration/env_infos/reward_ctrl Std                  0.0805527\n",
      "exploration/env_infos/reward_ctrl Max                 -0.121845\n",
      "exploration/env_infos/reward_ctrl Min                 -0.587737\n",
      "exploration/env_infos/final/height Mean               -0.416262\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.416262\n",
      "exploration/env_infos/final/height Min                -0.416262\n",
      "exploration/env_infos/initial/height Mean              0.0737205\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0737205\n",
      "exploration/env_infos/initial/height Min               0.0737205\n",
      "exploration/env_infos/height Mean                     -0.479543\n",
      "exploration/env_infos/height Std                       0.118678\n",
      "exploration/env_infos/height Max                       0.187983\n",
      "exploration/env_infos/height Min                      -0.592299\n",
      "exploration/env_infos/final/reward_angular Mean        0.426476\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.426476\n",
      "exploration/env_infos/final/reward_angular Min         0.426476\n",
      "exploration/env_infos/initial/reward_angular Mean      1.32804\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.32804\n",
      "exploration/env_infos/initial/reward_angular Min       1.32804\n",
      "exploration/env_infos/reward_angular Mean              0.134897\n",
      "exploration/env_infos/reward_angular Std               1.50559\n",
      "exploration/env_infos/reward_angular Max               5.03807\n",
      "exploration/env_infos/reward_angular Min              -4.23288\n",
      "evaluation/num steps total                             1.1e+06\n",
      "evaluation/num paths total                          1100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.117004\n",
      "evaluation/Rewards Std                                 0.990305\n",
      "evaluation/Rewards Max                                 5.29413\n",
      "evaluation/Rewards Min                                -5.64618\n",
      "evaluation/Returns Mean                             -117.004\n",
      "evaluation/Returns Std                               279.154\n",
      "evaluation/Returns Max                               637.379\n",
      "evaluation/Returns Min                              -564.792\n",
      "evaluation/Actions Mean                                0.238785\n",
      "evaluation/Actions Std                                 0.694221\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -117.004\n",
      "evaluation/env_infos/final/reward_run Mean            -0.374147\n",
      "evaluation/env_infos/final/reward_run Std              0.835541\n",
      "evaluation/env_infos/final/reward_run Max              1.32855\n",
      "evaluation/env_infos/final/reward_run Min             -1.93907\n",
      "evaluation/env_infos/initial/reward_run Mean           0.134466\n",
      "evaluation/env_infos/initial/reward_run Std            0.602433\n",
      "evaluation/env_infos/initial/reward_run Max            1.15849\n",
      "evaluation/env_infos/initial/reward_run Min           -0.906423\n",
      "evaluation/env_infos/reward_run Mean                  -0.274217\n",
      "evaluation/env_infos/reward_run Std                    0.773831\n",
      "evaluation/env_infos/reward_run Max                    2.41448\n",
      "evaluation/env_infos/reward_run Min                   -2.7959\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.336066\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.135305\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0826897\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.571911\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.341959\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.135249\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0557634\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.521855\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.323377\n",
      "evaluation/env_infos/reward_ctrl Std                   0.119887\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00855926\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.587168\n",
      "evaluation/env_infos/final/height Mean                -0.287104\n",
      "evaluation/env_infos/final/height Std                  0.209514\n",
      "evaluation/env_infos/final/height Max                  0.0060347\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00489759\n",
      "evaluation/env_infos/initial/height Std                0.0506007\n",
      "evaluation/env_infos/initial/height Max                0.0880479\n",
      "evaluation/env_infos/initial/height Min               -0.068067\n",
      "evaluation/env_infos/height Mean                      -0.264866\n",
      "evaluation/env_infos/height Std                        0.227276\n",
      "evaluation/env_infos/height Max                        0.424487\n",
      "evaluation/env_infos/height Min                       -0.597744\n",
      "evaluation/env_infos/final/reward_angular Mean         0.147599\n",
      "evaluation/env_infos/final/reward_angular Std          1.45892\n",
      "evaluation/env_infos/final/reward_angular Max          2.43297\n",
      "evaluation/env_infos/final/reward_angular Min         -2.96087\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.676369\n",
      "evaluation/env_infos/initial/reward_angular Std        1.00282\n",
      "evaluation/env_infos/initial/reward_angular Max        2.75499\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.53301\n",
      "evaluation/env_infos/reward_angular Mean               0.0501017\n",
      "evaluation/env_infos/reward_angular Std                1.44688\n",
      "evaluation/env_infos/reward_angular Max                7.80865\n",
      "evaluation/env_infos/reward_angular Min               -5.16026\n",
      "time/data storing (s)                                  0.0151542\n",
      "time/evaluation sampling (s)                          20.6152\n",
      "time/exploration sampling (s)                          0.977179\n",
      "time/logging (s)                                       0.243426\n",
      "time/saving (s)                                        0.0298675\n",
      "time/training (s)                                      3.86319\n",
      "time/epoch (s)                                        25.744\n",
      "time/total (s)                                      1372.32\n",
      "Epoch                                                 43\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:35:21.389890 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 44 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 46000\n",
      "trainer/QF1 Loss                                       0.74614\n",
      "trainer/QF2 Loss                                       0.783822\n",
      "trainer/Policy Loss                                    2.63997\n",
      "trainer/Q1 Predictions Mean                            3.16404\n",
      "trainer/Q1 Predictions Std                             4.98819\n",
      "trainer/Q1 Predictions Max                            34.7343\n",
      "trainer/Q1 Predictions Min                            -3.21067\n",
      "trainer/Q2 Predictions Mean                            3.23162\n",
      "trainer/Q2 Predictions Std                             4.89936\n",
      "trainer/Q2 Predictions Max                            32.1681\n",
      "trainer/Q2 Predictions Min                            -3.24838\n",
      "trainer/Q Targets Mean                                 3.01236\n",
      "trainer/Q Targets Std                                  5.09767\n",
      "trainer/Q Targets Max                                 34.8153\n",
      "trainer/Q Targets Min                                 -3.66509\n",
      "trainer/Log Pis Mean                                   6.1328\n",
      "trainer/Log Pis Std                                    6.03969\n",
      "trainer/Log Pis Max                                   33.8418\n",
      "trainer/Log Pis Min                                   -7.38654\n",
      "trainer/Policy mu Mean                                 0.138708\n",
      "trainer/Policy mu Std                                  1.59496\n",
      "trainer/Policy mu Max                                  5.67621\n",
      "trainer/Policy mu Min                                 -7.44188\n",
      "trainer/Policy log std Mean                           -0.767273\n",
      "trainer/Policy log std Std                             0.339595\n",
      "trainer/Policy log std Max                             0.154666\n",
      "trainer/Policy log std Min                            -2.92623\n",
      "trainer/Alpha                                          0.0116166\n",
      "trainer/Alpha Loss                                     0.591735\n",
      "exploration/num steps total                        46000\n",
      "exploration/num paths total                           46\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.253752\n",
      "exploration/Rewards Std                                0.70115\n",
      "exploration/Rewards Max                                1.62696\n",
      "exploration/Rewards Min                               -1.92308\n",
      "exploration/Returns Mean                            -253.752\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -253.752\n",
      "exploration/Returns Min                             -253.752\n",
      "exploration/Actions Mean                               0.280397\n",
      "exploration/Actions Std                                0.734012\n",
      "exploration/Actions Max                                0.999997\n",
      "exploration/Actions Min                               -0.999856\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -253.752\n",
      "exploration/env_infos/final/reward_run Mean           -0.111283\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.111283\n",
      "exploration/env_infos/final/reward_run Min            -0.111283\n",
      "exploration/env_infos/initial/reward_run Mean          0.952591\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.952591\n",
      "exploration/env_infos/initial/reward_run Min           0.952591\n",
      "exploration/env_infos/reward_run Mean                  0.17838\n",
      "exploration/env_infos/reward_run Std                   0.715826\n",
      "exploration/env_infos/reward_run Max                   2.41008\n",
      "exploration/env_infos/reward_run Min                  -1.7583\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.491778\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.491778\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.491778\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.497687\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.497687\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.497687\n",
      "exploration/env_infos/reward_ctrl Mean                -0.370438\n",
      "exploration/env_infos/reward_ctrl Std                  0.102496\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0595766\n",
      "exploration/env_infos/reward_ctrl Min                 -0.584864\n",
      "exploration/env_infos/final/height Mean               -0.492965\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.492965\n",
      "exploration/env_infos/final/height Min                -0.492965\n",
      "exploration/env_infos/initial/height Mean             -0.0285998\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0285998\n",
      "exploration/env_infos/initial/height Min              -0.0285998\n",
      "exploration/env_infos/height Mean                     -0.46203\n",
      "exploration/env_infos/height Std                       0.140922\n",
      "exploration/env_infos/height Max                       0.160332\n",
      "exploration/env_infos/height Min                      -0.585515\n",
      "exploration/env_infos/final/reward_angular Mean        1.72482\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.72482\n",
      "exploration/env_infos/final/reward_angular Min         1.72482\n",
      "exploration/env_infos/initial/reward_angular Mean      2.43794\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.43794\n",
      "exploration/env_infos/initial/reward_angular Min       2.43794\n",
      "exploration/env_infos/reward_angular Mean              0.0531803\n",
      "exploration/env_infos/reward_angular Std               1.75349\n",
      "exploration/env_infos/reward_angular Max               7.38082\n",
      "exploration/env_infos/reward_angular Min              -5.08461\n",
      "evaluation/num steps total                             1.125e+06\n",
      "evaluation/num paths total                          1125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0805305\n",
      "evaluation/Rewards Std                                 0.77959\n",
      "evaluation/Rewards Max                                 4.85374\n",
      "evaluation/Rewards Min                                -4.38556\n",
      "evaluation/Returns Mean                              -80.5305\n",
      "evaluation/Returns Std                               237.679\n",
      "evaluation/Returns Max                               493.625\n",
      "evaluation/Returns Min                              -457.108\n",
      "evaluation/Actions Mean                                0.203412\n",
      "evaluation/Actions Std                                 0.673221\n",
      "evaluation/Actions Max                                 0.999997\n",
      "evaluation/Actions Min                                -0.999993\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -80.5305\n",
      "evaluation/env_infos/final/reward_run Mean             0.0445674\n",
      "evaluation/env_infos/final/reward_run Std              0.532291\n",
      "evaluation/env_infos/final/reward_run Max              1.14288\n",
      "evaluation/env_infos/final/reward_run Min             -0.991918\n",
      "evaluation/env_infos/initial/reward_run Mean           0.266347\n",
      "evaluation/env_infos/initial/reward_run Std            0.548584\n",
      "evaluation/env_infos/initial/reward_run Max            0.965651\n",
      "evaluation/env_infos/initial/reward_run Min           -0.888563\n",
      "evaluation/env_infos/reward_run Mean                  -0.130148\n",
      "evaluation/env_infos/reward_run Std                    0.639918\n",
      "evaluation/env_infos/reward_run Max                    2.66862\n",
      "evaluation/env_infos/reward_run Min                   -1.9278\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.30007\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.122853\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.111591\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.499876\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.326944\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.135532\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0458376\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.496749\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.296762\n",
      "evaluation/env_infos/reward_ctrl Std                   0.136849\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00913744\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.592002\n",
      "evaluation/env_infos/final/height Mean                -0.270942\n",
      "evaluation/env_infos/final/height Std                  0.210971\n",
      "evaluation/env_infos/final/height Max                  0.0076082\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.0187425\n",
      "evaluation/env_infos/initial/height Std                0.0517135\n",
      "evaluation/env_infos/initial/height Max                0.0892951\n",
      "evaluation/env_infos/initial/height Min               -0.0659727\n",
      "evaluation/env_infos/height Mean                      -0.254888\n",
      "evaluation/env_infos/height Std                        0.214041\n",
      "evaluation/env_infos/height Max                        0.300341\n",
      "evaluation/env_infos/height Min                       -0.592651\n",
      "evaluation/env_infos/final/reward_angular Mean         0.26609\n",
      "evaluation/env_infos/final/reward_angular Std          1.32689\n",
      "evaluation/env_infos/final/reward_angular Max          4.61068\n",
      "evaluation/env_infos/final/reward_angular Min         -1.88615\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.507032\n",
      "evaluation/env_infos/initial/reward_angular Std        1.35661\n",
      "evaluation/env_infos/initial/reward_angular Max        3.47352\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.10162\n",
      "evaluation/env_infos/reward_angular Mean               0.0437839\n",
      "evaluation/env_infos/reward_angular Std                1.18944\n",
      "evaluation/env_infos/reward_angular Max                8.28717\n",
      "evaluation/env_infos/reward_angular Min               -5.10268\n",
      "time/data storing (s)                                  0.0147581\n",
      "time/evaluation sampling (s)                          20.909\n",
      "time/exploration sampling (s)                          0.960015\n",
      "time/logging (s)                                       0.237081\n",
      "time/saving (s)                                        0.0277731\n",
      "time/training (s)                                      3.94584\n",
      "time/epoch (s)                                        26.0945\n",
      "time/total (s)                                      1398.82\n",
      "Epoch                                                 44\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:35:47.899157 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 45 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 47000\n",
      "trainer/QF1 Loss                                       0.760813\n",
      "trainer/QF2 Loss                                       0.744448\n",
      "trainer/Policy Loss                                    2.52936\n",
      "trainer/Q1 Predictions Mean                            2.55306\n",
      "trainer/Q1 Predictions Std                             5.21975\n",
      "trainer/Q1 Predictions Max                            38.0303\n",
      "trainer/Q1 Predictions Min                            -4.37673\n",
      "trainer/Q2 Predictions Mean                            2.6103\n",
      "trainer/Q2 Predictions Std                             5.31602\n",
      "trainer/Q2 Predictions Max                            37.5563\n",
      "trainer/Q2 Predictions Min                            -4.77625\n",
      "trainer/Q Targets Mean                                 2.53543\n",
      "trainer/Q Targets Std                                  5.254\n",
      "trainer/Q Targets Max                                 37.1832\n",
      "trainer/Q Targets Min                                 -5.14703\n",
      "trainer/Log Pis Mean                                   5.49657\n",
      "trainer/Log Pis Std                                    5.89299\n",
      "trainer/Log Pis Max                                   35.9983\n",
      "trainer/Log Pis Min                                   -4.47492\n",
      "trainer/Policy mu Mean                                 0.168953\n",
      "trainer/Policy mu Std                                  1.50422\n",
      "trainer/Policy mu Max                                  5.91191\n",
      "trainer/Policy mu Min                                 -4.33568\n",
      "trainer/Policy log std Mean                           -0.784392\n",
      "trainer/Policy log std Std                             0.340243\n",
      "trainer/Policy log std Max                             0.0377598\n",
      "trainer/Policy log std Min                            -2.63281\n",
      "trainer/Alpha                                          0.0112275\n",
      "trainer/Alpha Loss                                    -2.25965\n",
      "exploration/num steps total                        47000\n",
      "exploration/num paths total                           47\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.14301\n",
      "exploration/Rewards Std                                0.68542\n",
      "exploration/Rewards Max                                1.8535\n",
      "exploration/Rewards Min                               -2.29009\n",
      "exploration/Returns Mean                            -143.01\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -143.01\n",
      "exploration/Returns Min                             -143.01\n",
      "exploration/Actions Mean                               0.175836\n",
      "exploration/Actions Std                                0.750584\n",
      "exploration/Actions Max                                0.999993\n",
      "exploration/Actions Min                               -0.999989\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -143.01\n",
      "exploration/env_infos/final/reward_run Mean            0.0133247\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.0133247\n",
      "exploration/env_infos/final/reward_run Min             0.0133247\n",
      "exploration/env_infos/initial/reward_run Mean          0.890211\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.890211\n",
      "exploration/env_infos/initial/reward_run Min           0.890211\n",
      "exploration/env_infos/reward_run Mean                  0.207464\n",
      "exploration/env_infos/reward_run Std                   0.720309\n",
      "exploration/env_infos/reward_run Max                   2.29627\n",
      "exploration/env_infos/reward_run Min                  -1.73732\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.273903\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.273903\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.273903\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.509676\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.509676\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.509676\n",
      "exploration/env_infos/reward_ctrl Mean                -0.356577\n",
      "exploration/env_infos/reward_ctrl Std                  0.0914634\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0651147\n",
      "exploration/env_infos/reward_ctrl Min                 -0.587389\n",
      "exploration/env_infos/final/height Mean               -0.489586\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.489586\n",
      "exploration/env_infos/final/height Min                -0.489586\n",
      "exploration/env_infos/initial/height Mean              0.0779978\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0779978\n",
      "exploration/env_infos/initial/height Min               0.0779978\n",
      "exploration/env_infos/height Mean                     -0.43562\n",
      "exploration/env_infos/height Std                       0.125172\n",
      "exploration/env_infos/height Max                       0.195936\n",
      "exploration/env_infos/height Min                      -0.584291\n",
      "exploration/env_infos/final/reward_angular Mean        1.6828\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.6828\n",
      "exploration/env_infos/final/reward_angular Min         1.6828\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.546827\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.546827\n",
      "exploration/env_infos/initial/reward_angular Min      -0.546827\n",
      "exploration/env_infos/reward_angular Mean              0.10404\n",
      "exploration/env_infos/reward_angular Std               2.3468\n",
      "exploration/env_infos/reward_angular Max               8.06523\n",
      "exploration/env_infos/reward_angular Min              -7.0829\n",
      "evaluation/num steps total                             1.15e+06\n",
      "evaluation/num paths total                          1150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.105993\n",
      "evaluation/Rewards Std                                 0.853922\n",
      "evaluation/Rewards Max                                 6.38541\n",
      "evaluation/Rewards Min                                -4.6286\n",
      "evaluation/Returns Mean                             -105.993\n",
      "evaluation/Returns Std                               305.188\n",
      "evaluation/Returns Max                               883.428\n",
      "evaluation/Returns Min                              -444.779\n",
      "evaluation/Actions Mean                                0.0246187\n",
      "evaluation/Actions Std                                 0.721839\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -0.999969\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -105.993\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0512709\n",
      "evaluation/env_infos/final/reward_run Std              0.788139\n",
      "evaluation/env_infos/final/reward_run Max              2.31964\n",
      "evaluation/env_infos/final/reward_run Min             -1.60545\n",
      "evaluation/env_infos/initial/reward_run Mean           0.2786\n",
      "evaluation/env_infos/initial/reward_run Std            0.666125\n",
      "evaluation/env_infos/initial/reward_run Max            1.10728\n",
      "evaluation/env_infos/initial/reward_run Min           -0.814879\n",
      "evaluation/env_infos/reward_run Mean                  -0.0967925\n",
      "evaluation/env_infos/reward_run Std                    0.824082\n",
      "evaluation/env_infos/reward_run Max                    3.71012\n",
      "evaluation/env_infos/reward_run Min                   -2.63213\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.327486\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.116029\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0871611\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.517524\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.344686\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.143156\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0969299\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.553438\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.312994\n",
      "evaluation/env_infos/reward_ctrl Std                   0.121397\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0137351\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.593854\n",
      "evaluation/env_infos/final/height Mean                -0.323333\n",
      "evaluation/env_infos/final/height Std                  0.210883\n",
      "evaluation/env_infos/final/height Max                 -0.00792247\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00643858\n",
      "evaluation/env_infos/initial/height Std                0.0491226\n",
      "evaluation/env_infos/initial/height Max                0.0997023\n",
      "evaluation/env_infos/initial/height Min               -0.0687508\n",
      "evaluation/env_infos/height Mean                      -0.284923\n",
      "evaluation/env_infos/height Std                        0.218168\n",
      "evaluation/env_infos/height Max                        0.324614\n",
      "evaluation/env_infos/height Min                       -0.592772\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.112825\n",
      "evaluation/env_infos/final/reward_angular Std          1.87408\n",
      "evaluation/env_infos/final/reward_angular Max          2.73944\n",
      "evaluation/env_infos/final/reward_angular Min         -4.10237\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.465461\n",
      "evaluation/env_infos/initial/reward_angular Std        1.20403\n",
      "evaluation/env_infos/initial/reward_angular Max        3.21308\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.81065\n",
      "evaluation/env_infos/reward_angular Mean               0.0387745\n",
      "evaluation/env_infos/reward_angular Std                1.46351\n",
      "evaluation/env_infos/reward_angular Max                9.31437\n",
      "evaluation/env_infos/reward_angular Min               -6.56269\n",
      "time/data storing (s)                                  0.0150949\n",
      "time/evaluation sampling (s)                          21.0618\n",
      "time/exploration sampling (s)                          0.975449\n",
      "time/logging (s)                                       0.238253\n",
      "time/saving (s)                                        0.02818\n",
      "time/training (s)                                      3.7792\n",
      "time/epoch (s)                                        26.098\n",
      "time/total (s)                                      1425.33\n",
      "Epoch                                                 45\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:14.337343 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 46 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 48000\n",
      "trainer/QF1 Loss                                       0.729661\n",
      "trainer/QF2 Loss                                       0.650295\n",
      "trainer/Policy Loss                                    2.10793\n",
      "trainer/Q1 Predictions Mean                            3.11496\n",
      "trainer/Q1 Predictions Std                             4.90208\n",
      "trainer/Q1 Predictions Max                            24.8306\n",
      "trainer/Q1 Predictions Min                            -4.42365\n",
      "trainer/Q2 Predictions Mean                            3.10866\n",
      "trainer/Q2 Predictions Std                             4.92327\n",
      "trainer/Q2 Predictions Max                            25.6103\n",
      "trainer/Q2 Predictions Min                            -4.19482\n",
      "trainer/Q Targets Mean                                 3.016\n",
      "trainer/Q Targets Std                                  4.91663\n",
      "trainer/Q Targets Max                                 22.8287\n",
      "trainer/Q Targets Min                                 -4.39847\n",
      "trainer/Log Pis Mean                                   5.54229\n",
      "trainer/Log Pis Std                                    5.12144\n",
      "trainer/Log Pis Max                                   27.7146\n",
      "trainer/Log Pis Min                                   -5.56626\n",
      "trainer/Policy mu Mean                                 0.02885\n",
      "trainer/Policy mu Std                                  1.56673\n",
      "trainer/Policy mu Max                                  4.51361\n",
      "trainer/Policy mu Min                                 -6.1737\n",
      "trainer/Policy log std Mean                           -0.736259\n",
      "trainer/Policy log std Std                             0.308387\n",
      "trainer/Policy log std Max                             0.0546195\n",
      "trainer/Policy log std Min                            -1.93039\n",
      "trainer/Alpha                                          0.0112247\n",
      "trainer/Alpha Loss                                    -2.05488\n",
      "exploration/num steps total                        48000\n",
      "exploration/num paths total                           48\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.145352\n",
      "exploration/Rewards Std                                0.792246\n",
      "exploration/Rewards Max                                2.47481\n",
      "exploration/Rewards Min                               -2.48391\n",
      "exploration/Returns Mean                            -145.352\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -145.352\n",
      "exploration/Returns Min                             -145.352\n",
      "exploration/Actions Mean                              -0.0113393\n",
      "exploration/Actions Std                                0.79628\n",
      "exploration/Actions Max                                0.999905\n",
      "exploration/Actions Min                               -0.999984\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -145.352\n",
      "exploration/env_infos/final/reward_run Mean            0.401889\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.401889\n",
      "exploration/env_infos/final/reward_run Min             0.401889\n",
      "exploration/env_infos/initial/reward_run Mean          0.364291\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.364291\n",
      "exploration/env_infos/initial/reward_run Min           0.364291\n",
      "exploration/env_infos/reward_run Mean                  0.0705637\n",
      "exploration/env_infos/reward_run Std                   0.519783\n",
      "exploration/env_infos/reward_run Max                   1.58266\n",
      "exploration/env_infos/reward_run Min                  -1.45608\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.383542\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.383542\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.383542\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.389585\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.389585\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.389585\n",
      "exploration/env_infos/reward_ctrl Mean                -0.380515\n",
      "exploration/env_infos/reward_ctrl Std                  0.0787506\n",
      "exploration/env_infos/reward_ctrl Max                 -0.129414\n",
      "exploration/env_infos/reward_ctrl Min                 -0.55551\n",
      "exploration/env_infos/final/height Mean               -0.567629\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.567629\n",
      "exploration/env_infos/final/height Min                -0.567629\n",
      "exploration/env_infos/initial/height Mean              0.0465731\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0465731\n",
      "exploration/env_infos/initial/height Min               0.0465731\n",
      "exploration/env_infos/height Mean                     -0.487426\n",
      "exploration/env_infos/height Std                       0.102941\n",
      "exploration/env_infos/height Max                       0.249952\n",
      "exploration/env_infos/height Min                      -0.587968\n",
      "exploration/env_infos/final/reward_angular Mean        1.2001\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.2001\n",
      "exploration/env_infos/final/reward_angular Min         1.2001\n",
      "exploration/env_infos/initial/reward_angular Mean      0.489309\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.489309\n",
      "exploration/env_infos/initial/reward_angular Min       0.489309\n",
      "exploration/env_infos/reward_angular Mean              0.104889\n",
      "exploration/env_infos/reward_angular Std               1.59544\n",
      "exploration/env_infos/reward_angular Max               5.75833\n",
      "exploration/env_infos/reward_angular Min              -4.61818\n",
      "evaluation/num steps total                             1.175e+06\n",
      "evaluation/num paths total                          1175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0552411\n",
      "evaluation/Rewards Std                                 0.964985\n",
      "evaluation/Rewards Max                                 5.87648\n",
      "evaluation/Rewards Min                                -4.75395\n",
      "evaluation/Returns Mean                              -55.2411\n",
      "evaluation/Returns Std                               383.886\n",
      "evaluation/Returns Max                               965.293\n",
      "evaluation/Returns Min                              -460.437\n",
      "evaluation/Actions Mean                                0.0858096\n",
      "evaluation/Actions Std                                 0.710415\n",
      "evaluation/Actions Max                                 0.999997\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -55.2411\n",
      "evaluation/env_infos/final/reward_run Mean            -0.506218\n",
      "evaluation/env_infos/final/reward_run Std              0.874486\n",
      "evaluation/env_infos/final/reward_run Max              1.08708\n",
      "evaluation/env_infos/final/reward_run Min             -2.26276\n",
      "evaluation/env_infos/initial/reward_run Mean           0.161644\n",
      "evaluation/env_infos/initial/reward_run Std            0.586839\n",
      "evaluation/env_infos/initial/reward_run Max            1.04001\n",
      "evaluation/env_infos/initial/reward_run Min           -0.796873\n",
      "evaluation/env_infos/reward_run Mean                  -0.485424\n",
      "evaluation/env_infos/reward_run Std                    0.960934\n",
      "evaluation/env_infos/reward_run Max                    2.47263\n",
      "evaluation/env_infos/reward_run Min                   -3.15023\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.315804\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.115028\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0913237\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.488625\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.310539\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.131523\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.078482\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.496336\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.307232\n",
      "evaluation/env_infos/reward_ctrl Std                   0.114406\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0135966\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.59729\n",
      "evaluation/env_infos/final/height Mean                -0.32641\n",
      "evaluation/env_infos/final/height Std                  0.198834\n",
      "evaluation/env_infos/final/height Max                 -0.0327311\n",
      "evaluation/env_infos/final/height Min                 -0.578807\n",
      "evaluation/env_infos/initial/height Mean               0.0133074\n",
      "evaluation/env_infos/initial/height Std                0.047988\n",
      "evaluation/env_infos/initial/height Max                0.091038\n",
      "evaluation/env_infos/initial/height Min               -0.0648124\n",
      "evaluation/env_infos/height Mean                      -0.297078\n",
      "evaluation/env_infos/height Std                        0.210048\n",
      "evaluation/env_infos/height Max                        0.361711\n",
      "evaluation/env_infos/height Min                       -0.591917\n",
      "evaluation/env_infos/final/reward_angular Mean         0.120219\n",
      "evaluation/env_infos/final/reward_angular Std          0.880312\n",
      "evaluation/env_infos/final/reward_angular Max          2.02828\n",
      "evaluation/env_infos/final/reward_angular Min         -1.86633\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.293671\n",
      "evaluation/env_infos/initial/reward_angular Std        1.03973\n",
      "evaluation/env_infos/initial/reward_angular Max        2.49823\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.06905\n",
      "evaluation/env_infos/reward_angular Mean               0.0407182\n",
      "evaluation/env_infos/reward_angular Std                1.38608\n",
      "evaluation/env_infos/reward_angular Max                7.84044\n",
      "evaluation/env_infos/reward_angular Min               -5.65827\n",
      "time/data storing (s)                                  0.0157605\n",
      "time/evaluation sampling (s)                          20.9604\n",
      "time/exploration sampling (s)                          1.02411\n",
      "time/logging (s)                                       0.234523\n",
      "time/saving (s)                                        0.0271962\n",
      "time/training (s)                                      3.75948\n",
      "time/epoch (s)                                        26.0214\n",
      "time/total (s)                                      1451.76\n",
      "Epoch                                                 46\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:36:40.715446 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 47 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 49000\n",
      "trainer/QF1 Loss                                       0.558057\n",
      "trainer/QF2 Loss                                       0.482507\n",
      "trainer/Policy Loss                                    2.48871\n",
      "trainer/Q1 Predictions Mean                            2.75535\n",
      "trainer/Q1 Predictions Std                             5.14068\n",
      "trainer/Q1 Predictions Max                            36.7821\n",
      "trainer/Q1 Predictions Min                            -6.94114\n",
      "trainer/Q2 Predictions Mean                            2.73563\n",
      "trainer/Q2 Predictions Std                             5.2564\n",
      "trainer/Q2 Predictions Max                            38.3845\n",
      "trainer/Q2 Predictions Min                            -6.92742\n",
      "trainer/Q Targets Mean                                 2.75398\n",
      "trainer/Q Targets Std                                  5.17958\n",
      "trainer/Q Targets Max                                 36.9095\n",
      "trainer/Q Targets Min                                 -7.79285\n",
      "trainer/Log Pis Mean                                   5.53271\n",
      "trainer/Log Pis Std                                    5.90282\n",
      "trainer/Log Pis Max                                   28.1411\n",
      "trainer/Log Pis Min                                   -6.36748\n",
      "trainer/Policy mu Mean                                 0.0356759\n",
      "trainer/Policy mu Std                                  1.53842\n",
      "trainer/Policy mu Max                                  5.23157\n",
      "trainer/Policy mu Min                                 -4.68383\n",
      "trainer/Policy log std Mean                           -0.778143\n",
      "trainer/Policy log std Std                             0.329027\n",
      "trainer/Policy log std Max                             0.117821\n",
      "trainer/Policy log std Min                            -3.12279\n",
      "trainer/Alpha                                          0.0110254\n",
      "trainer/Alpha Loss                                    -2.10602\n",
      "exploration/num steps total                        49000\n",
      "exploration/num paths total                           49\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.472386\n",
      "exploration/Rewards Std                                0.530764\n",
      "exploration/Rewards Max                                1.05393\n",
      "exploration/Rewards Min                               -2.34399\n",
      "exploration/Returns Mean                            -472.386\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -472.386\n",
      "exploration/Returns Min                             -472.386\n",
      "exploration/Actions Mean                               0.309037\n",
      "exploration/Actions Std                                0.65578\n",
      "exploration/Actions Max                                0.999683\n",
      "exploration/Actions Min                               -0.999939\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -472.386\n",
      "exploration/env_infos/final/reward_run Mean           -0.0079966\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.0079966\n",
      "exploration/env_infos/final/reward_run Min            -0.0079966\n",
      "exploration/env_infos/initial/reward_run Mean         -0.145333\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.145333\n",
      "exploration/env_infos/initial/reward_run Min          -0.145333\n",
      "exploration/env_infos/reward_run Mean                 -0.945916\n",
      "exploration/env_infos/reward_run Std                   0.754208\n",
      "exploration/env_infos/reward_run Max                   1.7706\n",
      "exploration/env_infos/reward_run Min                  -2.7505\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.39786\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.39786\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.39786\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.126539\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.126539\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.126539\n",
      "exploration/env_infos/reward_ctrl Mean                -0.315331\n",
      "exploration/env_infos/reward_ctrl Std                  0.0858478\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0340053\n",
      "exploration/env_infos/reward_ctrl Min                 -0.564915\n",
      "exploration/env_infos/final/height Mean               -0.153501\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.153501\n",
      "exploration/env_infos/final/height Min                -0.153501\n",
      "exploration/env_infos/initial/height Mean             -0.0443849\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0443849\n",
      "exploration/env_infos/initial/height Min              -0.0443849\n",
      "exploration/env_infos/height Mean                     -0.126251\n",
      "exploration/env_infos/height Std                       0.0676568\n",
      "exploration/env_infos/height Max                       0.148835\n",
      "exploration/env_infos/height Min                      -0.34414\n",
      "exploration/env_infos/final/reward_angular Mean        0.750453\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.750453\n",
      "exploration/env_infos/final/reward_angular Min         0.750453\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.439049\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.439049\n",
      "exploration/env_infos/initial/reward_angular Min      -0.439049\n",
      "exploration/env_infos/reward_angular Mean              0.0283993\n",
      "exploration/env_infos/reward_angular Std               1.31188\n",
      "exploration/env_infos/reward_angular Max               4.75919\n",
      "exploration/env_infos/reward_angular Min              -3.16398\n",
      "evaluation/num steps total                             1.2e+06\n",
      "evaluation/num paths total                          1200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0225059\n",
      "evaluation/Rewards Std                                 0.993639\n",
      "evaluation/Rewards Max                                 6.12231\n",
      "evaluation/Rewards Min                                -5.17532\n",
      "evaluation/Returns Mean                              -22.5059\n",
      "evaluation/Returns Std                               336.541\n",
      "evaluation/Returns Max                               941.471\n",
      "evaluation/Returns Min                              -467.122\n",
      "evaluation/Actions Mean                                0.0867651\n",
      "evaluation/Actions Std                                 0.710821\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -22.5059\n",
      "evaluation/env_infos/final/reward_run Mean            -0.546694\n",
      "evaluation/env_infos/final/reward_run Std              0.919552\n",
      "evaluation/env_infos/final/reward_run Max              1.26338\n",
      "evaluation/env_infos/final/reward_run Min             -2.62882\n",
      "evaluation/env_infos/initial/reward_run Mean           0.209296\n",
      "evaluation/env_infos/initial/reward_run Std            0.605783\n",
      "evaluation/env_infos/initial/reward_run Max            1.15328\n",
      "evaluation/env_infos/initial/reward_run Min           -0.794427\n",
      "evaluation/env_infos/reward_run Mean                  -0.438616\n",
      "evaluation/env_infos/reward_run Std                    0.939049\n",
      "evaluation/env_infos/reward_run Max                    2.85628\n",
      "evaluation/env_infos/reward_run Min                   -2.96145\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.323827\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.13525\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.037835\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.562664\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.298564\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.114106\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0924715\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.486506\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.307677\n",
      "evaluation/env_infos/reward_ctrl Std                   0.10949\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0198648\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.597619\n",
      "evaluation/env_infos/final/height Mean                -0.281645\n",
      "evaluation/env_infos/final/height Std                  0.250193\n",
      "evaluation/env_infos/final/height Max                  0.230318\n",
      "evaluation/env_infos/final/height Min                 -0.582273\n",
      "evaluation/env_infos/initial/height Mean              -0.0137173\n",
      "evaluation/env_infos/initial/height Std                0.0470989\n",
      "evaluation/env_infos/initial/height Max                0.0762514\n",
      "evaluation/env_infos/initial/height Min               -0.0924297\n",
      "evaluation/env_infos/height Mean                      -0.254147\n",
      "evaluation/env_infos/height Std                        0.231417\n",
      "evaluation/env_infos/height Max                        0.411391\n",
      "evaluation/env_infos/height Min                       -0.591458\n",
      "evaluation/env_infos/final/reward_angular Mean         0.10389\n",
      "evaluation/env_infos/final/reward_angular Std          0.993413\n",
      "evaluation/env_infos/final/reward_angular Max          2.73195\n",
      "evaluation/env_infos/final/reward_angular Min         -1.92772\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.473417\n",
      "evaluation/env_infos/initial/reward_angular Std        1.17902\n",
      "evaluation/env_infos/initial/reward_angular Max        3.17305\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.80822\n",
      "evaluation/env_infos/reward_angular Mean               0.0508168\n",
      "evaluation/env_infos/reward_angular Std                1.41774\n",
      "evaluation/env_infos/reward_angular Max                9.12001\n",
      "evaluation/env_infos/reward_angular Min               -5.79011\n",
      "time/data storing (s)                                  0.0148286\n",
      "time/evaluation sampling (s)                          20.8919\n",
      "time/exploration sampling (s)                          0.974699\n",
      "time/logging (s)                                       0.236812\n",
      "time/saving (s)                                        0.0279175\n",
      "time/training (s)                                      3.80977\n",
      "time/epoch (s)                                        25.9559\n",
      "time/total (s)                                      1478.14\n",
      "Epoch                                                 47\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:37:07.511415 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 48 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 50000\n",
      "trainer/QF1 Loss                                       0.632148\n",
      "trainer/QF2 Loss                                       0.560528\n",
      "trainer/Policy Loss                                    3.15049\n",
      "trainer/Q1 Predictions Mean                            2.43326\n",
      "trainer/Q1 Predictions Std                             5.40019\n",
      "trainer/Q1 Predictions Max                            36.4399\n",
      "trainer/Q1 Predictions Min                            -6.17632\n",
      "trainer/Q2 Predictions Mean                            2.39664\n",
      "trainer/Q2 Predictions Std                             5.51862\n",
      "trainer/Q2 Predictions Max                            38.3311\n",
      "trainer/Q2 Predictions Min                            -6.65731\n",
      "trainer/Q Targets Mean                                 2.4137\n",
      "trainer/Q Targets Std                                  5.59647\n",
      "trainer/Q Targets Max                                 40.9455\n",
      "trainer/Q Targets Min                                 -7.27938\n",
      "trainer/Log Pis Mean                                   5.85813\n",
      "trainer/Log Pis Std                                    5.23397\n",
      "trainer/Log Pis Max                                   21.8305\n",
      "trainer/Log Pis Min                                   -6.9416\n",
      "trainer/Policy mu Mean                                -0.152135\n",
      "trainer/Policy mu Std                                  1.56349\n",
      "trainer/Policy mu Max                                  4.56795\n",
      "trainer/Policy mu Min                                 -4.45179\n",
      "trainer/Policy log std Mean                           -0.752307\n",
      "trainer/Policy log std Std                             0.320427\n",
      "trainer/Policy log std Max                             0.257616\n",
      "trainer/Policy log std Min                            -2.42575\n",
      "trainer/Alpha                                          0.0111615\n",
      "trainer/Alpha Loss                                    -0.637773\n",
      "exploration/num steps total                        50000\n",
      "exploration/num paths total                           50\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.291674\n",
      "exploration/Rewards Std                                0.897288\n",
      "exploration/Rewards Max                                3.10037\n",
      "exploration/Rewards Min                               -2.87358\n",
      "exploration/Returns Mean                            -291.674\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -291.674\n",
      "exploration/Returns Min                             -291.674\n",
      "exploration/Actions Mean                              -0.179829\n",
      "exploration/Actions Std                                0.760173\n",
      "exploration/Actions Max                                0.999939\n",
      "exploration/Actions Min                               -0.999999\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -291.674\n",
      "exploration/env_infos/final/reward_run Mean           -1.19738\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.19738\n",
      "exploration/env_infos/final/reward_run Min            -1.19738\n",
      "exploration/env_infos/initial/reward_run Mean          0.259793\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.259793\n",
      "exploration/env_infos/initial/reward_run Min           0.259793\n",
      "exploration/env_infos/reward_run Mean                  0.0106664\n",
      "exploration/env_infos/reward_run Std                   0.624641\n",
      "exploration/env_infos/reward_run Max                   1.89818\n",
      "exploration/env_infos/reward_run Min                  -1.89707\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.49016\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.49016\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.49016\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.362208\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.362208\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.362208\n",
      "exploration/env_infos/reward_ctrl Mean                -0.366121\n",
      "exploration/env_infos/reward_ctrl Std                  0.0919135\n",
      "exploration/env_infos/reward_ctrl Max                 -0.108059\n",
      "exploration/env_infos/reward_ctrl Min                 -0.596215\n",
      "exploration/env_infos/final/height Mean               -0.490448\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.490448\n",
      "exploration/env_infos/final/height Min                -0.490448\n",
      "exploration/env_infos/initial/height Mean              0.00526031\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.00526031\n",
      "exploration/env_infos/initial/height Min               0.00526031\n",
      "exploration/env_infos/height Mean                     -0.464967\n",
      "exploration/env_infos/height Std                       0.106124\n",
      "exploration/env_infos/height Max                       0.136766\n",
      "exploration/env_infos/height Min                      -0.587096\n",
      "exploration/env_infos/final/reward_angular Mean       -3.28857\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -3.28857\n",
      "exploration/env_infos/final/reward_angular Min        -3.28857\n",
      "exploration/env_infos/initial/reward_angular Mean      0.197982\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.197982\n",
      "exploration/env_infos/initial/reward_angular Min       0.197982\n",
      "exploration/env_infos/reward_angular Mean              0.113318\n",
      "exploration/env_infos/reward_angular Std               2.04695\n",
      "exploration/env_infos/reward_angular Max               7.58571\n",
      "exploration/env_infos/reward_angular Min              -5.3379\n",
      "evaluation/num steps total                             1.225e+06\n",
      "evaluation/num paths total                          1225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.107752\n",
      "evaluation/Rewards Std                                 0.887131\n",
      "evaluation/Rewards Max                                 5.47616\n",
      "evaluation/Rewards Min                                -4.47417\n",
      "evaluation/Returns Mean                             -107.752\n",
      "evaluation/Returns Std                               352.257\n",
      "evaluation/Returns Max                               953.766\n",
      "evaluation/Returns Min                              -532.819\n",
      "evaluation/Actions Mean                               -0.0501416\n",
      "evaluation/Actions Std                                 0.729072\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -107.752\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0702532\n",
      "evaluation/env_infos/final/reward_run Std              0.757951\n",
      "evaluation/env_infos/final/reward_run Max              1.10534\n",
      "evaluation/env_infos/final/reward_run Min             -2.49108\n",
      "evaluation/env_infos/initial/reward_run Mean           0.289792\n",
      "evaluation/env_infos/initial/reward_run Std            0.468583\n",
      "evaluation/env_infos/initial/reward_run Max            0.89857\n",
      "evaluation/env_infos/initial/reward_run Min           -0.698012\n",
      "evaluation/env_infos/reward_run Mean                  -0.158069\n",
      "evaluation/env_infos/reward_run Std                    0.802677\n",
      "evaluation/env_infos/reward_run Max                    3.50301\n",
      "evaluation/env_infos/reward_run Min                   -3.04933\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.331333\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.109067\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0414571\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.515618\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.309874\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.122887\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0456727\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.491788\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.320436\n",
      "evaluation/env_infos/reward_ctrl Std                   0.117418\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00552638\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.597319\n",
      "evaluation/env_infos/final/height Mean                -0.406322\n",
      "evaluation/env_infos/final/height Std                  0.182503\n",
      "evaluation/env_infos/final/height Max                 -0.00252008\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00205257\n",
      "evaluation/env_infos/initial/height Std                0.0527464\n",
      "evaluation/env_infos/initial/height Max                0.0925996\n",
      "evaluation/env_infos/initial/height Min               -0.0946475\n",
      "evaluation/env_infos/height Mean                      -0.349203\n",
      "evaluation/env_infos/height Std                        0.206688\n",
      "evaluation/env_infos/height Max                        0.360822\n",
      "evaluation/env_infos/height Min                       -0.595991\n",
      "evaluation/env_infos/final/reward_angular Mean         0.33038\n",
      "evaluation/env_infos/final/reward_angular Std          1.00256\n",
      "evaluation/env_infos/final/reward_angular Max          2.17622\n",
      "evaluation/env_infos/final/reward_angular Min         -1.46365\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.229777\n",
      "evaluation/env_infos/initial/reward_angular Std        1.17453\n",
      "evaluation/env_infos/initial/reward_angular Max        2.23838\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.06834\n",
      "evaluation/env_infos/reward_angular Mean               0.0500694\n",
      "evaluation/env_infos/reward_angular Std                1.32135\n",
      "evaluation/env_infos/reward_angular Max                7.41112\n",
      "evaluation/env_infos/reward_angular Min               -6.2314\n",
      "time/data storing (s)                                  0.0149729\n",
      "time/evaluation sampling (s)                          21.3331\n",
      "time/exploration sampling (s)                          0.985166\n",
      "time/logging (s)                                       0.236649\n",
      "time/saving (s)                                        0.0276609\n",
      "time/training (s)                                      3.76533\n",
      "time/epoch (s)                                        26.3628\n",
      "time/total (s)                                      1504.94\n",
      "Epoch                                                 48\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:37:33.650471 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 49 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 51000\n",
      "trainer/QF1 Loss                                       0.488045\n",
      "trainer/QF2 Loss                                       0.531894\n",
      "trainer/Policy Loss                                    2.88445\n",
      "trainer/Q1 Predictions Mean                            2.36454\n",
      "trainer/Q1 Predictions Std                             5.16659\n",
      "trainer/Q1 Predictions Max                            28.8331\n",
      "trainer/Q1 Predictions Min                            -5.61564\n",
      "trainer/Q2 Predictions Mean                            2.3819\n",
      "trainer/Q2 Predictions Std                             5.32658\n",
      "trainer/Q2 Predictions Max                            29.6161\n",
      "trainer/Q2 Predictions Min                            -5.6812\n",
      "trainer/Q Targets Mean                                 2.50268\n",
      "trainer/Q Targets Std                                  5.27342\n",
      "trainer/Q Targets Max                                 28.8691\n",
      "trainer/Q Targets Min                                 -5.39115\n",
      "trainer/Log Pis Mean                                   5.52319\n",
      "trainer/Log Pis Std                                    5.79137\n",
      "trainer/Log Pis Max                                   29.5364\n",
      "trainer/Log Pis Min                                   -5.25736\n",
      "trainer/Policy mu Mean                                 0.17113\n",
      "trainer/Policy mu Std                                  1.55869\n",
      "trainer/Policy mu Max                                  7.88785\n",
      "trainer/Policy mu Min                                 -5.03738\n",
      "trainer/Policy log std Mean                           -0.7845\n",
      "trainer/Policy log std Std                             0.315608\n",
      "trainer/Policy log std Max                             0.124175\n",
      "trainer/Policy log std Min                            -2.5775\n",
      "trainer/Alpha                                          0.0105747\n",
      "trainer/Alpha Loss                                    -2.16941\n",
      "exploration/num steps total                        51000\n",
      "exploration/num paths total                           51\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0951777\n",
      "exploration/Rewards Std                                0.848622\n",
      "exploration/Rewards Max                                2.45741\n",
      "exploration/Rewards Min                               -2.40159\n",
      "exploration/Returns Mean                             -95.1777\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -95.1777\n",
      "exploration/Returns Min                              -95.1777\n",
      "exploration/Actions Mean                               0.133589\n",
      "exploration/Actions Std                                0.778835\n",
      "exploration/Actions Max                                0.999998\n",
      "exploration/Actions Min                               -0.999989\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -95.1777\n",
      "exploration/env_infos/final/reward_run Mean            0.133483\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.133483\n",
      "exploration/env_infos/final/reward_run Min             0.133483\n",
      "exploration/env_infos/initial/reward_run Mean          1.03853\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.03853\n",
      "exploration/env_infos/initial/reward_run Min           1.03853\n",
      "exploration/env_infos/reward_run Mean                  0.265811\n",
      "exploration/env_infos/reward_run Std                   0.771745\n",
      "exploration/env_infos/reward_run Max                   2.26709\n",
      "exploration/env_infos/reward_run Min                  -1.80409\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.450588\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.450588\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.450588\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.528807\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.528807\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.528807\n",
      "exploration/env_infos/reward_ctrl Mean                -0.374658\n",
      "exploration/env_infos/reward_ctrl Std                  0.0820623\n",
      "exploration/env_infos/reward_ctrl Max                 -0.147485\n",
      "exploration/env_infos/reward_ctrl Min                 -0.565987\n",
      "exploration/env_infos/final/height Mean               -0.545521\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.545521\n",
      "exploration/env_infos/final/height Min                -0.545521\n",
      "exploration/env_infos/initial/height Mean              0.069669\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.069669\n",
      "exploration/env_infos/initial/height Min               0.069669\n",
      "exploration/env_infos/height Mean                     -0.485222\n",
      "exploration/env_infos/height Std                       0.0994174\n",
      "exploration/env_infos/height Max                       0.237569\n",
      "exploration/env_infos/height Min                      -0.599583\n",
      "exploration/env_infos/final/reward_angular Mean        1.37487\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.37487\n",
      "exploration/env_infos/final/reward_angular Min         1.37487\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.0954143\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.0954143\n",
      "exploration/env_infos/initial/reward_angular Min      -0.0954143\n",
      "exploration/env_infos/reward_angular Mean              0.122814\n",
      "exploration/env_infos/reward_angular Std               1.71199\n",
      "exploration/env_infos/reward_angular Max               6.87172\n",
      "exploration/env_infos/reward_angular Min              -5.54891\n",
      "evaluation/num steps total                             1.25e+06\n",
      "evaluation/num paths total                          1250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0728751\n",
      "evaluation/Rewards Std                                 0.921048\n",
      "evaluation/Rewards Max                                 5.56194\n",
      "evaluation/Rewards Min                                -4.54901\n",
      "evaluation/Returns Mean                              -72.8751\n",
      "evaluation/Returns Std                               344.599\n",
      "evaluation/Returns Max                               935.738\n",
      "evaluation/Returns Min                              -441.65\n",
      "evaluation/Actions Mean                                0.0512808\n",
      "evaluation/Actions Std                                 0.746242\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -72.8751\n",
      "evaluation/env_infos/final/reward_run Mean            -0.233926\n",
      "evaluation/env_infos/final/reward_run Std              0.870203\n",
      "evaluation/env_infos/final/reward_run Max              1.25955\n",
      "evaluation/env_infos/final/reward_run Min             -1.90473\n",
      "evaluation/env_infos/initial/reward_run Mean           0.230772\n",
      "evaluation/env_infos/initial/reward_run Std            0.711398\n",
      "evaluation/env_infos/initial/reward_run Max            1.30163\n",
      "evaluation/env_infos/initial/reward_run Min           -0.845396\n",
      "evaluation/env_infos/reward_run Mean                  -0.213629\n",
      "evaluation/env_infos/reward_run Std                    1.00694\n",
      "evaluation/env_infos/reward_run Max                    3.259\n",
      "evaluation/env_infos/reward_run Min                   -3.31264\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.345795\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.130774\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0535145\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.523385\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.335433\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.136865\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0343639\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.505596\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.335704\n",
      "evaluation/env_infos/reward_ctrl Std                   0.113972\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0170809\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.5876\n",
      "evaluation/env_infos/final/height Mean                -0.396111\n",
      "evaluation/env_infos/final/height Std                  0.199702\n",
      "evaluation/env_infos/final/height Max                 -0.0655963\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean              -0.00761733\n",
      "evaluation/env_infos/initial/height Std                0.0510039\n",
      "evaluation/env_infos/initial/height Max                0.0809935\n",
      "evaluation/env_infos/initial/height Min               -0.0855183\n",
      "evaluation/env_infos/height Mean                      -0.32688\n",
      "evaluation/env_infos/height Std                        0.232302\n",
      "evaluation/env_infos/height Max                        0.427175\n",
      "evaluation/env_infos/height Min                       -0.596801\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.109987\n",
      "evaluation/env_infos/final/reward_angular Std          1.24925\n",
      "evaluation/env_infos/final/reward_angular Max          3.45346\n",
      "evaluation/env_infos/final/reward_angular Min         -2.40033\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.518698\n",
      "evaluation/env_infos/initial/reward_angular Std        1.15893\n",
      "evaluation/env_infos/initial/reward_angular Max        2.65449\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.41719\n",
      "evaluation/env_infos/reward_angular Mean               0.0463485\n",
      "evaluation/env_infos/reward_angular Std                1.44655\n",
      "evaluation/env_infos/reward_angular Max                8.12447\n",
      "evaluation/env_infos/reward_angular Min               -6.33115\n",
      "time/data storing (s)                                  0.0157383\n",
      "time/evaluation sampling (s)                          20.6815\n",
      "time/exploration sampling (s)                          0.9772\n",
      "time/logging (s)                                       0.237822\n",
      "time/saving (s)                                        0.0276788\n",
      "time/training (s)                                      3.76174\n",
      "time/epoch (s)                                        25.7016\n",
      "time/total (s)                                      1531.07\n",
      "Epoch                                                 49\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:38:00.632464 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 50 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 52000\n",
      "trainer/QF1 Loss                                       0.676529\n",
      "trainer/QF2 Loss                                       0.581465\n",
      "trainer/Policy Loss                                    2.45825\n",
      "trainer/Q1 Predictions Mean                            2.57578\n",
      "trainer/Q1 Predictions Std                             4.92124\n",
      "trainer/Q1 Predictions Max                            25.6103\n",
      "trainer/Q1 Predictions Min                            -5.08576\n",
      "trainer/Q2 Predictions Mean                            2.49313\n",
      "trainer/Q2 Predictions Std                             4.91746\n",
      "trainer/Q2 Predictions Max                            24.7967\n",
      "trainer/Q2 Predictions Min                            -5.2547\n",
      "trainer/Q Targets Mean                                 2.65247\n",
      "trainer/Q Targets Std                                  5.03417\n",
      "trainer/Q Targets Max                                 25.3067\n",
      "trainer/Q Targets Min                                 -5.4203\n",
      "trainer/Log Pis Mean                                   5.31044\n",
      "trainer/Log Pis Std                                    5.37338\n",
      "trainer/Log Pis Max                                   25.022\n",
      "trainer/Log Pis Min                                   -5.136\n",
      "trainer/Policy mu Mean                                 0.0845913\n",
      "trainer/Policy mu Std                                  1.49275\n",
      "trainer/Policy mu Max                                  5.32766\n",
      "trainer/Policy mu Min                                 -5.15356\n",
      "trainer/Policy log std Mean                           -0.781118\n",
      "trainer/Policy log std Std                             0.31439\n",
      "trainer/Policy log std Max                             0.119003\n",
      "trainer/Policy log std Min                            -2.26999\n",
      "trainer/Alpha                                          0.0109997\n",
      "trainer/Alpha Loss                                    -3.1093\n",
      "exploration/num steps total                        52000\n",
      "exploration/num paths total                           52\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.0167552\n",
      "exploration/Rewards Std                                1.60724\n",
      "exploration/Rewards Max                                4.18204\n",
      "exploration/Rewards Min                               -5.07914\n",
      "exploration/Returns Mean                              16.7552\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                               16.7552\n",
      "exploration/Returns Min                               16.7552\n",
      "exploration/Actions Mean                               0.273646\n",
      "exploration/Actions Std                                0.61577\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999996\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                           16.7552\n",
      "exploration/env_infos/final/reward_run Mean            0.454604\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.454604\n",
      "exploration/env_infos/final/reward_run Min             0.454604\n",
      "exploration/env_infos/initial/reward_run Mean          0.640931\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.640931\n",
      "exploration/env_infos/initial/reward_run Min           0.640931\n",
      "exploration/env_infos/reward_run Mean                  0.475805\n",
      "exploration/env_infos/reward_run Std                   0.861973\n",
      "exploration/env_infos/reward_run Max                   2.58088\n",
      "exploration/env_infos/reward_run Min                  -2.01989\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.268692\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.268692\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.268692\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.19442\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.19442\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.19442\n",
      "exploration/env_infos/reward_ctrl Mean                -0.272433\n",
      "exploration/env_infos/reward_ctrl Std                  0.0979476\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181247\n",
      "exploration/env_infos/reward_ctrl Min                 -0.519843\n",
      "exploration/env_infos/final/height Mean               -0.142337\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.142337\n",
      "exploration/env_infos/final/height Min                -0.142337\n",
      "exploration/env_infos/initial/height Mean              0.083031\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.083031\n",
      "exploration/env_infos/initial/height Min               0.083031\n",
      "exploration/env_infos/height Mean                     -0.118386\n",
      "exploration/env_infos/height Std                       0.106084\n",
      "exploration/env_infos/height Max                       0.157585\n",
      "exploration/env_infos/height Min                      -0.408277\n",
      "exploration/env_infos/final/reward_angular Mean       -1.609\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.609\n",
      "exploration/env_infos/final/reward_angular Min        -1.609\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.48279\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.48279\n",
      "exploration/env_infos/initial/reward_angular Min      -1.48279\n",
      "exploration/env_infos/reward_angular Mean             -0.0547526\n",
      "exploration/env_infos/reward_angular Std               1.66494\n",
      "exploration/env_infos/reward_angular Max               5.23717\n",
      "exploration/env_infos/reward_angular Min              -4.33718\n",
      "evaluation/num steps total                             1.275e+06\n",
      "evaluation/num paths total                          1275\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.00869839\n",
      "evaluation/Rewards Std                                 1.2252\n",
      "evaluation/Rewards Max                                 7.35864\n",
      "evaluation/Rewards Min                                -6.18596\n",
      "evaluation/Returns Mean                               -8.69839\n",
      "evaluation/Returns Std                               318.896\n",
      "evaluation/Returns Max                               878.111\n",
      "evaluation/Returns Min                              -447.71\n",
      "evaluation/Actions Mean                                0.0552964\n",
      "evaluation/Actions Std                                 0.672029\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            -8.69839\n",
      "evaluation/env_infos/final/reward_run Mean            -0.15882\n",
      "evaluation/env_infos/final/reward_run Std              1.04371\n",
      "evaluation/env_infos/final/reward_run Max              2.73075\n",
      "evaluation/env_infos/final/reward_run Min             -1.95214\n",
      "evaluation/env_infos/initial/reward_run Mean           0.235151\n",
      "evaluation/env_infos/initial/reward_run Std            0.665997\n",
      "evaluation/env_infos/initial/reward_run Max            1.03408\n",
      "evaluation/env_infos/initial/reward_run Min           -0.788884\n",
      "evaluation/env_infos/reward_run Mean                  -0.11133\n",
      "evaluation/env_infos/reward_run Std                    0.907933\n",
      "evaluation/env_infos/reward_run Max                    3.37056\n",
      "evaluation/env_infos/reward_run Min                   -2.58436\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.260678\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.116649\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0258578\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.46208\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.333874\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.142256\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0720541\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.55129\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.272808\n",
      "evaluation/env_infos/reward_ctrl Std                   0.115791\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00376567\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.595796\n",
      "evaluation/env_infos/final/height Mean                -0.257453\n",
      "evaluation/env_infos/final/height Std                  0.199807\n",
      "evaluation/env_infos/final/height Max                 -0.00401034\n",
      "evaluation/env_infos/final/height Min                 -0.57728\n",
      "evaluation/env_infos/initial/height Mean              -0.00222769\n",
      "evaluation/env_infos/initial/height Std                0.0463331\n",
      "evaluation/env_infos/initial/height Max                0.0927803\n",
      "evaluation/env_infos/initial/height Min               -0.0716972\n",
      "evaluation/env_infos/height Mean                      -0.240309\n",
      "evaluation/env_infos/height Std                        0.213875\n",
      "evaluation/env_infos/height Max                        0.417657\n",
      "evaluation/env_infos/height Min                       -0.601658\n",
      "evaluation/env_infos/final/reward_angular Mean         0.106366\n",
      "evaluation/env_infos/final/reward_angular Std          1.42991\n",
      "evaluation/env_infos/final/reward_angular Max          2.31517\n",
      "evaluation/env_infos/final/reward_angular Min         -4.03632\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.590151\n",
      "evaluation/env_infos/initial/reward_angular Std        1.12019\n",
      "evaluation/env_infos/initial/reward_angular Max        2.52982\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.58117\n",
      "evaluation/env_infos/reward_angular Mean               0.0575929\n",
      "evaluation/env_infos/reward_angular Std                1.78387\n",
      "evaluation/env_infos/reward_angular Max                8.90632\n",
      "evaluation/env_infos/reward_angular Min               -8.40316\n",
      "time/data storing (s)                                  0.0158499\n",
      "time/evaluation sampling (s)                          21.135\n",
      "time/exploration sampling (s)                          1.09755\n",
      "time/logging (s)                                       0.242411\n",
      "time/saving (s)                                        0.0542596\n",
      "time/training (s)                                      3.9955\n",
      "time/epoch (s)                                        26.5406\n",
      "time/total (s)                                      1558.06\n",
      "Epoch                                                 50\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:38:27.186616 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 51 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 53000\n",
      "trainer/QF1 Loss                                       0.682881\n",
      "trainer/QF2 Loss                                       0.661857\n",
      "trainer/Policy Loss                                    2.04424\n",
      "trainer/Q1 Predictions Mean                            2.55721\n",
      "trainer/Q1 Predictions Std                             5.50914\n",
      "trainer/Q1 Predictions Max                            33.2335\n",
      "trainer/Q1 Predictions Min                            -4.39532\n",
      "trainer/Q2 Predictions Mean                            2.62124\n",
      "trainer/Q2 Predictions Std                             5.54705\n",
      "trainer/Q2 Predictions Max                            34.7804\n",
      "trainer/Q2 Predictions Min                            -4.55605\n",
      "trainer/Q Targets Mean                                 2.74646\n",
      "trainer/Q Targets Std                                  5.74099\n",
      "trainer/Q Targets Max                                 34.179\n",
      "trainer/Q Targets Min                                 -5.46442\n",
      "trainer/Log Pis Mean                                   4.89548\n",
      "trainer/Log Pis Std                                    4.53232\n",
      "trainer/Log Pis Max                                   20.9387\n",
      "trainer/Log Pis Min                                   -6.15453\n",
      "trainer/Policy mu Mean                                 0.0752826\n",
      "trainer/Policy mu Std                                  1.44312\n",
      "trainer/Policy mu Max                                  4.67746\n",
      "trainer/Policy mu Min                                 -4.2552\n",
      "trainer/Policy log std Mean                           -0.807698\n",
      "trainer/Policy log std Std                             0.326585\n",
      "trainer/Policy log std Max                             0.00322551\n",
      "trainer/Policy log std Min                            -3.23511\n",
      "trainer/Alpha                                          0.0107585\n",
      "trainer/Alpha Loss                                    -5.00448\n",
      "exploration/num steps total                        53000\n",
      "exploration/num paths total                           53\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.362432\n",
      "exploration/Rewards Std                                0.871975\n",
      "exploration/Rewards Max                                2.55664\n",
      "exploration/Rewards Min                               -2.07571\n",
      "exploration/Returns Mean                             362.432\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              362.432\n",
      "exploration/Returns Min                              362.432\n",
      "exploration/Actions Mean                               0.101503\n",
      "exploration/Actions Std                                0.77971\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999915\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          362.432\n",
      "exploration/env_infos/final/reward_run Mean            1.44145\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.44145\n",
      "exploration/env_infos/final/reward_run Min             1.44145\n",
      "exploration/env_infos/initial/reward_run Mean          0.538431\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.538431\n",
      "exploration/env_infos/initial/reward_run Min           0.538431\n",
      "exploration/env_infos/reward_run Mean                  0.63173\n",
      "exploration/env_infos/reward_run Std                   0.867617\n",
      "exploration/env_infos/reward_run Max                   2.99678\n",
      "exploration/env_infos/reward_run Min                  -1.53893\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.328641\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.328641\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.328641\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.406315\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.406315\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.406315\n",
      "exploration/env_infos/reward_ctrl Mean                -0.37095\n",
      "exploration/env_infos/reward_ctrl Std                  0.0890266\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0967031\n",
      "exploration/env_infos/reward_ctrl Min                 -0.588734\n",
      "exploration/env_infos/final/height Mean               -0.519448\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.519448\n",
      "exploration/env_infos/final/height Min                -0.519448\n",
      "exploration/env_infos/initial/height Mean              0.0693053\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0693053\n",
      "exploration/env_infos/initial/height Min               0.0693053\n",
      "exploration/env_infos/height Mean                     -0.445887\n",
      "exploration/env_infos/height Std                       0.162386\n",
      "exploration/env_infos/height Max                       0.590293\n",
      "exploration/env_infos/height Min                      -0.591607\n",
      "exploration/env_infos/final/reward_angular Mean        1.71737\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.71737\n",
      "exploration/env_infos/final/reward_angular Min         1.71737\n",
      "exploration/env_infos/initial/reward_angular Mean      0.0872132\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.0872132\n",
      "exploration/env_infos/initial/reward_angular Min       0.0872132\n",
      "exploration/env_infos/reward_angular Mean              0.120379\n",
      "exploration/env_infos/reward_angular Std               2.07082\n",
      "exploration/env_infos/reward_angular Max               8.64302\n",
      "exploration/env_infos/reward_angular Min              -5.40142\n",
      "evaluation/num steps total                             1.3e+06\n",
      "evaluation/num paths total                          1300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.164872\n",
      "evaluation/Rewards Std                                 0.965038\n",
      "evaluation/Rewards Max                                 5.9208\n",
      "evaluation/Rewards Min                                -5.69848\n",
      "evaluation/Returns Mean                             -164.872\n",
      "evaluation/Returns Std                               251.826\n",
      "evaluation/Returns Max                               708.867\n",
      "evaluation/Returns Min                              -665.922\n",
      "evaluation/Actions Mean                                0.00976719\n",
      "evaluation/Actions Std                                 0.725585\n",
      "evaluation/Actions Max                                 0.999995\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -164.872\n",
      "evaluation/env_infos/final/reward_run Mean             0.0169445\n",
      "evaluation/env_infos/final/reward_run Std              0.725726\n",
      "evaluation/env_infos/final/reward_run Max              1.4671\n",
      "evaluation/env_infos/final/reward_run Min             -1.72212\n",
      "evaluation/env_infos/initial/reward_run Mean           0.365394\n",
      "evaluation/env_infos/initial/reward_run Std            0.56007\n",
      "evaluation/env_infos/initial/reward_run Max            1.10186\n",
      "evaluation/env_infos/initial/reward_run Min           -0.582434\n",
      "evaluation/env_infos/reward_run Mean                  -0.065582\n",
      "evaluation/env_infos/reward_run Std                    0.706137\n",
      "evaluation/env_infos/reward_run Max                    2.90423\n",
      "evaluation/env_infos/reward_run Min                   -2.64691\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.345267\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.152927\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0427781\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.524063\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.301278\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.129882\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0834784\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.512755\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.315941\n",
      "evaluation/env_infos/reward_ctrl Std                   0.140335\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.021276\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599067\n",
      "evaluation/env_infos/final/height Mean                -0.404497\n",
      "evaluation/env_infos/final/height Std                  0.191217\n",
      "evaluation/env_infos/final/height Max                 -0.0533552\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean              -0.0097243\n",
      "evaluation/env_infos/initial/height Std                0.0588537\n",
      "evaluation/env_infos/initial/height Max                0.0887892\n",
      "evaluation/env_infos/initial/height Min               -0.096328\n",
      "evaluation/env_infos/height Mean                      -0.355075\n",
      "evaluation/env_infos/height Std                        0.221079\n",
      "evaluation/env_infos/height Max                        0.658326\n",
      "evaluation/env_infos/height Min                       -0.594096\n",
      "evaluation/env_infos/final/reward_angular Mean         0.124277\n",
      "evaluation/env_infos/final/reward_angular Std          1.1463\n",
      "evaluation/env_infos/final/reward_angular Max          2.27416\n",
      "evaluation/env_infos/final/reward_angular Min         -3.12729\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.244426\n",
      "evaluation/env_infos/initial/reward_angular Std        1.09743\n",
      "evaluation/env_infos/initial/reward_angular Max        2.54038\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.25546\n",
      "evaluation/env_infos/reward_angular Mean               0.0184628\n",
      "evaluation/env_infos/reward_angular Std                1.41459\n",
      "evaluation/env_infos/reward_angular Max                8.49269\n",
      "evaluation/env_infos/reward_angular Min               -6.99912\n",
      "time/data storing (s)                                  0.0146573\n",
      "time/evaluation sampling (s)                          21.0691\n",
      "time/exploration sampling (s)                          0.982206\n",
      "time/logging (s)                                       0.234647\n",
      "time/saving (s)                                        0.0289823\n",
      "time/training (s)                                      3.76256\n",
      "time/epoch (s)                                        26.0922\n",
      "time/total (s)                                      1584.6\n",
      "Epoch                                                 51\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:38:53.673044 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 52 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 54000\n",
      "trainer/QF1 Loss                                       0.961767\n",
      "trainer/QF2 Loss                                       1.05239\n",
      "trainer/Policy Loss                                    2.69761\n",
      "trainer/Q1 Predictions Mean                            2.5554\n",
      "trainer/Q1 Predictions Std                             5.65614\n",
      "trainer/Q1 Predictions Max                            34.3161\n",
      "trainer/Q1 Predictions Min                            -5.50808\n",
      "trainer/Q2 Predictions Mean                            2.63393\n",
      "trainer/Q2 Predictions Std                             5.62649\n",
      "trainer/Q2 Predictions Max                            35.1211\n",
      "trainer/Q2 Predictions Min                            -5.35818\n",
      "trainer/Q Targets Mean                                 2.48588\n",
      "trainer/Q Targets Std                                  5.63074\n",
      "trainer/Q Targets Max                                 32.2488\n",
      "trainer/Q Targets Min                                 -5.72897\n",
      "trainer/Log Pis Mean                                   5.59733\n",
      "trainer/Log Pis Std                                    5.76602\n",
      "trainer/Log Pis Max                                   31.1566\n",
      "trainer/Log Pis Min                                   -5.71547\n",
      "trainer/Policy mu Mean                                 0.0884252\n",
      "trainer/Policy mu Std                                  1.57382\n",
      "trainer/Policy mu Max                                  4.831\n",
      "trainer/Policy mu Min                                 -5.20057\n",
      "trainer/Policy log std Mean                           -0.748178\n",
      "trainer/Policy log std Std                             0.31069\n",
      "trainer/Policy log std Max                             0.301011\n",
      "trainer/Policy log std Min                            -2.05354\n",
      "trainer/Alpha                                          0.0107572\n",
      "trainer/Alpha Loss                                    -1.82468\n",
      "exploration/num steps total                        54000\n",
      "exploration/num paths total                           54\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.160419\n",
      "exploration/Rewards Std                                1.58542\n",
      "exploration/Rewards Max                                6.53463\n",
      "exploration/Rewards Min                               -4.55572\n",
      "exploration/Returns Mean                            -160.419\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -160.419\n",
      "exploration/Returns Min                             -160.419\n",
      "exploration/Actions Mean                              -0.0177705\n",
      "exploration/Actions Std                                0.761956\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999992\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -160.419\n",
      "exploration/env_infos/final/reward_run Mean           -0.214054\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.214054\n",
      "exploration/env_infos/final/reward_run Min            -0.214054\n",
      "exploration/env_infos/initial/reward_run Mean          1.01105\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.01105\n",
      "exploration/env_infos/initial/reward_run Min           1.01105\n",
      "exploration/env_infos/reward_run Mean                  0.0621484\n",
      "exploration/env_infos/reward_run Std                   0.544258\n",
      "exploration/env_infos/reward_run Max                   1.76024\n",
      "exploration/env_infos/reward_run Min                  -1.95443\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.450652\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.450652\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.450652\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.527808\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.527808\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.527808\n",
      "exploration/env_infos/reward_ctrl Mean                -0.348535\n",
      "exploration/env_infos/reward_ctrl Std                  0.104589\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0691895\n",
      "exploration/env_infos/reward_ctrl Min                 -0.586879\n",
      "exploration/env_infos/final/height Mean               -0.501968\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.501968\n",
      "exploration/env_infos/final/height Min                -0.501968\n",
      "exploration/env_infos/initial/height Mean              0.0711899\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0711899\n",
      "exploration/env_infos/initial/height Min               0.0711899\n",
      "exploration/env_infos/height Mean                     -0.413486\n",
      "exploration/env_infos/height Std                       0.172284\n",
      "exploration/env_infos/height Max                       0.324228\n",
      "exploration/env_infos/height Min                      -0.596182\n",
      "exploration/env_infos/final/reward_angular Mean        0.305972\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.305972\n",
      "exploration/env_infos/final/reward_angular Min         0.305972\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.371916\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.371916\n",
      "exploration/env_infos/initial/reward_angular Min      -0.371916\n",
      "exploration/env_infos/reward_angular Mean              0.111108\n",
      "exploration/env_infos/reward_angular Std               1.98566\n",
      "exploration/env_infos/reward_angular Max               8.32719\n",
      "exploration/env_infos/reward_angular Min              -5.5488\n",
      "evaluation/num steps total                             1.325e+06\n",
      "evaluation/num paths total                          1325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.234041\n",
      "evaluation/Rewards Std                                 0.943387\n",
      "evaluation/Rewards Max                                 6.78526\n",
      "evaluation/Rewards Min                                -6.35107\n",
      "evaluation/Returns Mean                             -234.041\n",
      "evaluation/Returns Std                               279.005\n",
      "evaluation/Returns Max                               852.69\n",
      "evaluation/Returns Min                              -608.445\n",
      "evaluation/Actions Mean                               -0.0118144\n",
      "evaluation/Actions Std                                 0.781252\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -234.041\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0258295\n",
      "evaluation/env_infos/final/reward_run Std              0.601092\n",
      "evaluation/env_infos/final/reward_run Max              1.22716\n",
      "evaluation/env_infos/final/reward_run Min             -1.84965\n",
      "evaluation/env_infos/initial/reward_run Mean           0.298625\n",
      "evaluation/env_infos/initial/reward_run Std            0.574221\n",
      "evaluation/env_infos/initial/reward_run Max            1.19225\n",
      "evaluation/env_infos/initial/reward_run Min           -0.659774\n",
      "evaluation/env_infos/reward_run Mean                   0.0209121\n",
      "evaluation/env_infos/reward_run Std                    0.63923\n",
      "evaluation/env_infos/reward_run Max                    2.88447\n",
      "evaluation/env_infos/reward_run Min                   -2.78043\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.35889\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.14083\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0821838\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.570931\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.316445\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.167676\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.083982\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.543372\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.366296\n",
      "evaluation/env_infos/reward_ctrl Std                   0.131112\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0154774\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.593797\n",
      "evaluation/env_infos/final/height Mean                -0.447411\n",
      "evaluation/env_infos/final/height Std                  0.18141\n",
      "evaluation/env_infos/final/height Max                 -0.0663786\n",
      "evaluation/env_infos/final/height Min                 -0.591401\n",
      "evaluation/env_infos/initial/height Mean               0.0121653\n",
      "evaluation/env_infos/initial/height Std                0.051058\n",
      "evaluation/env_infos/initial/height Max                0.0962934\n",
      "evaluation/env_infos/initial/height Min               -0.0716199\n",
      "evaluation/env_infos/height Mean                      -0.415485\n",
      "evaluation/env_infos/height Std                        0.203892\n",
      "evaluation/env_infos/height Max                        0.533181\n",
      "evaluation/env_infos/height Min                       -0.592673\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.28553\n",
      "evaluation/env_infos/final/reward_angular Std          1.16694\n",
      "evaluation/env_infos/final/reward_angular Max          2.16859\n",
      "evaluation/env_infos/final/reward_angular Min         -3.33496\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.620159\n",
      "evaluation/env_infos/initial/reward_angular Std        1.00093\n",
      "evaluation/env_infos/initial/reward_angular Max        2.67873\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.37732\n",
      "evaluation/env_infos/reward_angular Mean               0.00773398\n",
      "evaluation/env_infos/reward_angular Std                1.37116\n",
      "evaluation/env_infos/reward_angular Max                8.92028\n",
      "evaluation/env_infos/reward_angular Min               -7.12071\n",
      "time/data storing (s)                                  0.0158019\n",
      "time/evaluation sampling (s)                          20.8655\n",
      "time/exploration sampling (s)                          0.996736\n",
      "time/logging (s)                                       0.239899\n",
      "time/saving (s)                                        0.0294231\n",
      "time/training (s)                                      3.893\n",
      "time/epoch (s)                                        26.0404\n",
      "time/total (s)                                      1611.09\n",
      "Epoch                                                 52\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:39:19.856901 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 53 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 55000\n",
      "trainer/QF1 Loss                                       0.767686\n",
      "trainer/QF2 Loss                                       0.724679\n",
      "trainer/Policy Loss                                    3.04955\n",
      "trainer/Q1 Predictions Mean                            2.58041\n",
      "trainer/Q1 Predictions Std                             6.30549\n",
      "trainer/Q1 Predictions Max                            34.0654\n",
      "trainer/Q1 Predictions Min                            -5.40129\n",
      "trainer/Q2 Predictions Mean                            2.69018\n",
      "trainer/Q2 Predictions Std                             6.41781\n",
      "trainer/Q2 Predictions Max                            36.2801\n",
      "trainer/Q2 Predictions Min                            -4.8838\n",
      "trainer/Q Targets Mean                                 2.68352\n",
      "trainer/Q Targets Std                                  6.34058\n",
      "trainer/Q Targets Max                                 34.7883\n",
      "trainer/Q Targets Min                                 -5.93727\n",
      "trainer/Log Pis Mean                                   6.04231\n",
      "trainer/Log Pis Std                                    5.63992\n",
      "trainer/Log Pis Max                                   24.7911\n",
      "trainer/Log Pis Min                                   -5.88858\n",
      "trainer/Policy mu Mean                                 0.194151\n",
      "trainer/Policy mu Std                                  1.57694\n",
      "trainer/Policy mu Max                                  4.86299\n",
      "trainer/Policy mu Min                                 -5.52096\n",
      "trainer/Policy log std Mean                           -0.787531\n",
      "trainer/Policy log std Std                             0.341744\n",
      "trainer/Policy log std Max                             0.469938\n",
      "trainer/Policy log std Min                            -2.72591\n",
      "trainer/Alpha                                          0.0111206\n",
      "trainer/Alpha Loss                                     0.190367\n",
      "exploration/num steps total                        55000\n",
      "exploration/num paths total                           55\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.141844\n",
      "exploration/Rewards Std                                0.968281\n",
      "exploration/Rewards Max                                4.09449\n",
      "exploration/Rewards Min                               -3.83736\n",
      "exploration/Returns Mean                             141.844\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              141.844\n",
      "exploration/Returns Min                              141.844\n",
      "exploration/Actions Mean                              -0.0952564\n",
      "exploration/Actions Std                                0.738372\n",
      "exploration/Actions Max                                0.999808\n",
      "exploration/Actions Min                               -0.999838\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          141.844\n",
      "exploration/env_infos/final/reward_run Mean           -0.0420927\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.0420927\n",
      "exploration/env_infos/final/reward_run Min            -0.0420927\n",
      "exploration/env_infos/initial/reward_run Mean         -0.752826\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.752826\n",
      "exploration/env_infos/initial/reward_run Min          -0.752826\n",
      "exploration/env_infos/reward_run Mean                 -0.613948\n",
      "exploration/env_infos/reward_run Std                   0.92268\n",
      "exploration/env_infos/reward_run Max                   1.46284\n",
      "exploration/env_infos/reward_run Min                  -3.48309\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.428941\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.428941\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.428941\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.189959\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.189959\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.189959\n",
      "exploration/env_infos/reward_ctrl Mean                -0.33256\n",
      "exploration/env_infos/reward_ctrl Std                  0.0851105\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0720429\n",
      "exploration/env_infos/reward_ctrl Min                 -0.537979\n",
      "exploration/env_infos/final/height Mean               -0.577231\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.577231\n",
      "exploration/env_infos/final/height Min                -0.577231\n",
      "exploration/env_infos/initial/height Mean             -0.0724324\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0724324\n",
      "exploration/env_infos/initial/height Min              -0.0724324\n",
      "exploration/env_infos/height Mean                     -0.331437\n",
      "exploration/env_infos/height Std                       0.268914\n",
      "exploration/env_infos/height Max                       0.411799\n",
      "exploration/env_infos/height Min                      -0.595557\n",
      "exploration/env_infos/final/reward_angular Mean        0.00186629\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.00186629\n",
      "exploration/env_infos/final/reward_angular Min         0.00186629\n",
      "exploration/env_infos/initial/reward_angular Mean      1.62088\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.62088\n",
      "exploration/env_infos/initial/reward_angular Min       1.62088\n",
      "exploration/env_infos/reward_angular Mean             -0.0567685\n",
      "exploration/env_infos/reward_angular Std               1.47465\n",
      "exploration/env_infos/reward_angular Max               5.621\n",
      "exploration/env_infos/reward_angular Min              -5.95403\n",
      "evaluation/num steps total                             1.35e+06\n",
      "evaluation/num paths total                          1350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0909331\n",
      "evaluation/Rewards Std                                 0.974167\n",
      "evaluation/Rewards Max                                 6.7659\n",
      "evaluation/Rewards Min                                -5.00065\n",
      "evaluation/Returns Mean                              -90.9331\n",
      "evaluation/Returns Std                               323.304\n",
      "evaluation/Returns Max                               719.995\n",
      "evaluation/Returns Min                              -559.083\n",
      "evaluation/Actions Mean                                0.0609268\n",
      "evaluation/Actions Std                                 0.715157\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -90.9331\n",
      "evaluation/env_infos/final/reward_run Mean             0.0416955\n",
      "evaluation/env_infos/final/reward_run Std              0.81169\n",
      "evaluation/env_infos/final/reward_run Max              1.46978\n",
      "evaluation/env_infos/final/reward_run Min             -1.72897\n",
      "evaluation/env_infos/initial/reward_run Mean           0.283715\n",
      "evaluation/env_infos/initial/reward_run Std            0.64244\n",
      "evaluation/env_infos/initial/reward_run Max            1.2042\n",
      "evaluation/env_infos/initial/reward_run Min           -0.756572\n",
      "evaluation/env_infos/reward_run Mean                  -0.159637\n",
      "evaluation/env_infos/reward_run Std                    0.943957\n",
      "evaluation/env_infos/reward_run Max                    3.09228\n",
      "evaluation/env_infos/reward_run Min                   -4.19999\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.343322\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.111073\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.109321\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.522749\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.298152\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.160019\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0666879\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.564537\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.309097\n",
      "evaluation/env_infos/reward_ctrl Std                   0.121783\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00948744\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598419\n",
      "evaluation/env_infos/final/height Mean                -0.428024\n",
      "evaluation/env_infos/final/height Std                  0.172649\n",
      "evaluation/env_infos/final/height Max                 -0.0191689\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean              -0.0147304\n",
      "evaluation/env_infos/initial/height Std                0.0520266\n",
      "evaluation/env_infos/initial/height Max                0.0885887\n",
      "evaluation/env_infos/initial/height Min               -0.0900925\n",
      "evaluation/env_infos/height Mean                      -0.359675\n",
      "evaluation/env_infos/height Std                        0.2083\n",
      "evaluation/env_infos/height Max                        0.446937\n",
      "evaluation/env_infos/height Min                       -0.596552\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.185892\n",
      "evaluation/env_infos/final/reward_angular Std          1.06398\n",
      "evaluation/env_infos/final/reward_angular Max          1.95535\n",
      "evaluation/env_infos/final/reward_angular Min         -2.57664\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.321935\n",
      "evaluation/env_infos/initial/reward_angular Std        0.975378\n",
      "evaluation/env_infos/initial/reward_angular Max        2.28725\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.66417\n",
      "evaluation/env_infos/reward_angular Mean               0.022027\n",
      "evaluation/env_infos/reward_angular Std                1.48685\n",
      "evaluation/env_infos/reward_angular Max                7.42604\n",
      "evaluation/env_infos/reward_angular Min               -6.86438\n",
      "time/data storing (s)                                  0.0149805\n",
      "time/evaluation sampling (s)                          20.6131\n",
      "time/exploration sampling (s)                          0.982291\n",
      "time/logging (s)                                       0.242292\n",
      "time/saving (s)                                        0.0288563\n",
      "time/training (s)                                      3.8167\n",
      "time/epoch (s)                                        25.6983\n",
      "time/total (s)                                      1637.28\n",
      "Epoch                                                 53\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:39:46.290184 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 54 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 56000\n",
      "trainer/QF1 Loss                                       0.637457\n",
      "trainer/QF2 Loss                                       0.650075\n",
      "trainer/Policy Loss                                    3.58603\n",
      "trainer/Q1 Predictions Mean                            2.09616\n",
      "trainer/Q1 Predictions Std                             5.21951\n",
      "trainer/Q1 Predictions Max                            27.1852\n",
      "trainer/Q1 Predictions Min                            -5.88933\n",
      "trainer/Q2 Predictions Mean                            2.15198\n",
      "trainer/Q2 Predictions Std                             5.28847\n",
      "trainer/Q2 Predictions Max                            28.28\n",
      "trainer/Q2 Predictions Min                            -5.85559\n",
      "trainer/Q Targets Mean                                 1.95702\n",
      "trainer/Q Targets Std                                  5.1956\n",
      "trainer/Q Targets Max                                 26.4015\n",
      "trainer/Q Targets Min                                 -6.77093\n",
      "trainer/Log Pis Mean                                   5.98062\n",
      "trainer/Log Pis Std                                    5.51286\n",
      "trainer/Log Pis Max                                   25.9976\n",
      "trainer/Log Pis Min                                   -4.86823\n",
      "trainer/Policy mu Mean                                 0.0441231\n",
      "trainer/Policy mu Std                                  1.62556\n",
      "trainer/Policy mu Max                                  5.66349\n",
      "trainer/Policy mu Min                                 -6.42963\n",
      "trainer/Policy log std Mean                           -0.71238\n",
      "trainer/Policy log std Std                             0.306087\n",
      "trainer/Policy log std Max                             0.395959\n",
      "trainer/Policy log std Min                            -1.86261\n",
      "trainer/Alpha                                          0.0116811\n",
      "trainer/Alpha Loss                                    -0.0862442\n",
      "exploration/num steps total                        56000\n",
      "exploration/num paths total                           56\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.45514\n",
      "exploration/Rewards Std                                0.33956\n",
      "exploration/Rewards Max                                1.93839\n",
      "exploration/Rewards Min                               -1.74183\n",
      "exploration/Returns Mean                            -455.14\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -455.14\n",
      "exploration/Returns Min                             -455.14\n",
      "exploration/Actions Mean                              -0.448331\n",
      "exploration/Actions Std                                0.730696\n",
      "exploration/Actions Max                                0.999013\n",
      "exploration/Actions Min                               -0.998766\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -455.14\n",
      "exploration/env_infos/final/reward_run Mean           -0.00934376\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.00934376\n",
      "exploration/env_infos/final/reward_run Min            -0.00934376\n",
      "exploration/env_infos/initial/reward_run Mean         -0.0585934\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.0585934\n",
      "exploration/env_infos/initial/reward_run Min          -0.0585934\n",
      "exploration/env_infos/reward_run Mean                 -0.0323246\n",
      "exploration/env_infos/reward_run Std                   0.224221\n",
      "exploration/env_infos/reward_run Max                   1.50229\n",
      "exploration/env_infos/reward_run Min                  -1.80439\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.404772\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.404772\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.404772\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.26569\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.26569\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.26569\n",
      "exploration/env_infos/reward_ctrl Mean                -0.44095\n",
      "exploration/env_infos/reward_ctrl Std                  0.0549311\n",
      "exploration/env_infos/reward_ctrl Max                 -0.128564\n",
      "exploration/env_infos/reward_ctrl Min                 -0.534261\n",
      "exploration/env_infos/final/height Mean               -0.577205\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.577205\n",
      "exploration/env_infos/final/height Min                -0.577205\n",
      "exploration/env_infos/initial/height Mean              0.0105409\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0105409\n",
      "exploration/env_infos/initial/height Min               0.0105409\n",
      "exploration/env_infos/height Mean                     -0.533734\n",
      "exploration/env_infos/height Std                       0.15725\n",
      "exploration/env_infos/height Max                       0.32979\n",
      "exploration/env_infos/height Min                      -0.577381\n",
      "exploration/env_infos/final/reward_angular Mean       -0.00526237\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.00526237\n",
      "exploration/env_infos/final/reward_angular Min        -0.00526237\n",
      "exploration/env_infos/initial/reward_angular Mean      1.38199\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.38199\n",
      "exploration/env_infos/initial/reward_angular Min       1.38199\n",
      "exploration/env_infos/reward_angular Mean             -0.0637499\n",
      "exploration/env_infos/reward_angular Std               0.578609\n",
      "exploration/env_infos/reward_angular Max               4.51841\n",
      "exploration/env_infos/reward_angular Min              -4.08806\n",
      "evaluation/num steps total                             1.375e+06\n",
      "evaluation/num paths total                          1375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.129992\n",
      "evaluation/Rewards Std                                 0.908956\n",
      "evaluation/Rewards Max                                 5.3976\n",
      "evaluation/Rewards Min                                -5.01619\n",
      "evaluation/Returns Mean                             -129.992\n",
      "evaluation/Returns Std                               373.657\n",
      "evaluation/Returns Max                               969.187\n",
      "evaluation/Returns Min                              -663.55\n",
      "evaluation/Actions Mean                               -0.0379682\n",
      "evaluation/Actions Std                                 0.75124\n",
      "evaluation/Actions Max                                 0.999989\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -129.992\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0141327\n",
      "evaluation/env_infos/final/reward_run Std              1.0047\n",
      "evaluation/env_infos/final/reward_run Max              2.60045\n",
      "evaluation/env_infos/final/reward_run Min             -2.35632\n",
      "evaluation/env_infos/initial/reward_run Mean           0.435304\n",
      "evaluation/env_infos/initial/reward_run Std            0.509817\n",
      "evaluation/env_infos/initial/reward_run Max            1.28304\n",
      "evaluation/env_infos/initial/reward_run Min           -0.3153\n",
      "evaluation/env_infos/reward_run Mean                  -0.0586943\n",
      "evaluation/env_infos/reward_run Std                    0.868128\n",
      "evaluation/env_infos/reward_run Max                    3.2134\n",
      "evaluation/env_infos/reward_run Min                   -2.89846\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.334229\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.137497\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0444506\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.510709\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.322054\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.182112\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0519195\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.575447\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.339482\n",
      "evaluation/env_infos/reward_ctrl Std                   0.128984\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00972949\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.59016\n",
      "evaluation/env_infos/final/height Mean                -0.428215\n",
      "evaluation/env_infos/final/height Std                  0.182518\n",
      "evaluation/env_infos/final/height Max                 -0.0595442\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00794003\n",
      "evaluation/env_infos/initial/height Std                0.0559114\n",
      "evaluation/env_infos/initial/height Max                0.102047\n",
      "evaluation/env_infos/initial/height Min               -0.113651\n",
      "evaluation/env_infos/height Mean                      -0.397884\n",
      "evaluation/env_infos/height Std                        0.213528\n",
      "evaluation/env_infos/height Max                        0.426468\n",
      "evaluation/env_infos/height Min                       -0.591988\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.525991\n",
      "evaluation/env_infos/final/reward_angular Std          1.56738\n",
      "evaluation/env_infos/final/reward_angular Max          1.43744\n",
      "evaluation/env_infos/final/reward_angular Min         -4.87174\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.263009\n",
      "evaluation/env_infos/initial/reward_angular Std        1.07245\n",
      "evaluation/env_infos/initial/reward_angular Max        2.27924\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.81607\n",
      "evaluation/env_infos/reward_angular Mean               0.030158\n",
      "evaluation/env_infos/reward_angular Std                1.33543\n",
      "evaluation/env_infos/reward_angular Max                8.42806\n",
      "evaluation/env_infos/reward_angular Min               -7.62582\n",
      "time/data storing (s)                                  0.0148191\n",
      "time/evaluation sampling (s)                          20.9125\n",
      "time/exploration sampling (s)                          1.00227\n",
      "time/logging (s)                                       0.239985\n",
      "time/saving (s)                                        0.0292451\n",
      "time/training (s)                                      3.76813\n",
      "time/epoch (s)                                        25.967\n",
      "time/total (s)                                      1663.71\n",
      "Epoch                                                 54\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:40:12.622728 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 55 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 57000\n",
      "trainer/QF1 Loss                                       0.854451\n",
      "trainer/QF2 Loss                                       0.763917\n",
      "trainer/Policy Loss                                    2.68863\n",
      "trainer/Q1 Predictions Mean                            2.58009\n",
      "trainer/Q1 Predictions Std                             5.87222\n",
      "trainer/Q1 Predictions Max                            31.7192\n",
      "trainer/Q1 Predictions Min                            -5.39622\n",
      "trainer/Q2 Predictions Mean                            2.56015\n",
      "trainer/Q2 Predictions Std                             5.91963\n",
      "trainer/Q2 Predictions Max                            30.784\n",
      "trainer/Q2 Predictions Min                            -5.58851\n",
      "trainer/Q Targets Mean                                 2.55986\n",
      "trainer/Q Targets Std                                  5.89761\n",
      "trainer/Q Targets Max                                 27.9492\n",
      "trainer/Q Targets Min                                 -5.86448\n",
      "trainer/Log Pis Mean                                   5.43526\n",
      "trainer/Log Pis Std                                    4.80233\n",
      "trainer/Log Pis Max                                   26.0665\n",
      "trainer/Log Pis Min                                   -3.2973\n",
      "trainer/Policy mu Mean                                 0.23509\n",
      "trainer/Policy mu Std                                  1.48491\n",
      "trainer/Policy mu Max                                  4.77582\n",
      "trainer/Policy mu Min                                 -5.58949\n",
      "trainer/Policy log std Mean                           -0.766798\n",
      "trainer/Policy log std Std                             0.307671\n",
      "trainer/Policy log std Max                             0.353875\n",
      "trainer/Policy log std Min                            -1.91899\n",
      "trainer/Alpha                                          0.0113903\n",
      "trainer/Alpha Loss                                    -2.52682\n",
      "exploration/num steps total                        57000\n",
      "exploration/num paths total                           57\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.419626\n",
      "exploration/Rewards Std                                1.27562\n",
      "exploration/Rewards Max                                3.98277\n",
      "exploration/Rewards Min                               -4.19777\n",
      "exploration/Returns Mean                             419.626\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              419.626\n",
      "exploration/Returns Min                              419.626\n",
      "exploration/Actions Mean                               0.140448\n",
      "exploration/Actions Std                                0.678468\n",
      "exploration/Actions Max                                0.999946\n",
      "exploration/Actions Min                               -0.999234\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          419.626\n",
      "exploration/env_infos/final/reward_run Mean           -1.5581\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.5581\n",
      "exploration/env_infos/final/reward_run Min            -1.5581\n",
      "exploration/env_infos/initial/reward_run Mean         -0.183909\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.183909\n",
      "exploration/env_infos/initial/reward_run Min          -0.183909\n",
      "exploration/env_infos/reward_run Mean                 -1.50028\n",
      "exploration/env_infos/reward_run Std                   0.612341\n",
      "exploration/env_infos/reward_run Max                   1.69089\n",
      "exploration/env_infos/reward_run Min                  -3.13842\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.272526\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.272526\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.272526\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.066606\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.066606\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.066606\n",
      "exploration/env_infos/reward_ctrl Mean                -0.288027\n",
      "exploration/env_infos/reward_ctrl Std                  0.0842713\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0617399\n",
      "exploration/env_infos/reward_ctrl Min                 -0.52325\n",
      "exploration/env_infos/final/height Mean               -0.0115566\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0115566\n",
      "exploration/env_infos/final/height Min                -0.0115566\n",
      "exploration/env_infos/initial/height Mean              0.0230243\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0230243\n",
      "exploration/env_infos/initial/height Min               0.0230243\n",
      "exploration/env_infos/height Mean                     -0.103135\n",
      "exploration/env_infos/height Std                       0.0638272\n",
      "exploration/env_infos/height Max                       0.234955\n",
      "exploration/env_infos/height Min                      -0.26262\n",
      "exploration/env_infos/final/reward_angular Mean       -0.111642\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.111642\n",
      "exploration/env_infos/final/reward_angular Min        -0.111642\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.054198\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.054198\n",
      "exploration/env_infos/initial/reward_angular Min      -0.054198\n",
      "exploration/env_infos/reward_angular Mean              0.0284386\n",
      "exploration/env_infos/reward_angular Std               1.5217\n",
      "exploration/env_infos/reward_angular Max               4.97261\n",
      "exploration/env_infos/reward_angular Min              -3.87487\n",
      "evaluation/num steps total                             1.4e+06\n",
      "evaluation/num paths total                          1400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0729086\n",
      "evaluation/Rewards Std                                 1.04919\n",
      "evaluation/Rewards Max                                 6.49269\n",
      "evaluation/Rewards Min                                -5.76989\n",
      "evaluation/Returns Mean                              -72.9086\n",
      "evaluation/Returns Std                               364.583\n",
      "evaluation/Returns Max                              1182.41\n",
      "evaluation/Returns Min                              -590.291\n",
      "evaluation/Actions Mean                                0.0921824\n",
      "evaluation/Actions Std                                 0.704634\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -0.999987\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -72.9086\n",
      "evaluation/env_infos/final/reward_run Mean            -0.213685\n",
      "evaluation/env_infos/final/reward_run Std              0.935696\n",
      "evaluation/env_infos/final/reward_run Max              1.45147\n",
      "evaluation/env_infos/final/reward_run Min             -2.19568\n",
      "evaluation/env_infos/initial/reward_run Mean           0.449795\n",
      "evaluation/env_infos/initial/reward_run Std            0.5319\n",
      "evaluation/env_infos/initial/reward_run Max            1.34076\n",
      "evaluation/env_infos/initial/reward_run Min           -0.424119\n",
      "evaluation/env_infos/reward_run Mean                  -0.110572\n",
      "evaluation/env_infos/reward_run Std                    0.930263\n",
      "evaluation/env_infos/reward_run Max                    3.26858\n",
      "evaluation/env_infos/reward_run Min                   -2.97255\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.283936\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.100839\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0420539\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.410558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.341671\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.184772\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0968617\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.580444\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.303004\n",
      "evaluation/env_infos/reward_ctrl Std                   0.107134\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0208504\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.594091\n",
      "evaluation/env_infos/final/height Mean                -0.368215\n",
      "evaluation/env_infos/final/height Std                  0.198267\n",
      "evaluation/env_infos/final/height Max                 -0.001205\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00791077\n",
      "evaluation/env_infos/initial/height Std                0.0505656\n",
      "evaluation/env_infos/initial/height Max                0.0862575\n",
      "evaluation/env_infos/initial/height Min               -0.0609197\n",
      "evaluation/env_infos/height Mean                      -0.359309\n",
      "evaluation/env_infos/height Std                        0.20411\n",
      "evaluation/env_infos/height Max                        0.371495\n",
      "evaluation/env_infos/height Min                       -0.594021\n",
      "evaluation/env_infos/final/reward_angular Mean         0.110204\n",
      "evaluation/env_infos/final/reward_angular Std          1.36648\n",
      "evaluation/env_infos/final/reward_angular Max          2.36691\n",
      "evaluation/env_infos/final/reward_angular Min         -3.71153\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.166395\n",
      "evaluation/env_infos/initial/reward_angular Std        0.921187\n",
      "evaluation/env_infos/initial/reward_angular Max        1.8086\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.51152\n",
      "evaluation/env_infos/reward_angular Mean               0.029167\n",
      "evaluation/env_infos/reward_angular Std                1.64564\n",
      "evaluation/env_infos/reward_angular Max                8.69133\n",
      "evaluation/env_infos/reward_angular Min               -7.45365\n",
      "time/data storing (s)                                  0.0152425\n",
      "time/evaluation sampling (s)                          20.7543\n",
      "time/exploration sampling (s)                          1.01841\n",
      "time/logging (s)                                       0.238341\n",
      "time/saving (s)                                        0.0280069\n",
      "time/training (s)                                      3.80309\n",
      "time/epoch (s)                                        25.8574\n",
      "time/total (s)                                      1690.04\n",
      "Epoch                                                 55\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:40:39.898350 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 56 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 58000\n",
      "trainer/QF1 Loss                                       0.609372\n",
      "trainer/QF2 Loss                                       0.648349\n",
      "trainer/Policy Loss                                    2.5743\n",
      "trainer/Q1 Predictions Mean                            3.3273\n",
      "trainer/Q1 Predictions Std                             7.73471\n",
      "trainer/Q1 Predictions Max                            38.1527\n",
      "trainer/Q1 Predictions Min                            -6.86604\n",
      "trainer/Q2 Predictions Mean                            3.40934\n",
      "trainer/Q2 Predictions Std                             7.68039\n",
      "trainer/Q2 Predictions Max                            35.6984\n",
      "trainer/Q2 Predictions Min                            -6.89462\n",
      "trainer/Q Targets Mean                                 3.38404\n",
      "trainer/Q Targets Std                                  7.67643\n",
      "trainer/Q Targets Max                                 38.8305\n",
      "trainer/Q Targets Min                                 -7.14668\n",
      "trainer/Log Pis Mean                                   6.26613\n",
      "trainer/Log Pis Std                                    5.94806\n",
      "trainer/Log Pis Max                                   44.8257\n",
      "trainer/Log Pis Min                                   -4.57649\n",
      "trainer/Policy mu Mean                                 0.314488\n",
      "trainer/Policy mu Std                                  1.61238\n",
      "trainer/Policy mu Max                                  4.71182\n",
      "trainer/Policy mu Min                                 -7.92877\n",
      "trainer/Policy log std Mean                           -0.747095\n",
      "trainer/Policy log std Std                             0.314323\n",
      "trainer/Policy log std Max                             1.83386\n",
      "trainer/Policy log std Min                            -1.92422\n",
      "trainer/Alpha                                          0.011676\n",
      "trainer/Alpha Loss                                     1.18478\n",
      "exploration/num steps total                        58000\n",
      "exploration/num paths total                           58\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.858027\n",
      "exploration/Rewards Std                                1.20189\n",
      "exploration/Rewards Max                                3.59424\n",
      "exploration/Rewards Min                               -2.81029\n",
      "exploration/Returns Mean                             858.027\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              858.027\n",
      "exploration/Returns Min                              858.027\n",
      "exploration/Actions Mean                               0.219402\n",
      "exploration/Actions Std                                0.772366\n",
      "exploration/Actions Max                                0.999999\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          858.027\n",
      "exploration/env_infos/final/reward_run Mean            1.58212\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.58212\n",
      "exploration/env_infos/final/reward_run Min             1.58212\n",
      "exploration/env_infos/initial/reward_run Mean          0.619948\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.619948\n",
      "exploration/env_infos/initial/reward_run Min           0.619948\n",
      "exploration/env_infos/reward_run Mean                  1.26723\n",
      "exploration/env_infos/reward_run Std                   0.718728\n",
      "exploration/env_infos/reward_run Max                   2.9697\n",
      "exploration/env_infos/reward_run Min                  -0.943459\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.381658\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.381658\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.381658\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.188879\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.188879\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.188879\n",
      "exploration/env_infos/reward_ctrl Mean                -0.386812\n",
      "exploration/env_infos/reward_ctrl Std                  0.0949083\n",
      "exploration/env_infos/reward_ctrl Max                 -0.097311\n",
      "exploration/env_infos/reward_ctrl Min                 -0.590656\n",
      "exploration/env_infos/final/height Mean               -0.0615024\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0615024\n",
      "exploration/env_infos/final/height Min                -0.0615024\n",
      "exploration/env_infos/initial/height Mean             -0.0163406\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0163406\n",
      "exploration/env_infos/initial/height Min              -0.0163406\n",
      "exploration/env_infos/height Mean                     -0.212915\n",
      "exploration/env_infos/height Std                       0.0974645\n",
      "exploration/env_infos/height Max                       0.0696904\n",
      "exploration/env_infos/height Min                      -0.423896\n",
      "exploration/env_infos/final/reward_angular Mean       -0.371803\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.371803\n",
      "exploration/env_infos/final/reward_angular Min        -0.371803\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.61392\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.61392\n",
      "exploration/env_infos/initial/reward_angular Min      -1.61392\n",
      "exploration/env_infos/reward_angular Mean             -0.0758217\n",
      "exploration/env_infos/reward_angular Std               2.13623\n",
      "exploration/env_infos/reward_angular Max               7.26158\n",
      "exploration/env_infos/reward_angular Min              -5.03964\n",
      "evaluation/num steps total                             1.425e+06\n",
      "evaluation/num paths total                          1425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.152658\n",
      "evaluation/Rewards Std                                 0.919962\n",
      "evaluation/Rewards Max                                 5.77187\n",
      "evaluation/Rewards Min                                -4.73837\n",
      "evaluation/Returns Mean                             -152.658\n",
      "evaluation/Returns Std                               273.771\n",
      "evaluation/Returns Max                               880.337\n",
      "evaluation/Returns Min                              -628.495\n",
      "evaluation/Actions Mean                                0.0644082\n",
      "evaluation/Actions Std                                 0.743391\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -152.658\n",
      "evaluation/env_infos/final/reward_run Mean             0.0818079\n",
      "evaluation/env_infos/final/reward_run Std              0.920724\n",
      "evaluation/env_infos/final/reward_run Max              2.14086\n",
      "evaluation/env_infos/final/reward_run Min             -2.36005\n",
      "evaluation/env_infos/initial/reward_run Mean           0.278618\n",
      "evaluation/env_infos/initial/reward_run Std            0.686941\n",
      "evaluation/env_infos/initial/reward_run Max            1.22576\n",
      "evaluation/env_infos/initial/reward_run Min           -0.897409\n",
      "evaluation/env_infos/reward_run Mean                  -0.0466415\n",
      "evaluation/env_infos/reward_run Std                    0.953166\n",
      "evaluation/env_infos/reward_run Max                    3.1215\n",
      "evaluation/env_infos/reward_run Min                   -3.66603\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.320447\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0903977\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.105865\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.483952\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.359116\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.156652\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.119779\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.569211\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.334067\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0910078\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.018239\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.594373\n",
      "evaluation/env_infos/final/height Mean                -0.365228\n",
      "evaluation/env_infos/final/height Std                  0.210567\n",
      "evaluation/env_infos/final/height Max                  0.00491777\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean               0.00911607\n",
      "evaluation/env_infos/initial/height Std                0.0410056\n",
      "evaluation/env_infos/initial/height Max                0.0706366\n",
      "evaluation/env_infos/initial/height Min               -0.0702951\n",
      "evaluation/env_infos/height Mean                      -0.339102\n",
      "evaluation/env_infos/height Std                        0.227753\n",
      "evaluation/env_infos/height Max                        0.363662\n",
      "evaluation/env_infos/height Min                       -0.594848\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.0707075\n",
      "evaluation/env_infos/final/reward_angular Std          1.25037\n",
      "evaluation/env_infos/final/reward_angular Max          2.74749\n",
      "evaluation/env_infos/final/reward_angular Min         -3.68361\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.239149\n",
      "evaluation/env_infos/initial/reward_angular Std        1.06872\n",
      "evaluation/env_infos/initial/reward_angular Max        2.6409\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.51232\n",
      "evaluation/env_infos/reward_angular Mean               0.0269882\n",
      "evaluation/env_infos/reward_angular Std                1.48152\n",
      "evaluation/env_infos/reward_angular Max                7.96556\n",
      "evaluation/env_infos/reward_angular Min               -8.60061\n",
      "time/data storing (s)                                  0.0154616\n",
      "time/evaluation sampling (s)                          21.2956\n",
      "time/exploration sampling (s)                          1.0216\n",
      "time/logging (s)                                       0.249496\n",
      "time/saving (s)                                        0.0314065\n",
      "time/training (s)                                      4.19273\n",
      "time/epoch (s)                                        26.8063\n",
      "time/total (s)                                      1717.32\n",
      "Epoch                                                 56\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:41:07.291438 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 57 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 59000\n",
      "trainer/QF1 Loss                                       0.663939\n",
      "trainer/QF2 Loss                                       0.729954\n",
      "trainer/Policy Loss                                    2.76267\n",
      "trainer/Q1 Predictions Mean                            3.3059\n",
      "trainer/Q1 Predictions Std                             6.27733\n",
      "trainer/Q1 Predictions Max                            33.4204\n",
      "trainer/Q1 Predictions Min                            -7.94315\n",
      "trainer/Q2 Predictions Mean                            3.30244\n",
      "trainer/Q2 Predictions Std                             6.35526\n",
      "trainer/Q2 Predictions Max                            32.262\n",
      "trainer/Q2 Predictions Min                            -7.89191\n",
      "trainer/Q Targets Mean                                 3.23369\n",
      "trainer/Q Targets Std                                  6.35259\n",
      "trainer/Q Targets Max                                 32.9269\n",
      "trainer/Q Targets Min                                 -8.64694\n",
      "trainer/Log Pis Mean                                   6.37499\n",
      "trainer/Log Pis Std                                    6.06368\n",
      "trainer/Log Pis Max                                   34.5911\n",
      "trainer/Log Pis Min                                   -4.10754\n",
      "trainer/Policy mu Mean                                 0.0997335\n",
      "trainer/Policy mu Std                                  1.67872\n",
      "trainer/Policy mu Max                                  5.89453\n",
      "trainer/Policy mu Min                                 -4.84362\n",
      "trainer/Policy log std Mean                           -0.697233\n",
      "trainer/Policy log std Std                             0.3026\n",
      "trainer/Policy log std Max                             0.52135\n",
      "trainer/Policy log std Min                            -1.84128\n",
      "trainer/Alpha                                          0.0120197\n",
      "trainer/Alpha Loss                                     1.65859\n",
      "exploration/num steps total                        59000\n",
      "exploration/num paths total                           59\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.0577454\n",
      "exploration/Rewards Std                                0.58866\n",
      "exploration/Rewards Max                                1.96869\n",
      "exploration/Rewards Min                               -1.61216\n",
      "exploration/Returns Mean                              57.7454\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                               57.7454\n",
      "exploration/Returns Min                               57.7454\n",
      "exploration/Actions Mean                              -0.0529208\n",
      "exploration/Actions Std                                0.745977\n",
      "exploration/Actions Max                                0.999992\n",
      "exploration/Actions Min                               -0.999894\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                           57.7454\n",
      "exploration/env_infos/final/reward_run Mean            0.316554\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.316554\n",
      "exploration/env_infos/final/reward_run Min             0.316554\n",
      "exploration/env_infos/initial/reward_run Mean         -0.179665\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.179665\n",
      "exploration/env_infos/initial/reward_run Min          -0.179665\n",
      "exploration/env_infos/reward_run Mean                 -0.244706\n",
      "exploration/env_infos/reward_run Std                   0.600525\n",
      "exploration/env_infos/reward_run Max                   1.56988\n",
      "exploration/env_infos/reward_run Min                  -2.15524\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.469931\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.469931\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.469931\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.449057\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.449057\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.449057\n",
      "exploration/env_infos/reward_ctrl Mean                -0.335569\n",
      "exploration/env_infos/reward_ctrl Std                  0.0977487\n",
      "exploration/env_infos/reward_ctrl Max                 -0.077013\n",
      "exploration/env_infos/reward_ctrl Min                 -0.563344\n",
      "exploration/env_infos/final/height Mean               -0.533336\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.533336\n",
      "exploration/env_infos/final/height Min                -0.533336\n",
      "exploration/env_infos/initial/height Mean              0.0533035\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0533035\n",
      "exploration/env_infos/initial/height Min               0.0533035\n",
      "exploration/env_infos/height Mean                     -0.464371\n",
      "exploration/env_infos/height Std                       0.156389\n",
      "exploration/env_infos/height Max                       0.0533035\n",
      "exploration/env_infos/height Min                      -0.582475\n",
      "exploration/env_infos/final/reward_angular Mean       -1.07318\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.07318\n",
      "exploration/env_infos/final/reward_angular Min        -1.07318\n",
      "exploration/env_infos/initial/reward_angular Mean      1.22469\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.22469\n",
      "exploration/env_infos/initial/reward_angular Min       1.22469\n",
      "exploration/env_infos/reward_angular Mean             -0.0484307\n",
      "exploration/env_infos/reward_angular Std               1.44267\n",
      "exploration/env_infos/reward_angular Max               4.30444\n",
      "exploration/env_infos/reward_angular Min              -4.9958\n",
      "evaluation/num steps total                             1.45e+06\n",
      "evaluation/num paths total                          1450\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.178108\n",
      "evaluation/Rewards Std                                 0.830956\n",
      "evaluation/Rewards Max                                 5.29018\n",
      "evaluation/Rewards Min                                -4.15462\n",
      "evaluation/Returns Mean                             -178.108\n",
      "evaluation/Returns Std                               257.899\n",
      "evaluation/Returns Max                               810.676\n",
      "evaluation/Returns Min                              -623.088\n",
      "evaluation/Actions Mean                               -0.0177188\n",
      "evaluation/Actions Std                                 0.738396\n",
      "evaluation/Actions Max                                 0.999995\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -178.108\n",
      "evaluation/env_infos/final/reward_run Mean             0.0323183\n",
      "evaluation/env_infos/final/reward_run Std              0.508032\n",
      "evaluation/env_infos/final/reward_run Max              1.1495\n",
      "evaluation/env_infos/final/reward_run Min             -1.15601\n",
      "evaluation/env_infos/initial/reward_run Mean           0.504855\n",
      "evaluation/env_infos/initial/reward_run Std            0.598902\n",
      "evaluation/env_infos/initial/reward_run Max            1.39913\n",
      "evaluation/env_infos/initial/reward_run Min           -0.497717\n",
      "evaluation/env_infos/reward_run Mean                   0.000469244\n",
      "evaluation/env_infos/reward_run Std                    0.67577\n",
      "evaluation/env_infos/reward_run Max                    2.74324\n",
      "evaluation/env_infos/reward_run Min                   -2.80847\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.344592\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.101494\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0816493\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.534958\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.350355\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.176961\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.115398\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.589142\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.327325\n",
      "evaluation/env_infos/reward_ctrl Std                   0.104327\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0125194\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.593786\n",
      "evaluation/env_infos/final/height Mean                -0.450547\n",
      "evaluation/env_infos/final/height Std                  0.14453\n",
      "evaluation/env_infos/final/height Max                 -0.0976863\n",
      "evaluation/env_infos/final/height Min                 -0.577304\n",
      "evaluation/env_infos/initial/height Mean              -0.00592945\n",
      "evaluation/env_infos/initial/height Std                0.049694\n",
      "evaluation/env_infos/initial/height Max                0.0799326\n",
      "evaluation/env_infos/initial/height Min               -0.102122\n",
      "evaluation/env_infos/height Mean                      -0.395736\n",
      "evaluation/env_infos/height Std                        0.18306\n",
      "evaluation/env_infos/height Max                        0.305202\n",
      "evaluation/env_infos/height Min                       -0.592852\n",
      "evaluation/env_infos/final/reward_angular Mean         0.385874\n",
      "evaluation/env_infos/final/reward_angular Std          1.10554\n",
      "evaluation/env_infos/final/reward_angular Max          3.28287\n",
      "evaluation/env_infos/final/reward_angular Min         -1.34906\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.300677\n",
      "evaluation/env_infos/initial/reward_angular Std        1.0098\n",
      "evaluation/env_infos/initial/reward_angular Max        2.48791\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.36246\n",
      "evaluation/env_infos/reward_angular Mean               0.0207792\n",
      "evaluation/env_infos/reward_angular Std                1.30477\n",
      "evaluation/env_infos/reward_angular Max                7.3944\n",
      "evaluation/env_infos/reward_angular Min               -5.47711\n",
      "time/data storing (s)                                  0.0147209\n",
      "time/evaluation sampling (s)                          21.8282\n",
      "time/exploration sampling (s)                          0.973041\n",
      "time/logging (s)                                       0.237469\n",
      "time/saving (s)                                        0.0281311\n",
      "time/training (s)                                      3.78676\n",
      "time/epoch (s)                                        26.8683\n",
      "time/total (s)                                      1744.7\n",
      "Epoch                                                 57\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:41:33.760575 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 58 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 60000\n",
      "trainer/QF1 Loss                                       0.530563\n",
      "trainer/QF2 Loss                                       0.633003\n",
      "trainer/Policy Loss                                    2.85833\n",
      "trainer/Q1 Predictions Mean                            2.61269\n",
      "trainer/Q1 Predictions Std                             6.47432\n",
      "trainer/Q1 Predictions Max                            36.8132\n",
      "trainer/Q1 Predictions Min                            -5.65099\n",
      "trainer/Q2 Predictions Mean                            2.57554\n",
      "trainer/Q2 Predictions Std                             6.50311\n",
      "trainer/Q2 Predictions Max                            37.6256\n",
      "trainer/Q2 Predictions Min                            -5.68719\n",
      "trainer/Q Targets Mean                                 2.73119\n",
      "trainer/Q Targets Std                                  6.58654\n",
      "trainer/Q Targets Max                                 36.3828\n",
      "trainer/Q Targets Min                                 -5.95696\n",
      "trainer/Log Pis Mean                                   5.68293\n",
      "trainer/Log Pis Std                                    5.16364\n",
      "trainer/Log Pis Max                                   26.2124\n",
      "trainer/Log Pis Min                                   -5.28896\n",
      "trainer/Policy mu Mean                                 0.116257\n",
      "trainer/Policy mu Std                                  1.56161\n",
      "trainer/Policy mu Max                                  4.34952\n",
      "trainer/Policy mu Min                                 -4.91788\n",
      "trainer/Policy log std Mean                           -0.738908\n",
      "trainer/Policy log std Std                             0.293169\n",
      "trainer/Policy log std Max                             0.261367\n",
      "trainer/Policy log std Min                            -1.93481\n",
      "trainer/Alpha                                          0.0122619\n",
      "trainer/Alpha Loss                                    -1.3953\n",
      "exploration/num steps total                        60000\n",
      "exploration/num paths total                           60\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.124075\n",
      "exploration/Rewards Std                                1.17284\n",
      "exploration/Rewards Max                                4.07546\n",
      "exploration/Rewards Min                               -3.19131\n",
      "exploration/Returns Mean                             124.075\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              124.075\n",
      "exploration/Returns Min                              124.075\n",
      "exploration/Actions Mean                               0.355885\n",
      "exploration/Actions Std                                0.692105\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999999\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          124.075\n",
      "exploration/env_infos/final/reward_run Mean            1.22169\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.22169\n",
      "exploration/env_infos/final/reward_run Min             1.22169\n",
      "exploration/env_infos/initial/reward_run Mean          0.546331\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.546331\n",
      "exploration/env_infos/initial/reward_run Min           0.546331\n",
      "exploration/env_infos/reward_run Mean                  0.5597\n",
      "exploration/env_infos/reward_run Std                   0.433622\n",
      "exploration/env_infos/reward_run Max                   2.24904\n",
      "exploration/env_infos/reward_run Min                  -0.891572\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.402461\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.402461\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.402461\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.185408\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.185408\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.185408\n",
      "exploration/env_infos/reward_ctrl Mean                -0.363398\n",
      "exploration/env_infos/reward_ctrl Std                  0.086421\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0673719\n",
      "exploration/env_infos/reward_ctrl Min                 -0.573727\n",
      "exploration/env_infos/final/height Mean               -0.288756\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.288756\n",
      "exploration/env_infos/final/height Min                -0.288756\n",
      "exploration/env_infos/initial/height Mean             -0.0729572\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0729572\n",
      "exploration/env_infos/initial/height Min              -0.0729572\n",
      "exploration/env_infos/height Mean                     -0.223543\n",
      "exploration/env_infos/height Std                       0.0817078\n",
      "exploration/env_infos/height Max                      -0.0109223\n",
      "exploration/env_infos/height Min                      -0.446663\n",
      "exploration/env_infos/final/reward_angular Mean       -1.23285\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.23285\n",
      "exploration/env_infos/final/reward_angular Min        -1.23285\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.48562\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.48562\n",
      "exploration/env_infos/initial/reward_angular Min      -1.48562\n",
      "exploration/env_infos/reward_angular Mean             -0.0194165\n",
      "exploration/env_infos/reward_angular Std               1.33776\n",
      "exploration/env_infos/reward_angular Max               3.74404\n",
      "exploration/env_infos/reward_angular Min              -4.60927\n",
      "evaluation/num steps total                             1.475e+06\n",
      "evaluation/num paths total                          1475\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0823664\n",
      "evaluation/Rewards Std                                 1.13966\n",
      "evaluation/Rewards Max                                 7.6952\n",
      "evaluation/Rewards Min                                -5.75055\n",
      "evaluation/Returns Mean                              -82.3664\n",
      "evaluation/Returns Std                               261.041\n",
      "evaluation/Returns Max                               735.838\n",
      "evaluation/Returns Min                              -483.91\n",
      "evaluation/Actions Mean                                0.0605898\n",
      "evaluation/Actions Std                                 0.734023\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -0.999997\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -82.3664\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0922084\n",
      "evaluation/env_infos/final/reward_run Std              1.0223\n",
      "evaluation/env_infos/final/reward_run Max              2.11308\n",
      "evaluation/env_infos/final/reward_run Min             -1.97949\n",
      "evaluation/env_infos/initial/reward_run Mean           0.295234\n",
      "evaluation/env_infos/initial/reward_run Std            0.633385\n",
      "evaluation/env_infos/initial/reward_run Max            1.0555\n",
      "evaluation/env_infos/initial/reward_run Min           -0.814122\n",
      "evaluation/env_infos/reward_run Mean                  -0.231245\n",
      "evaluation/env_infos/reward_run Std                    1.05111\n",
      "evaluation/env_infos/reward_run Max                    2.88552\n",
      "evaluation/env_infos/reward_run Min                   -3.76316\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.339995\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0953986\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.109546\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.523714\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.31935\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.165449\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0842659\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.559669\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.325476\n",
      "evaluation/env_infos/reward_ctrl Std                   0.104635\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0125889\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.597239\n",
      "evaluation/env_infos/final/height Mean                -0.385388\n",
      "evaluation/env_infos/final/height Std                  0.213318\n",
      "evaluation/env_infos/final/height Max                  0.0291294\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.008439\n",
      "evaluation/env_infos/initial/height Std                0.0570016\n",
      "evaluation/env_infos/initial/height Max                0.0831768\n",
      "evaluation/env_infos/initial/height Min               -0.111049\n",
      "evaluation/env_infos/height Mean                      -0.336139\n",
      "evaluation/env_infos/height Std                        0.204853\n",
      "evaluation/env_infos/height Max                        0.393005\n",
      "evaluation/env_infos/height Min                       -0.599928\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.56169\n",
      "evaluation/env_infos/final/reward_angular Std          1.58406\n",
      "evaluation/env_infos/final/reward_angular Max          2.36478\n",
      "evaluation/env_infos/final/reward_angular Min         -4.24846\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.052635\n",
      "evaluation/env_infos/initial/reward_angular Std        0.949401\n",
      "evaluation/env_infos/initial/reward_angular Max        1.78459\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.72657\n",
      "evaluation/env_infos/reward_angular Mean               0.0362576\n",
      "evaluation/env_infos/reward_angular Std                1.67393\n",
      "evaluation/env_infos/reward_angular Max                9.48951\n",
      "evaluation/env_infos/reward_angular Min               -7.07288\n",
      "time/data storing (s)                                  0.0155422\n",
      "time/evaluation sampling (s)                          20.8548\n",
      "time/exploration sampling (s)                          1.02901\n",
      "time/logging (s)                                       0.235572\n",
      "time/saving (s)                                        0.0277757\n",
      "time/training (s)                                      3.80526\n",
      "time/epoch (s)                                        25.968\n",
      "time/total (s)                                      1771.17\n",
      "Epoch                                                 58\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:41:59.960324 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 59 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 61000\n",
      "trainer/QF1 Loss                                       0.644854\n",
      "trainer/QF2 Loss                                       0.658223\n",
      "trainer/Policy Loss                                    2.77164\n",
      "trainer/Q1 Predictions Mean                            3.22665\n",
      "trainer/Q1 Predictions Std                             6.41118\n",
      "trainer/Q1 Predictions Max                            35.2439\n",
      "trainer/Q1 Predictions Min                            -7.68944\n",
      "trainer/Q2 Predictions Mean                            3.35602\n",
      "trainer/Q2 Predictions Std                             6.44377\n",
      "trainer/Q2 Predictions Max                            35.0008\n",
      "trainer/Q2 Predictions Min                            -7.38126\n",
      "trainer/Q Targets Mean                                 3.29787\n",
      "trainer/Q Targets Std                                  6.4971\n",
      "trainer/Q Targets Max                                 36.5232\n",
      "trainer/Q Targets Min                                 -8.12639\n",
      "trainer/Log Pis Mean                                   6.34447\n",
      "trainer/Log Pis Std                                    6.43458\n",
      "trainer/Log Pis Max                                   39.2556\n",
      "trainer/Log Pis Min                                   -4.24887\n",
      "trainer/Policy mu Mean                                 0.199557\n",
      "trainer/Policy mu Std                                  1.72878\n",
      "trainer/Policy mu Max                                  5.40439\n",
      "trainer/Policy mu Min                                -10.0326\n",
      "trainer/Policy log std Mean                           -0.718732\n",
      "trainer/Policy log std Std                             0.286104\n",
      "trainer/Policy log std Max                             0.452276\n",
      "trainer/Policy log std Min                            -1.78725\n",
      "trainer/Alpha                                          0.0129485\n",
      "trainer/Alpha Loss                                     1.49769\n",
      "exploration/num steps total                        61000\n",
      "exploration/num paths total                           61\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.4209\n",
      "exploration/Rewards Std                                0.896866\n",
      "exploration/Rewards Max                                3.22494\n",
      "exploration/Rewards Min                               -1.95149\n",
      "exploration/Returns Mean                             420.9\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              420.9\n",
      "exploration/Returns Min                              420.9\n",
      "exploration/Actions Mean                               0.228409\n",
      "exploration/Actions Std                                0.814937\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999985\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          420.9\n",
      "exploration/env_infos/final/reward_run Mean           -0.395349\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.395349\n",
      "exploration/env_infos/final/reward_run Min            -0.395349\n",
      "exploration/env_infos/initial/reward_run Mean          0.816069\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.816069\n",
      "exploration/env_infos/initial/reward_run Min           0.816069\n",
      "exploration/env_infos/reward_run Mean                  0.601681\n",
      "exploration/env_infos/reward_run Std                   0.926106\n",
      "exploration/env_infos/reward_run Max                   3.4375\n",
      "exploration/env_infos/reward_run Min                  -1.6188\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.432674\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.432674\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.432674\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.447479\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.447479\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.447479\n",
      "exploration/env_infos/reward_ctrl Mean                -0.429776\n",
      "exploration/env_infos/reward_ctrl Std                  0.094116\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0838336\n",
      "exploration/env_infos/reward_ctrl Min                 -0.597401\n",
      "exploration/env_infos/final/height Mean               -0.446291\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.446291\n",
      "exploration/env_infos/final/height Min                -0.446291\n",
      "exploration/env_infos/initial/height Mean              0.000937657\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.000937657\n",
      "exploration/env_infos/initial/height Min               0.000937657\n",
      "exploration/env_infos/height Mean                     -0.427034\n",
      "exploration/env_infos/height Std                       0.149105\n",
      "exploration/env_infos/height Max                       0.124364\n",
      "exploration/env_infos/height Min                      -0.57846\n",
      "exploration/env_infos/final/reward_angular Mean       -0.863423\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.863423\n",
      "exploration/env_infos/final/reward_angular Min        -0.863423\n",
      "exploration/env_infos/initial/reward_angular Mean      0.0119633\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.0119633\n",
      "exploration/env_infos/initial/reward_angular Min       0.0119633\n",
      "exploration/env_infos/reward_angular Mean              0.0351673\n",
      "exploration/env_infos/reward_angular Std               2.17182\n",
      "exploration/env_infos/reward_angular Max               7.4855\n",
      "exploration/env_infos/reward_angular Min              -7.7357\n",
      "evaluation/num steps total                             1.5e+06\n",
      "evaluation/num paths total                          1500\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.146879\n",
      "evaluation/Rewards Std                                 1.07063\n",
      "evaluation/Rewards Max                                 7.10868\n",
      "evaluation/Rewards Min                                -4.88447\n",
      "evaluation/Returns Mean                             -146.879\n",
      "evaluation/Returns Std                               298.393\n",
      "evaluation/Returns Max                              1028.17\n",
      "evaluation/Returns Min                              -510.03\n",
      "evaluation/Actions Mean                                0.0331441\n",
      "evaluation/Actions Std                                 0.766879\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -146.879\n",
      "evaluation/env_infos/final/reward_run Mean             0.173069\n",
      "evaluation/env_infos/final/reward_run Std              0.680519\n",
      "evaluation/env_infos/final/reward_run Max              1.90951\n",
      "evaluation/env_infos/final/reward_run Min             -0.921838\n",
      "evaluation/env_infos/initial/reward_run Mean           0.248673\n",
      "evaluation/env_infos/initial/reward_run Std            0.631584\n",
      "evaluation/env_infos/initial/reward_run Max            1.23256\n",
      "evaluation/env_infos/initial/reward_run Min           -0.727142\n",
      "evaluation/env_infos/reward_run Mean                   0.11606\n",
      "evaluation/env_infos/reward_run Std                    0.812642\n",
      "evaluation/env_infos/reward_run Max                    3.14065\n",
      "evaluation/env_infos/reward_run Min                   -4.9163\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.354894\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.110859\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.117236\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.541429\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.334129\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.15038\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0650986\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.55676\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.353521\n",
      "evaluation/env_infos/reward_ctrl Std                   0.102527\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0167458\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.592976\n",
      "evaluation/env_infos/final/height Mean                -0.474693\n",
      "evaluation/env_infos/final/height Std                  0.126199\n",
      "evaluation/env_infos/final/height Max                 -0.173098\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00179428\n",
      "evaluation/env_infos/initial/height Std                0.0446965\n",
      "evaluation/env_infos/initial/height Max                0.0889235\n",
      "evaluation/env_infos/initial/height Min               -0.079857\n",
      "evaluation/env_infos/height Mean                      -0.420596\n",
      "evaluation/env_infos/height Std                        0.17513\n",
      "evaluation/env_infos/height Max                        0.316038\n",
      "evaluation/env_infos/height Min                       -0.597359\n",
      "evaluation/env_infos/final/reward_angular Mean         0.144411\n",
      "evaluation/env_infos/final/reward_angular Std          1.60507\n",
      "evaluation/env_infos/final/reward_angular Max          3.61353\n",
      "evaluation/env_infos/final/reward_angular Min         -4.03683\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.801976\n",
      "evaluation/env_infos/initial/reward_angular Std        0.783629\n",
      "evaluation/env_infos/initial/reward_angular Max        2.01101\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.611604\n",
      "evaluation/env_infos/reward_angular Mean               0.0150397\n",
      "evaluation/env_infos/reward_angular Std                1.70087\n",
      "evaluation/env_infos/reward_angular Max                8.70333\n",
      "evaluation/env_infos/reward_angular Min               -8.18553\n",
      "time/data storing (s)                                  0.0145128\n",
      "time/evaluation sampling (s)                          20.6092\n",
      "time/exploration sampling (s)                          0.97199\n",
      "time/logging (s)                                       0.239973\n",
      "time/saving (s)                                        0.0286552\n",
      "time/training (s)                                      3.79166\n",
      "time/epoch (s)                                        25.656\n",
      "time/total (s)                                      1797.37\n",
      "Epoch                                                 59\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:42:26.720559 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 60 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 62000\n",
      "trainer/QF1 Loss                                       0.702845\n",
      "trainer/QF2 Loss                                       0.605831\n",
      "trainer/Policy Loss                                    2.14371\n",
      "trainer/Q1 Predictions Mean                            3.53288\n",
      "trainer/Q1 Predictions Std                             6.68364\n",
      "trainer/Q1 Predictions Max                            44.1411\n",
      "trainer/Q1 Predictions Min                            -5.60336\n",
      "trainer/Q2 Predictions Mean                            3.59307\n",
      "trainer/Q2 Predictions Std                             6.74574\n",
      "trainer/Q2 Predictions Max                            42.5018\n",
      "trainer/Q2 Predictions Min                            -5.69957\n",
      "trainer/Q Targets Mean                                 3.46438\n",
      "trainer/Q Targets Std                                  6.81431\n",
      "trainer/Q Targets Max                                 44.9037\n",
      "trainer/Q Targets Min                                 -6.96258\n",
      "trainer/Log Pis Mean                                   5.98089\n",
      "trainer/Log Pis Std                                    5.79101\n",
      "trainer/Log Pis Max                                   29.8164\n",
      "trainer/Log Pis Min                                   -7.55747\n",
      "trainer/Policy mu Mean                                 0.113076\n",
      "trainer/Policy mu Std                                  1.59267\n",
      "trainer/Policy mu Max                                  4.3479\n",
      "trainer/Policy mu Min                                 -6.92105\n",
      "trainer/Policy log std Mean                           -0.761466\n",
      "trainer/Policy log std Std                             0.281108\n",
      "trainer/Policy log std Max                             0.348642\n",
      "trainer/Policy log std Min                            -2.01573\n",
      "trainer/Alpha                                          0.0130963\n",
      "trainer/Alpha Loss                                    -0.0828508\n",
      "exploration/num steps total                        62000\n",
      "exploration/num paths total                           62\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.770596\n",
      "exploration/Rewards Std                                0.769424\n",
      "exploration/Rewards Max                                2.54589\n",
      "exploration/Rewards Min                               -1.78907\n",
      "exploration/Returns Mean                             770.596\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              770.596\n",
      "exploration/Returns Min                              770.596\n",
      "exploration/Actions Mean                               0.139678\n",
      "exploration/Actions Std                                0.80981\n",
      "exploration/Actions Max                                0.999989\n",
      "exploration/Actions Min                               -0.999998\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          770.596\n",
      "exploration/env_infos/final/reward_run Mean            2.0109\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             2.0109\n",
      "exploration/env_infos/final/reward_run Min             2.0109\n",
      "exploration/env_infos/initial/reward_run Mean         -0.0419066\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.0419066\n",
      "exploration/env_infos/initial/reward_run Min          -0.0419066\n",
      "exploration/env_infos/reward_run Mean                  1.23498\n",
      "exploration/env_infos/reward_run Std                   0.517509\n",
      "exploration/env_infos/reward_run Max                   2.46802\n",
      "exploration/env_infos/reward_run Min                  -0.487941\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.322388\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.322388\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.322388\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.346886\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.346886\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.346886\n",
      "exploration/env_infos/reward_ctrl Mean                -0.405181\n",
      "exploration/env_infos/reward_ctrl Std                  0.0896039\n",
      "exploration/env_infos/reward_ctrl Max                 -0.117911\n",
      "exploration/env_infos/reward_ctrl Min                 -0.595464\n",
      "exploration/env_infos/final/height Mean               -0.309439\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.309439\n",
      "exploration/env_infos/final/height Min                -0.309439\n",
      "exploration/env_infos/initial/height Mean              0.0603527\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0603527\n",
      "exploration/env_infos/initial/height Min               0.0603527\n",
      "exploration/env_infos/height Mean                     -0.246962\n",
      "exploration/env_infos/height Std                       0.0731609\n",
      "exploration/env_infos/height Max                       0.0603527\n",
      "exploration/env_infos/height Min                      -0.444726\n",
      "exploration/env_infos/final/reward_angular Mean       -1.60292\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.60292\n",
      "exploration/env_infos/final/reward_angular Min        -1.60292\n",
      "exploration/env_infos/initial/reward_angular Mean      0.905546\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.905546\n",
      "exploration/env_infos/initial/reward_angular Min       0.905546\n",
      "exploration/env_infos/reward_angular Mean             -0.0446428\n",
      "exploration/env_infos/reward_angular Std               1.80253\n",
      "exploration/env_infos/reward_angular Max               5.55003\n",
      "exploration/env_infos/reward_angular Min              -4.26822\n",
      "evaluation/num steps total                             1.525e+06\n",
      "evaluation/num paths total                          1525\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.151963\n",
      "evaluation/Rewards Std                                 1.17621\n",
      "evaluation/Rewards Max                                 7.32761\n",
      "evaluation/Rewards Min                                -6.19152\n",
      "evaluation/Returns Mean                             -151.963\n",
      "evaluation/Returns Std                               308.652\n",
      "evaluation/Returns Max                              1075.41\n",
      "evaluation/Returns Min                              -590.625\n",
      "evaluation/Actions Mean                               -0.0119855\n",
      "evaluation/Actions Std                                 0.751159\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -151.963\n",
      "evaluation/env_infos/final/reward_run Mean             0.336723\n",
      "evaluation/env_infos/final/reward_run Std              0.914983\n",
      "evaluation/env_infos/final/reward_run Max              2.05944\n",
      "evaluation/env_infos/final/reward_run Min             -2.3973\n",
      "evaluation/env_infos/initial/reward_run Mean           0.244332\n",
      "evaluation/env_infos/initial/reward_run Std            0.671259\n",
      "evaluation/env_infos/initial/reward_run Max            1.1445\n",
      "evaluation/env_infos/initial/reward_run Min           -0.828101\n",
      "evaluation/env_infos/reward_run Mean                   0.0580368\n",
      "evaluation/env_infos/reward_run Std                    0.989384\n",
      "evaluation/env_infos/reward_run Max                    3.00023\n",
      "evaluation/env_infos/reward_run Min                   -4.15167\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.333195\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0981416\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0849096\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.484899\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.333578\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.152109\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0940256\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.573975\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.338631\n",
      "evaluation/env_infos/reward_ctrl Std                   0.109152\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0167413\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598322\n",
      "evaluation/env_infos/final/height Mean                -0.392752\n",
      "evaluation/env_infos/final/height Std                  0.16498\n",
      "evaluation/env_infos/final/height Max                 -0.0488698\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean              -0.00673548\n",
      "evaluation/env_infos/initial/height Std                0.0499727\n",
      "evaluation/env_infos/initial/height Max                0.0970991\n",
      "evaluation/env_infos/initial/height Min               -0.0801578\n",
      "evaluation/env_infos/height Mean                      -0.383863\n",
      "evaluation/env_infos/height Std                        0.188768\n",
      "evaluation/env_infos/height Max                        0.400966\n",
      "evaluation/env_infos/height Min                       -0.594672\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.0581304\n",
      "evaluation/env_infos/final/reward_angular Std          2.19514\n",
      "evaluation/env_infos/final/reward_angular Max          6.07148\n",
      "evaluation/env_infos/final/reward_angular Min         -3.35604\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.649986\n",
      "evaluation/env_infos/initial/reward_angular Std        1.11316\n",
      "evaluation/env_infos/initial/reward_angular Max        2.96503\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.46665\n",
      "evaluation/env_infos/reward_angular Mean               0.0155486\n",
      "evaluation/env_infos/reward_angular Std                1.78294\n",
      "evaluation/env_infos/reward_angular Max               10.1982\n",
      "evaluation/env_infos/reward_angular Min               -7.54197\n",
      "time/data storing (s)                                  0.0158919\n",
      "time/evaluation sampling (s)                          21.1366\n",
      "time/exploration sampling (s)                          0.974705\n",
      "time/logging (s)                                       0.231982\n",
      "time/saving (s)                                        0.028685\n",
      "time/training (s)                                      3.8306\n",
      "time/epoch (s)                                        26.2184\n",
      "time/total (s)                                      1824.12\n",
      "Epoch                                                 60\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:42:53.224247 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 61 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 63000\n",
      "trainer/QF1 Loss                                       0.725063\n",
      "trainer/QF2 Loss                                       0.895377\n",
      "trainer/Policy Loss                                    2.73478\n",
      "trainer/Q1 Predictions Mean                            2.95265\n",
      "trainer/Q1 Predictions Std                             6.82189\n",
      "trainer/Q1 Predictions Max                            34.0981\n",
      "trainer/Q1 Predictions Min                            -7.9956\n",
      "trainer/Q2 Predictions Mean                            3.11751\n",
      "trainer/Q2 Predictions Std                             6.94052\n",
      "trainer/Q2 Predictions Max                            35.2275\n",
      "trainer/Q2 Predictions Min                            -7.95914\n",
      "trainer/Q Targets Mean                                 2.99116\n",
      "trainer/Q Targets Std                                  6.99463\n",
      "trainer/Q Targets Max                                 36.8059\n",
      "trainer/Q Targets Min                                 -7.7079\n",
      "trainer/Log Pis Mean                                   6.05025\n",
      "trainer/Log Pis Std                                    6.3765\n",
      "trainer/Log Pis Max                                   38.2456\n",
      "trainer/Log Pis Min                                  -10.2523\n",
      "trainer/Policy mu Mean                                 0.0350889\n",
      "trainer/Policy mu Std                                  1.74159\n",
      "trainer/Policy mu Max                                  4.65365\n",
      "trainer/Policy mu Min                                 -9.72368\n",
      "trainer/Policy log std Mean                           -0.705386\n",
      "trainer/Policy log std Std                             0.304764\n",
      "trainer/Policy log std Max                             0.936839\n",
      "trainer/Policy log std Min                            -1.64594\n",
      "trainer/Alpha                                          0.0137167\n",
      "trainer/Alpha Loss                                     0.215582\n",
      "exploration/num steps total                        63000\n",
      "exploration/num paths total                           63\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.313819\n",
      "exploration/Rewards Std                                0.771544\n",
      "exploration/Rewards Max                                2.76054\n",
      "exploration/Rewards Min                               -2.69545\n",
      "exploration/Returns Mean                            -313.819\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -313.819\n",
      "exploration/Returns Min                             -313.819\n",
      "exploration/Actions Mean                               0.113872\n",
      "exploration/Actions Std                                0.761428\n",
      "exploration/Actions Max                                0.999699\n",
      "exploration/Actions Min                               -0.999988\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -313.819\n",
      "exploration/env_infos/final/reward_run Mean            1.54447\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.54447\n",
      "exploration/env_infos/final/reward_run Min             1.54447\n",
      "exploration/env_infos/initial/reward_run Mean          1.17859\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.17859\n",
      "exploration/env_infos/initial/reward_run Min           1.17859\n",
      "exploration/env_infos/reward_run Mean                  0.407806\n",
      "exploration/env_infos/reward_run Std                   0.909193\n",
      "exploration/env_infos/reward_run Max                   3.05395\n",
      "exploration/env_infos/reward_run Min                  -2.24732\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.131195\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.131195\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.131195\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.495837\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.495837\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.495837\n",
      "exploration/env_infos/reward_ctrl Mean                -0.355643\n",
      "exploration/env_infos/reward_ctrl Std                  0.0982547\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0706085\n",
      "exploration/env_infos/reward_ctrl Min                 -0.5744\n",
      "exploration/env_infos/final/height Mean               -0.425784\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.425784\n",
      "exploration/env_infos/final/height Min                -0.425784\n",
      "exploration/env_infos/initial/height Mean             -0.050437\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.050437\n",
      "exploration/env_infos/initial/height Min              -0.050437\n",
      "exploration/env_infos/height Mean                     -0.453086\n",
      "exploration/env_infos/height Std                       0.108155\n",
      "exploration/env_infos/height Max                       0.137441\n",
      "exploration/env_infos/height Min                      -0.592595\n",
      "exploration/env_infos/final/reward_angular Mean       -0.125528\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.125528\n",
      "exploration/env_infos/final/reward_angular Min        -0.125528\n",
      "exploration/env_infos/initial/reward_angular Mean      1.66282\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.66282\n",
      "exploration/env_infos/initial/reward_angular Min       1.66282\n",
      "exploration/env_infos/reward_angular Mean              0.144825\n",
      "exploration/env_infos/reward_angular Std               1.83756\n",
      "exploration/env_infos/reward_angular Max               7.24302\n",
      "exploration/env_infos/reward_angular Min              -5.24931\n",
      "evaluation/num steps total                             1.55e+06\n",
      "evaluation/num paths total                          1550\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.14098\n",
      "evaluation/Rewards Std                                 1.16263\n",
      "evaluation/Rewards Max                                 7.17861\n",
      "evaluation/Rewards Min                                -6.1157\n",
      "evaluation/Returns Mean                             -140.98\n",
      "evaluation/Returns Std                               340.36\n",
      "evaluation/Returns Max                              1145.51\n",
      "evaluation/Returns Min                              -578.76\n",
      "evaluation/Actions Mean                               -0.0533517\n",
      "evaluation/Actions Std                                 0.767979\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -140.98\n",
      "evaluation/env_infos/final/reward_run Mean             0.153894\n",
      "evaluation/env_infos/final/reward_run Std              1.10536\n",
      "evaluation/env_infos/final/reward_run Max              2.05987\n",
      "evaluation/env_infos/final/reward_run Min             -2.99484\n",
      "evaluation/env_infos/initial/reward_run Mean           0.115735\n",
      "evaluation/env_infos/initial/reward_run Std            0.710223\n",
      "evaluation/env_infos/initial/reward_run Max            1.18213\n",
      "evaluation/env_infos/initial/reward_run Min           -1.23537\n",
      "evaluation/env_infos/reward_run Mean                   0.190986\n",
      "evaluation/env_infos/reward_run Std                    0.97458\n",
      "evaluation/env_infos/reward_run Max                    3.43599\n",
      "evaluation/env_infos/reward_run Min                   -4.17069\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.365079\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0577521\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.250134\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.463613\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.339261\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.112208\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.122124\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.520396\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.355583\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0973576\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0166628\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596873\n",
      "evaluation/env_infos/final/height Mean                -0.416414\n",
      "evaluation/env_infos/final/height Std                  0.186687\n",
      "evaluation/env_infos/final/height Max                 -0.073723\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00283432\n",
      "evaluation/env_infos/initial/height Std                0.0606465\n",
      "evaluation/env_infos/initial/height Max                0.105918\n",
      "evaluation/env_infos/initial/height Min               -0.0722632\n",
      "evaluation/env_infos/height Mean                      -0.386065\n",
      "evaluation/env_infos/height Std                        0.206608\n",
      "evaluation/env_infos/height Max                        0.325346\n",
      "evaluation/env_infos/height Min                       -0.59731\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.266083\n",
      "evaluation/env_infos/final/reward_angular Std          1.54544\n",
      "evaluation/env_infos/final/reward_angular Max          4.81601\n",
      "evaluation/env_infos/final/reward_angular Min         -3.14108\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.709\n",
      "evaluation/env_infos/initial/reward_angular Std        1.22728\n",
      "evaluation/env_infos/initial/reward_angular Max        3.85457\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.734299\n",
      "evaluation/env_infos/reward_angular Mean               0.0134151\n",
      "evaluation/env_infos/reward_angular Std                1.7727\n",
      "evaluation/env_infos/reward_angular Max                9.05774\n",
      "evaluation/env_infos/reward_angular Min               -6.99667\n",
      "time/data storing (s)                                  0.0158227\n",
      "time/evaluation sampling (s)                          20.9228\n",
      "time/exploration sampling (s)                          1.01202\n",
      "time/logging (s)                                       0.239051\n",
      "time/saving (s)                                        0.0311137\n",
      "time/training (s)                                      3.79012\n",
      "time/epoch (s)                                        26.0109\n",
      "time/total (s)                                      1850.63\n",
      "Epoch                                                 61\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:43:20.853753 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 62 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 64000\n",
      "trainer/QF1 Loss                                       0.663032\n",
      "trainer/QF2 Loss                                       0.673086\n",
      "trainer/Policy Loss                                    2.18273\n",
      "trainer/Q1 Predictions Mean                            3.77333\n",
      "trainer/Q1 Predictions Std                             7.71219\n",
      "trainer/Q1 Predictions Max                            42.232\n",
      "trainer/Q1 Predictions Min                            -6.48534\n",
      "trainer/Q2 Predictions Mean                            3.56637\n",
      "trainer/Q2 Predictions Std                             7.55323\n",
      "trainer/Q2 Predictions Max                            40.1201\n",
      "trainer/Q2 Predictions Min                            -6.27351\n",
      "trainer/Q Targets Mean                                 3.72192\n",
      "trainer/Q Targets Std                                  7.65355\n",
      "trainer/Q Targets Max                                 42.6967\n",
      "trainer/Q Targets Min                                 -5.90949\n",
      "trainer/Log Pis Mean                                   6.22857\n",
      "trainer/Log Pis Std                                    5.49283\n",
      "trainer/Log Pis Max                                   39.4765\n",
      "trainer/Log Pis Min                                  -10.2371\n",
      "trainer/Policy mu Mean                                 0.330506\n",
      "trainer/Policy mu Std                                  1.5762\n",
      "trainer/Policy mu Max                                  3.93625\n",
      "trainer/Policy mu Min                                 -8.88297\n",
      "trainer/Policy log std Mean                           -0.775421\n",
      "trainer/Policy log std Std                             0.318249\n",
      "trainer/Policy log std Max                             0.288622\n",
      "trainer/Policy log std Min                            -2.24761\n",
      "trainer/Alpha                                          0.0132095\n",
      "trainer/Alpha Loss                                     0.989074\n",
      "exploration/num steps total                        64000\n",
      "exploration/num paths total                           64\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.444732\n",
      "exploration/Rewards Std                                0.278121\n",
      "exploration/Rewards Max                                0.713401\n",
      "exploration/Rewards Min                               -1.32611\n",
      "exploration/Returns Mean                            -444.732\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -444.732\n",
      "exploration/Returns Min                             -444.732\n",
      "exploration/Actions Mean                              -0.155928\n",
      "exploration/Actions Std                                0.694861\n",
      "exploration/Actions Max                                0.999618\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -444.732\n",
      "exploration/env_infos/final/reward_run Mean           -1.05005\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.05005\n",
      "exploration/env_infos/final/reward_run Min            -1.05005\n",
      "exploration/env_infos/initial/reward_run Mean         -0.874346\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.874346\n",
      "exploration/env_infos/initial/reward_run Min          -0.874346\n",
      "exploration/env_infos/reward_run Mean                 -0.208247\n",
      "exploration/env_infos/reward_run Std                   0.647232\n",
      "exploration/env_infos/reward_run Max                   1.80987\n",
      "exploration/env_infos/reward_run Min                  -2.58236\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.387904\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.387904\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.387904\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.260634\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.260634\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.260634\n",
      "exploration/env_infos/reward_ctrl Mean                -0.304287\n",
      "exploration/env_infos/reward_ctrl Std                  0.0951532\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0541222\n",
      "exploration/env_infos/reward_ctrl Min                 -0.530559\n",
      "exploration/env_infos/final/height Mean               -0.357473\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.357473\n",
      "exploration/env_infos/final/height Min                -0.357473\n",
      "exploration/env_infos/initial/height Mean             -0.0587143\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0587143\n",
      "exploration/env_infos/initial/height Min              -0.0587143\n",
      "exploration/env_infos/height Mean                     -0.500545\n",
      "exploration/env_infos/height Std                       0.117145\n",
      "exploration/env_infos/height Max                       0.220334\n",
      "exploration/env_infos/height Min                      -0.592723\n",
      "exploration/env_infos/final/reward_angular Mean       -1.18075\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.18075\n",
      "exploration/env_infos/final/reward_angular Min        -1.18075\n",
      "exploration/env_infos/initial/reward_angular Mean      1.87469\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.87469\n",
      "exploration/env_infos/initial/reward_angular Min       1.87469\n",
      "exploration/env_infos/reward_angular Mean             -0.0660885\n",
      "exploration/env_infos/reward_angular Std               1.43618\n",
      "exploration/env_infos/reward_angular Max               4.28253\n",
      "exploration/env_infos/reward_angular Min              -5.53762\n",
      "evaluation/num steps total                             1.575e+06\n",
      "evaluation/num paths total                          1575\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.116405\n",
      "evaluation/Rewards Std                                 1.17144\n",
      "evaluation/Rewards Max                                 7.21975\n",
      "evaluation/Rewards Min                                -6.19755\n",
      "evaluation/Returns Mean                             -116.405\n",
      "evaluation/Returns Std                               314.497\n",
      "evaluation/Returns Max                              1153.91\n",
      "evaluation/Returns Min                              -539.329\n",
      "evaluation/Actions Mean                                0.0548653\n",
      "evaluation/Actions Std                                 0.752466\n",
      "evaluation/Actions Max                                 0.999997\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -116.405\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0265984\n",
      "evaluation/env_infos/final/reward_run Std              1.23922\n",
      "evaluation/env_infos/final/reward_run Max              2.08638\n",
      "evaluation/env_infos/final/reward_run Min             -3.04645\n",
      "evaluation/env_infos/initial/reward_run Mean           0.377097\n",
      "evaluation/env_infos/initial/reward_run Std            0.652145\n",
      "evaluation/env_infos/initial/reward_run Max            1.13475\n",
      "evaluation/env_infos/initial/reward_run Min           -0.824143\n",
      "evaluation/env_infos/reward_run Mean                  -0.0807151\n",
      "evaluation/env_infos/reward_run Std                    1.16375\n",
      "evaluation/env_infos/reward_run Max                    3.68118\n",
      "evaluation/env_infos/reward_run Min                   -4.28447\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.337667\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.098137\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.139501\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.544515\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.345236\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.159383\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0977567\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.585033\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.341529\n",
      "evaluation/env_infos/reward_ctrl Std                   0.113521\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0282757\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599125\n",
      "evaluation/env_infos/final/height Mean                -0.363165\n",
      "evaluation/env_infos/final/height Std                  0.181657\n",
      "evaluation/env_infos/final/height Max                  0.00720944\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.00921684\n",
      "evaluation/env_infos/initial/height Std                0.048508\n",
      "evaluation/env_infos/initial/height Max                0.0835184\n",
      "evaluation/env_infos/initial/height Min               -0.0707953\n",
      "evaluation/env_infos/height Mean                      -0.362273\n",
      "evaluation/env_infos/height Std                        0.190636\n",
      "evaluation/env_infos/height Max                        0.412854\n",
      "evaluation/env_infos/height Min                       -0.60413\n",
      "evaluation/env_infos/final/reward_angular Mean         0.489256\n",
      "evaluation/env_infos/final/reward_angular Std          1.65953\n",
      "evaluation/env_infos/final/reward_angular Max          4.86985\n",
      "evaluation/env_infos/final/reward_angular Min         -3.68329\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.47125\n",
      "evaluation/env_infos/initial/reward_angular Std        1.17067\n",
      "evaluation/env_infos/initial/reward_angular Max        2.41422\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.46833\n",
      "evaluation/env_infos/reward_angular Mean               0.0406229\n",
      "evaluation/env_infos/reward_angular Std                1.91199\n",
      "evaluation/env_infos/reward_angular Max                8.82227\n",
      "evaluation/env_infos/reward_angular Min               -9.20114\n",
      "time/data storing (s)                                  0.0160043\n",
      "time/evaluation sampling (s)                          21.8718\n",
      "time/exploration sampling (s)                          1.03031\n",
      "time/logging (s)                                       0.234132\n",
      "time/saving (s)                                        0.027543\n",
      "time/training (s)                                      3.85791\n",
      "time/epoch (s)                                        27.0377\n",
      "time/total (s)                                      1878.25\n",
      "Epoch                                                 62\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:43:46.977261 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 63 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 65000\n",
      "trainer/QF1 Loss                                       0.568576\n",
      "trainer/QF2 Loss                                       0.58806\n",
      "trainer/Policy Loss                                    2.50805\n",
      "trainer/Q1 Predictions Mean                            3.62501\n",
      "trainer/Q1 Predictions Std                             7.70155\n",
      "trainer/Q1 Predictions Max                            38.193\n",
      "trainer/Q1 Predictions Min                            -6.13621\n",
      "trainer/Q2 Predictions Mean                            3.5253\n",
      "trainer/Q2 Predictions Std                             7.57074\n",
      "trainer/Q2 Predictions Max                            35.4114\n",
      "trainer/Q2 Predictions Min                            -6.23178\n",
      "trainer/Q Targets Mean                                 3.49116\n",
      "trainer/Q Targets Std                                  7.72626\n",
      "trainer/Q Targets Max                                 39.8811\n",
      "trainer/Q Targets Min                                 -6.45719\n",
      "trainer/Log Pis Mean                                   6.48604\n",
      "trainer/Log Pis Std                                    6.23583\n",
      "trainer/Log Pis Max                                   32.7987\n",
      "trainer/Log Pis Min                                   -6.51905\n",
      "trainer/Policy mu Mean                                 0.190758\n",
      "trainer/Policy mu Std                                  1.68442\n",
      "trainer/Policy mu Max                                  4.61515\n",
      "trainer/Policy mu Min                                 -9.24149\n",
      "trainer/Policy log std Mean                           -0.72372\n",
      "trainer/Policy log std Std                             0.276103\n",
      "trainer/Policy log std Max                             0.430349\n",
      "trainer/Policy log std Min                            -2.28423\n",
      "trainer/Alpha                                          0.0129486\n",
      "trainer/Alpha Loss                                     2.11359\n",
      "exploration/num steps total                        65000\n",
      "exploration/num paths total                           65\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.360491\n",
      "exploration/Rewards Std                                0.354106\n",
      "exploration/Rewards Max                                0.597478\n",
      "exploration/Rewards Min                               -1.51552\n",
      "exploration/Returns Mean                            -360.491\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -360.491\n",
      "exploration/Returns Min                             -360.491\n",
      "exploration/Actions Mean                               0.0963831\n",
      "exploration/Actions Std                                0.759191\n",
      "exploration/Actions Max                                0.999996\n",
      "exploration/Actions Min                               -0.999994\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -360.491\n",
      "exploration/env_infos/final/reward_run Mean           -0.911637\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.911637\n",
      "exploration/env_infos/final/reward_run Min            -0.911637\n",
      "exploration/env_infos/initial/reward_run Mean          0.831353\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.831353\n",
      "exploration/env_infos/initial/reward_run Min           0.831353\n",
      "exploration/env_infos/reward_run Mean                  0.56128\n",
      "exploration/env_infos/reward_run Std                   0.843481\n",
      "exploration/env_infos/reward_run Max                   2.73027\n",
      "exploration/env_infos/reward_run Min                  -1.4813\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.393885\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.393885\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.393885\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.342793\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.342793\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.342793\n",
      "exploration/env_infos/reward_ctrl Mean                -0.351396\n",
      "exploration/env_infos/reward_ctrl Std                  0.109931\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0375172\n",
      "exploration/env_infos/reward_ctrl Min                 -0.594838\n",
      "exploration/env_infos/final/height Mean               -0.575078\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.575078\n",
      "exploration/env_infos/final/height Min                -0.575078\n",
      "exploration/env_infos/initial/height Mean             -0.0441914\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0441914\n",
      "exploration/env_infos/initial/height Min              -0.0441914\n",
      "exploration/env_infos/height Mean                     -0.460881\n",
      "exploration/env_infos/height Std                       0.133106\n",
      "exploration/env_infos/height Max                       0.218359\n",
      "exploration/env_infos/height Min                      -0.589531\n",
      "exploration/env_infos/final/reward_angular Mean       -1.34482\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.34482\n",
      "exploration/env_infos/final/reward_angular Min        -1.34482\n",
      "exploration/env_infos/initial/reward_angular Mean      0.0452971\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.0452971\n",
      "exploration/env_infos/initial/reward_angular Min       0.0452971\n",
      "exploration/env_infos/reward_angular Mean              0.119054\n",
      "exploration/env_infos/reward_angular Std               1.80378\n",
      "exploration/env_infos/reward_angular Max               6.31629\n",
      "exploration/env_infos/reward_angular Min              -5.06352\n",
      "evaluation/num steps total                             1.6e+06\n",
      "evaluation/num paths total                          1600\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.15535\n",
      "evaluation/Rewards Std                                 1.31999\n",
      "evaluation/Rewards Max                                 7.93825\n",
      "evaluation/Rewards Min                                -8.67866\n",
      "evaluation/Returns Mean                             -155.35\n",
      "evaluation/Returns Std                               348.912\n",
      "evaluation/Returns Max                              1247.66\n",
      "evaluation/Returns Min                              -562.019\n",
      "evaluation/Actions Mean                                0.10483\n",
      "evaluation/Actions Std                                 0.741371\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -155.35\n",
      "evaluation/env_infos/final/reward_run Mean             0.0938134\n",
      "evaluation/env_infos/final/reward_run Std              1.19252\n",
      "evaluation/env_infos/final/reward_run Max              2.48929\n",
      "evaluation/env_infos/final/reward_run Min             -3.49215\n",
      "evaluation/env_infos/initial/reward_run Mean           0.272894\n",
      "evaluation/env_infos/initial/reward_run Std            0.666512\n",
      "evaluation/env_infos/initial/reward_run Max            1.10132\n",
      "evaluation/env_infos/initial/reward_run Min           -0.80478\n",
      "evaluation/env_infos/reward_run Mean                  -0.0454946\n",
      "evaluation/env_infos/reward_run Std                    1.10315\n",
      "evaluation/env_infos/reward_run Max                    3.33201\n",
      "evaluation/env_infos/reward_run Min                   -4.14985\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.330881\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.122426\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.122338\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.597249\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.33849\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.157672\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0512123\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.572562\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.336372\n",
      "evaluation/env_infos/reward_ctrl Std                   0.113737\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0129313\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599922\n",
      "evaluation/env_infos/final/height Mean                -0.398773\n",
      "evaluation/env_infos/final/height Std                  0.189451\n",
      "evaluation/env_infos/final/height Max                 -0.0318807\n",
      "evaluation/env_infos/final/height Min                 -0.580146\n",
      "evaluation/env_infos/initial/height Mean              -0.0053447\n",
      "evaluation/env_infos/initial/height Std                0.0478566\n",
      "evaluation/env_infos/initial/height Max                0.0805165\n",
      "evaluation/env_infos/initial/height Min               -0.0721682\n",
      "evaluation/env_infos/height Mean                      -0.367738\n",
      "evaluation/env_infos/height Std                        0.201831\n",
      "evaluation/env_infos/height Max                        0.384532\n",
      "evaluation/env_infos/height Min                       -0.595768\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.486857\n",
      "evaluation/env_infos/final/reward_angular Std          1.90386\n",
      "evaluation/env_infos/final/reward_angular Max          5.05377\n",
      "evaluation/env_infos/final/reward_angular Min         -4.32525\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.560438\n",
      "evaluation/env_infos/initial/reward_angular Std        1.10099\n",
      "evaluation/env_infos/initial/reward_angular Max        2.36511\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.40892\n",
      "evaluation/env_infos/reward_angular Mean              -0.00533665\n",
      "evaluation/env_infos/reward_angular Std                1.95239\n",
      "evaluation/env_infos/reward_angular Max               10.1047\n",
      "evaluation/env_infos/reward_angular Min              -10.387\n",
      "time/data storing (s)                                  0.0146688\n",
      "time/evaluation sampling (s)                          20.5201\n",
      "time/exploration sampling (s)                          0.99772\n",
      "time/logging (s)                                       0.237652\n",
      "time/saving (s)                                        0.0268422\n",
      "time/training (s)                                      3.80374\n",
      "time/epoch (s)                                        25.6008\n",
      "time/total (s)                                      1904.37\n",
      "Epoch                                                 63\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:44:13.534825 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 64 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 66000\n",
      "trainer/QF1 Loss                                       0.74919\n",
      "trainer/QF2 Loss                                       0.72006\n",
      "trainer/Policy Loss                                    2.52883\n",
      "trainer/Q1 Predictions Mean                            3.46535\n",
      "trainer/Q1 Predictions Std                             7.41654\n",
      "trainer/Q1 Predictions Max                            40.2513\n",
      "trainer/Q1 Predictions Min                            -7.27615\n",
      "trainer/Q2 Predictions Mean                            3.34428\n",
      "trainer/Q2 Predictions Std                             7.45322\n",
      "trainer/Q2 Predictions Max                            42.6025\n",
      "trainer/Q2 Predictions Min                            -7.58545\n",
      "trainer/Q Targets Mean                                 3.37731\n",
      "trainer/Q Targets Std                                  7.42187\n",
      "trainer/Q Targets Max                                 41.2014\n",
      "trainer/Q Targets Min                                 -7.39484\n",
      "trainer/Log Pis Mean                                   6.26856\n",
      "trainer/Log Pis Std                                    6.06935\n",
      "trainer/Log Pis Max                                   31.2292\n",
      "trainer/Log Pis Min                                   -5.20407\n",
      "trainer/Policy mu Mean                                 0.0846678\n",
      "trainer/Policy mu Std                                  1.69025\n",
      "trainer/Policy mu Max                                  4.77355\n",
      "trainer/Policy mu Min                                 -7.97988\n",
      "trainer/Policy log std Mean                           -0.733579\n",
      "trainer/Policy log std Std                             0.304467\n",
      "trainer/Policy log std Max                             0.547464\n",
      "trainer/Policy log std Min                            -2.17389\n",
      "trainer/Alpha                                          0.0139364\n",
      "trainer/Alpha Loss                                     1.14804\n",
      "exploration/num steps total                        66000\n",
      "exploration/num paths total                           66\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.490781\n",
      "exploration/Rewards Std                                0.87784\n",
      "exploration/Rewards Max                                2.71652\n",
      "exploration/Rewards Min                               -2.18674\n",
      "exploration/Returns Mean                             490.781\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              490.781\n",
      "exploration/Returns Min                              490.781\n",
      "exploration/Actions Mean                              -0.0063772\n",
      "exploration/Actions Std                                0.820614\n",
      "exploration/Actions Max                                0.999987\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          490.781\n",
      "exploration/env_infos/final/reward_run Mean            1.32112\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.32112\n",
      "exploration/env_infos/final/reward_run Min             1.32112\n",
      "exploration/env_infos/initial/reward_run Mean          0.647907\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.647907\n",
      "exploration/env_infos/initial/reward_run Min           0.647907\n",
      "exploration/env_infos/reward_run Mean                  1.28212\n",
      "exploration/env_infos/reward_run Std                   0.948395\n",
      "exploration/env_infos/reward_run Max                   3.74966\n",
      "exploration/env_infos/reward_run Min                  -1.83539\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.421636\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.421636\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.421636\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.305708\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.305708\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.305708\n",
      "exploration/env_infos/reward_ctrl Mean                -0.404069\n",
      "exploration/env_infos/reward_ctrl Std                  0.108075\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0596697\n",
      "exploration/env_infos/reward_ctrl Min                 -0.594594\n",
      "exploration/env_infos/final/height Mean               -0.184744\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.184744\n",
      "exploration/env_infos/final/height Min                -0.184744\n",
      "exploration/env_infos/initial/height Mean              0.0510677\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0510677\n",
      "exploration/env_infos/initial/height Min               0.0510677\n",
      "exploration/env_infos/height Mean                     -0.114517\n",
      "exploration/env_infos/height Std                       0.128989\n",
      "exploration/env_infos/height Max                       0.298267\n",
      "exploration/env_infos/height Min                      -0.454757\n",
      "exploration/env_infos/final/reward_angular Mean        0.691209\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.691209\n",
      "exploration/env_infos/final/reward_angular Min         0.691209\n",
      "exploration/env_infos/initial/reward_angular Mean      0.309731\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.309731\n",
      "exploration/env_infos/initial/reward_angular Min       0.309731\n",
      "exploration/env_infos/reward_angular Mean             -0.0692356\n",
      "exploration/env_infos/reward_angular Std               2.32711\n",
      "exploration/env_infos/reward_angular Max               6.67338\n",
      "exploration/env_infos/reward_angular Min              -5.89168\n",
      "evaluation/num steps total                             1.625e+06\n",
      "evaluation/num paths total                          1625\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.104728\n",
      "evaluation/Rewards Std                                 1.17872\n",
      "evaluation/Rewards Max                                 7.16634\n",
      "evaluation/Rewards Min                                -6.61299\n",
      "evaluation/Returns Mean                             -104.728\n",
      "evaluation/Returns Std                               347.522\n",
      "evaluation/Returns Max                              1281.21\n",
      "evaluation/Returns Min                              -521.035\n",
      "evaluation/Actions Mean                                0.0196514\n",
      "evaluation/Actions Std                                 0.736727\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          -104.728\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0710066\n",
      "evaluation/env_infos/final/reward_run Std              1.20476\n",
      "evaluation/env_infos/final/reward_run Max              2.76245\n",
      "evaluation/env_infos/final/reward_run Min             -2.84644\n",
      "evaluation/env_infos/initial/reward_run Mean           0.271543\n",
      "evaluation/env_infos/initial/reward_run Std            0.598477\n",
      "evaluation/env_infos/initial/reward_run Max            1.05145\n",
      "evaluation/env_infos/initial/reward_run Min           -0.756366\n",
      "evaluation/env_infos/reward_run Mean                  -0.127996\n",
      "evaluation/env_infos/reward_run Std                    1.08077\n",
      "evaluation/env_infos/reward_run Max                    3.78656\n",
      "evaluation/env_infos/reward_run Min                   -3.9892\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.307191\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.104381\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.133305\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.506333\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.321298\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.142809\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0787236\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.558283\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.325892\n",
      "evaluation/env_infos/reward_ctrl Std                   0.119396\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0127723\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599862\n",
      "evaluation/env_infos/final/height Mean                -0.366497\n",
      "evaluation/env_infos/final/height Std                  0.206375\n",
      "evaluation/env_infos/final/height Max                  0.0752187\n",
      "evaluation/env_infos/final/height Min                 -0.57729\n",
      "evaluation/env_infos/initial/height Mean               0.00523477\n",
      "evaluation/env_infos/initial/height Std                0.047278\n",
      "evaluation/env_infos/initial/height Max                0.0834971\n",
      "evaluation/env_infos/initial/height Min               -0.0747385\n",
      "evaluation/env_infos/height Mean                      -0.354203\n",
      "evaluation/env_infos/height Std                        0.217648\n",
      "evaluation/env_infos/height Max                        0.467168\n",
      "evaluation/env_infos/height Min                       -0.600444\n",
      "evaluation/env_infos/final/reward_angular Mean         0.389975\n",
      "evaluation/env_infos/final/reward_angular Std          1.8214\n",
      "evaluation/env_infos/final/reward_angular Max          3.90738\n",
      "evaluation/env_infos/final/reward_angular Min         -3.37723\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.535925\n",
      "evaluation/env_infos/initial/reward_angular Std        1.12105\n",
      "evaluation/env_infos/initial/reward_angular Max        3.10459\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.695054\n",
      "evaluation/env_infos/reward_angular Mean               0.0326153\n",
      "evaluation/env_infos/reward_angular Std                1.81067\n",
      "evaluation/env_infos/reward_angular Max                8.13683\n",
      "evaluation/env_infos/reward_angular Min               -7.75517\n",
      "time/data storing (s)                                  0.0145147\n",
      "time/evaluation sampling (s)                          20.929\n",
      "time/exploration sampling (s)                          0.982397\n",
      "time/logging (s)                                       0.237401\n",
      "time/saving (s)                                        0.0279631\n",
      "time/training (s)                                      3.82492\n",
      "time/epoch (s)                                        26.0162\n",
      "time/total (s)                                      1930.93\n",
      "Epoch                                                 64\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:44:39.842708 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 65 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 67000\n",
      "trainer/QF1 Loss                                       0.737363\n",
      "trainer/QF2 Loss                                       0.7548\n",
      "trainer/Policy Loss                                    2.21601\n",
      "trainer/Q1 Predictions Mean                            3.37471\n",
      "trainer/Q1 Predictions Std                             8.44025\n",
      "trainer/Q1 Predictions Max                            45.0547\n",
      "trainer/Q1 Predictions Min                            -5.97119\n",
      "trainer/Q2 Predictions Mean                            3.35925\n",
      "trainer/Q2 Predictions Std                             8.35284\n",
      "trainer/Q2 Predictions Max                            44.8928\n",
      "trainer/Q2 Predictions Min                            -6.39633\n",
      "trainer/Q Targets Mean                                 3.46966\n",
      "trainer/Q Targets Std                                  8.42995\n",
      "trainer/Q Targets Max                                 46.9498\n",
      "trainer/Q Targets Min                                 -6.34343\n",
      "trainer/Log Pis Mean                                   5.8942\n",
      "trainer/Log Pis Std                                    6.50136\n",
      "trainer/Log Pis Max                                   31.3706\n",
      "trainer/Log Pis Min                                   -7.01809\n",
      "trainer/Policy mu Mean                                 0.23952\n",
      "trainer/Policy mu Std                                  1.64693\n",
      "trainer/Policy mu Max                                  5.53682\n",
      "trainer/Policy mu Min                                 -7.50894\n",
      "trainer/Policy log std Mean                           -0.700509\n",
      "trainer/Policy log std Std                             0.30221\n",
      "trainer/Policy log std Max                             0.422662\n",
      "trainer/Policy log std Min                            -2.05454\n",
      "trainer/Alpha                                          0.0150393\n",
      "trainer/Alpha Loss                                    -0.444104\n",
      "exploration/num steps total                        67000\n",
      "exploration/num paths total                           67\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.263002\n",
      "exploration/Rewards Std                                0.883341\n",
      "exploration/Rewards Max                                4.019\n",
      "exploration/Rewards Min                               -4.42718\n",
      "exploration/Returns Mean                            -263.002\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -263.002\n",
      "exploration/Returns Min                             -263.002\n",
      "exploration/Actions Mean                               0.0733064\n",
      "exploration/Actions Std                                0.710676\n",
      "exploration/Actions Max                                0.99866\n",
      "exploration/Actions Min                               -0.999885\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -263.002\n",
      "exploration/env_infos/final/reward_run Mean            0.337982\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.337982\n",
      "exploration/env_infos/final/reward_run Min             0.337982\n",
      "exploration/env_infos/initial/reward_run Mean         -0.363259\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.363259\n",
      "exploration/env_infos/initial/reward_run Min          -0.363259\n",
      "exploration/env_infos/reward_run Mean                 -0.0800615\n",
      "exploration/env_infos/reward_run Std                   0.474259\n",
      "exploration/env_infos/reward_run Max                   1.4118\n",
      "exploration/env_infos/reward_run Min                  -2.60471\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.261357\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.261357\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.261357\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.215087\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.215087\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.215087\n",
      "exploration/env_infos/reward_ctrl Mean                -0.306261\n",
      "exploration/env_infos/reward_ctrl Std                  0.0732727\n",
      "exploration/env_infos/reward_ctrl Max                 -0.047576\n",
      "exploration/env_infos/reward_ctrl Min                 -0.537399\n",
      "exploration/env_infos/final/height Mean               -0.504864\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.504864\n",
      "exploration/env_infos/final/height Min                -0.504864\n",
      "exploration/env_infos/initial/height Mean              0.018565\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.018565\n",
      "exploration/env_infos/initial/height Min               0.018565\n",
      "exploration/env_infos/height Mean                     -0.518841\n",
      "exploration/env_infos/height Std                       0.115349\n",
      "exploration/env_infos/height Max                       0.108398\n",
      "exploration/env_infos/height Min                      -0.580274\n",
      "exploration/env_infos/final/reward_angular Mean        2.45281\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         2.45281\n",
      "exploration/env_infos/final/reward_angular Min         2.45281\n",
      "exploration/env_infos/initial/reward_angular Mean      0.245828\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.245828\n",
      "exploration/env_infos/initial/reward_angular Min       0.245828\n",
      "exploration/env_infos/reward_angular Mean             -0.0751861\n",
      "exploration/env_infos/reward_angular Std               1.27757\n",
      "exploration/env_infos/reward_angular Max               6.02487\n",
      "exploration/env_infos/reward_angular Min              -6.10568\n",
      "evaluation/num steps total                             1.65e+06\n",
      "evaluation/num paths total                          1650\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0477342\n",
      "evaluation/Rewards Std                                 1.37192\n",
      "evaluation/Rewards Max                                 8.22011\n",
      "evaluation/Rewards Min                                -6.46422\n",
      "evaluation/Returns Mean                              -47.7342\n",
      "evaluation/Returns Std                               359.052\n",
      "evaluation/Returns Max                              1102.59\n",
      "evaluation/Returns Min                              -506.188\n",
      "evaluation/Actions Mean                                0.152519\n",
      "evaluation/Actions Std                                 0.732316\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -47.7342\n",
      "evaluation/env_infos/final/reward_run Mean            -0.384096\n",
      "evaluation/env_infos/final/reward_run Std              1.03959\n",
      "evaluation/env_infos/final/reward_run Max              0.938717\n",
      "evaluation/env_infos/final/reward_run Min             -2.97217\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0740522\n",
      "evaluation/env_infos/initial/reward_run Std            0.545142\n",
      "evaluation/env_infos/initial/reward_run Max            0.909356\n",
      "evaluation/env_infos/initial/reward_run Min           -0.919019\n",
      "evaluation/env_infos/reward_run Mean                  -0.291135\n",
      "evaluation/env_infos/reward_run Std                    1.22816\n",
      "evaluation/env_infos/reward_run Max                    3.42604\n",
      "evaluation/env_infos/reward_run Min                   -4.54819\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.339856\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0897167\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.17115\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.581049\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.293882\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.138262\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0832438\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.508326\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.335729\n",
      "evaluation/env_infos/reward_ctrl Std                   0.104352\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0154638\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599723\n",
      "evaluation/env_infos/final/height Mean                -0.367549\n",
      "evaluation/env_infos/final/height Std                  0.17292\n",
      "evaluation/env_infos/final/height Max                 -0.0465664\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean              -0.00356245\n",
      "evaluation/env_infos/initial/height Std                0.0557942\n",
      "evaluation/env_infos/initial/height Max                0.099911\n",
      "evaluation/env_infos/initial/height Min               -0.0991407\n",
      "evaluation/env_infos/height Mean                      -0.32467\n",
      "evaluation/env_infos/height Std                        0.191157\n",
      "evaluation/env_infos/height Max                        0.283175\n",
      "evaluation/env_infos/height Min                       -0.601836\n",
      "evaluation/env_infos/final/reward_angular Mean         0.142716\n",
      "evaluation/env_infos/final/reward_angular Std          1.8467\n",
      "evaluation/env_infos/final/reward_angular Max          3.33183\n",
      "evaluation/env_infos/final/reward_angular Min         -5.46498\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.340529\n",
      "evaluation/env_infos/initial/reward_angular Std        1.01604\n",
      "evaluation/env_infos/initial/reward_angular Max        2.35831\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.34533\n",
      "evaluation/env_infos/reward_angular Mean               0.0213927\n",
      "evaluation/env_infos/reward_angular Std                2.05745\n",
      "evaluation/env_infos/reward_angular Max                9.34206\n",
      "evaluation/env_infos/reward_angular Min               -7.91039\n",
      "time/data storing (s)                                  0.0151477\n",
      "time/evaluation sampling (s)                          20.5136\n",
      "time/exploration sampling (s)                          0.978661\n",
      "time/logging (s)                                       0.235623\n",
      "time/saving (s)                                        0.0280569\n",
      "time/training (s)                                      3.99454\n",
      "time/epoch (s)                                        25.7656\n",
      "time/total (s)                                      1957.23\n",
      "Epoch                                                 65\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:45:06.076065 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 66 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 68000\n",
      "trainer/QF1 Loss                                       0.565181\n",
      "trainer/QF2 Loss                                       0.569692\n",
      "trainer/Policy Loss                                    2.1098\n",
      "trainer/Q1 Predictions Mean                            3.20622\n",
      "trainer/Q1 Predictions Std                             7.63391\n",
      "trainer/Q1 Predictions Max                            41.3952\n",
      "trainer/Q1 Predictions Min                            -7.57989\n",
      "trainer/Q2 Predictions Mean                            3.31405\n",
      "trainer/Q2 Predictions Std                             7.77499\n",
      "trainer/Q2 Predictions Max                            42.1449\n",
      "trainer/Q2 Predictions Min                            -6.9998\n",
      "trainer/Q Targets Mean                                 3.25187\n",
      "trainer/Q Targets Std                                  7.78055\n",
      "trainer/Q Targets Max                                 40.5688\n",
      "trainer/Q Targets Min                                 -7.99128\n",
      "trainer/Log Pis Mean                                   5.65956\n",
      "trainer/Log Pis Std                                    4.89483\n",
      "trainer/Log Pis Max                                   24.2004\n",
      "trainer/Log Pis Min                                   -4.53367\n",
      "trainer/Policy mu Mean                                 0.114314\n",
      "trainer/Policy mu Std                                  1.55472\n",
      "trainer/Policy mu Max                                  3.97471\n",
      "trainer/Policy mu Min                                 -5.44182\n",
      "trainer/Policy log std Mean                           -0.703223\n",
      "trainer/Policy log std Std                             0.272026\n",
      "trainer/Policy log std Max                             0.460708\n",
      "trainer/Policy log std Min                            -2.29013\n",
      "trainer/Alpha                                          0.014837\n",
      "trainer/Alpha Loss                                    -1.4337\n",
      "exploration/num steps total                        68000\n",
      "exploration/num paths total                           68\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.248451\n",
      "exploration/Rewards Std                                1.34901\n",
      "exploration/Rewards Max                                3.25713\n",
      "exploration/Rewards Min                               -5.68862\n",
      "exploration/Returns Mean                            -248.451\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -248.451\n",
      "exploration/Returns Min                             -248.451\n",
      "exploration/Actions Mean                              -0.0375038\n",
      "exploration/Actions Std                                0.678454\n",
      "exploration/Actions Max                                0.999939\n",
      "exploration/Actions Min                               -0.999988\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -248.451\n",
      "exploration/env_infos/final/reward_run Mean            0.443991\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.443991\n",
      "exploration/env_infos/final/reward_run Min             0.443991\n",
      "exploration/env_infos/initial/reward_run Mean         -0.627537\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.627537\n",
      "exploration/env_infos/initial/reward_run Min          -0.627537\n",
      "exploration/env_infos/reward_run Mean                 -0.482527\n",
      "exploration/env_infos/reward_run Std                   1.04692\n",
      "exploration/env_infos/reward_run Max                   2.3015\n",
      "exploration/env_infos/reward_run Min                  -3.71865\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.333085\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.333085\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.333085\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.329456\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.329456\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.329456\n",
      "exploration/env_infos/reward_ctrl Mean                -0.277024\n",
      "exploration/env_infos/reward_ctrl Std                  0.0958927\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0650981\n",
      "exploration/env_infos/reward_ctrl Min                 -0.564384\n",
      "exploration/env_infos/final/height Mean               -0.0232949\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0232949\n",
      "exploration/env_infos/final/height Min                -0.0232949\n",
      "exploration/env_infos/initial/height Mean              0.0666094\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0666094\n",
      "exploration/env_infos/initial/height Min               0.0666094\n",
      "exploration/env_infos/height Mean                     -0.029764\n",
      "exploration/env_infos/height Std                       0.0918994\n",
      "exploration/env_infos/height Max                       0.449725\n",
      "exploration/env_infos/height Min                      -0.221287\n",
      "exploration/env_infos/final/reward_angular Mean       -1.51874\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.51874\n",
      "exploration/env_infos/final/reward_angular Min        -1.51874\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.16858\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.16858\n",
      "exploration/env_infos/initial/reward_angular Min      -0.16858\n",
      "exploration/env_infos/reward_angular Mean              0.0205901\n",
      "exploration/env_infos/reward_angular Std               1.79791\n",
      "exploration/env_infos/reward_angular Max               7.27786\n",
      "exploration/env_infos/reward_angular Min              -4.72689\n",
      "evaluation/num steps total                             1.675e+06\n",
      "evaluation/num paths total                          1675\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.190889\n",
      "evaluation/Rewards Std                                 1.25599\n",
      "evaluation/Rewards Max                                 7.03381\n",
      "evaluation/Rewards Min                                -6.17278\n",
      "evaluation/Returns Mean                              190.889\n",
      "evaluation/Returns Std                               556.223\n",
      "evaluation/Returns Max                              1371.3\n",
      "evaluation/Returns Min                              -385.152\n",
      "evaluation/Actions Mean                                0.0707366\n",
      "evaluation/Actions Std                                 0.748054\n",
      "evaluation/Actions Max                                 0.999987\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           190.889\n",
      "evaluation/env_infos/final/reward_run Mean            -0.402144\n",
      "evaluation/env_infos/final/reward_run Std              1.57834\n",
      "evaluation/env_infos/final/reward_run Max              2.47166\n",
      "evaluation/env_infos/final/reward_run Min             -3.45032\n",
      "evaluation/env_infos/initial/reward_run Mean           0.0796428\n",
      "evaluation/env_infos/initial/reward_run Std            0.55513\n",
      "evaluation/env_infos/initial/reward_run Max            1.16252\n",
      "evaluation/env_infos/initial/reward_run Min           -0.887041\n",
      "evaluation/env_infos/reward_run Mean                  -0.479601\n",
      "evaluation/env_infos/reward_run Std                    1.50169\n",
      "evaluation/env_infos/reward_run Max                    3.64141\n",
      "evaluation/env_infos/reward_run Min                   -4.62569\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.347955\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.110289\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0597251\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.516123\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.290191\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.102831\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.094497\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.462053\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.338753\n",
      "evaluation/env_infos/reward_ctrl Std                   0.101792\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0178967\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.588961\n",
      "evaluation/env_infos/final/height Mean                -0.298082\n",
      "evaluation/env_infos/final/height Std                  0.17319\n",
      "evaluation/env_infos/final/height Max                  0.0136222\n",
      "evaluation/env_infos/final/height Min                 -0.576991\n",
      "evaluation/env_infos/initial/height Mean              -0.000954834\n",
      "evaluation/env_infos/initial/height Std                0.0510102\n",
      "evaluation/env_infos/initial/height Max                0.0778209\n",
      "evaluation/env_infos/initial/height Min               -0.0894518\n",
      "evaluation/env_infos/height Mean                      -0.270588\n",
      "evaluation/env_infos/height Std                        0.187869\n",
      "evaluation/env_infos/height Max                        0.27896\n",
      "evaluation/env_infos/height Min                       -0.597435\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.186055\n",
      "evaluation/env_infos/final/reward_angular Std          1.34571\n",
      "evaluation/env_infos/final/reward_angular Max          2.30301\n",
      "evaluation/env_infos/final/reward_angular Min         -3.03491\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.169744\n",
      "evaluation/env_infos/initial/reward_angular Std        0.957621\n",
      "evaluation/env_infos/initial/reward_angular Max        2.79981\n",
      "evaluation/env_infos/initial/reward_angular Min       -0.932428\n",
      "evaluation/env_infos/reward_angular Mean               0.0383235\n",
      "evaluation/env_infos/reward_angular Std                1.76688\n",
      "evaluation/env_infos/reward_angular Max                8.26062\n",
      "evaluation/env_infos/reward_angular Min               -7.19619\n",
      "time/data storing (s)                                  0.015425\n",
      "time/evaluation sampling (s)                          20.598\n",
      "time/exploration sampling (s)                          0.975596\n",
      "time/logging (s)                                       0.236365\n",
      "time/saving (s)                                        0.0277367\n",
      "time/training (s)                                      3.8324\n",
      "time/epoch (s)                                        25.6855\n",
      "time/total (s)                                      1983.47\n",
      "Epoch                                                 66\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:45:32.731722 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 67 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 69000\n",
      "trainer/QF1 Loss                                       0.610545\n",
      "trainer/QF2 Loss                                       0.617896\n",
      "trainer/Policy Loss                                    1.65703\n",
      "trainer/Q1 Predictions Mean                            3.44433\n",
      "trainer/Q1 Predictions Std                             8.09872\n",
      "trainer/Q1 Predictions Max                            45.8324\n",
      "trainer/Q1 Predictions Min                            -7.32282\n",
      "trainer/Q2 Predictions Mean                            3.40324\n",
      "trainer/Q2 Predictions Std                             8.06744\n",
      "trainer/Q2 Predictions Max                            44.4043\n",
      "trainer/Q2 Predictions Min                            -7.31938\n",
      "trainer/Q Targets Mean                                 3.42932\n",
      "trainer/Q Targets Std                                  8.17784\n",
      "trainer/Q Targets Max                                 45.0984\n",
      "trainer/Q Targets Min                                 -8.42097\n",
      "trainer/Log Pis Mean                                   5.27651\n",
      "trainer/Log Pis Std                                    5.85533\n",
      "trainer/Log Pis Max                                   27.6221\n",
      "trainer/Log Pis Min                                   -6.47545\n",
      "trainer/Policy mu Mean                                -0.112375\n",
      "trainer/Policy mu Std                                  1.62938\n",
      "trainer/Policy mu Max                                  3.97814\n",
      "trainer/Policy mu Min                                 -8.92097\n",
      "trainer/Policy log std Mean                           -0.700816\n",
      "trainer/Policy log std Std                             0.298531\n",
      "trainer/Policy log std Max                             0.643411\n",
      "trainer/Policy log std Min                            -1.77386\n",
      "trainer/Alpha                                          0.0138073\n",
      "trainer/Alpha Loss                                    -3.09779\n",
      "exploration/num steps total                        69000\n",
      "exploration/num paths total                           69\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.304388\n",
      "exploration/Rewards Std                                0.352574\n",
      "exploration/Rewards Max                                1.06795\n",
      "exploration/Rewards Min                               -2.13273\n",
      "exploration/Returns Mean                            -304.388\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -304.388\n",
      "exploration/Returns Min                             -304.388\n",
      "exploration/Actions Mean                              -0.347804\n",
      "exploration/Actions Std                                0.567576\n",
      "exploration/Actions Max                                0.995388\n",
      "exploration/Actions Min                               -0.999741\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -304.388\n",
      "exploration/env_infos/final/reward_run Mean           -0.655423\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.655423\n",
      "exploration/env_infos/final/reward_run Min            -0.655423\n",
      "exploration/env_infos/initial/reward_run Mean         -0.893498\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.893498\n",
      "exploration/env_infos/initial/reward_run Min          -0.893498\n",
      "exploration/env_infos/reward_run Mean                  0.143634\n",
      "exploration/env_infos/reward_run Std                   0.578429\n",
      "exploration/env_infos/reward_run Max                   3.03791\n",
      "exploration/env_infos/reward_run Min                  -1.59784\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.240566\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.240566\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.240566\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.34661\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.34661\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.34661\n",
      "exploration/env_infos/reward_ctrl Mean                -0.265866\n",
      "exploration/env_infos/reward_ctrl Std                  0.0822574\n",
      "exploration/env_infos/reward_ctrl Max                 -0.020241\n",
      "exploration/env_infos/reward_ctrl Min                 -0.546941\n",
      "exploration/env_infos/final/height Mean               -0.566556\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.566556\n",
      "exploration/env_infos/final/height Min                -0.566556\n",
      "exploration/env_infos/initial/height Mean             -0.0439359\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0439359\n",
      "exploration/env_infos/initial/height Min              -0.0439359\n",
      "exploration/env_infos/height Mean                     -0.341848\n",
      "exploration/env_infos/height Std                       0.275481\n",
      "exploration/env_infos/height Max                       0.341228\n",
      "exploration/env_infos/height Min                      -0.580268\n",
      "exploration/env_infos/final/reward_angular Mean       -0.0377775\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.0377775\n",
      "exploration/env_infos/final/reward_angular Min        -0.0377775\n",
      "exploration/env_infos/initial/reward_angular Mean      2.22922\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.22922\n",
      "exploration/env_infos/initial/reward_angular Min       2.22922\n",
      "exploration/env_infos/reward_angular Mean              0.0730799\n",
      "exploration/env_infos/reward_angular Std               1.14936\n",
      "exploration/env_infos/reward_angular Max               5.2318\n",
      "exploration/env_infos/reward_angular Min              -4.86854\n",
      "evaluation/num steps total                             1.7e+06\n",
      "evaluation/num paths total                          1700\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.00558837\n",
      "evaluation/Rewards Std                                 1.21564\n",
      "evaluation/Rewards Max                                 7.0744\n",
      "evaluation/Rewards Min                                -6.4198\n",
      "evaluation/Returns Mean                                5.58837\n",
      "evaluation/Returns Std                               362.334\n",
      "evaluation/Returns Max                              1380.59\n",
      "evaluation/Returns Min                              -421.15\n",
      "evaluation/Actions Mean                               -0.0330178\n",
      "evaluation/Actions Std                                 0.728042\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                             5.58837\n",
      "evaluation/env_infos/final/reward_run Mean            -0.36607\n",
      "evaluation/env_infos/final/reward_run Std              0.988022\n",
      "evaluation/env_infos/final/reward_run Max              1.92886\n",
      "evaluation/env_infos/final/reward_run Min             -2.03122\n",
      "evaluation/env_infos/initial/reward_run Mean           0.153803\n",
      "evaluation/env_infos/initial/reward_run Std            0.555061\n",
      "evaluation/env_infos/initial/reward_run Max            1.17618\n",
      "evaluation/env_infos/initial/reward_run Min           -0.783789\n",
      "evaluation/env_infos/reward_run Mean                  -0.140967\n",
      "evaluation/env_infos/reward_run Std                    1.15032\n",
      "evaluation/env_infos/reward_run Max                    4.29126\n",
      "evaluation/env_infos/reward_run Min                   -3.18123\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.33227\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0912841\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.178535\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.545554\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.252267\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.135775\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0544018\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.463367\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.318681\n",
      "evaluation/env_infos/reward_ctrl Std                   0.108066\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00928177\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.594655\n",
      "evaluation/env_infos/final/height Mean                -0.234077\n",
      "evaluation/env_infos/final/height Std                  0.170897\n",
      "evaluation/env_infos/final/height Max                  0.0484825\n",
      "evaluation/env_infos/final/height Min                 -0.554772\n",
      "evaluation/env_infos/initial/height Mean               0.000636345\n",
      "evaluation/env_infos/initial/height Std                0.0570578\n",
      "evaluation/env_infos/initial/height Max                0.0919351\n",
      "evaluation/env_infos/initial/height Min               -0.0938835\n",
      "evaluation/env_infos/height Mean                      -0.24789\n",
      "evaluation/env_infos/height Std                        0.168081\n",
      "evaluation/env_infos/height Max                        0.33821\n",
      "evaluation/env_infos/height Min                       -0.595998\n",
      "evaluation/env_infos/final/reward_angular Mean         0.157295\n",
      "evaluation/env_infos/final/reward_angular Std          1.72759\n",
      "evaluation/env_infos/final/reward_angular Max          2.98784\n",
      "evaluation/env_infos/final/reward_angular Min         -4.39567\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.291323\n",
      "evaluation/env_infos/initial/reward_angular Std        0.974069\n",
      "evaluation/env_infos/initial/reward_angular Max        2.6149\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.11834\n",
      "evaluation/env_infos/reward_angular Mean               0.0503779\n",
      "evaluation/env_infos/reward_angular Std                1.81625\n",
      "evaluation/env_infos/reward_angular Max                8.7512\n",
      "evaluation/env_infos/reward_angular Min               -8.25769\n",
      "time/data storing (s)                                  0.0151466\n",
      "time/evaluation sampling (s)                          20.8206\n",
      "time/exploration sampling (s)                          1.00027\n",
      "time/logging (s)                                       0.235967\n",
      "time/saving (s)                                        0.0268676\n",
      "time/training (s)                                      3.94966\n",
      "time/epoch (s)                                        26.0485\n",
      "time/total (s)                                      2010.12\n",
      "Epoch                                                 67\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:45:58.964547 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 68 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 70000\n",
      "trainer/QF1 Loss                                       0.624905\n",
      "trainer/QF2 Loss                                       0.635944\n",
      "trainer/Policy Loss                                    1.59109\n",
      "trainer/Q1 Predictions Mean                            3.54227\n",
      "trainer/Q1 Predictions Std                             8.4401\n",
      "trainer/Q1 Predictions Max                            47.4089\n",
      "trainer/Q1 Predictions Min                            -7.22264\n",
      "trainer/Q2 Predictions Mean                            3.58993\n",
      "trainer/Q2 Predictions Std                             8.60037\n",
      "trainer/Q2 Predictions Max                            49.043\n",
      "trainer/Q2 Predictions Min                            -7.39722\n",
      "trainer/Q Targets Mean                                 3.45715\n",
      "trainer/Q Targets Std                                  8.53443\n",
      "trainer/Q Targets Max                                 48.7744\n",
      "trainer/Q Targets Min                                 -7.1384\n",
      "trainer/Log Pis Mean                                   5.32182\n",
      "trainer/Log Pis Std                                    5.0276\n",
      "trainer/Log Pis Max                                   24.9485\n",
      "trainer/Log Pis Min                                   -4.42064\n",
      "trainer/Policy mu Mean                                 0.200337\n",
      "trainer/Policy mu Std                                  1.51304\n",
      "trainer/Policy mu Max                                  5.01221\n",
      "trainer/Policy mu Min                                 -5.91939\n",
      "trainer/Policy log std Mean                           -0.741788\n",
      "trainer/Policy log std Std                             0.29496\n",
      "trainer/Policy log std Max                             0.373442\n",
      "trainer/Policy log std Min                            -2.27296\n",
      "trainer/Alpha                                          0.0133689\n",
      "trainer/Alpha Loss                                    -2.9251\n",
      "exploration/num steps total                        70000\n",
      "exploration/num paths total                           70\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.194672\n",
      "exploration/Rewards Std                                0.804647\n",
      "exploration/Rewards Max                                2.83711\n",
      "exploration/Rewards Min                               -1.95462\n",
      "exploration/Returns Mean                             194.672\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              194.672\n",
      "exploration/Returns Min                              194.672\n",
      "exploration/Actions Mean                               0.371534\n",
      "exploration/Actions Std                                0.580624\n",
      "exploration/Actions Max                                0.999983\n",
      "exploration/Actions Min                               -0.998964\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          194.672\n",
      "exploration/env_infos/final/reward_run Mean            0.346925\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.346925\n",
      "exploration/env_infos/final/reward_run Min             0.346925\n",
      "exploration/env_infos/initial/reward_run Mean         -0.572555\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.572555\n",
      "exploration/env_infos/initial/reward_run Min          -0.572555\n",
      "exploration/env_infos/reward_run Mean                 -0.461688\n",
      "exploration/env_infos/reward_run Std                   0.807569\n",
      "exploration/env_infos/reward_run Max                   1.09931\n",
      "exploration/env_infos/reward_run Min                  -3.06905\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.408853\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.408853\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.408853\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.319661\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.319661\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.319661\n",
      "exploration/env_infos/reward_ctrl Mean                -0.285097\n",
      "exploration/env_infos/reward_ctrl Std                  0.0902494\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0407387\n",
      "exploration/env_infos/reward_ctrl Min                 -0.554902\n",
      "exploration/env_infos/final/height Mean               -0.55769\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.55769\n",
      "exploration/env_infos/final/height Min                -0.55769\n",
      "exploration/env_infos/initial/height Mean             -0.00701422\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.00701422\n",
      "exploration/env_infos/initial/height Min              -0.00701422\n",
      "exploration/env_infos/height Mean                     -0.389598\n",
      "exploration/env_infos/height Std                       0.246513\n",
      "exploration/env_infos/height Max                       0.383593\n",
      "exploration/env_infos/height Min                      -0.584382\n",
      "exploration/env_infos/final/reward_angular Mean       -0.746019\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.746019\n",
      "exploration/env_infos/final/reward_angular Min        -0.746019\n",
      "exploration/env_infos/initial/reward_angular Mean      2.37861\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.37861\n",
      "exploration/env_infos/initial/reward_angular Min       2.37861\n",
      "exploration/env_infos/reward_angular Mean             -0.0354445\n",
      "exploration/env_infos/reward_angular Std               1.27522\n",
      "exploration/env_infos/reward_angular Max               5.79769\n",
      "exploration/env_infos/reward_angular Min              -6.31197\n",
      "evaluation/num steps total                             1.725e+06\n",
      "evaluation/num paths total                          1725\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0509919\n",
      "evaluation/Rewards Std                                 1.24477\n",
      "evaluation/Rewards Max                                 7.06799\n",
      "evaluation/Rewards Min                                -6.30299\n",
      "evaluation/Returns Mean                              -50.9919\n",
      "evaluation/Returns Std                               353.411\n",
      "evaluation/Returns Max                              1317.23\n",
      "evaluation/Returns Min                              -448.316\n",
      "evaluation/Actions Mean                                0.108618\n",
      "evaluation/Actions Std                                 0.71497\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999982\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -50.9919\n",
      "evaluation/env_infos/final/reward_run Mean             0.214\n",
      "evaluation/env_infos/final/reward_run Std              1.07841\n",
      "evaluation/env_infos/final/reward_run Max              2.57815\n",
      "evaluation/env_infos/final/reward_run Min             -1.7664\n",
      "evaluation/env_infos/initial/reward_run Mean           0.225918\n",
      "evaluation/env_infos/initial/reward_run Std            0.539542\n",
      "evaluation/env_infos/initial/reward_run Max            1.12276\n",
      "evaluation/env_infos/initial/reward_run Min           -0.72464\n",
      "evaluation/env_infos/reward_run Mean                   0.0747409\n",
      "evaluation/env_infos/reward_run Std                    1.09052\n",
      "evaluation/env_infos/reward_run Max                    4.11673\n",
      "evaluation/env_infos/reward_run Min                   -5.19739\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.341725\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.102369\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.153507\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.519864\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.283704\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.145794\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.104066\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.542835\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.313788\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0989974\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0124754\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.591458\n",
      "evaluation/env_infos/final/height Mean                -0.394913\n",
      "evaluation/env_infos/final/height Std                  0.163165\n",
      "evaluation/env_infos/final/height Max                 -0.0182643\n",
      "evaluation/env_infos/final/height Min                 -0.578433\n",
      "evaluation/env_infos/initial/height Mean              -0.00690996\n",
      "evaluation/env_infos/initial/height Std                0.0507998\n",
      "evaluation/env_infos/initial/height Max                0.0852642\n",
      "evaluation/env_infos/initial/height Min               -0.091498\n",
      "evaluation/env_infos/height Mean                      -0.372659\n",
      "evaluation/env_infos/height Std                        0.184367\n",
      "evaluation/env_infos/height Max                        0.292724\n",
      "evaluation/env_infos/height Min                       -0.597207\n",
      "evaluation/env_infos/final/reward_angular Mean         0.292331\n",
      "evaluation/env_infos/final/reward_angular Std          1.65425\n",
      "evaluation/env_infos/final/reward_angular Max          6.92466\n",
      "evaluation/env_infos/final/reward_angular Min         -2.80156\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.228095\n",
      "evaluation/env_infos/initial/reward_angular Std        1.21061\n",
      "evaluation/env_infos/initial/reward_angular Max        2.2913\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.60441\n",
      "evaluation/env_infos/reward_angular Mean               0.0157351\n",
      "evaluation/env_infos/reward_angular Std                1.81785\n",
      "evaluation/env_infos/reward_angular Max                9.1465\n",
      "evaluation/env_infos/reward_angular Min               -7.2125\n",
      "time/data storing (s)                                  0.0150398\n",
      "time/evaluation sampling (s)                          20.4924\n",
      "time/exploration sampling (s)                          0.978917\n",
      "time/logging (s)                                       0.238392\n",
      "time/saving (s)                                        0.0280857\n",
      "time/training (s)                                      3.92558\n",
      "time/epoch (s)                                        25.6784\n",
      "time/total (s)                                      2036.35\n",
      "Epoch                                                 68\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:46:25.291455 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 69 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 71000\n",
      "trainer/QF1 Loss                                       0.707868\n",
      "trainer/QF2 Loss                                       0.738678\n",
      "trainer/Policy Loss                                    2.99327\n",
      "trainer/Q1 Predictions Mean                            3.10001\n",
      "trainer/Q1 Predictions Std                             7.74732\n",
      "trainer/Q1 Predictions Max                            38.0497\n",
      "trainer/Q1 Predictions Min                            -7.9108\n",
      "trainer/Q2 Predictions Mean                            3.04919\n",
      "trainer/Q2 Predictions Std                             7.73956\n",
      "trainer/Q2 Predictions Max                            37.0929\n",
      "trainer/Q2 Predictions Min                            -7.50968\n",
      "trainer/Q Targets Mean                                 3.26298\n",
      "trainer/Q Targets Std                                  7.91794\n",
      "trainer/Q Targets Max                                 38.8031\n",
      "trainer/Q Targets Min                                 -7.42853\n",
      "trainer/Log Pis Mean                                   6.29987\n",
      "trainer/Log Pis Std                                    5.55519\n",
      "trainer/Log Pis Max                                   26.0114\n",
      "trainer/Log Pis Min                                   -3.93648\n",
      "trainer/Policy mu Mean                                 0.134484\n",
      "trainer/Policy mu Std                                  1.59497\n",
      "trainer/Policy mu Max                                  4.78419\n",
      "trainer/Policy mu Min                                 -5.97283\n",
      "trainer/Policy log std Mean                           -0.773659\n",
      "trainer/Policy log std Std                             0.2897\n",
      "trainer/Policy log std Max                             0.240094\n",
      "trainer/Policy log std Min                            -2.24736\n",
      "trainer/Alpha                                          0.0125996\n",
      "trainer/Alpha Loss                                     1.3121\n",
      "exploration/num steps total                        71000\n",
      "exploration/num paths total                           71\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.255625\n",
      "exploration/Rewards Std                                1.61414\n",
      "exploration/Rewards Max                                5.8298\n",
      "exploration/Rewards Min                               -3.77125\n",
      "exploration/Returns Mean                            -255.625\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -255.625\n",
      "exploration/Returns Min                             -255.625\n",
      "exploration/Actions Mean                               0.0591276\n",
      "exploration/Actions Std                                0.788057\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -255.625\n",
      "exploration/env_infos/final/reward_run Mean            0.250751\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.250751\n",
      "exploration/env_infos/final/reward_run Min             0.250751\n",
      "exploration/env_infos/initial/reward_run Mean          0.828329\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.828329\n",
      "exploration/env_infos/initial/reward_run Min           0.828329\n",
      "exploration/env_infos/reward_run Mean                 -0.111632\n",
      "exploration/env_infos/reward_run Std                   0.631646\n",
      "exploration/env_infos/reward_run Max                   1.81786\n",
      "exploration/env_infos/reward_run Min                  -2.04526\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.295266\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.295266\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.295266\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.564111\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.564111\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.564111\n",
      "exploration/env_infos/reward_ctrl Mean                -0.374718\n",
      "exploration/env_infos/reward_ctrl Std                  0.100639\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0836366\n",
      "exploration/env_infos/reward_ctrl Min                 -0.585941\n",
      "exploration/env_infos/final/height Mean               -0.301352\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.301352\n",
      "exploration/env_infos/final/height Min                -0.301352\n",
      "exploration/env_infos/initial/height Mean              0.0807662\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0807662\n",
      "exploration/env_infos/initial/height Min               0.0807662\n",
      "exploration/env_infos/height Mean                     -0.432851\n",
      "exploration/env_infos/height Std                       0.112684\n",
      "exploration/env_infos/height Max                       0.150388\n",
      "exploration/env_infos/height Min                      -0.587279\n",
      "exploration/env_infos/final/reward_angular Mean       -1.75372\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.75372\n",
      "exploration/env_infos/final/reward_angular Min        -1.75372\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.75336\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.75336\n",
      "exploration/env_infos/initial/reward_angular Min      -0.75336\n",
      "exploration/env_infos/reward_angular Mean              0.156408\n",
      "exploration/env_infos/reward_angular Std               2.21559\n",
      "exploration/env_infos/reward_angular Max               8.73569\n",
      "exploration/env_infos/reward_angular Min              -4.47674\n",
      "evaluation/num steps total                             1.75e+06\n",
      "evaluation/num paths total                          1750\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0259581\n",
      "evaluation/Rewards Std                                 1.27247\n",
      "evaluation/Rewards Max                                 6.72773\n",
      "evaluation/Rewards Min                                -6.42281\n",
      "evaluation/Returns Mean                              -25.9581\n",
      "evaluation/Returns Std                               356.842\n",
      "evaluation/Returns Max                              1389.6\n",
      "evaluation/Returns Min                              -390.627\n",
      "evaluation/Actions Mean                                0.123756\n",
      "evaluation/Actions Std                                 0.743581\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -0.999997\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -25.9581\n",
      "evaluation/env_infos/final/reward_run Mean             0.0306106\n",
      "evaluation/env_infos/final/reward_run Std              0.746078\n",
      "evaluation/env_infos/final/reward_run Max              1.69777\n",
      "evaluation/env_infos/final/reward_run Min             -1.22743\n",
      "evaluation/env_infos/initial/reward_run Mean           0.301058\n",
      "evaluation/env_infos/initial/reward_run Std            0.567513\n",
      "evaluation/env_infos/initial/reward_run Max            1.12596\n",
      "evaluation/env_infos/initial/reward_run Min           -0.863991\n",
      "evaluation/env_infos/reward_run Mean                   0.136026\n",
      "evaluation/env_infos/reward_run Std                    1.07211\n",
      "evaluation/env_infos/reward_run Max                    3.96157\n",
      "evaluation/env_infos/reward_run Min                   -4.71347\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.361755\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0922563\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.185682\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.541307\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.309123\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.156925\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0830191\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.525298\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.340937\n",
      "evaluation/env_infos/reward_ctrl Std                   0.11033\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00551983\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.591956\n",
      "evaluation/env_infos/final/height Mean                -0.337027\n",
      "evaluation/env_infos/final/height Std                  0.148877\n",
      "evaluation/env_infos/final/height Max                 -0.0473798\n",
      "evaluation/env_infos/final/height Min                 -0.577503\n",
      "evaluation/env_infos/initial/height Mean              -0.0126466\n",
      "evaluation/env_infos/initial/height Std                0.056443\n",
      "evaluation/env_infos/initial/height Max                0.0984644\n",
      "evaluation/env_infos/initial/height Min               -0.0983519\n",
      "evaluation/env_infos/height Mean                      -0.30845\n",
      "evaluation/env_infos/height Std                        0.179414\n",
      "evaluation/env_infos/height Max                        0.335764\n",
      "evaluation/env_infos/height Min                       -0.605281\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.392962\n",
      "evaluation/env_infos/final/reward_angular Std          1.44198\n",
      "evaluation/env_infos/final/reward_angular Max          2.87043\n",
      "evaluation/env_infos/final/reward_angular Min         -3.83443\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.505426\n",
      "evaluation/env_infos/initial/reward_angular Std        1.15907\n",
      "evaluation/env_infos/initial/reward_angular Max        3.41397\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.34201\n",
      "evaluation/env_infos/reward_angular Mean               0.0155101\n",
      "evaluation/env_infos/reward_angular Std                1.92343\n",
      "evaluation/env_infos/reward_angular Max                9.35766\n",
      "evaluation/env_infos/reward_angular Min               -8.06268\n",
      "time/data storing (s)                                  0.0160098\n",
      "time/evaluation sampling (s)                          20.5488\n",
      "time/exploration sampling (s)                          1.03607\n",
      "time/logging (s)                                       0.242485\n",
      "time/saving (s)                                        0.0259166\n",
      "time/training (s)                                      3.8993\n",
      "time/epoch (s)                                        25.7686\n",
      "time/total (s)                                      2062.68\n",
      "Epoch                                                 69\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:46:51.937685 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 70 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 72000\n",
      "trainer/QF1 Loss                                       0.742152\n",
      "trainer/QF2 Loss                                       0.677193\n",
      "trainer/Policy Loss                                    1.72748\n",
      "trainer/Q1 Predictions Mean                            3.85419\n",
      "trainer/Q1 Predictions Std                             8.474\n",
      "trainer/Q1 Predictions Max                            41.3223\n",
      "trainer/Q1 Predictions Min                            -6.77742\n",
      "trainer/Q2 Predictions Mean                            3.68327\n",
      "trainer/Q2 Predictions Std                             8.48781\n",
      "trainer/Q2 Predictions Max                            41.1159\n",
      "trainer/Q2 Predictions Min                            -7.93954\n",
      "trainer/Q Targets Mean                                 3.71656\n",
      "trainer/Q Targets Std                                  8.45058\n",
      "trainer/Q Targets Max                                 40.7303\n",
      "trainer/Q Targets Min                                 -7.07445\n",
      "trainer/Log Pis Mean                                   5.7557\n",
      "trainer/Log Pis Std                                    5.45181\n",
      "trainer/Log Pis Max                                   26.8964\n",
      "trainer/Log Pis Min                                   -6.40027\n",
      "trainer/Policy mu Mean                                 0.143293\n",
      "trainer/Policy mu Std                                  1.60382\n",
      "trainer/Policy mu Max                                  4.26418\n",
      "trainer/Policy mu Min                                 -7.58468\n",
      "trainer/Policy log std Mean                           -0.769311\n",
      "trainer/Policy log std Std                             0.259651\n",
      "trainer/Policy log std Max                             0.148832\n",
      "trainer/Policy log std Min                            -2.03431\n",
      "trainer/Alpha                                          0.0125005\n",
      "trainer/Alpha Loss                                    -1.07041\n",
      "exploration/num steps total                        72000\n",
      "exploration/num paths total                           72\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.305761\n",
      "exploration/Rewards Std                                0.984699\n",
      "exploration/Rewards Max                                2.87283\n",
      "exploration/Rewards Min                               -3.15841\n",
      "exploration/Returns Mean                            -305.761\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -305.761\n",
      "exploration/Returns Min                             -305.761\n",
      "exploration/Actions Mean                               0.0571792\n",
      "exploration/Actions Std                                0.719491\n",
      "exploration/Actions Max                                0.999951\n",
      "exploration/Actions Min                               -0.999814\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -305.761\n",
      "exploration/env_infos/final/reward_run Mean            0.0687263\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.0687263\n",
      "exploration/env_infos/final/reward_run Min             0.0687263\n",
      "exploration/env_infos/initial/reward_run Mean         -0.295634\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.295634\n",
      "exploration/env_infos/initial/reward_run Min          -0.295634\n",
      "exploration/env_infos/reward_run Mean                 -0.0920284\n",
      "exploration/env_infos/reward_run Std                   0.645099\n",
      "exploration/env_infos/reward_run Max                   1.73928\n",
      "exploration/env_infos/reward_run Min                  -2.0272\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.308983\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.308983\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.308983\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.126912\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.126912\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.126912\n",
      "exploration/env_infos/reward_ctrl Mean                -0.312562\n",
      "exploration/env_infos/reward_ctrl Std                  0.0838567\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0599209\n",
      "exploration/env_infos/reward_ctrl Min                 -0.530487\n",
      "exploration/env_infos/final/height Mean               -0.535593\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.535593\n",
      "exploration/env_infos/final/height Min                -0.535593\n",
      "exploration/env_infos/initial/height Mean              0.0477289\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0477289\n",
      "exploration/env_infos/initial/height Min               0.0477289\n",
      "exploration/env_infos/height Mean                     -0.441875\n",
      "exploration/env_infos/height Std                       0.150804\n",
      "exploration/env_infos/height Max                       0.313239\n",
      "exploration/env_infos/height Min                      -0.591592\n",
      "exploration/env_infos/final/reward_angular Mean        1.22658\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.22658\n",
      "exploration/env_infos/final/reward_angular Min         1.22658\n",
      "exploration/env_infos/initial/reward_angular Mean      0.60593\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.60593\n",
      "exploration/env_infos/initial/reward_angular Min       0.60593\n",
      "exploration/env_infos/reward_angular Mean              0.12009\n",
      "exploration/env_infos/reward_angular Std               1.91136\n",
      "exploration/env_infos/reward_angular Max               6.19086\n",
      "exploration/env_infos/reward_angular Min              -5.48755\n",
      "evaluation/num steps total                             1.775e+06\n",
      "evaluation/num paths total                          1775\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.00500477\n",
      "evaluation/Rewards Std                                 1.18014\n",
      "evaluation/Rewards Max                                 6.45737\n",
      "evaluation/Rewards Min                                -5.43663\n",
      "evaluation/Returns Mean                                5.00477\n",
      "evaluation/Returns Std                               400.629\n",
      "evaluation/Returns Max                              1408.05\n",
      "evaluation/Returns Min                              -404.411\n",
      "evaluation/Actions Mean                                0.0877978\n",
      "evaluation/Actions Std                                 0.735053\n",
      "evaluation/Actions Max                                 0.999985\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                             5.00477\n",
      "evaluation/env_infos/final/reward_run Mean             0.470283\n",
      "evaluation/env_infos/final/reward_run Std              1.12815\n",
      "evaluation/env_infos/final/reward_run Max              2.87302\n",
      "evaluation/env_infos/final/reward_run Min             -2.12306\n",
      "evaluation/env_infos/initial/reward_run Mean           0.320027\n",
      "evaluation/env_infos/initial/reward_run Std            0.593528\n",
      "evaluation/env_infos/initial/reward_run Max            1.09673\n",
      "evaluation/env_infos/initial/reward_run Min           -0.689656\n",
      "evaluation/env_infos/reward_run Mean                   0.082567\n",
      "evaluation/env_infos/reward_run Std                    1.19147\n",
      "evaluation/env_infos/reward_run Max                    4.30397\n",
      "evaluation/env_infos/reward_run Min                   -6.01908\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.310635\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0986671\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.120025\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.480659\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.307811\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.142605\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.082266\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.549083\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.328807\n",
      "evaluation/env_infos/reward_ctrl Std                   0.108357\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0102033\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596174\n",
      "evaluation/env_infos/final/height Mean                -0.313288\n",
      "evaluation/env_infos/final/height Std                  0.183636\n",
      "evaluation/env_infos/final/height Max                 -0.0118711\n",
      "evaluation/env_infos/final/height Min                 -0.57686\n",
      "evaluation/env_infos/initial/height Mean              -0.0135078\n",
      "evaluation/env_infos/initial/height Std                0.0465745\n",
      "evaluation/env_infos/initial/height Max                0.0737348\n",
      "evaluation/env_infos/initial/height Min               -0.0919446\n",
      "evaluation/env_infos/height Mean                      -0.311703\n",
      "evaluation/env_infos/height Std                        0.196031\n",
      "evaluation/env_infos/height Max                        0.291576\n",
      "evaluation/env_infos/height Min                       -0.596886\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.445543\n",
      "evaluation/env_infos/final/reward_angular Std          1.66837\n",
      "evaluation/env_infos/final/reward_angular Max          3.06277\n",
      "evaluation/env_infos/final/reward_angular Min         -4.87013\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.563053\n",
      "evaluation/env_infos/initial/reward_angular Std        1.25454\n",
      "evaluation/env_infos/initial/reward_angular Max        3.40106\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.48218\n",
      "evaluation/env_infos/reward_angular Mean               0.0088035\n",
      "evaluation/env_infos/reward_angular Std                1.81036\n",
      "evaluation/env_infos/reward_angular Max                8.18234\n",
      "evaluation/env_infos/reward_angular Min               -6.9908\n",
      "time/data storing (s)                                  0.0156708\n",
      "time/evaluation sampling (s)                          20.8655\n",
      "time/exploration sampling (s)                          1.01662\n",
      "time/logging (s)                                       0.2366\n",
      "time/saving (s)                                        0.0283842\n",
      "time/training (s)                                      3.89763\n",
      "time/epoch (s)                                        26.0604\n",
      "time/total (s)                                      2089.32\n",
      "Epoch                                                 70\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:47:18.445973 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 71 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 73000\n",
      "trainer/QF1 Loss                                       0.458404\n",
      "trainer/QF2 Loss                                       0.51249\n",
      "trainer/Policy Loss                                    4.02976\n",
      "trainer/Q1 Predictions Mean                            2.51551\n",
      "trainer/Q1 Predictions Std                             7.17503\n",
      "trainer/Q1 Predictions Max                            33.8422\n",
      "trainer/Q1 Predictions Min                            -7.03863\n",
      "trainer/Q2 Predictions Mean                            2.58676\n",
      "trainer/Q2 Predictions Std                             7.02215\n",
      "trainer/Q2 Predictions Max                            33.2597\n",
      "trainer/Q2 Predictions Min                            -6.6886\n",
      "trainer/Q Targets Mean                                 2.49968\n",
      "trainer/Q Targets Std                                  7.2176\n",
      "trainer/Q Targets Max                                 36.0732\n",
      "trainer/Q Targets Min                                 -8.06535\n",
      "trainer/Log Pis Mean                                   6.73617\n",
      "trainer/Log Pis Std                                    5.24768\n",
      "trainer/Log Pis Max                                   24.9262\n",
      "trainer/Log Pis Min                                   -3.81507\n",
      "trainer/Policy mu Mean                                 0.457401\n",
      "trainer/Policy mu Std                                  1.61133\n",
      "trainer/Policy mu Max                                  4.81529\n",
      "trainer/Policy mu Min                                 -5.6604\n",
      "trainer/Policy log std Mean                           -0.749957\n",
      "trainer/Policy log std Std                             0.251923\n",
      "trainer/Policy log std Max                             0.22213\n",
      "trainer/Policy log std Min                            -1.54157\n",
      "trainer/Alpha                                          0.0126068\n",
      "trainer/Alpha Loss                                     3.21995\n",
      "exploration/num steps total                        73000\n",
      "exploration/num paths total                           73\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.173055\n",
      "exploration/Rewards Std                                0.918627\n",
      "exploration/Rewards Max                                2.47765\n",
      "exploration/Rewards Min                               -2.81211\n",
      "exploration/Returns Mean                            -173.055\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -173.055\n",
      "exploration/Returns Min                             -173.055\n",
      "exploration/Actions Mean                               0.26347\n",
      "exploration/Actions Std                                0.760427\n",
      "exploration/Actions Max                                0.999975\n",
      "exploration/Actions Min                               -0.999973\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -173.055\n",
      "exploration/env_infos/final/reward_run Mean            1.66058\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.66058\n",
      "exploration/env_infos/final/reward_run Min             1.66058\n",
      "exploration/env_infos/initial/reward_run Mean          0.628475\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.628475\n",
      "exploration/env_infos/initial/reward_run Min           0.628475\n",
      "exploration/env_infos/reward_run Mean                  0.517675\n",
      "exploration/env_infos/reward_run Std                   0.867772\n",
      "exploration/env_infos/reward_run Max                   2.94772\n",
      "exploration/env_infos/reward_run Min                  -2.12953\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.297815\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.297815\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.297815\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.417666\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.417666\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.417666\n",
      "exploration/env_infos/reward_ctrl Mean                -0.388599\n",
      "exploration/env_infos/reward_ctrl Std                  0.100548\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0336471\n",
      "exploration/env_infos/reward_ctrl Min                 -0.594441\n",
      "exploration/env_infos/final/height Mean               -0.307837\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.307837\n",
      "exploration/env_infos/final/height Min                -0.307837\n",
      "exploration/env_infos/initial/height Mean              0.0524285\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0524285\n",
      "exploration/env_infos/initial/height Min               0.0524285\n",
      "exploration/env_infos/height Mean                     -0.432177\n",
      "exploration/env_infos/height Std                       0.126188\n",
      "exploration/env_infos/height Max                       0.175813\n",
      "exploration/env_infos/height Min                      -0.579791\n",
      "exploration/env_infos/final/reward_angular Mean        1.14771\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.14771\n",
      "exploration/env_infos/final/reward_angular Min         1.14771\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.402111\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.402111\n",
      "exploration/env_infos/initial/reward_angular Min      -0.402111\n",
      "exploration/env_infos/reward_angular Mean              0.0877401\n",
      "exploration/env_infos/reward_angular Std               2.33235\n",
      "exploration/env_infos/reward_angular Max               8.52911\n",
      "exploration/env_infos/reward_angular Min              -6.04678\n",
      "evaluation/num steps total                             1.8e+06\n",
      "evaluation/num paths total                          1800\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0754695\n",
      "evaluation/Rewards Std                                 1.2064\n",
      "evaluation/Rewards Max                                 7.44011\n",
      "evaluation/Rewards Min                                -7.52256\n",
      "evaluation/Returns Mean                              -75.4695\n",
      "evaluation/Returns Std                               327.615\n",
      "evaluation/Returns Max                              1238.66\n",
      "evaluation/Returns Min                              -365.256\n",
      "evaluation/Actions Mean                                0.106401\n",
      "evaluation/Actions Std                                 0.769289\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999997\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -75.4695\n",
      "evaluation/env_infos/final/reward_run Mean             0.0536842\n",
      "evaluation/env_infos/final/reward_run Std              0.870158\n",
      "evaluation/env_infos/final/reward_run Max              2.07751\n",
      "evaluation/env_infos/final/reward_run Min             -1.67235\n",
      "evaluation/env_infos/initial/reward_run Mean           0.369589\n",
      "evaluation/env_infos/initial/reward_run Std            0.547545\n",
      "evaluation/env_infos/initial/reward_run Max            1.13013\n",
      "evaluation/env_infos/initial/reward_run Min           -0.707969\n",
      "evaluation/env_infos/reward_run Mean                   0.10462\n",
      "evaluation/env_infos/reward_run Std                    0.980471\n",
      "evaluation/env_infos/reward_run Max                    3.53069\n",
      "evaluation/env_infos/reward_run Min                   -5.29442\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.379769\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.108595\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.151197\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.516998\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.327295\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.147648\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0922768\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.557715\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.361876\n",
      "evaluation/env_infos/reward_ctrl Std                   0.115613\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0159994\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.592617\n",
      "evaluation/env_infos/final/height Mean                -0.351625\n",
      "evaluation/env_infos/final/height Std                  0.166593\n",
      "evaluation/env_infos/final/height Max                 -0.0142449\n",
      "evaluation/env_infos/final/height Min                 -0.577274\n",
      "evaluation/env_infos/initial/height Mean               0.0106692\n",
      "evaluation/env_infos/initial/height Std                0.0438477\n",
      "evaluation/env_infos/initial/height Max                0.0802267\n",
      "evaluation/env_infos/initial/height Min               -0.0619297\n",
      "evaluation/env_infos/height Mean                      -0.338838\n",
      "evaluation/env_infos/height Std                        0.176008\n",
      "evaluation/env_infos/height Max                        0.324732\n",
      "evaluation/env_infos/height Min                       -0.598163\n",
      "evaluation/env_infos/final/reward_angular Mean         0.38628\n",
      "evaluation/env_infos/final/reward_angular Std          1.58359\n",
      "evaluation/env_infos/final/reward_angular Max          3.80414\n",
      "evaluation/env_infos/final/reward_angular Min         -1.91083\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.0374328\n",
      "evaluation/env_infos/initial/reward_angular Std        1.14351\n",
      "evaluation/env_infos/initial/reward_angular Max        2.62239\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.71616\n",
      "evaluation/env_infos/reward_angular Mean               0.0116547\n",
      "evaluation/env_infos/reward_angular Std                1.82524\n",
      "evaluation/env_infos/reward_angular Max                8.72634\n",
      "evaluation/env_infos/reward_angular Min               -8.23651\n",
      "time/data storing (s)                                  0.0148651\n",
      "time/evaluation sampling (s)                          20.7923\n",
      "time/exploration sampling (s)                          0.990443\n",
      "time/logging (s)                                       0.238071\n",
      "time/saving (s)                                        0.0271019\n",
      "time/training (s)                                      3.8563\n",
      "time/epoch (s)                                        25.9191\n",
      "time/total (s)                                      2115.83\n",
      "Epoch                                                 71\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:47:44.713029 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 72 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 74000\n",
      "trainer/QF1 Loss                                       0.640858\n",
      "trainer/QF2 Loss                                       0.564046\n",
      "trainer/Policy Loss                                    2.79555\n",
      "trainer/Q1 Predictions Mean                            2.46311\n",
      "trainer/Q1 Predictions Std                             7.2403\n",
      "trainer/Q1 Predictions Max                            43.6361\n",
      "trainer/Q1 Predictions Min                            -7.44663\n",
      "trainer/Q2 Predictions Mean                            2.25352\n",
      "trainer/Q2 Predictions Std                             7.03018\n",
      "trainer/Q2 Predictions Max                            41.8158\n",
      "trainer/Q2 Predictions Min                            -7.84016\n",
      "trainer/Q Targets Mean                                 2.41152\n",
      "trainer/Q Targets Std                                  7.08173\n",
      "trainer/Q Targets Max                                 41.1739\n",
      "trainer/Q Targets Min                                 -8.42463\n",
      "trainer/Log Pis Mean                                   5.26651\n",
      "trainer/Log Pis Std                                    5.36264\n",
      "trainer/Log Pis Max                                   20.9257\n",
      "trainer/Log Pis Min                                   -6.10298\n",
      "trainer/Policy mu Mean                                 0.104529\n",
      "trainer/Policy mu Std                                  1.52053\n",
      "trainer/Policy mu Max                                  4.6337\n",
      "trainer/Policy mu Min                                 -5.68521\n",
      "trainer/Policy log std Mean                           -0.767212\n",
      "trainer/Policy log std Std                             0.258571\n",
      "trainer/Policy log std Max                             0.366781\n",
      "trainer/Policy log std Min                            -1.64573\n",
      "trainer/Alpha                                          0.0121988\n",
      "trainer/Alpha Loss                                    -3.23152\n",
      "exploration/num steps total                        74000\n",
      "exploration/num paths total                           74\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0261415\n",
      "exploration/Rewards Std                                1.65018\n",
      "exploration/Rewards Max                                4.6316\n",
      "exploration/Rewards Min                               -5.88823\n",
      "exploration/Returns Mean                             -26.1415\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -26.1415\n",
      "exploration/Returns Min                              -26.1415\n",
      "exploration/Actions Mean                               0.257033\n",
      "exploration/Actions Std                                0.636694\n",
      "exploration/Actions Max                                0.999374\n",
      "exploration/Actions Min                               -0.999998\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -26.1415\n",
      "exploration/env_infos/final/reward_run Mean           -0.423639\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.423639\n",
      "exploration/env_infos/final/reward_run Min            -0.423639\n",
      "exploration/env_infos/initial/reward_run Mean          0.720001\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.720001\n",
      "exploration/env_infos/initial/reward_run Min           0.720001\n",
      "exploration/env_infos/reward_run Mean                 -0.0893201\n",
      "exploration/env_infos/reward_run Std                   0.758321\n",
      "exploration/env_infos/reward_run Max                   2.26554\n",
      "exploration/env_infos/reward_run Min                  -2.33972\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.355826\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.355826\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.355826\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.321431\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.321431\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.321431\n",
      "exploration/env_infos/reward_ctrl Mean                -0.282867\n",
      "exploration/env_infos/reward_ctrl Std                  0.0956757\n",
      "exploration/env_infos/reward_ctrl Max                 -0.037937\n",
      "exploration/env_infos/reward_ctrl Min                 -0.549782\n",
      "exploration/env_infos/final/height Mean                0.167525\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                 0.167525\n",
      "exploration/env_infos/final/height Min                 0.167525\n",
      "exploration/env_infos/initial/height Mean             -0.0551978\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0551978\n",
      "exploration/env_infos/initial/height Min              -0.0551978\n",
      "exploration/env_infos/height Mean                     -0.0299455\n",
      "exploration/env_infos/height Std                       0.0922135\n",
      "exploration/env_infos/height Max                       0.361312\n",
      "exploration/env_infos/height Min                      -0.283866\n",
      "exploration/env_infos/final/reward_angular Mean       -0.758702\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.758702\n",
      "exploration/env_infos/final/reward_angular Min        -0.758702\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.8003\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.8003\n",
      "exploration/env_infos/initial/reward_angular Min      -1.8003\n",
      "exploration/env_infos/reward_angular Mean             -0.0409617\n",
      "exploration/env_infos/reward_angular Std               1.85677\n",
      "exploration/env_infos/reward_angular Max               6.3172\n",
      "exploration/env_infos/reward_angular Min              -5.113\n",
      "evaluation/num steps total                             1.825e+06\n",
      "evaluation/num paths total                          1825\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0978347\n",
      "evaluation/Rewards Std                                 1.24012\n",
      "evaluation/Rewards Max                                 6.87147\n",
      "evaluation/Rewards Min                                -6.61172\n",
      "evaluation/Returns Mean                               97.8347\n",
      "evaluation/Returns Std                               472.387\n",
      "evaluation/Returns Max                              1532.52\n",
      "evaluation/Returns Min                              -425.009\n",
      "evaluation/Actions Mean                                0.0230434\n",
      "evaluation/Actions Std                                 0.758347\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            97.8347\n",
      "evaluation/env_infos/final/reward_run Mean             0.121196\n",
      "evaluation/env_infos/final/reward_run Std              1.07751\n",
      "evaluation/env_infos/final/reward_run Max              3.64568\n",
      "evaluation/env_infos/final/reward_run Min             -2.446\n",
      "evaluation/env_infos/initial/reward_run Mean           0.261845\n",
      "evaluation/env_infos/initial/reward_run Std            0.581716\n",
      "evaluation/env_infos/initial/reward_run Max            1.13128\n",
      "evaluation/env_infos/initial/reward_run Min           -0.743328\n",
      "evaluation/env_infos/reward_run Mean                  -0.107178\n",
      "evaluation/env_infos/reward_run Std                    1.45989\n",
      "evaluation/env_infos/reward_run Max                    4.65118\n",
      "evaluation/env_infos/reward_run Min                   -5.09003\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.369658\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.130704\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.048549\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.54964\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.290277\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.110346\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0876917\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.481253\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.345373\n",
      "evaluation/env_infos/reward_ctrl Std                   0.126815\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00765149\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.5978\n",
      "evaluation/env_infos/final/height Mean                -0.3443\n",
      "evaluation/env_infos/final/height Std                  0.184254\n",
      "evaluation/env_infos/final/height Max                  0.00243741\n",
      "evaluation/env_infos/final/height Min                 -0.577271\n",
      "evaluation/env_infos/initial/height Mean              -0.00416596\n",
      "evaluation/env_infos/initial/height Std                0.0511634\n",
      "evaluation/env_infos/initial/height Max                0.089094\n",
      "evaluation/env_infos/initial/height Min               -0.100944\n",
      "evaluation/env_infos/height Mean                      -0.250116\n",
      "evaluation/env_infos/height Std                        0.202224\n",
      "evaluation/env_infos/height Max                        0.350938\n",
      "evaluation/env_infos/height Min                       -0.59304\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.601559\n",
      "evaluation/env_infos/final/reward_angular Std          1.4786\n",
      "evaluation/env_infos/final/reward_angular Max          2.60353\n",
      "evaluation/env_infos/final/reward_angular Min         -4.06576\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.277821\n",
      "evaluation/env_infos/initial/reward_angular Std        1.12502\n",
      "evaluation/env_infos/initial/reward_angular Max        2.70094\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.01945\n",
      "evaluation/env_infos/reward_angular Mean              -0.000121697\n",
      "evaluation/env_infos/reward_angular Std                1.75238\n",
      "evaluation/env_infos/reward_angular Max                9.28449\n",
      "evaluation/env_infos/reward_angular Min               -7.04907\n",
      "time/data storing (s)                                  0.0144158\n",
      "time/evaluation sampling (s)                          20.6541\n",
      "time/exploration sampling (s)                          0.963093\n",
      "time/logging (s)                                       0.238569\n",
      "time/saving (s)                                        0.0271736\n",
      "time/training (s)                                      3.80275\n",
      "time/epoch (s)                                        25.7001\n",
      "time/total (s)                                      2142.09\n",
      "Epoch                                                 72\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:48:11.444055 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 73 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 75000\n",
      "trainer/QF1 Loss                                       0.645799\n",
      "trainer/QF2 Loss                                       0.489337\n",
      "trainer/Policy Loss                                    2.631\n",
      "trainer/Q1 Predictions Mean                            3.00197\n",
      "trainer/Q1 Predictions Std                             7.82356\n",
      "trainer/Q1 Predictions Max                            33.6739\n",
      "trainer/Q1 Predictions Min                            -8.03646\n",
      "trainer/Q2 Predictions Mean                            3.01677\n",
      "trainer/Q2 Predictions Std                             7.85185\n",
      "trainer/Q2 Predictions Max                            34.6103\n",
      "trainer/Q2 Predictions Min                            -7.15372\n",
      "trainer/Q Targets Mean                                 3.01609\n",
      "trainer/Q Targets Std                                  7.83896\n",
      "trainer/Q Targets Max                                 35.2415\n",
      "trainer/Q Targets Min                                 -7.74846\n",
      "trainer/Log Pis Mean                                   5.90648\n",
      "trainer/Log Pis Std                                    6.01217\n",
      "trainer/Log Pis Max                                   41.4726\n",
      "trainer/Log Pis Min                                   -6.21167\n",
      "trainer/Policy mu Mean                                 0.225606\n",
      "trainer/Policy mu Std                                  1.60189\n",
      "trainer/Policy mu Max                                  5.69394\n",
      "trainer/Policy mu Min                                 -6.86538\n",
      "trainer/Policy log std Mean                           -0.750358\n",
      "trainer/Policy log std Std                             0.280418\n",
      "trainer/Policy log std Max                             0.36684\n",
      "trainer/Policy log std Min                            -2.7664\n",
      "trainer/Alpha                                          0.01222\n",
      "trainer/Alpha Loss                                    -0.411823\n",
      "exploration/num steps total                        75000\n",
      "exploration/num paths total                           75\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.534974\n",
      "exploration/Rewards Std                                1.18052\n",
      "exploration/Rewards Max                                3.7\n",
      "exploration/Rewards Min                               -2.88941\n",
      "exploration/Returns Mean                             534.974\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              534.974\n",
      "exploration/Returns Min                              534.974\n",
      "exploration/Actions Mean                               0.187275\n",
      "exploration/Actions Std                                0.770091\n",
      "exploration/Actions Max                                0.999992\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          534.974\n",
      "exploration/env_infos/final/reward_run Mean            2.44156\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             2.44156\n",
      "exploration/env_infos/final/reward_run Min             2.44156\n",
      "exploration/env_infos/initial/reward_run Mean          0.64497\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.64497\n",
      "exploration/env_infos/initial/reward_run Min           0.64497\n",
      "exploration/env_infos/reward_run Mean                  1.74542\n",
      "exploration/env_infos/reward_run Std                   0.878096\n",
      "exploration/env_infos/reward_run Max                   3.92143\n",
      "exploration/env_infos/reward_run Min                  -0.918959\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.376283\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.376283\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.376283\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.289882\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.289882\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.289882\n",
      "exploration/env_infos/reward_ctrl Mean                -0.376868\n",
      "exploration/env_infos/reward_ctrl Std                  0.10329\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0360684\n",
      "exploration/env_infos/reward_ctrl Min                 -0.594427\n",
      "exploration/env_infos/final/height Mean               -0.329189\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.329189\n",
      "exploration/env_infos/final/height Min                -0.329189\n",
      "exploration/env_infos/initial/height Mean             -0.0348366\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0348366\n",
      "exploration/env_infos/initial/height Min              -0.0348366\n",
      "exploration/env_infos/height Mean                     -0.140439\n",
      "exploration/env_infos/height Std                       0.103295\n",
      "exploration/env_infos/height Max                       0.186274\n",
      "exploration/env_infos/height Min                      -0.411641\n",
      "exploration/env_infos/final/reward_angular Mean        0.63287\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.63287\n",
      "exploration/env_infos/final/reward_angular Min         0.63287\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.46659\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.46659\n",
      "exploration/env_infos/initial/reward_angular Min      -1.46659\n",
      "exploration/env_infos/reward_angular Mean             -0.0443471\n",
      "exploration/env_infos/reward_angular Std               1.57169\n",
      "exploration/env_infos/reward_angular Max               4.8261\n",
      "exploration/env_infos/reward_angular Min              -4.46728\n",
      "evaluation/num steps total                             1.85e+06\n",
      "evaluation/num paths total                          1850\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.00799344\n",
      "evaluation/Rewards Std                                 1.12616\n",
      "evaluation/Rewards Max                                 7.68589\n",
      "evaluation/Rewards Min                                -7.01438\n",
      "evaluation/Returns Mean                               -7.99344\n",
      "evaluation/Returns Std                               417.803\n",
      "evaluation/Returns Max                              1502.71\n",
      "evaluation/Returns Min                              -448.002\n",
      "evaluation/Actions Mean                                0.159893\n",
      "evaluation/Actions Std                                 0.71251\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            -7.99344\n",
      "evaluation/env_infos/final/reward_run Mean             0.44884\n",
      "evaluation/env_infos/final/reward_run Std              0.922185\n",
      "evaluation/env_infos/final/reward_run Max              2.42582\n",
      "evaluation/env_infos/final/reward_run Min             -1.22869\n",
      "evaluation/env_infos/initial/reward_run Mean           0.320482\n",
      "evaluation/env_infos/initial/reward_run Std            0.573497\n",
      "evaluation/env_infos/initial/reward_run Max            1.20167\n",
      "evaluation/env_infos/initial/reward_run Min           -0.594785\n",
      "evaluation/env_infos/reward_run Mean                   0.185533\n",
      "evaluation/env_infos/reward_run Std                    1.20951\n",
      "evaluation/env_infos/reward_run Max                    4.29372\n",
      "evaluation/env_infos/reward_run Min                   -4.24247\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.318901\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0936144\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.11067\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.576445\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.297867\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.137085\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0322993\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.500738\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.319942\n",
      "evaluation/env_infos/reward_ctrl Std                   0.122949\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0134119\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596933\n",
      "evaluation/env_infos/final/height Mean                -0.437975\n",
      "evaluation/env_infos/final/height Std                  0.162597\n",
      "evaluation/env_infos/final/height Max                 -0.0179959\n",
      "evaluation/env_infos/final/height Min                 -0.593127\n",
      "evaluation/env_infos/initial/height Mean              -0.0178101\n",
      "evaluation/env_infos/initial/height Std                0.0415406\n",
      "evaluation/env_infos/initial/height Max                0.0852225\n",
      "evaluation/env_infos/initial/height Min               -0.0843376\n",
      "evaluation/env_infos/height Mean                      -0.355181\n",
      "evaluation/env_infos/height Std                        0.201773\n",
      "evaluation/env_infos/height Max                        0.366032\n",
      "evaluation/env_infos/height Min                       -0.593127\n",
      "evaluation/env_infos/final/reward_angular Mean         0.0959038\n",
      "evaluation/env_infos/final/reward_angular Std          1.62375\n",
      "evaluation/env_infos/final/reward_angular Max          3.538\n",
      "evaluation/env_infos/final/reward_angular Min         -3.74109\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.125264\n",
      "evaluation/env_infos/initial/reward_angular Std        1.11012\n",
      "evaluation/env_infos/initial/reward_angular Max        2.29501\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.01765\n",
      "evaluation/env_infos/reward_angular Mean              -0.00367688\n",
      "evaluation/env_infos/reward_angular Std                1.58388\n",
      "evaluation/env_infos/reward_angular Max                8.84797\n",
      "evaluation/env_infos/reward_angular Min               -9.32531\n",
      "time/data storing (s)                                  0.0148669\n",
      "time/evaluation sampling (s)                          20.8027\n",
      "time/exploration sampling (s)                          1.03925\n",
      "time/logging (s)                                       0.243158\n",
      "time/saving (s)                                        0.0262511\n",
      "time/training (s)                                      4.01043\n",
      "time/epoch (s)                                        26.1366\n",
      "time/total (s)                                      2168.82\n",
      "Epoch                                                 73\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:48:38.522691 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 74 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 76000\n",
      "trainer/QF1 Loss                                       0.671626\n",
      "trainer/QF2 Loss                                       0.60951\n",
      "trainer/Policy Loss                                    2.21395\n",
      "trainer/Q1 Predictions Mean                            3.81992\n",
      "trainer/Q1 Predictions Std                             9.04928\n",
      "trainer/Q1 Predictions Max                            44.5978\n",
      "trainer/Q1 Predictions Min                            -7.60325\n",
      "trainer/Q2 Predictions Mean                            3.95846\n",
      "trainer/Q2 Predictions Std                             9.24227\n",
      "trainer/Q2 Predictions Max                            45.385\n",
      "trainer/Q2 Predictions Min                            -7.48509\n",
      "trainer/Q Targets Mean                                 3.85271\n",
      "trainer/Q Targets Std                                  9.15871\n",
      "trainer/Q Targets Max                                 44.6218\n",
      "trainer/Q Targets Min                                 -7.37051\n",
      "trainer/Log Pis Mean                                   6.30306\n",
      "trainer/Log Pis Std                                    5.41379\n",
      "trainer/Log Pis Max                                   27.2159\n",
      "trainer/Log Pis Min                                   -5.02972\n",
      "trainer/Policy mu Mean                                 0.204177\n",
      "trainer/Policy mu Std                                  1.60778\n",
      "trainer/Policy mu Max                                  4.97857\n",
      "trainer/Policy mu Min                                 -5.09168\n",
      "trainer/Policy log std Mean                           -0.783664\n",
      "trainer/Policy log std Std                             0.308927\n",
      "trainer/Policy log std Max                             0.438971\n",
      "trainer/Policy log std Min                            -2.39103\n",
      "trainer/Alpha                                          0.0124653\n",
      "trainer/Alpha Loss                                     1.32908\n",
      "exploration/num steps total                        76000\n",
      "exploration/num paths total                           76\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               2.37723\n",
      "exploration/Rewards Std                                0.864618\n",
      "exploration/Rewards Max                                4.78155\n",
      "exploration/Rewards Min                               -0.248126\n",
      "exploration/Returns Mean                            2377.23\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             2377.23\n",
      "exploration/Returns Min                             2377.23\n",
      "exploration/Actions Mean                               0.250595\n",
      "exploration/Actions Std                                0.762423\n",
      "exploration/Actions Max                                0.999995\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         2377.23\n",
      "exploration/env_infos/final/reward_run Mean           -2.1411\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -2.1411\n",
      "exploration/env_infos/final/reward_run Min            -2.1411\n",
      "exploration/env_infos/initial/reward_run Mean          0.0855807\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0855807\n",
      "exploration/env_infos/initial/reward_run Min           0.0855807\n",
      "exploration/env_infos/reward_run Mean                 -2.50973\n",
      "exploration/env_infos/reward_run Std                   0.8694\n",
      "exploration/env_infos/reward_run Max                   0.448506\n",
      "exploration/env_infos/reward_run Min                  -4.83615\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.423381\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.423381\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.423381\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.172192\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.172192\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.172192\n",
      "exploration/env_infos/reward_ctrl Mean                -0.386452\n",
      "exploration/env_infos/reward_ctrl Std                  0.0747103\n",
      "exploration/env_infos/reward_ctrl Max                 -0.160649\n",
      "exploration/env_infos/reward_ctrl Min                 -0.578385\n",
      "exploration/env_infos/final/height Mean               -0.0514665\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0514665\n",
      "exploration/env_infos/final/height Min                -0.0514665\n",
      "exploration/env_infos/initial/height Mean             -0.0013066\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0013066\n",
      "exploration/env_infos/initial/height Min              -0.0013066\n",
      "exploration/env_infos/height Mean                     -0.0700369\n",
      "exploration/env_infos/height Std                       0.103585\n",
      "exploration/env_infos/height Max                       0.237473\n",
      "exploration/env_infos/height Min                      -0.364454\n",
      "exploration/env_infos/final/reward_angular Mean        1.06893\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.06893\n",
      "exploration/env_infos/final/reward_angular Min         1.06893\n",
      "exploration/env_infos/initial/reward_angular Mean      0.1804\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.1804\n",
      "exploration/env_infos/initial/reward_angular Min       0.1804\n",
      "exploration/env_infos/reward_angular Mean              0.113966\n",
      "exploration/env_infos/reward_angular Std               2.30322\n",
      "exploration/env_infos/reward_angular Max               6.1574\n",
      "exploration/env_infos/reward_angular Min              -6.54869\n",
      "evaluation/num steps total                             1.875e+06\n",
      "evaluation/num paths total                          1875\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.133412\n",
      "evaluation/Rewards Std                                 1.3249\n",
      "evaluation/Rewards Max                                 7.71802\n",
      "evaluation/Rewards Min                                -6.81132\n",
      "evaluation/Returns Mean                              133.412\n",
      "evaluation/Returns Std                               509.339\n",
      "evaluation/Returns Max                              1458.03\n",
      "evaluation/Returns Min                              -370.972\n",
      "evaluation/Actions Mean                                0.0566\n",
      "evaluation/Actions Std                                 0.791258\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           133.412\n",
      "evaluation/env_infos/final/reward_run Mean             0.277582\n",
      "evaluation/env_infos/final/reward_run Std              1.2746\n",
      "evaluation/env_infos/final/reward_run Max              2.99018\n",
      "evaluation/env_infos/final/reward_run Min             -2.7555\n",
      "evaluation/env_infos/initial/reward_run Mean           0.247717\n",
      "evaluation/env_infos/initial/reward_run Std            0.626442\n",
      "evaluation/env_infos/initial/reward_run Max            1.20999\n",
      "evaluation/env_infos/initial/reward_run Min           -0.885977\n",
      "evaluation/env_infos/reward_run Mean                   0.0154393\n",
      "evaluation/env_infos/reward_run Std                    1.47717\n",
      "evaluation/env_infos/reward_run Max                    4.71507\n",
      "evaluation/env_infos/reward_run Min                   -4.84129\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.397135\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.101598\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.181701\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.547553\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.261838\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.107874\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0566131\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.43122\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.377575\n",
      "evaluation/env_infos/reward_ctrl Std                   0.113509\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0203906\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598256\n",
      "evaluation/env_infos/final/height Mean                -0.322159\n",
      "evaluation/env_infos/final/height Std                  0.175985\n",
      "evaluation/env_infos/final/height Max                  0.0499779\n",
      "evaluation/env_infos/final/height Min                 -0.579233\n",
      "evaluation/env_infos/initial/height Mean              -0.00156079\n",
      "evaluation/env_infos/initial/height Std                0.0514531\n",
      "evaluation/env_infos/initial/height Max                0.0743828\n",
      "evaluation/env_infos/initial/height Min               -0.0942063\n",
      "evaluation/env_infos/height Mean                      -0.271028\n",
      "evaluation/env_infos/height Std                        0.205413\n",
      "evaluation/env_infos/height Max                        0.38005\n",
      "evaluation/env_infos/height Min                       -0.596875\n",
      "evaluation/env_infos/final/reward_angular Mean         0.0857633\n",
      "evaluation/env_infos/final/reward_angular Std          1.29626\n",
      "evaluation/env_infos/final/reward_angular Max          3.21589\n",
      "evaluation/env_infos/final/reward_angular Min         -3.55359\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.0628793\n",
      "evaluation/env_infos/initial/reward_angular Std        1.07859\n",
      "evaluation/env_infos/initial/reward_angular Max        2.86822\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.50883\n",
      "evaluation/env_infos/reward_angular Mean               0.0184814\n",
      "evaluation/env_infos/reward_angular Std                1.90174\n",
      "evaluation/env_infos/reward_angular Max                9.26232\n",
      "evaluation/env_infos/reward_angular Min               -8.49992\n",
      "time/data storing (s)                                  0.0159517\n",
      "time/evaluation sampling (s)                          21.3267\n",
      "time/exploration sampling (s)                          0.995573\n",
      "time/logging (s)                                       0.238581\n",
      "time/saving (s)                                        0.0274264\n",
      "time/training (s)                                      3.84937\n",
      "time/epoch (s)                                        26.4536\n",
      "time/total (s)                                      2195.9\n",
      "Epoch                                                 74\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:49:05.259900 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 75 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 77000\n",
      "trainer/QF1 Loss                                       0.629503\n",
      "trainer/QF2 Loss                                       0.623743\n",
      "trainer/Policy Loss                                    2.99275\n",
      "trainer/Q1 Predictions Mean                            2.99513\n",
      "trainer/Q1 Predictions Std                             8.71364\n",
      "trainer/Q1 Predictions Max                            41.2957\n",
      "trainer/Q1 Predictions Min                            -8.18131\n",
      "trainer/Q2 Predictions Mean                            3.02241\n",
      "trainer/Q2 Predictions Std                             8.80134\n",
      "trainer/Q2 Predictions Max                            40.3576\n",
      "trainer/Q2 Predictions Min                            -7.63128\n",
      "trainer/Q Targets Mean                                 3.00977\n",
      "trainer/Q Targets Std                                  8.83858\n",
      "trainer/Q Targets Max                                 40.3899\n",
      "trainer/Q Targets Min                                 -9.40264\n",
      "trainer/Log Pis Mean                                   6.20893\n",
      "trainer/Log Pis Std                                    5.67582\n",
      "trainer/Log Pis Max                                   25.2326\n",
      "trainer/Log Pis Min                                   -6.12053\n",
      "trainer/Policy mu Mean                                 0.0854838\n",
      "trainer/Policy mu Std                                  1.63563\n",
      "trainer/Policy mu Max                                  4.82998\n",
      "trainer/Policy mu Min                                 -4.98631\n",
      "trainer/Policy log std Mean                           -0.71238\n",
      "trainer/Policy log std Std                             0.280535\n",
      "trainer/Policy log std Max                             0.280484\n",
      "trainer/Policy log std Min                            -1.90195\n",
      "trainer/Alpha                                          0.0129553\n",
      "trainer/Alpha Loss                                     0.908247\n",
      "exploration/num steps total                        77000\n",
      "exploration/num paths total                           77\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.265629\n",
      "exploration/Rewards Std                                0.269109\n",
      "exploration/Rewards Max                                0.449979\n",
      "exploration/Rewards Min                               -1.25614\n",
      "exploration/Returns Mean                            -265.629\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -265.629\n",
      "exploration/Returns Min                             -265.629\n",
      "exploration/Actions Mean                              -0.0568723\n",
      "exploration/Actions Std                                0.639386\n",
      "exploration/Actions Max                                0.999887\n",
      "exploration/Actions Min                               -0.999997\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -265.629\n",
      "exploration/env_infos/final/reward_run Mean           -0.474615\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.474615\n",
      "exploration/env_infos/final/reward_run Min            -0.474615\n",
      "exploration/env_infos/initial/reward_run Mean         -0.652778\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.652778\n",
      "exploration/env_infos/initial/reward_run Min          -0.652778\n",
      "exploration/env_infos/reward_run Mean                  0.19268\n",
      "exploration/env_infos/reward_run Std                   0.761565\n",
      "exploration/env_infos/reward_run Max                   2.84793\n",
      "exploration/env_infos/reward_run Min                  -2.18106\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.296748\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.296748\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.296748\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.34772\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.34772\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.34772\n",
      "exploration/env_infos/reward_ctrl Mean                -0.247229\n",
      "exploration/env_infos/reward_ctrl Std                  0.1051\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0157911\n",
      "exploration/env_infos/reward_ctrl Min                 -0.577116\n",
      "exploration/env_infos/final/height Mean               -0.0354952\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0354952\n",
      "exploration/env_infos/final/height Min                -0.0354952\n",
      "exploration/env_infos/initial/height Mean              0.0209183\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0209183\n",
      "exploration/env_infos/initial/height Min               0.0209183\n",
      "exploration/env_infos/height Mean                     -0.044405\n",
      "exploration/env_infos/height Std                       0.103246\n",
      "exploration/env_infos/height Max                       0.248333\n",
      "exploration/env_infos/height Min                      -0.367275\n",
      "exploration/env_infos/final/reward_angular Mean        1.49105\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.49105\n",
      "exploration/env_infos/final/reward_angular Min         1.49105\n",
      "exploration/env_infos/initial/reward_angular Mean      2.12453\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.12453\n",
      "exploration/env_infos/initial/reward_angular Min       2.12453\n",
      "exploration/env_infos/reward_angular Mean              0.0161433\n",
      "exploration/env_infos/reward_angular Std               1.55217\n",
      "exploration/env_infos/reward_angular Max               4.86201\n",
      "exploration/env_infos/reward_angular Min              -6.30089\n",
      "evaluation/num steps total                             1.9e+06\n",
      "evaluation/num paths total                          1900\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.0503207\n",
      "evaluation/Rewards Std                                 1.13852\n",
      "evaluation/Rewards Max                                 7.51472\n",
      "evaluation/Rewards Min                                -9.44827\n",
      "evaluation/Returns Mean                              -50.3207\n",
      "evaluation/Returns Std                               413.505\n",
      "evaluation/Returns Max                              1578.5\n",
      "evaluation/Returns Min                              -476.629\n",
      "evaluation/Actions Mean                                0.0475978\n",
      "evaluation/Actions Std                                 0.781351\n",
      "evaluation/Actions Max                                 0.999997\n",
      "evaluation/Actions Min                                -0.999998\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           -50.3207\n",
      "evaluation/env_infos/final/reward_run Mean             0.487554\n",
      "evaluation/env_infos/final/reward_run Std              0.927252\n",
      "evaluation/env_infos/final/reward_run Max              2.41318\n",
      "evaluation/env_infos/final/reward_run Min             -1.18668\n",
      "evaluation/env_infos/initial/reward_run Mean           0.448991\n",
      "evaluation/env_infos/initial/reward_run Std            0.521239\n",
      "evaluation/env_infos/initial/reward_run Max            1.08148\n",
      "evaluation/env_infos/initial/reward_run Min           -0.732353\n",
      "evaluation/env_infos/reward_run Mean                   0.496968\n",
      "evaluation/env_infos/reward_run Std                    1.02627\n",
      "evaluation/env_infos/reward_run Max                    4.38543\n",
      "evaluation/env_infos/reward_run Min                   -3.34592\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.39156\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0733317\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.245606\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.503741\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.33522\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.151042\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0729654\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.537788\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.367665\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0872378\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00830597\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.592824\n",
      "evaluation/env_infos/final/height Mean                -0.407613\n",
      "evaluation/env_infos/final/height Std                  0.164806\n",
      "evaluation/env_infos/final/height Max                  0.0748244\n",
      "evaluation/env_infos/final/height Min                 -0.577275\n",
      "evaluation/env_infos/initial/height Mean              -0.0162728\n",
      "evaluation/env_infos/initial/height Std                0.0511565\n",
      "evaluation/env_infos/initial/height Max                0.0865064\n",
      "evaluation/env_infos/initial/height Min               -0.0935921\n",
      "evaluation/env_infos/height Mean                      -0.394658\n",
      "evaluation/env_infos/height Std                        0.173847\n",
      "evaluation/env_infos/height Max                        0.291377\n",
      "evaluation/env_infos/height Min                       -0.597456\n",
      "evaluation/env_infos/final/reward_angular Mean         0.0101259\n",
      "evaluation/env_infos/final/reward_angular Std          1.67588\n",
      "evaluation/env_infos/final/reward_angular Max          2.80227\n",
      "evaluation/env_infos/final/reward_angular Min         -4.55073\n",
      "evaluation/env_infos/initial/reward_angular Mean       0.231313\n",
      "evaluation/env_infos/initial/reward_angular Std        1.45798\n",
      "evaluation/env_infos/initial/reward_angular Max        4.34584\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.664\n",
      "evaluation/env_infos/reward_angular Mean              -0.00848186\n",
      "evaluation/env_infos/reward_angular Std                1.69489\n",
      "evaluation/env_infos/reward_angular Max                8.95669\n",
      "evaluation/env_infos/reward_angular Min              -10.5514\n",
      "time/data storing (s)                                  0.0154603\n",
      "time/evaluation sampling (s)                          20.9933\n",
      "time/exploration sampling (s)                          1.01046\n",
      "time/logging (s)                                       0.237373\n",
      "time/saving (s)                                        0.026349\n",
      "time/training (s)                                      3.84767\n",
      "time/epoch (s)                                        26.1306\n",
      "time/total (s)                                      2222.63\n",
      "Epoch                                                 75\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:49:31.623872 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 76 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 78000\n",
      "trainer/QF1 Loss                                       0.593577\n",
      "trainer/QF2 Loss                                       0.582467\n",
      "trainer/Policy Loss                                    2.71448\n",
      "trainer/Q1 Predictions Mean                            4.02605\n",
      "trainer/Q1 Predictions Std                             9.78865\n",
      "trainer/Q1 Predictions Max                            44.5545\n",
      "trainer/Q1 Predictions Min                            -8.03051\n",
      "trainer/Q2 Predictions Mean                            4.0974\n",
      "trainer/Q2 Predictions Std                             9.64594\n",
      "trainer/Q2 Predictions Max                            43.7294\n",
      "trainer/Q2 Predictions Min                            -8.29231\n",
      "trainer/Q Targets Mean                                 4.03136\n",
      "trainer/Q Targets Std                                  9.79019\n",
      "trainer/Q Targets Max                                 46.0613\n",
      "trainer/Q Targets Min                                 -8.54726\n",
      "trainer/Log Pis Mean                                   7.03339\n",
      "trainer/Log Pis Std                                    5.65641\n",
      "trainer/Log Pis Max                                   29.3781\n",
      "trainer/Log Pis Min                                   -6.97978\n",
      "trainer/Policy mu Mean                                 0.172546\n",
      "trainer/Policy mu Std                                  1.69115\n",
      "trainer/Policy mu Max                                  5.4913\n",
      "trainer/Policy mu Min                                 -6.44127\n",
      "trainer/Policy log std Mean                           -0.752758\n",
      "trainer/Policy log std Std                             0.319703\n",
      "trainer/Policy log std Max                             0.290971\n",
      "trainer/Policy log std Min                            -3.01928\n",
      "trainer/Alpha                                          0.0119662\n",
      "trainer/Alpha Loss                                     4.57489\n",
      "exploration/num steps total                        78000\n",
      "exploration/num paths total                           78\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.18177\n",
      "exploration/Rewards Std                                1.06993\n",
      "exploration/Rewards Max                                4.32913\n",
      "exploration/Rewards Min                               -2.95435\n",
      "exploration/Returns Mean                            -181.77\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -181.77\n",
      "exploration/Returns Min                             -181.77\n",
      "exploration/Actions Mean                              -0.18004\n",
      "exploration/Actions Std                                0.689055\n",
      "exploration/Actions Max                                0.999647\n",
      "exploration/Actions Min                               -0.999885\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -181.77\n",
      "exploration/env_infos/final/reward_run Mean           -0.845435\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.845435\n",
      "exploration/env_infos/final/reward_run Min            -0.845435\n",
      "exploration/env_infos/initial/reward_run Mean         -0.202647\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.202647\n",
      "exploration/env_infos/initial/reward_run Min          -0.202647\n",
      "exploration/env_infos/reward_run Mean                 -0.0557067\n",
      "exploration/env_infos/reward_run Std                   0.706654\n",
      "exploration/env_infos/reward_run Max                   1.84207\n",
      "exploration/env_infos/reward_run Min                  -2.68931\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.318608\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.318608\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.318608\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.25985\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.25985\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.25985\n",
      "exploration/env_infos/reward_ctrl Mean                -0.304327\n",
      "exploration/env_infos/reward_ctrl Std                  0.0906099\n",
      "exploration/env_infos/reward_ctrl Max                 -0.036058\n",
      "exploration/env_infos/reward_ctrl Min                 -0.588725\n",
      "exploration/env_infos/final/height Mean               -0.562529\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.562529\n",
      "exploration/env_infos/final/height Min                -0.562529\n",
      "exploration/env_infos/initial/height Mean             -0.0675903\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0675903\n",
      "exploration/env_infos/initial/height Min              -0.0675903\n",
      "exploration/env_infos/height Mean                     -0.402879\n",
      "exploration/env_infos/height Std                       0.207158\n",
      "exploration/env_infos/height Max                       0.314016\n",
      "exploration/env_infos/height Min                      -0.586864\n",
      "exploration/env_infos/final/reward_angular Mean        0.485813\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.485813\n",
      "exploration/env_infos/final/reward_angular Min         0.485813\n",
      "exploration/env_infos/initial/reward_angular Mean      2.21008\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.21008\n",
      "exploration/env_infos/initial/reward_angular Min       2.21008\n",
      "exploration/env_infos/reward_angular Mean              0.105494\n",
      "exploration/env_infos/reward_angular Std               1.74849\n",
      "exploration/env_infos/reward_angular Max               6.88421\n",
      "exploration/env_infos/reward_angular Min              -4.44862\n",
      "evaluation/num steps total                             1.925e+06\n",
      "evaluation/num paths total                          1925\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0808466\n",
      "evaluation/Rewards Std                                 1.13403\n",
      "evaluation/Rewards Max                                 7.44304\n",
      "evaluation/Rewards Min                                -6.94964\n",
      "evaluation/Returns Mean                               80.8466\n",
      "evaluation/Returns Std                               366.618\n",
      "evaluation/Returns Max                              1413.56\n",
      "evaluation/Returns Min                              -354.551\n",
      "evaluation/Actions Mean                                0.0741049\n",
      "evaluation/Actions Std                                 0.75502\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            80.8466\n",
      "evaluation/env_infos/final/reward_run Mean             0.137911\n",
      "evaluation/env_infos/final/reward_run Std              1.13938\n",
      "evaluation/env_infos/final/reward_run Max              1.97474\n",
      "evaluation/env_infos/final/reward_run Min             -2.83719\n",
      "evaluation/env_infos/initial/reward_run Mean           0.384017\n",
      "evaluation/env_infos/initial/reward_run Std            0.465017\n",
      "evaluation/env_infos/initial/reward_run Max            1.18877\n",
      "evaluation/env_infos/initial/reward_run Min           -0.62235\n",
      "evaluation/env_infos/reward_run Mean                   0.0375222\n",
      "evaluation/env_infos/reward_run Std                    1.49021\n",
      "evaluation/env_infos/reward_run Max                    4.5708\n",
      "evaluation/env_infos/reward_run Min                   -5.04437\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.363182\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.1043\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.13349\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.589578\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.315962\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0912408\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0499249\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.398758\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.345328\n",
      "evaluation/env_infos/reward_ctrl Std                   0.103293\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0177136\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598164\n",
      "evaluation/env_infos/final/height Mean                -0.306111\n",
      "evaluation/env_infos/final/height Std                  0.182435\n",
      "evaluation/env_infos/final/height Max                  0.0290459\n",
      "evaluation/env_infos/final/height Min                 -0.577273\n",
      "evaluation/env_infos/initial/height Mean              -0.0161253\n",
      "evaluation/env_infos/initial/height Std                0.0476562\n",
      "evaluation/env_infos/initial/height Max                0.0795613\n",
      "evaluation/env_infos/initial/height Min               -0.0843676\n",
      "evaluation/env_infos/height Mean                      -0.265415\n",
      "evaluation/env_infos/height Std                        0.199008\n",
      "evaluation/env_infos/height Max                        0.386416\n",
      "evaluation/env_infos/height Min                       -0.595842\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.06718\n",
      "evaluation/env_infos/final/reward_angular Std          1.40061\n",
      "evaluation/env_infos/final/reward_angular Max          2.12742\n",
      "evaluation/env_infos/final/reward_angular Min         -2.83064\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.613613\n",
      "evaluation/env_infos/initial/reward_angular Std        1.2395\n",
      "evaluation/env_infos/initial/reward_angular Max        3.45152\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.79441\n",
      "evaluation/env_infos/reward_angular Mean               0.0242119\n",
      "evaluation/env_infos/reward_angular Std                1.77812\n",
      "evaluation/env_infos/reward_angular Max                8.83758\n",
      "evaluation/env_infos/reward_angular Min               -9.20403\n",
      "time/data storing (s)                                  0.0157718\n",
      "time/evaluation sampling (s)                          20.5563\n",
      "time/exploration sampling (s)                          0.988975\n",
      "time/logging (s)                                       0.246355\n",
      "time/saving (s)                                        0.0277148\n",
      "time/training (s)                                      3.92438\n",
      "time/epoch (s)                                        25.7595\n",
      "time/total (s)                                      2249\n",
      "Epoch                                                 76\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:49:58.164381 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 77 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 79000\n",
      "trainer/QF1 Loss                                       0.630679\n",
      "trainer/QF2 Loss                                       0.561887\n",
      "trainer/Policy Loss                                    1.33596\n",
      "trainer/Q1 Predictions Mean                            4.25697\n",
      "trainer/Q1 Predictions Std                            10.1135\n",
      "trainer/Q1 Predictions Max                            48.1778\n",
      "trainer/Q1 Predictions Min                            -9.78212\n",
      "trainer/Q2 Predictions Mean                            4.13348\n",
      "trainer/Q2 Predictions Std                            10.0816\n",
      "trainer/Q2 Predictions Max                            46.5171\n",
      "trainer/Q2 Predictions Min                           -10.1658\n",
      "trainer/Q Targets Mean                                 4.08303\n",
      "trainer/Q Targets Std                                 10.1501\n",
      "trainer/Q Targets Max                                 46.6282\n",
      "trainer/Q Targets Min                                -11.0586\n",
      "trainer/Log Pis Mean                                   5.70641\n",
      "trainer/Log Pis Std                                    5.23674\n",
      "trainer/Log Pis Max                                   23.6644\n",
      "trainer/Log Pis Min                                   -5.11189\n",
      "trainer/Policy mu Mean                                 0.37358\n",
      "trainer/Policy mu Std                                  1.52383\n",
      "trainer/Policy mu Max                                  4.90665\n",
      "trainer/Policy mu Min                                 -5.64265\n",
      "trainer/Policy log std Mean                           -0.773172\n",
      "trainer/Policy log std Std                             0.282888\n",
      "trainer/Policy log std Max                             0.174333\n",
      "trainer/Policy log std Min                            -1.95621\n",
      "trainer/Alpha                                          0.0115753\n",
      "trainer/Alpha Loss                                    -1.30869\n",
      "exploration/num steps total                        79000\n",
      "exploration/num paths total                           79\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0693914\n",
      "exploration/Rewards Std                                1.54032\n",
      "exploration/Rewards Max                                6.08259\n",
      "exploration/Rewards Min                               -5.36373\n",
      "exploration/Returns Mean                             -69.3914\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -69.3914\n",
      "exploration/Returns Min                              -69.3914\n",
      "exploration/Actions Mean                               0.124072\n",
      "exploration/Actions Std                                0.768291\n",
      "exploration/Actions Max                                0.999994\n",
      "exploration/Actions Min                               -0.999996\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -69.3914\n",
      "exploration/env_infos/final/reward_run Mean           -0.228672\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.228672\n",
      "exploration/env_infos/final/reward_run Min            -0.228672\n",
      "exploration/env_infos/initial/reward_run Mean          1.3484\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.3484\n",
      "exploration/env_infos/initial/reward_run Min           1.3484\n",
      "exploration/env_infos/reward_run Mean                  0.502135\n",
      "exploration/env_infos/reward_run Std                   0.826875\n",
      "exploration/env_infos/reward_run Max                   2.49647\n",
      "exploration/env_infos/reward_run Min                  -1.84849\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.551499\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.551499\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.551499\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.464109\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.464109\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.464109\n",
      "exploration/env_infos/reward_ctrl Mean                -0.363399\n",
      "exploration/env_infos/reward_ctrl Std                  0.106729\n",
      "exploration/env_infos/reward_ctrl Max                 -0.044308\n",
      "exploration/env_infos/reward_ctrl Min                 -0.596296\n",
      "exploration/env_infos/final/height Mean               -0.57466\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.57466\n",
      "exploration/env_infos/final/height Min                -0.57466\n",
      "exploration/env_infos/initial/height Mean             -0.0356684\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0356684\n",
      "exploration/env_infos/initial/height Min              -0.0356684\n",
      "exploration/env_infos/height Mean                     -0.401925\n",
      "exploration/env_infos/height Std                       0.146777\n",
      "exploration/env_infos/height Max                       0.238827\n",
      "exploration/env_infos/height Min                      -0.592637\n",
      "exploration/env_infos/final/reward_angular Mean       -0.0287847\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.0287847\n",
      "exploration/env_infos/final/reward_angular Min        -0.0287847\n",
      "exploration/env_infos/initial/reward_angular Mean      1.56296\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.56296\n",
      "exploration/env_infos/initial/reward_angular Min       1.56296\n",
      "exploration/env_infos/reward_angular Mean              0.176832\n",
      "exploration/env_infos/reward_angular Std               2.15712\n",
      "exploration/env_infos/reward_angular Max               8.52855\n",
      "exploration/env_infos/reward_angular Min              -6.71794\n",
      "evaluation/num steps total                             1.95e+06\n",
      "evaluation/num paths total                          1950\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.175417\n",
      "evaluation/Rewards Std                                 1.2616\n",
      "evaluation/Rewards Max                                 7.00234\n",
      "evaluation/Rewards Min                                -6.08362\n",
      "evaluation/Returns Mean                              175.417\n",
      "evaluation/Returns Std                               531.312\n",
      "evaluation/Returns Max                              1675.95\n",
      "evaluation/Returns Min                              -377.287\n",
      "evaluation/Actions Mean                                0.181479\n",
      "evaluation/Actions Std                                 0.741085\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           175.417\n",
      "evaluation/env_infos/final/reward_run Mean             0.183573\n",
      "evaluation/env_infos/final/reward_run Std              1.24871\n",
      "evaluation/env_infos/final/reward_run Max              2.85779\n",
      "evaluation/env_infos/final/reward_run Min             -2.63103\n",
      "evaluation/env_infos/initial/reward_run Mean           0.415741\n",
      "evaluation/env_infos/initial/reward_run Std            0.511252\n",
      "evaluation/env_infos/initial/reward_run Max            1.3155\n",
      "evaluation/env_infos/initial/reward_run Min           -0.485062\n",
      "evaluation/env_infos/reward_run Mean                   0.0801394\n",
      "evaluation/env_infos/reward_run Std                    1.51621\n",
      "evaluation/env_infos/reward_run Max                    4.6855\n",
      "evaluation/env_infos/reward_run Min                   -5.07295\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.35725\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.124017\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0734449\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.548726\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.379172\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.102684\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.128283\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.532148\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.349285\n",
      "evaluation/env_infos/reward_ctrl Std                   0.106689\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0193693\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596767\n",
      "evaluation/env_infos/final/height Mean                -0.333041\n",
      "evaluation/env_infos/final/height Std                  0.188846\n",
      "evaluation/env_infos/final/height Max                  0.028246\n",
      "evaluation/env_infos/final/height Min                 -0.577955\n",
      "evaluation/env_infos/initial/height Mean              -0.00537922\n",
      "evaluation/env_infos/initial/height Std                0.0555392\n",
      "evaluation/env_infos/initial/height Max                0.0945368\n",
      "evaluation/env_infos/initial/height Min               -0.0871733\n",
      "evaluation/env_infos/height Mean                      -0.280241\n",
      "evaluation/env_infos/height Std                        0.203483\n",
      "evaluation/env_infos/height Max                        0.390587\n",
      "evaluation/env_infos/height Min                       -0.59269\n",
      "evaluation/env_infos/final/reward_angular Mean         0.371225\n",
      "evaluation/env_infos/final/reward_angular Std          1.75467\n",
      "evaluation/env_infos/final/reward_angular Max          4.09431\n",
      "evaluation/env_infos/final/reward_angular Min         -4.10457\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.249018\n",
      "evaluation/env_infos/initial/reward_angular Std        1.44501\n",
      "evaluation/env_infos/initial/reward_angular Max        3.47056\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.49419\n",
      "evaluation/env_infos/reward_angular Mean               0.0210797\n",
      "evaluation/env_infos/reward_angular Std                1.79905\n",
      "evaluation/env_infos/reward_angular Max                9.57018\n",
      "evaluation/env_infos/reward_angular Min               -7.16248\n",
      "time/data storing (s)                                  0.0158708\n",
      "time/evaluation sampling (s)                          20.7916\n",
      "time/exploration sampling (s)                          0.969255\n",
      "time/logging (s)                                       0.234175\n",
      "time/saving (s)                                        0.0279711\n",
      "time/training (s)                                      3.84304\n",
      "time/epoch (s)                                        25.8819\n",
      "time/total (s)                                      2275.53\n",
      "Epoch                                                 77\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:50:25.093055 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 78 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 80000\n",
      "trainer/QF1 Loss                                       0.535387\n",
      "trainer/QF2 Loss                                       0.476372\n",
      "trainer/Policy Loss                                    3.76954\n",
      "trainer/Q1 Predictions Mean                            2.8717\n",
      "trainer/Q1 Predictions Std                             7.99125\n",
      "trainer/Q1 Predictions Max                            35.659\n",
      "trainer/Q1 Predictions Min                            -8.17464\n",
      "trainer/Q2 Predictions Mean                            3.07216\n",
      "trainer/Q2 Predictions Std                             8.08915\n",
      "trainer/Q2 Predictions Max                            35.1756\n",
      "trainer/Q2 Predictions Min                            -8.58943\n",
      "trainer/Q Targets Mean                                 2.99458\n",
      "trainer/Q Targets Std                                  7.99938\n",
      "trainer/Q Targets Max                                 35.3563\n",
      "trainer/Q Targets Min                                 -8.47473\n",
      "trainer/Log Pis Mean                                   6.97661\n",
      "trainer/Log Pis Std                                    5.87462\n",
      "trainer/Log Pis Max                                   26.0133\n",
      "trainer/Log Pis Min                                   -3.94968\n",
      "trainer/Policy mu Mean                                 0.170348\n",
      "trainer/Policy mu Std                                  1.71085\n",
      "trainer/Policy mu Max                                  5.98134\n",
      "trainer/Policy mu Min                                 -6.34524\n",
      "trainer/Policy log std Mean                           -0.74263\n",
      "trainer/Policy log std Std                             0.28438\n",
      "trainer/Policy log std Max                             0.118531\n",
      "trainer/Policy log std Min                            -2.05544\n",
      "trainer/Alpha                                          0.0120391\n",
      "trainer/Alpha Loss                                     4.31978\n",
      "exploration/num steps total                        80000\n",
      "exploration/num paths total                           80\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.421662\n",
      "exploration/Rewards Std                                0.668835\n",
      "exploration/Rewards Max                                2.18114\n",
      "exploration/Rewards Min                               -2.24735\n",
      "exploration/Returns Mean                            -421.662\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -421.662\n",
      "exploration/Returns Min                             -421.662\n",
      "exploration/Actions Mean                              -0.0733498\n",
      "exploration/Actions Std                                0.641022\n",
      "exploration/Actions Max                                0.998233\n",
      "exploration/Actions Min                               -0.99983\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -421.662\n",
      "exploration/env_infos/final/reward_run Mean           -0.357126\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.357126\n",
      "exploration/env_infos/final/reward_run Min            -0.357126\n",
      "exploration/env_infos/initial/reward_run Mean          1.05311\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.05311\n",
      "exploration/env_infos/initial/reward_run Min           1.05311\n",
      "exploration/env_infos/reward_run Mean                  0.15512\n",
      "exploration/env_infos/reward_run Std                   0.708836\n",
      "exploration/env_infos/reward_run Max                   2.10678\n",
      "exploration/env_infos/reward_run Min                  -1.92903\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.240186\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.240186\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.240186\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.500328\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.500328\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.500328\n",
      "exploration/env_infos/reward_ctrl Mean                -0.249774\n",
      "exploration/env_infos/reward_ctrl Std                  0.0952955\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0226161\n",
      "exploration/env_infos/reward_ctrl Min                 -0.516424\n",
      "exploration/env_infos/final/height Mean               -0.0813701\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0813701\n",
      "exploration/env_infos/final/height Min                -0.0813701\n",
      "exploration/env_infos/initial/height Mean             -0.0131351\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0131351\n",
      "exploration/env_infos/initial/height Min              -0.0131351\n",
      "exploration/env_infos/height Mean                     -0.426761\n",
      "exploration/env_infos/height Std                       0.163601\n",
      "exploration/env_infos/height Max                       0.268169\n",
      "exploration/env_infos/height Min                      -0.586783\n",
      "exploration/env_infos/final/reward_angular Mean       -0.942126\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.942126\n",
      "exploration/env_infos/final/reward_angular Min        -0.942126\n",
      "exploration/env_infos/initial/reward_angular Mean      2.14282\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.14282\n",
      "exploration/env_infos/initial/reward_angular Min       2.14282\n",
      "exploration/env_infos/reward_angular Mean              0.0880689\n",
      "exploration/env_infos/reward_angular Std               1.70903\n",
      "exploration/env_infos/reward_angular Max               7.34035\n",
      "exploration/env_infos/reward_angular Min              -4.58559\n",
      "evaluation/num steps total                             1.975e+06\n",
      "evaluation/num paths total                          1975\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.102952\n",
      "evaluation/Rewards Std                                 1.25112\n",
      "evaluation/Rewards Max                                 6.63196\n",
      "evaluation/Rewards Min                                -6.60529\n",
      "evaluation/Returns Mean                              102.952\n",
      "evaluation/Returns Std                               410.655\n",
      "evaluation/Returns Max                              1728.13\n",
      "evaluation/Returns Min                              -320.659\n",
      "evaluation/Actions Mean                                0.0790525\n",
      "evaluation/Actions Std                                 0.797607\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           102.952\n",
      "evaluation/env_infos/final/reward_run Mean             0.451511\n",
      "evaluation/env_infos/final/reward_run Std              1.17498\n",
      "evaluation/env_infos/final/reward_run Max              3.47748\n",
      "evaluation/env_infos/final/reward_run Min             -1.91712\n",
      "evaluation/env_infos/initial/reward_run Mean           0.53685\n",
      "evaluation/env_infos/initial/reward_run Std            0.507506\n",
      "evaluation/env_infos/initial/reward_run Max            1.35548\n",
      "evaluation/env_infos/initial/reward_run Min           -0.26157\n",
      "evaluation/env_infos/reward_run Mean                   0.203142\n",
      "evaluation/env_infos/reward_run Std                    1.61832\n",
      "evaluation/env_infos/reward_run Max                    4.7678\n",
      "evaluation/env_infos/reward_run Min                   -5.49921\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.429028\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.119601\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.180591\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.556765\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.406763\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.138774\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0672835\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.586713\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.385456\n",
      "evaluation/env_infos/reward_ctrl Std                   0.117318\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0333589\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.595133\n",
      "evaluation/env_infos/final/height Mean                -0.380439\n",
      "evaluation/env_infos/final/height Std                  0.189411\n",
      "evaluation/env_infos/final/height Max                 -0.0267198\n",
      "evaluation/env_infos/final/height Min                 -0.577269\n",
      "evaluation/env_infos/initial/height Mean              -0.0194446\n",
      "evaluation/env_infos/initial/height Std                0.0381591\n",
      "evaluation/env_infos/initial/height Max                0.045606\n",
      "evaluation/env_infos/initial/height Min               -0.0678488\n",
      "evaluation/env_infos/height Mean                      -0.287387\n",
      "evaluation/env_infos/height Std                        0.229067\n",
      "evaluation/env_infos/height Max                        0.456333\n",
      "evaluation/env_infos/height Min                       -0.596484\n",
      "evaluation/env_infos/final/reward_angular Mean         0.241914\n",
      "evaluation/env_infos/final/reward_angular Std          1.34486\n",
      "evaluation/env_infos/final/reward_angular Max          3.11318\n",
      "evaluation/env_infos/final/reward_angular Min         -2.89921\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.114685\n",
      "evaluation/env_infos/initial/reward_angular Std        1.27649\n",
      "evaluation/env_infos/initial/reward_angular Max        1.91716\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.22305\n",
      "evaluation/env_infos/reward_angular Mean               0.013966\n",
      "evaluation/env_infos/reward_angular Std                1.8507\n",
      "evaluation/env_infos/reward_angular Max                8.70558\n",
      "evaluation/env_infos/reward_angular Min               -7.55881\n",
      "time/data storing (s)                                  0.0152408\n",
      "time/evaluation sampling (s)                          20.8571\n",
      "time/exploration sampling (s)                          1.06375\n",
      "time/logging (s)                                       0.238428\n",
      "time/saving (s)                                        0.026071\n",
      "time/training (s)                                      4.11315\n",
      "time/epoch (s)                                        26.3137\n",
      "time/total (s)                                      2302.46\n",
      "Epoch                                                 78\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:50:51.789377 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 79 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 81000\n",
      "trainer/QF1 Loss                                       0.613457\n",
      "trainer/QF2 Loss                                       0.485087\n",
      "trainer/Policy Loss                                    3.43656\n",
      "trainer/Q1 Predictions Mean                            3.79484\n",
      "trainer/Q1 Predictions Std                             9.62293\n",
      "trainer/Q1 Predictions Max                            42.7544\n",
      "trainer/Q1 Predictions Min                           -22.1249\n",
      "trainer/Q2 Predictions Mean                            3.76129\n",
      "trainer/Q2 Predictions Std                             9.48959\n",
      "trainer/Q2 Predictions Max                            39.9559\n",
      "trainer/Q2 Predictions Min                           -18.8152\n",
      "trainer/Q Targets Mean                                 3.83705\n",
      "trainer/Q Targets Std                                  9.60201\n",
      "trainer/Q Targets Max                                 42.2689\n",
      "trainer/Q Targets Min                                -17.5722\n",
      "trainer/Log Pis Mean                                   7.54912\n",
      "trainer/Log Pis Std                                    5.95563\n",
      "trainer/Log Pis Max                                   25.4846\n",
      "trainer/Log Pis Min                                   -4.54963\n",
      "trainer/Policy mu Mean                                 0.263993\n",
      "trainer/Policy mu Std                                  1.72689\n",
      "trainer/Policy mu Max                                  4.88253\n",
      "trainer/Policy mu Min                                 -5.3169\n",
      "trainer/Policy log std Mean                           -0.768391\n",
      "trainer/Policy log std Std                             0.29282\n",
      "trainer/Policy log std Max                             0.5027\n",
      "trainer/Policy log std Min                            -2.31395\n",
      "trainer/Alpha                                          0.0125718\n",
      "trainer/Alpha Loss                                     6.78577\n",
      "exploration/num steps total                        81000\n",
      "exploration/num paths total                           81\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.15225\n",
      "exploration/Rewards Std                                1.29385\n",
      "exploration/Rewards Max                                5.31108\n",
      "exploration/Rewards Min                               -4.44139\n",
      "exploration/Returns Mean                             152.25\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              152.25\n",
      "exploration/Returns Min                              152.25\n",
      "exploration/Actions Mean                              -0.1077\n",
      "exploration/Actions Std                                0.789716\n",
      "exploration/Actions Max                                0.999997\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          152.25\n",
      "exploration/env_infos/final/reward_run Mean            0.08631\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.08631\n",
      "exploration/env_infos/final/reward_run Min             0.08631\n",
      "exploration/env_infos/initial/reward_run Mean          0.690524\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.690524\n",
      "exploration/env_infos/initial/reward_run Min           0.690524\n",
      "exploration/env_infos/reward_run Mean                  0.289152\n",
      "exploration/env_infos/reward_run Std                   0.537855\n",
      "exploration/env_infos/reward_run Max                   2.69818\n",
      "exploration/env_infos/reward_run Min                  -3.08618\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.327214\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.327214\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.327214\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.489014\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.489014\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.489014\n",
      "exploration/env_infos/reward_ctrl Mean                -0.381151\n",
      "exploration/env_infos/reward_ctrl Std                  0.0892003\n",
      "exploration/env_infos/reward_ctrl Max                 -0.134871\n",
      "exploration/env_infos/reward_ctrl Min                 -0.596768\n",
      "exploration/env_infos/final/height Mean               -0.552134\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.552134\n",
      "exploration/env_infos/final/height Min                -0.552134\n",
      "exploration/env_infos/initial/height Mean              0.028341\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.028341\n",
      "exploration/env_infos/initial/height Min               0.028341\n",
      "exploration/env_infos/height Mean                     -0.131034\n",
      "exploration/env_infos/height Std                       0.185201\n",
      "exploration/env_infos/height Max                       0.322065\n",
      "exploration/env_infos/height Min                      -0.591065\n",
      "exploration/env_infos/final/reward_angular Mean        0.305213\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.305213\n",
      "exploration/env_infos/final/reward_angular Min         0.305213\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.235368\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.235368\n",
      "exploration/env_infos/initial/reward_angular Min      -0.235368\n",
      "exploration/env_infos/reward_angular Mean              0.196571\n",
      "exploration/env_infos/reward_angular Std               1.3981\n",
      "exploration/env_infos/reward_angular Max               6.50629\n",
      "exploration/env_infos/reward_angular Min              -4.98725\n",
      "evaluation/num steps total                             2e+06\n",
      "evaluation/num paths total                          2000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                               -0.00111373\n",
      "evaluation/Rewards Std                                 1.14069\n",
      "evaluation/Rewards Max                                 8.86892\n",
      "evaluation/Rewards Min                                -6.75092\n",
      "evaluation/Returns Mean                               -1.11373\n",
      "evaluation/Returns Std                               416.637\n",
      "evaluation/Returns Max                              1560.75\n",
      "evaluation/Returns Min                              -637.006\n",
      "evaluation/Actions Mean                                0.018452\n",
      "evaluation/Actions Std                                 0.819223\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            -1.11373\n",
      "evaluation/env_infos/final/reward_run Mean             0.358825\n",
      "evaluation/env_infos/final/reward_run Std              1.17991\n",
      "evaluation/env_infos/final/reward_run Max              2.34546\n",
      "evaluation/env_infos/final/reward_run Min             -2.53101\n",
      "evaluation/env_infos/initial/reward_run Mean           0.476068\n",
      "evaluation/env_infos/initial/reward_run Std            0.502518\n",
      "evaluation/env_infos/initial/reward_run Max            1.17198\n",
      "evaluation/env_infos/initial/reward_run Min           -0.688577\n",
      "evaluation/env_infos/reward_run Mean                   0.433583\n",
      "evaluation/env_infos/reward_run Std                    1.27641\n",
      "evaluation/env_infos/reward_run Max                    4.40172\n",
      "evaluation/env_infos/reward_run Min                   -5.10672\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.438476\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0832754\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.27448\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.578283\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.386795\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.116533\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.135412\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.560645\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.402881\n",
      "evaluation/env_infos/reward_ctrl Std                   0.105508\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0338682\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.59879\n",
      "evaluation/env_infos/final/height Mean                -0.351355\n",
      "evaluation/env_infos/final/height Std                  0.215389\n",
      "evaluation/env_infos/final/height Max                  0.0761859\n",
      "evaluation/env_infos/final/height Min                 -0.57727\n",
      "evaluation/env_infos/initial/height Mean               0.00360696\n",
      "evaluation/env_infos/initial/height Std                0.0492615\n",
      "evaluation/env_infos/initial/height Max                0.0869178\n",
      "evaluation/env_infos/initial/height Min               -0.0935589\n",
      "evaluation/env_infos/height Mean                      -0.317801\n",
      "evaluation/env_infos/height Std                        0.217003\n",
      "evaluation/env_infos/height Max                        0.511458\n",
      "evaluation/env_infos/height Min                       -0.59699\n",
      "evaluation/env_infos/final/reward_angular Mean         0.508145\n",
      "evaluation/env_infos/final/reward_angular Std          1.36426\n",
      "evaluation/env_infos/final/reward_angular Max          3.91478\n",
      "evaluation/env_infos/final/reward_angular Min         -1.35596\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.363688\n",
      "evaluation/env_infos/initial/reward_angular Std        1.24563\n",
      "evaluation/env_infos/initial/reward_angular Max        2.86827\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.15566\n",
      "evaluation/env_infos/reward_angular Mean               0.00145661\n",
      "evaluation/env_infos/reward_angular Std                1.80604\n",
      "evaluation/env_infos/reward_angular Max                9.94638\n",
      "evaluation/env_infos/reward_angular Min               -9.52788\n",
      "time/data storing (s)                                  0.0153993\n",
      "time/evaluation sampling (s)                          20.858\n",
      "time/exploration sampling (s)                          1.02275\n",
      "time/logging (s)                                       0.239504\n",
      "time/saving (s)                                        0.0268779\n",
      "time/training (s)                                      3.8475\n",
      "time/epoch (s)                                        26.0101\n",
      "time/total (s)                                      2329.15\n",
      "Epoch                                                 79\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:51:18.119007 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 80 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 82000\n",
      "trainer/QF1 Loss                                       0.560091\n",
      "trainer/QF2 Loss                                       0.635598\n",
      "trainer/Policy Loss                                    2.62316\n",
      "trainer/Q1 Predictions Mean                            2.70272\n",
      "trainer/Q1 Predictions Std                             8.87354\n",
      "trainer/Q1 Predictions Max                            36.1204\n",
      "trainer/Q1 Predictions Min                           -18.5613\n",
      "trainer/Q2 Predictions Mean                            2.67794\n",
      "trainer/Q2 Predictions Std                             8.96025\n",
      "trainer/Q2 Predictions Max                            36.4178\n",
      "trainer/Q2 Predictions Min                           -17.6826\n",
      "trainer/Q Targets Mean                                 2.76093\n",
      "trainer/Q Targets Std                                  9.03005\n",
      "trainer/Q Targets Max                                 35.5744\n",
      "trainer/Q Targets Min                                -18.5244\n",
      "trainer/Log Pis Mean                                   5.5822\n",
      "trainer/Log Pis Std                                    5.43705\n",
      "trainer/Log Pis Max                                   22.8717\n",
      "trainer/Log Pis Min                                   -3.99238\n",
      "trainer/Policy mu Mean                                 0.17508\n",
      "trainer/Policy mu Std                                  1.57329\n",
      "trainer/Policy mu Max                                  4.86274\n",
      "trainer/Policy mu Min                                 -5.35767\n",
      "trainer/Policy log std Mean                           -0.724435\n",
      "trainer/Policy log std Std                             0.259817\n",
      "trainer/Policy log std Max                             0.463963\n",
      "trainer/Policy log std Min                            -1.76124\n",
      "trainer/Alpha                                          0.0129429\n",
      "trainer/Alpha Loss                                    -1.81609\n",
      "exploration/num steps total                        82000\n",
      "exploration/num paths total                           82\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.494863\n",
      "exploration/Rewards Std                                1.41794\n",
      "exploration/Rewards Max                                4.39474\n",
      "exploration/Rewards Min                               -4.59775\n",
      "exploration/Returns Mean                             494.863\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              494.863\n",
      "exploration/Returns Min                              494.863\n",
      "exploration/Actions Mean                               0.0319258\n",
      "exploration/Actions Std                                0.842556\n",
      "exploration/Actions Max                                0.999981\n",
      "exploration/Actions Min                               -0.999995\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          494.863\n",
      "exploration/env_infos/final/reward_run Mean           -0.810442\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.810442\n",
      "exploration/env_infos/final/reward_run Min            -0.810442\n",
      "exploration/env_infos/initial/reward_run Mean          0.0178319\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0178319\n",
      "exploration/env_infos/initial/reward_run Min           0.0178319\n",
      "exploration/env_infos/reward_run Mean                 -1.57683\n",
      "exploration/env_infos/reward_run Std                   1.51487\n",
      "exploration/env_infos/reward_run Max                   2.56512\n",
      "exploration/env_infos/reward_run Min                  -4.79009\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.270712\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.270712\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.270712\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.361254\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.361254\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.361254\n",
      "exploration/env_infos/reward_ctrl Mean                -0.426551\n",
      "exploration/env_infos/reward_ctrl Std                  0.0941028\n",
      "exploration/env_infos/reward_ctrl Max                 -0.116648\n",
      "exploration/env_infos/reward_ctrl Min                 -0.598261\n",
      "exploration/env_infos/final/height Mean               -0.558843\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.558843\n",
      "exploration/env_infos/final/height Min                -0.558843\n",
      "exploration/env_infos/initial/height Mean              0.0422461\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0422461\n",
      "exploration/env_infos/initial/height Min               0.0422461\n",
      "exploration/env_infos/height Mean                     -0.229965\n",
      "exploration/env_infos/height Std                       0.255784\n",
      "exploration/env_infos/height Max                       0.352097\n",
      "exploration/env_infos/height Min                      -0.583872\n",
      "exploration/env_infos/final/reward_angular Mean       -1.10371\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.10371\n",
      "exploration/env_infos/final/reward_angular Min        -1.10371\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.877304\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.877304\n",
      "exploration/env_infos/initial/reward_angular Min      -0.877304\n",
      "exploration/env_infos/reward_angular Mean             -0.029618\n",
      "exploration/env_infos/reward_angular Std               2.25847\n",
      "exploration/env_infos/reward_angular Max               7.57201\n",
      "exploration/env_infos/reward_angular Min              -6.48589\n",
      "evaluation/num steps total                             2.025e+06\n",
      "evaluation/num paths total                          2025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.416258\n",
      "evaluation/Rewards Std                                 1.42079\n",
      "evaluation/Rewards Max                                 6.99852\n",
      "evaluation/Rewards Min                                -6.56169\n",
      "evaluation/Returns Mean                              416.258\n",
      "evaluation/Returns Std                               619.775\n",
      "evaluation/Returns Max                              1774.59\n",
      "evaluation/Returns Min                              -291.426\n",
      "evaluation/Actions Mean                                0.106624\n",
      "evaluation/Actions Std                                 0.773245\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           416.258\n",
      "evaluation/env_infos/final/reward_run Mean            -0.525352\n",
      "evaluation/env_infos/final/reward_run Std              2.1316\n",
      "evaluation/env_infos/final/reward_run Max              2.54592\n",
      "evaluation/env_infos/final/reward_run Min             -3.62089\n",
      "evaluation/env_infos/initial/reward_run Mean           0.48221\n",
      "evaluation/env_infos/initial/reward_run Std            0.570368\n",
      "evaluation/env_infos/initial/reward_run Max            1.16063\n",
      "evaluation/env_infos/initial/reward_run Min           -0.747972\n",
      "evaluation/env_infos/reward_run Mean                  -0.39492\n",
      "evaluation/env_infos/reward_run Std                    2.02653\n",
      "evaluation/env_infos/reward_run Max                    4.53587\n",
      "evaluation/env_infos/reward_run Min                   -5.2365\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.388514\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0995968\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.161344\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.557869\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.356494\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.122677\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0577453\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.551752\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.365566\n",
      "evaluation/env_infos/reward_ctrl Std                   0.105515\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0198385\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.595323\n",
      "evaluation/env_infos/final/height Mean                -0.115394\n",
      "evaluation/env_infos/final/height Std                  0.160014\n",
      "evaluation/env_infos/final/height Max                  0.258095\n",
      "evaluation/env_infos/final/height Min                 -0.548037\n",
      "evaluation/env_infos/initial/height Mean              -0.0133174\n",
      "evaluation/env_infos/initial/height Std                0.049743\n",
      "evaluation/env_infos/initial/height Max                0.0792389\n",
      "evaluation/env_infos/initial/height Min               -0.0825116\n",
      "evaluation/env_infos/height Mean                      -0.164443\n",
      "evaluation/env_infos/height Std                        0.168969\n",
      "evaluation/env_infos/height Max                        0.371775\n",
      "evaluation/env_infos/height Min                       -0.597959\n",
      "evaluation/env_infos/final/reward_angular Mean         0.221809\n",
      "evaluation/env_infos/final/reward_angular Std          2.00083\n",
      "evaluation/env_infos/final/reward_angular Max          4.3948\n",
      "evaluation/env_infos/final/reward_angular Min         -3.79878\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.0194049\n",
      "evaluation/env_infos/initial/reward_angular Std        1.4519\n",
      "evaluation/env_infos/initial/reward_angular Max        4.1675\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.16005\n",
      "evaluation/env_infos/reward_angular Mean               0.0546498\n",
      "evaluation/env_infos/reward_angular Std                2.12756\n",
      "evaluation/env_infos/reward_angular Max                9.66272\n",
      "evaluation/env_infos/reward_angular Min               -9.97689\n",
      "time/data storing (s)                                  0.0153046\n",
      "time/evaluation sampling (s)                          20.6211\n",
      "time/exploration sampling (s)                          0.974379\n",
      "time/logging (s)                                       0.237869\n",
      "time/saving (s)                                        0.0281899\n",
      "time/training (s)                                      3.82427\n",
      "time/epoch (s)                                        25.7011\n",
      "time/total (s)                                      2355.48\n",
      "Epoch                                                 80\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:51:44.563312 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 81 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 83000\n",
      "trainer/QF1 Loss                                       0.639644\n",
      "trainer/QF2 Loss                                       0.801097\n",
      "trainer/Policy Loss                                    1.47688\n",
      "trainer/Q1 Predictions Mean                            4.68783\n",
      "trainer/Q1 Predictions Std                            11.5052\n",
      "trainer/Q1 Predictions Max                            48.3823\n",
      "trainer/Q1 Predictions Min                           -25.1541\n",
      "trainer/Q2 Predictions Mean                            4.52293\n",
      "trainer/Q2 Predictions Std                            11.3316\n",
      "trainer/Q2 Predictions Max                            47.5535\n",
      "trainer/Q2 Predictions Min                           -22.4163\n",
      "trainer/Q Targets Mean                                 4.59614\n",
      "trainer/Q Targets Std                                 11.4325\n",
      "trainer/Q Targets Max                                 47.8772\n",
      "trainer/Q Targets Min                                -27.084\n",
      "trainer/Log Pis Mean                                   6.31524\n",
      "trainer/Log Pis Std                                    5.32812\n",
      "trainer/Log Pis Max                                   25.364\n",
      "trainer/Log Pis Min                                   -3.62128\n",
      "trainer/Policy mu Mean                                 0.158928\n",
      "trainer/Policy mu Std                                  1.59143\n",
      "trainer/Policy mu Max                                  4.12535\n",
      "trainer/Policy mu Min                                 -5.77714\n",
      "trainer/Policy log std Mean                           -0.769724\n",
      "trainer/Policy log std Std                             0.27756\n",
      "trainer/Policy log std Max                             0.0824268\n",
      "trainer/Policy log std Min                            -1.94258\n",
      "trainer/Alpha                                          0.0125353\n",
      "trainer/Alpha Loss                                     1.38052\n",
      "exploration/num steps total                        83000\n",
      "exploration/num paths total                           83\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.057035\n",
      "exploration/Rewards Std                                0.52106\n",
      "exploration/Rewards Max                                1.74612\n",
      "exploration/Rewards Min                               -1.89798\n",
      "exploration/Returns Mean                             -57.035\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -57.035\n",
      "exploration/Returns Min                              -57.035\n",
      "exploration/Actions Mean                               0.0669598\n",
      "exploration/Actions Std                                0.566913\n",
      "exploration/Actions Max                                0.997529\n",
      "exploration/Actions Min                               -0.999931\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -57.035\n",
      "exploration/env_infos/final/reward_run Mean            0.199583\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.199583\n",
      "exploration/env_infos/final/reward_run Min             0.199583\n",
      "exploration/env_infos/initial/reward_run Mean          0.871456\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.871456\n",
      "exploration/env_infos/initial/reward_run Min           0.871456\n",
      "exploration/env_infos/reward_run Mean                 -0.390579\n",
      "exploration/env_infos/reward_run Std                   0.726867\n",
      "exploration/env_infos/reward_run Max                   1.94043\n",
      "exploration/env_infos/reward_run Min                  -2.54353\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.200278\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.200278\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.200278\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.343532\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.343532\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.343532\n",
      "exploration/env_infos/reward_ctrl Mean                -0.195524\n",
      "exploration/env_infos/reward_ctrl Std                  0.0932498\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00414827\n",
      "exploration/env_infos/reward_ctrl Min                 -0.553738\n",
      "exploration/env_infos/final/height Mean               -0.0713995\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0713995\n",
      "exploration/env_infos/final/height Min                -0.0713995\n",
      "exploration/env_infos/initial/height Mean              0.0045665\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0045665\n",
      "exploration/env_infos/initial/height Min               0.0045665\n",
      "exploration/env_infos/height Mean                     -0.0625644\n",
      "exploration/env_infos/height Std                       0.0783606\n",
      "exploration/env_infos/height Max                       0.322941\n",
      "exploration/env_infos/height Min                      -0.228499\n",
      "exploration/env_infos/final/reward_angular Mean        0.283832\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.283832\n",
      "exploration/env_infos/final/reward_angular Min         0.283832\n",
      "exploration/env_infos/initial/reward_angular Mean      1.31926\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.31926\n",
      "exploration/env_infos/initial/reward_angular Min       1.31926\n",
      "exploration/env_infos/reward_angular Mean              0.00950288\n",
      "exploration/env_infos/reward_angular Std               1.48525\n",
      "exploration/env_infos/reward_angular Max               5.5842\n",
      "exploration/env_infos/reward_angular Min              -5.0036\n",
      "evaluation/num steps total                             2.05e+06\n",
      "evaluation/num paths total                          2050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.292357\n",
      "evaluation/Rewards Std                                 1.30225\n",
      "evaluation/Rewards Max                                 6.96317\n",
      "evaluation/Rewards Min                                -5.47607\n",
      "evaluation/Returns Mean                              292.357\n",
      "evaluation/Returns Std                               511.855\n",
      "evaluation/Returns Max                              1580.03\n",
      "evaluation/Returns Min                              -324.595\n",
      "evaluation/Actions Mean                                0.134501\n",
      "evaluation/Actions Std                                 0.765227\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -0.999994\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           292.357\n",
      "evaluation/env_infos/final/reward_run Mean             0.152511\n",
      "evaluation/env_infos/final/reward_run Std              1.60673\n",
      "evaluation/env_infos/final/reward_run Max              3.05037\n",
      "evaluation/env_infos/final/reward_run Min             -3.57142\n",
      "evaluation/env_infos/initial/reward_run Mean           0.44839\n",
      "evaluation/env_infos/initial/reward_run Std            0.387619\n",
      "evaluation/env_infos/initial/reward_run Max            1.04374\n",
      "evaluation/env_infos/initial/reward_run Min           -0.153355\n",
      "evaluation/env_infos/reward_run Mean                  -0.166524\n",
      "evaluation/env_infos/reward_run Std                    1.82928\n",
      "evaluation/env_infos/reward_run Max                    4.62644\n",
      "evaluation/env_infos/reward_run Min                   -5.27752\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.368785\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.121957\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0906988\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.550465\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.368546\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0916195\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.169018\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.552532\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.362197\n",
      "evaluation/env_infos/reward_ctrl Std                   0.114059\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0189951\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.597164\n",
      "evaluation/env_infos/final/height Mean                -0.187732\n",
      "evaluation/env_infos/final/height Std                  0.190016\n",
      "evaluation/env_infos/final/height Max                  0.0863667\n",
      "evaluation/env_infos/final/height Min                 -0.577458\n",
      "evaluation/env_infos/initial/height Mean              -0.00607948\n",
      "evaluation/env_infos/initial/height Std                0.0511143\n",
      "evaluation/env_infos/initial/height Max                0.0857384\n",
      "evaluation/env_infos/initial/height Min               -0.0899494\n",
      "evaluation/env_infos/height Mean                      -0.181858\n",
      "evaluation/env_infos/height Std                        0.174887\n",
      "evaluation/env_infos/height Max                        0.37194\n",
      "evaluation/env_infos/height Min                       -0.595168\n",
      "evaluation/env_infos/final/reward_angular Mean         0.160485\n",
      "evaluation/env_infos/final/reward_angular Std          2.3363\n",
      "evaluation/env_infos/final/reward_angular Max          6.2825\n",
      "evaluation/env_infos/final/reward_angular Min         -4.68194\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.671903\n",
      "evaluation/env_infos/initial/reward_angular Std        1.03562\n",
      "evaluation/env_infos/initial/reward_angular Max        2.15219\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.12783\n",
      "evaluation/env_infos/reward_angular Mean               0.0324305\n",
      "evaluation/env_infos/reward_angular Std                2.18785\n",
      "evaluation/env_infos/reward_angular Max                9.49484\n",
      "evaluation/env_infos/reward_angular Min               -8.61294\n",
      "time/data storing (s)                                  0.0155591\n",
      "time/evaluation sampling (s)                          20.7081\n",
      "time/exploration sampling (s)                          1.00997\n",
      "time/logging (s)                                       0.238391\n",
      "time/saving (s)                                        0.0277928\n",
      "time/training (s)                                      3.79407\n",
      "time/epoch (s)                                        25.7938\n",
      "time/total (s)                                      2381.92\n",
      "Epoch                                                 81\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:52:11.860977 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 82 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 84000\n",
      "trainer/QF1 Loss                                       0.861831\n",
      "trainer/QF2 Loss                                       1.00076\n",
      "trainer/Policy Loss                                    2.32209\n",
      "trainer/Q1 Predictions Mean                            2.91521\n",
      "trainer/Q1 Predictions Std                            10.8257\n",
      "trainer/Q1 Predictions Max                            49.8216\n",
      "trainer/Q1 Predictions Min                           -25.5294\n",
      "trainer/Q2 Predictions Mean                            3.06243\n",
      "trainer/Q2 Predictions Std                            10.9545\n",
      "trainer/Q2 Predictions Max                            52.0162\n",
      "trainer/Q2 Predictions Min                           -26.2296\n",
      "trainer/Q Targets Mean                                 3.03307\n",
      "trainer/Q Targets Std                                 10.9626\n",
      "trainer/Q Targets Max                                 50.2168\n",
      "trainer/Q Targets Min                                -29.9892\n",
      "trainer/Log Pis Mean                                   5.45292\n",
      "trainer/Log Pis Std                                    4.76191\n",
      "trainer/Log Pis Max                                   22.0759\n",
      "trainer/Log Pis Min                                   -4.21231\n",
      "trainer/Policy mu Mean                                 0.132781\n",
      "trainer/Policy mu Std                                  1.52585\n",
      "trainer/Policy mu Max                                  4.3763\n",
      "trainer/Policy mu Min                                 -5.41972\n",
      "trainer/Policy log std Mean                           -0.742851\n",
      "trainer/Policy log std Std                             0.263065\n",
      "trainer/Policy log std Max                             0.0648448\n",
      "trainer/Policy log std Min                            -2.31772\n",
      "trainer/Alpha                                          0.0120309\n",
      "trainer/Alpha Loss                                    -2.41724\n",
      "exploration/num steps total                        84000\n",
      "exploration/num paths total                           84\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.295272\n",
      "exploration/Rewards Std                                1.61405\n",
      "exploration/Rewards Max                                6.0069\n",
      "exploration/Rewards Min                               -4.51913\n",
      "exploration/Returns Mean                             295.272\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              295.272\n",
      "exploration/Returns Min                              295.272\n",
      "exploration/Actions Mean                              -0.0107595\n",
      "exploration/Actions Std                                0.75229\n",
      "exploration/Actions Max                                0.999977\n",
      "exploration/Actions Min                               -0.999972\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          295.272\n",
      "exploration/env_infos/final/reward_run Mean           -0.59765\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.59765\n",
      "exploration/env_infos/final/reward_run Min            -0.59765\n",
      "exploration/env_infos/initial/reward_run Mean         -0.166736\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.166736\n",
      "exploration/env_infos/initial/reward_run Min          -0.166736\n",
      "exploration/env_infos/reward_run Mean                 -0.882585\n",
      "exploration/env_infos/reward_run Std                   1.39424\n",
      "exploration/env_infos/reward_run Max                   1.69909\n",
      "exploration/env_infos/reward_run Min                  -5.11356\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.259893\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.259893\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.259893\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.421125\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.421125\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.421125\n",
      "exploration/env_infos/reward_ctrl Mean                -0.339634\n",
      "exploration/env_infos/reward_ctrl Std                  0.0955844\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0573373\n",
      "exploration/env_infos/reward_ctrl Min                 -0.593725\n",
      "exploration/env_infos/final/height Mean               -0.426252\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.426252\n",
      "exploration/env_infos/final/height Min                -0.426252\n",
      "exploration/env_infos/initial/height Mean              0.0514288\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0514288\n",
      "exploration/env_infos/initial/height Min               0.0514288\n",
      "exploration/env_infos/height Mean                     -0.321909\n",
      "exploration/env_infos/height Std                       0.222043\n",
      "exploration/env_infos/height Max                       0.366834\n",
      "exploration/env_infos/height Min                      -0.587454\n",
      "exploration/env_infos/final/reward_angular Mean        1.22231\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.22231\n",
      "exploration/env_infos/final/reward_angular Min         1.22231\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.33313\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.33313\n",
      "exploration/env_infos/initial/reward_angular Min      -1.33313\n",
      "exploration/env_infos/reward_angular Mean             -0.0468767\n",
      "exploration/env_infos/reward_angular Std               2.28557\n",
      "exploration/env_infos/reward_angular Max               7.17092\n",
      "exploration/env_infos/reward_angular Min              -8.15092\n",
      "evaluation/num steps total                             2.075e+06\n",
      "evaluation/num paths total                          2075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.324035\n",
      "evaluation/Rewards Std                                 1.25352\n",
      "evaluation/Rewards Max                                 6.60396\n",
      "evaluation/Rewards Min                                -6.94937\n",
      "evaluation/Returns Mean                              324.035\n",
      "evaluation/Returns Std                               529.973\n",
      "evaluation/Returns Max                              1732.73\n",
      "evaluation/Returns Min                              -300.09\n",
      "evaluation/Actions Mean                                0.137968\n",
      "evaluation/Actions Std                                 0.756671\n",
      "evaluation/Actions Max                                 0.999968\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           324.035\n",
      "evaluation/env_infos/final/reward_run Mean            -0.122646\n",
      "evaluation/env_infos/final/reward_run Std              1.62691\n",
      "evaluation/env_infos/final/reward_run Max              3.1632\n",
      "evaluation/env_infos/final/reward_run Min             -3.3036\n",
      "evaluation/env_infos/initial/reward_run Mean           0.51892\n",
      "evaluation/env_infos/initial/reward_run Std            0.413011\n",
      "evaluation/env_infos/initial/reward_run Max            1.05173\n",
      "evaluation/env_infos/initial/reward_run Min           -0.287696\n",
      "evaluation/env_infos/reward_run Mean                  -0.270386\n",
      "evaluation/env_infos/reward_run Std                    1.7895\n",
      "evaluation/env_infos/reward_run Max                    4.18809\n",
      "evaluation/env_infos/reward_run Min                   -5.59548\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.327088\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.119465\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0812145\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.485828\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.298753\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0811183\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0869732\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.461227\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.354952\n",
      "evaluation/env_infos/reward_ctrl Std                   0.105546\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0155302\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598095\n",
      "evaluation/env_infos/final/height Mean                -0.201014\n",
      "evaluation/env_infos/final/height Std                  0.194273\n",
      "evaluation/env_infos/final/height Max                  0.089687\n",
      "evaluation/env_infos/final/height Min                 -0.577268\n",
      "evaluation/env_infos/initial/height Mean              -0.0230764\n",
      "evaluation/env_infos/initial/height Std                0.0547297\n",
      "evaluation/env_infos/initial/height Max                0.0820268\n",
      "evaluation/env_infos/initial/height Min               -0.127755\n",
      "evaluation/env_infos/height Mean                      -0.180271\n",
      "evaluation/env_infos/height Std                        0.192489\n",
      "evaluation/env_infos/height Max                        0.450481\n",
      "evaluation/env_infos/height Min                       -0.596362\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.304145\n",
      "evaluation/env_infos/final/reward_angular Std          1.44459\n",
      "evaluation/env_infos/final/reward_angular Max          2.58362\n",
      "evaluation/env_infos/final/reward_angular Min         -2.88733\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.481112\n",
      "evaluation/env_infos/initial/reward_angular Std        1.34042\n",
      "evaluation/env_infos/initial/reward_angular Max        2.8907\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.26761\n",
      "evaluation/env_infos/reward_angular Mean               0.0422521\n",
      "evaluation/env_infos/reward_angular Std                2.02824\n",
      "evaluation/env_infos/reward_angular Max                9.99436\n",
      "evaluation/env_infos/reward_angular Min               -7.49535\n",
      "time/data storing (s)                                  0.0159146\n",
      "time/evaluation sampling (s)                          21.3741\n",
      "time/exploration sampling (s)                          1.03725\n",
      "time/logging (s)                                       0.238597\n",
      "time/saving (s)                                        0.027942\n",
      "time/training (s)                                      3.94466\n",
      "time/epoch (s)                                        26.6385\n",
      "time/total (s)                                      2409.21\n",
      "Epoch                                                 82\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:52:48.172559 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 83 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 85000\n",
      "trainer/QF1 Loss                                       0.637799\n",
      "trainer/QF2 Loss                                       0.727658\n",
      "trainer/Policy Loss                                    2.72122\n",
      "trainer/Q1 Predictions Mean                            2.82726\n",
      "trainer/Q1 Predictions Std                            10.1853\n",
      "trainer/Q1 Predictions Max                            49.533\n",
      "trainer/Q1 Predictions Min                           -15.9547\n",
      "trainer/Q2 Predictions Mean                            2.71502\n",
      "trainer/Q2 Predictions Std                            10.1986\n",
      "trainer/Q2 Predictions Max                            49.5745\n",
      "trainer/Q2 Predictions Min                           -16.4227\n",
      "trainer/Q Targets Mean                                 2.89084\n",
      "trainer/Q Targets Std                                 10.3036\n",
      "trainer/Q Targets Max                                 49.4196\n",
      "trainer/Q Targets Min                                -16.401\n",
      "trainer/Log Pis Mean                                   5.64023\n",
      "trainer/Log Pis Std                                    5.53031\n",
      "trainer/Log Pis Max                                   28.3136\n",
      "trainer/Log Pis Min                                   -5.03011\n",
      "trainer/Policy mu Mean                                 0.0466144\n",
      "trainer/Policy mu Std                                  1.55637\n",
      "trainer/Policy mu Max                                  5.26224\n",
      "trainer/Policy mu Min                                 -6.16097\n",
      "trainer/Policy log std Mean                           -0.764993\n",
      "trainer/Policy log std Std                             0.273785\n",
      "trainer/Policy log std Max                             0.16185\n",
      "trainer/Policy log std Min                            -1.87863\n",
      "trainer/Alpha                                          0.012668\n",
      "trainer/Alpha Loss                                    -1.5714\n",
      "exploration/num steps total                        85000\n",
      "exploration/num paths total                           85\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.129184\n",
      "exploration/Rewards Std                                0.83762\n",
      "exploration/Rewards Max                                3.65677\n",
      "exploration/Rewards Min                               -2.21398\n",
      "exploration/Returns Mean                            -129.184\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -129.184\n",
      "exploration/Returns Min                             -129.184\n",
      "exploration/Actions Mean                               0.0359425\n",
      "exploration/Actions Std                                0.716174\n",
      "exploration/Actions Max                                0.999994\n",
      "exploration/Actions Min                               -0.999955\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -129.184\n",
      "exploration/env_infos/final/reward_run Mean            0.103294\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.103294\n",
      "exploration/env_infos/final/reward_run Min             0.103294\n",
      "exploration/env_infos/initial/reward_run Mean          0.314345\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.314345\n",
      "exploration/env_infos/initial/reward_run Min           0.314345\n",
      "exploration/env_infos/reward_run Mean                  0.0315247\n",
      "exploration/env_infos/reward_run Std                   0.478895\n",
      "exploration/env_infos/reward_run Max                   1.21608\n",
      "exploration/env_infos/reward_run Min                  -1.57213\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.320319\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.320319\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.320319\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.357814\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.357814\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.357814\n",
      "exploration/env_infos/reward_ctrl Mean                -0.308519\n",
      "exploration/env_infos/reward_ctrl Std                  0.079859\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0181374\n",
      "exploration/env_infos/reward_ctrl Min                 -0.568145\n",
      "exploration/env_infos/final/height Mean               -0.549404\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.549404\n",
      "exploration/env_infos/final/height Min                -0.549404\n",
      "exploration/env_infos/initial/height Mean              0.00389473\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.00389473\n",
      "exploration/env_infos/initial/height Min               0.00389473\n",
      "exploration/env_infos/height Mean                     -0.517089\n",
      "exploration/env_infos/height Std                       0.151001\n",
      "exploration/env_infos/height Max                       0.326751\n",
      "exploration/env_infos/height Min                      -0.581557\n",
      "exploration/env_infos/final/reward_angular Mean       -0.657436\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -0.657436\n",
      "exploration/env_infos/final/reward_angular Min        -0.657436\n",
      "exploration/env_infos/initial/reward_angular Mean      0.3738\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.3738\n",
      "exploration/env_infos/initial/reward_angular Min       0.3738\n",
      "exploration/env_infos/reward_angular Mean              0.0979796\n",
      "exploration/env_infos/reward_angular Std               1.05452\n",
      "exploration/env_infos/reward_angular Max               5.3074\n",
      "exploration/env_infos/reward_angular Min              -3.0372\n",
      "evaluation/num steps total                             2.1e+06\n",
      "evaluation/num paths total                          2100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0317858\n",
      "evaluation/Rewards Std                                 1.09816\n",
      "evaluation/Rewards Max                                 6.38574\n",
      "evaluation/Rewards Min                                -6.03371\n",
      "evaluation/Returns Mean                               31.7858\n",
      "evaluation/Returns Std                               415.666\n",
      "evaluation/Returns Max                              1673.37\n",
      "evaluation/Returns Min                              -383.277\n",
      "evaluation/Actions Mean                                0.016876\n",
      "evaluation/Actions Std                                 0.72314\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            31.7858\n",
      "evaluation/env_infos/final/reward_run Mean             0.436511\n",
      "evaluation/env_infos/final/reward_run Std              0.99803\n",
      "evaluation/env_infos/final/reward_run Max              2.84943\n",
      "evaluation/env_infos/final/reward_run Min             -1.80716\n",
      "evaluation/env_infos/initial/reward_run Mean           0.559983\n",
      "evaluation/env_infos/initial/reward_run Std            0.532104\n",
      "evaluation/env_infos/initial/reward_run Max            1.44922\n",
      "evaluation/env_infos/initial/reward_run Min           -0.517666\n",
      "evaluation/env_infos/reward_run Mean                   0.543943\n",
      "evaluation/env_infos/reward_run Std                    1.21071\n",
      "evaluation/env_infos/reward_run Max                    4.39119\n",
      "evaluation/env_infos/reward_run Min                   -5.55614\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.28273\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0981068\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.084141\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.484231\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.368768\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.112328\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.105217\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.521378\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.31393\n",
      "evaluation/env_infos/reward_ctrl Std                   0.100261\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0224839\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.593751\n",
      "evaluation/env_infos/final/height Mean                -0.329482\n",
      "evaluation/env_infos/final/height Std                  0.183457\n",
      "evaluation/env_infos/final/height Max                 -0.0225867\n",
      "evaluation/env_infos/final/height Min                 -0.576836\n",
      "evaluation/env_infos/initial/height Mean              -0.0128625\n",
      "evaluation/env_infos/initial/height Std                0.0542083\n",
      "evaluation/env_infos/initial/height Max                0.0973313\n",
      "evaluation/env_infos/initial/height Min               -0.0948688\n",
      "evaluation/env_infos/height Mean                      -0.325915\n",
      "evaluation/env_infos/height Std                        0.208353\n",
      "evaluation/env_infos/height Max                        0.438679\n",
      "evaluation/env_infos/height Min                       -0.596791\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.961172\n",
      "evaluation/env_infos/final/reward_angular Std          1.7633\n",
      "evaluation/env_infos/final/reward_angular Max          3.15019\n",
      "evaluation/env_infos/final/reward_angular Min         -4.48036\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.470065\n",
      "evaluation/env_infos/initial/reward_angular Std        1.55568\n",
      "evaluation/env_infos/initial/reward_angular Max        2.49088\n",
      "evaluation/env_infos/initial/reward_angular Min       -3.53215\n",
      "evaluation/env_infos/reward_angular Mean              -0.000830111\n",
      "evaluation/env_infos/reward_angular Std                1.7364\n",
      "evaluation/env_infos/reward_angular Max                8.82047\n",
      "evaluation/env_infos/reward_angular Min               -7.58054\n",
      "time/data storing (s)                                  0.0156381\n",
      "time/evaluation sampling (s)                          30.009\n",
      "time/exploration sampling (s)                          1.27418\n",
      "time/logging (s)                                       0.238752\n",
      "time/saving (s)                                        0.0262603\n",
      "time/training (s)                                      4.0766\n",
      "time/epoch (s)                                        35.6405\n",
      "time/total (s)                                      2445.52\n",
      "Epoch                                                 83\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:53:16.413488 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 84 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 86000\n",
      "trainer/QF1 Loss                                       0.657109\n",
      "trainer/QF2 Loss                                       0.647652\n",
      "trainer/Policy Loss                                    2.25276\n",
      "trainer/Q1 Predictions Mean                            3.67976\n",
      "trainer/Q1 Predictions Std                            10.2791\n",
      "trainer/Q1 Predictions Max                            46.8826\n",
      "trainer/Q1 Predictions Min                           -16.7644\n",
      "trainer/Q2 Predictions Mean                            3.76231\n",
      "trainer/Q2 Predictions Std                            10.1821\n",
      "trainer/Q2 Predictions Max                            44.9641\n",
      "trainer/Q2 Predictions Min                           -15.88\n",
      "trainer/Q Targets Mean                                 3.61435\n",
      "trainer/Q Targets Std                                 10.2319\n",
      "trainer/Q Targets Max                                 45.9881\n",
      "trainer/Q Targets Min                                -15.7609\n",
      "trainer/Log Pis Mean                                   6.19343\n",
      "trainer/Log Pis Std                                    5.1912\n",
      "trainer/Log Pis Max                                   22.9607\n",
      "trainer/Log Pis Min                                   -4.28835\n",
      "trainer/Policy mu Mean                                 0.253817\n",
      "trainer/Policy mu Std                                  1.60206\n",
      "trainer/Policy mu Max                                  4.38588\n",
      "trainer/Policy mu Min                                 -5.75755\n",
      "trainer/Policy log std Mean                           -0.769141\n",
      "trainer/Policy log std Std                             0.287029\n",
      "trainer/Policy log std Max                             0.0325748\n",
      "trainer/Policy log std Min                            -2.1377\n",
      "trainer/Alpha                                          0.0116531\n",
      "trainer/Alpha Loss                                     0.861024\n",
      "exploration/num steps total                        86000\n",
      "exploration/num paths total                           86\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.219339\n",
      "exploration/Rewards Std                                1.26143\n",
      "exploration/Rewards Max                                3.46541\n",
      "exploration/Rewards Min                               -3.77901\n",
      "exploration/Returns Mean                            -219.339\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -219.339\n",
      "exploration/Returns Min                             -219.339\n",
      "exploration/Actions Mean                               0.0266043\n",
      "exploration/Actions Std                                0.697576\n",
      "exploration/Actions Max                                0.999922\n",
      "exploration/Actions Min                               -0.99989\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -219.339\n",
      "exploration/env_infos/final/reward_run Mean           -1.41773\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -1.41773\n",
      "exploration/env_infos/final/reward_run Min            -1.41773\n",
      "exploration/env_infos/initial/reward_run Mean         -0.0643624\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.0643624\n",
      "exploration/env_infos/initial/reward_run Min          -0.0643624\n",
      "exploration/env_infos/reward_run Mean                 -0.504966\n",
      "exploration/env_infos/reward_run Std                   1.10152\n",
      "exploration/env_infos/reward_run Max                   1.91138\n",
      "exploration/env_infos/reward_run Min                  -4.05838\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.338663\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.338663\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.338663\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.333054\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.333054\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.333054\n",
      "exploration/env_infos/reward_ctrl Mean                -0.292392\n",
      "exploration/env_infos/reward_ctrl Std                  0.104885\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0214004\n",
      "exploration/env_infos/reward_ctrl Min                 -0.568366\n",
      "exploration/env_infos/final/height Mean               -0.569221\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.569221\n",
      "exploration/env_infos/final/height Min                -0.569221\n",
      "exploration/env_infos/initial/height Mean              0.0402168\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0402168\n",
      "exploration/env_infos/initial/height Min               0.0402168\n",
      "exploration/env_infos/height Mean                     -0.342437\n",
      "exploration/env_infos/height Std                       0.215197\n",
      "exploration/env_infos/height Max                       0.310428\n",
      "exploration/env_infos/height Min                      -0.592363\n",
      "exploration/env_infos/final/reward_angular Mean        0.0607163\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.0607163\n",
      "exploration/env_infos/final/reward_angular Min         0.0607163\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.95735\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.95735\n",
      "exploration/env_infos/initial/reward_angular Min      -1.95735\n",
      "exploration/env_infos/reward_angular Mean             -0.0544424\n",
      "exploration/env_infos/reward_angular Std               2.31742\n",
      "exploration/env_infos/reward_angular Max               7.65448\n",
      "exploration/env_infos/reward_angular Min              -6.36008\n",
      "evaluation/num steps total                             2.125e+06\n",
      "evaluation/num paths total                          2125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.00269413\n",
      "evaluation/Rewards Std                                 1.08984\n",
      "evaluation/Rewards Max                                 7.01409\n",
      "evaluation/Rewards Min                                -6.43499\n",
      "evaluation/Returns Mean                                2.69413\n",
      "evaluation/Returns Std                               364.269\n",
      "evaluation/Returns Max                              1500.47\n",
      "evaluation/Returns Min                              -353.326\n",
      "evaluation/Actions Mean                                0.092868\n",
      "evaluation/Actions Std                                 0.711996\n",
      "evaluation/Actions Max                                 0.999975\n",
      "evaluation/Actions Min                                -0.999999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                             2.69413\n",
      "evaluation/env_infos/final/reward_run Mean             0.75412\n",
      "evaluation/env_infos/final/reward_run Std              0.832046\n",
      "evaluation/env_infos/final/reward_run Max              2.19949\n",
      "evaluation/env_infos/final/reward_run Min             -0.63521\n",
      "evaluation/env_infos/initial/reward_run Mean           0.478557\n",
      "evaluation/env_infos/initial/reward_run Std            0.535433\n",
      "evaluation/env_infos/initial/reward_run Max            1.37029\n",
      "evaluation/env_infos/initial/reward_run Min           -0.546898\n",
      "evaluation/env_infos/reward_run Mean                   0.45113\n",
      "evaluation/env_infos/reward_run Std                    1.08811\n",
      "evaluation/env_infos/reward_run Max                    4.12984\n",
      "evaluation/env_infos/reward_run Min                   -5.14127\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.30301\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.115619\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.13116\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.564127\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.334593\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0913436\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0998884\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.485673\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.309338\n",
      "evaluation/env_infos/reward_ctrl Std                   0.10673\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.019197\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.591351\n",
      "evaluation/env_infos/final/height Mean                -0.340447\n",
      "evaluation/env_infos/final/height Std                  0.159836\n",
      "evaluation/env_infos/final/height Max                 -0.04302\n",
      "evaluation/env_infos/final/height Min                 -0.577433\n",
      "evaluation/env_infos/initial/height Mean              -0.0118322\n",
      "evaluation/env_infos/initial/height Std                0.0543328\n",
      "evaluation/env_infos/initial/height Max                0.0848822\n",
      "evaluation/env_infos/initial/height Min               -0.0951594\n",
      "evaluation/env_infos/height Mean                      -0.332862\n",
      "evaluation/env_infos/height Std                        0.18206\n",
      "evaluation/env_infos/height Max                        0.399552\n",
      "evaluation/env_infos/height Min                       -0.598478\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.265507\n",
      "evaluation/env_infos/final/reward_angular Std          1.49907\n",
      "evaluation/env_infos/final/reward_angular Max          1.73082\n",
      "evaluation/env_infos/final/reward_angular Min         -4.56469\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.513777\n",
      "evaluation/env_infos/initial/reward_angular Std        1.51957\n",
      "evaluation/env_infos/initial/reward_angular Max        3.24496\n",
      "evaluation/env_infos/initial/reward_angular Min       -3.01589\n",
      "evaluation/env_infos/reward_angular Mean              -0.00566434\n",
      "evaluation/env_infos/reward_angular Std                1.82023\n",
      "evaluation/env_infos/reward_angular Max                8.5386\n",
      "evaluation/env_infos/reward_angular Min               -8.60557\n",
      "time/data storing (s)                                  0.0155019\n",
      "time/evaluation sampling (s)                          21.6414\n",
      "time/exploration sampling (s)                          1.02161\n",
      "time/logging (s)                                       0.253443\n",
      "time/saving (s)                                        0.029604\n",
      "time/training (s)                                      4.60996\n",
      "time/epoch (s)                                        27.5716\n",
      "time/total (s)                                      2473.78\n",
      "Epoch                                                 84\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:53:47.469871 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 85 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 87000\n",
      "trainer/QF1 Loss                                       0.790995\n",
      "trainer/QF2 Loss                                       0.822627\n",
      "trainer/Policy Loss                                    2.23132\n",
      "trainer/Q1 Predictions Mean                            3.8874\n",
      "trainer/Q1 Predictions Std                            11.0976\n",
      "trainer/Q1 Predictions Max                            54.0429\n",
      "trainer/Q1 Predictions Min                            -9.184\n",
      "trainer/Q2 Predictions Mean                            3.79134\n",
      "trainer/Q2 Predictions Std                            11.0795\n",
      "trainer/Q2 Predictions Max                            53.0348\n",
      "trainer/Q2 Predictions Min                            -9.25472\n",
      "trainer/Q Targets Mean                                 3.87451\n",
      "trainer/Q Targets Std                                 11.1231\n",
      "trainer/Q Targets Max                                 53.2584\n",
      "trainer/Q Targets Min                                 -9.36879\n",
      "trainer/Log Pis Mean                                   6.32162\n",
      "trainer/Log Pis Std                                    5.93833\n",
      "trainer/Log Pis Max                                   31.5068\n",
      "trainer/Log Pis Min                                   -5.9611\n",
      "trainer/Policy mu Mean                                 0.321151\n",
      "trainer/Policy mu Std                                  1.6283\n",
      "trainer/Policy mu Max                                  6.06513\n",
      "trainer/Policy mu Min                                 -7.14098\n",
      "trainer/Policy log std Mean                           -0.759852\n",
      "trainer/Policy log std Std                             0.265327\n",
      "trainer/Policy log std Max                             0.212686\n",
      "trainer/Policy log std Min                            -1.82866\n",
      "trainer/Alpha                                          0.0119806\n",
      "trainer/Alpha Loss                                     1.42266\n",
      "exploration/num steps total                        87000\n",
      "exploration/num paths total                           87\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.221857\n",
      "exploration/Rewards Std                                0.0977295\n",
      "exploration/Rewards Max                                0.0584852\n",
      "exploration/Rewards Min                               -0.550778\n",
      "exploration/Returns Mean                            -221.857\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -221.857\n",
      "exploration/Returns Min                             -221.857\n",
      "exploration/Actions Mean                               0.212727\n",
      "exploration/Actions Std                                0.735199\n",
      "exploration/Actions Max                                0.999991\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -221.857\n",
      "exploration/env_infos/final/reward_run Mean            2.08581\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             2.08581\n",
      "exploration/env_infos/final/reward_run Min             2.08581\n",
      "exploration/env_infos/initial/reward_run Mean          0.579703\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.579703\n",
      "exploration/env_infos/initial/reward_run Min           0.579703\n",
      "exploration/env_infos/reward_run Mean                  1.75866\n",
      "exploration/env_infos/reward_run Std                   0.758956\n",
      "exploration/env_infos/reward_run Max                   4.22763\n",
      "exploration/env_infos/reward_run Min                  -0.816347\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.159603\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.159603\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.159603\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.272946\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.272946\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.272946\n",
      "exploration/env_infos/reward_ctrl Mean                -0.351462\n",
      "exploration/env_infos/reward_ctrl Std                  0.0935986\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0291973\n",
      "exploration/env_infos/reward_ctrl Min                 -0.574366\n",
      "exploration/env_infos/final/height Mean               -0.0923702\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0923702\n",
      "exploration/env_infos/final/height Min                -0.0923702\n",
      "exploration/env_infos/initial/height Mean             -0.071714\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.071714\n",
      "exploration/env_infos/initial/height Min              -0.071714\n",
      "exploration/env_infos/height Mean                     -0.105815\n",
      "exploration/env_infos/height Std                       0.0968611\n",
      "exploration/env_infos/height Max                       0.115057\n",
      "exploration/env_infos/height Min                      -0.365963\n",
      "exploration/env_infos/final/reward_angular Mean        0.601966\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.601966\n",
      "exploration/env_infos/final/reward_angular Min         0.601966\n",
      "exploration/env_infos/initial/reward_angular Mean      0.416005\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.416005\n",
      "exploration/env_infos/initial/reward_angular Min       0.416005\n",
      "exploration/env_infos/reward_angular Mean             -0.0369257\n",
      "exploration/env_infos/reward_angular Std               1.71967\n",
      "exploration/env_infos/reward_angular Max               5.34519\n",
      "exploration/env_infos/reward_angular Min              -5.09404\n",
      "evaluation/num steps total                             2.15e+06\n",
      "evaluation/num paths total                          2150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.107566\n",
      "evaluation/Rewards Std                                 1.10694\n",
      "evaluation/Rewards Max                                 6.04451\n",
      "evaluation/Rewards Min                                -6.47271\n",
      "evaluation/Returns Mean                              107.566\n",
      "evaluation/Returns Std                               432.26\n",
      "evaluation/Returns Max                              1835.34\n",
      "evaluation/Returns Min                              -355.888\n",
      "evaluation/Actions Mean                                0.144828\n",
      "evaluation/Actions Std                                 0.777923\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           107.566\n",
      "evaluation/env_infos/final/reward_run Mean             0.453247\n",
      "evaluation/env_infos/final/reward_run Std              1.63027\n",
      "evaluation/env_infos/final/reward_run Max              4.2473\n",
      "evaluation/env_infos/final/reward_run Min             -2.53418\n",
      "evaluation/env_infos/initial/reward_run Mean           0.54153\n",
      "evaluation/env_infos/initial/reward_run Std            0.453511\n",
      "evaluation/env_infos/initial/reward_run Max            1.15535\n",
      "evaluation/env_infos/initial/reward_run Min           -0.22992\n",
      "evaluation/env_infos/reward_run Mean                   0.443024\n",
      "evaluation/env_infos/reward_run Std                    1.41969\n",
      "evaluation/env_infos/reward_run Max                    5.09854\n",
      "evaluation/env_infos/reward_run Min                   -5.09134\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.406898\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.097573\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.17366\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.541456\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.345885\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.109769\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.11407\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.52539\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.375684\n",
      "evaluation/env_infos/reward_ctrl Std                   0.114125\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0174539\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.591442\n",
      "evaluation/env_infos/final/height Mean                -0.219808\n",
      "evaluation/env_infos/final/height Std                  0.164361\n",
      "evaluation/env_infos/final/height Max                  0.0699658\n",
      "evaluation/env_infos/final/height Min                 -0.559148\n",
      "evaluation/env_infos/initial/height Mean               0.0107425\n",
      "evaluation/env_infos/initial/height Std                0.0502988\n",
      "evaluation/env_infos/initial/height Max                0.103858\n",
      "evaluation/env_infos/initial/height Min               -0.0822907\n",
      "evaluation/env_infos/height Mean                      -0.215626\n",
      "evaluation/env_infos/height Std                        0.189489\n",
      "evaluation/env_infos/height Max                        0.529976\n",
      "evaluation/env_infos/height Min                       -0.596653\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.125503\n",
      "evaluation/env_infos/final/reward_angular Std          1.31617\n",
      "evaluation/env_infos/final/reward_angular Max          3.36718\n",
      "evaluation/env_infos/final/reward_angular Min         -2.83438\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.531849\n",
      "evaluation/env_infos/initial/reward_angular Std        0.869571\n",
      "evaluation/env_infos/initial/reward_angular Max        1.32435\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.23455\n",
      "evaluation/env_infos/reward_angular Mean              -0.010609\n",
      "evaluation/env_infos/reward_angular Std                1.82852\n",
      "evaluation/env_infos/reward_angular Max                9.18303\n",
      "evaluation/env_infos/reward_angular Min               -7.92124\n",
      "time/data storing (s)                                  0.0163385\n",
      "time/evaluation sampling (s)                          24.9173\n",
      "time/exploration sampling (s)                          1.13941\n",
      "time/logging (s)                                       0.244948\n",
      "time/saving (s)                                        0.029934\n",
      "time/training (s)                                      3.99729\n",
      "time/epoch (s)                                        30.3452\n",
      "time/total (s)                                      2504.82\n",
      "Epoch                                                 85\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:54:15.708957 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 86 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 88000\n",
      "trainer/QF1 Loss                                       0.761459\n",
      "trainer/QF2 Loss                                       0.638391\n",
      "trainer/Policy Loss                                    2.70792\n",
      "trainer/Q1 Predictions Mean                            2.39515\n",
      "trainer/Q1 Predictions Std                            10.5704\n",
      "trainer/Q1 Predictions Max                            48.6166\n",
      "trainer/Q1 Predictions Min                           -25.1562\n",
      "trainer/Q2 Predictions Mean                            2.73799\n",
      "trainer/Q2 Predictions Std                            10.6024\n",
      "trainer/Q2 Predictions Max                            48.5744\n",
      "trainer/Q2 Predictions Min                           -23.4062\n",
      "trainer/Q Targets Mean                                 2.53379\n",
      "trainer/Q Targets Std                                 10.6915\n",
      "trainer/Q Targets Max                                 46.6117\n",
      "trainer/Q Targets Min                                -24.1708\n",
      "trainer/Log Pis Mean                                   5.39156\n",
      "trainer/Log Pis Std                                    5.37017\n",
      "trainer/Log Pis Max                                   23.8264\n",
      "trainer/Log Pis Min                                   -4.56387\n",
      "trainer/Policy mu Mean                                 0.0835593\n",
      "trainer/Policy mu Std                                  1.51864\n",
      "trainer/Policy mu Max                                  3.75457\n",
      "trainer/Policy mu Min                                 -6.69799\n",
      "trainer/Policy log std Mean                           -0.807339\n",
      "trainer/Policy log std Std                             0.267059\n",
      "trainer/Policy log std Max                             0.306171\n",
      "trainer/Policy log std Min                            -2.13816\n",
      "trainer/Alpha                                          0.011982\n",
      "trainer/Alpha Loss                                    -2.68998\n",
      "exploration/num steps total                        88000\n",
      "exploration/num paths total                           88\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0111689\n",
      "exploration/Rewards Std                                0.732315\n",
      "exploration/Rewards Max                                2.07705\n",
      "exploration/Rewards Min                               -2.31875\n",
      "exploration/Returns Mean                             -11.1689\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -11.1689\n",
      "exploration/Returns Min                              -11.1689\n",
      "exploration/Actions Mean                               0.14594\n",
      "exploration/Actions Std                                0.757763\n",
      "exploration/Actions Max                                0.999927\n",
      "exploration/Actions Min                               -0.999991\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -11.1689\n",
      "exploration/env_infos/final/reward_run Mean            3.32004\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             3.32004\n",
      "exploration/env_infos/final/reward_run Min             3.32004\n",
      "exploration/env_infos/initial/reward_run Mean          0.334143\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.334143\n",
      "exploration/env_infos/initial/reward_run Min           0.334143\n",
      "exploration/env_infos/reward_run Mean                  2.01472\n",
      "exploration/env_infos/reward_run Std                   0.815196\n",
      "exploration/env_infos/reward_run Max                   4.07576\n",
      "exploration/env_infos/reward_run Min                  -0.431032\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.438439\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.438439\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.438439\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.222409\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.222409\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.222409\n",
      "exploration/env_infos/reward_ctrl Mean                -0.357302\n",
      "exploration/env_infos/reward_ctrl Std                  0.0766837\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0779722\n",
      "exploration/env_infos/reward_ctrl Min                 -0.530433\n",
      "exploration/env_infos/final/height Mean               -0.147165\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.147165\n",
      "exploration/env_infos/final/height Min                -0.147165\n",
      "exploration/env_infos/initial/height Mean              0.00608483\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.00608483\n",
      "exploration/env_infos/initial/height Min               0.00608483\n",
      "exploration/env_infos/height Mean                     -0.106149\n",
      "exploration/env_infos/height Std                       0.0908061\n",
      "exploration/env_infos/height Max                       0.158726\n",
      "exploration/env_infos/height Min                      -0.329213\n",
      "exploration/env_infos/final/reward_angular Mean        1.36148\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.36148\n",
      "exploration/env_infos/final/reward_angular Min         1.36148\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.57216\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.57216\n",
      "exploration/env_infos/initial/reward_angular Min      -1.57216\n",
      "exploration/env_infos/reward_angular Mean             -0.065785\n",
      "exploration/env_infos/reward_angular Std               1.43199\n",
      "exploration/env_infos/reward_angular Max               4.55278\n",
      "exploration/env_infos/reward_angular Min              -4.16457\n",
      "evaluation/num steps total                             2.175e+06\n",
      "evaluation/num paths total                          2175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0862177\n",
      "evaluation/Rewards Std                                 1.15104\n",
      "evaluation/Rewards Max                                 6.52898\n",
      "evaluation/Rewards Min                                -6.09549\n",
      "evaluation/Returns Mean                               86.2177\n",
      "evaluation/Returns Std                               444.214\n",
      "evaluation/Returns Max                              1711.54\n",
      "evaluation/Returns Min                              -420.414\n",
      "evaluation/Actions Mean                                0.00755623\n",
      "evaluation/Actions Std                                 0.717759\n",
      "evaluation/Actions Max                                 0.999983\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            86.2177\n",
      "evaluation/env_infos/final/reward_run Mean             0.318532\n",
      "evaluation/env_infos/final/reward_run Std              1.30668\n",
      "evaluation/env_infos/final/reward_run Max              3.00197\n",
      "evaluation/env_infos/final/reward_run Min             -1.91269\n",
      "evaluation/env_infos/initial/reward_run Mean           0.369002\n",
      "evaluation/env_infos/initial/reward_run Std            0.475061\n",
      "evaluation/env_infos/initial/reward_run Max            1.04835\n",
      "evaluation/env_infos/initial/reward_run Min           -0.592714\n",
      "evaluation/env_infos/reward_run Mean                   0.45443\n",
      "evaluation/env_infos/reward_run Std                    1.39173\n",
      "evaluation/env_infos/reward_run Max                    4.02349\n",
      "evaluation/env_infos/reward_run Min                   -5.50213\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.31065\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.140712\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0102927\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.478282\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.309635\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0728417\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.125728\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.418998\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.309141\n",
      "evaluation/env_infos/reward_ctrl Std                   0.121149\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0101567\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596575\n",
      "evaluation/env_infos/final/height Mean                -0.328108\n",
      "evaluation/env_infos/final/height Std                  0.211565\n",
      "evaluation/env_infos/final/height Max                  0.13521\n",
      "evaluation/env_infos/final/height Min                 -0.575332\n",
      "evaluation/env_infos/initial/height Mean              -0.0139862\n",
      "evaluation/env_infos/initial/height Std                0.0469543\n",
      "evaluation/env_infos/initial/height Max                0.0771751\n",
      "evaluation/env_infos/initial/height Min               -0.0809827\n",
      "evaluation/env_infos/height Mean                      -0.305052\n",
      "evaluation/env_infos/height Std                        0.214055\n",
      "evaluation/env_infos/height Max                        0.497878\n",
      "evaluation/env_infos/height Min                       -0.597332\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.227703\n",
      "evaluation/env_infos/final/reward_angular Std          1.73634\n",
      "evaluation/env_infos/final/reward_angular Max          3.2918\n",
      "evaluation/env_infos/final/reward_angular Min         -4.62961\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.287519\n",
      "evaluation/env_infos/initial/reward_angular Std        1.18751\n",
      "evaluation/env_infos/initial/reward_angular Max        2.97109\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.96075\n",
      "evaluation/env_infos/reward_angular Mean              -0.00636227\n",
      "evaluation/env_infos/reward_angular Std                1.75124\n",
      "evaluation/env_infos/reward_angular Max                8.67649\n",
      "evaluation/env_infos/reward_angular Min               -8.42748\n",
      "time/data storing (s)                                  0.0156051\n",
      "time/evaluation sampling (s)                          22.3924\n",
      "time/exploration sampling (s)                          1.00322\n",
      "time/logging (s)                                       0.237845\n",
      "time/saving (s)                                        0.0294961\n",
      "time/training (s)                                      3.83861\n",
      "time/epoch (s)                                        27.5172\n",
      "time/total (s)                                      2533.05\n",
      "Epoch                                                 86\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:54:49.487469 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 87 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 89000\n",
      "trainer/QF1 Loss                                       1.05041\n",
      "trainer/QF2 Loss                                       0.810051\n",
      "trainer/Policy Loss                                    2.30612\n",
      "trainer/Q1 Predictions Mean                            3.87951\n",
      "trainer/Q1 Predictions Std                            11.2687\n",
      "trainer/Q1 Predictions Max                            42.8136\n",
      "trainer/Q1 Predictions Min                           -15.4821\n",
      "trainer/Q2 Predictions Mean                            3.88396\n",
      "trainer/Q2 Predictions Std                            11.2443\n",
      "trainer/Q2 Predictions Max                            43.9698\n",
      "trainer/Q2 Predictions Min                           -15.7085\n",
      "trainer/Q Targets Mean                                 3.91612\n",
      "trainer/Q Targets Std                                 11.2573\n",
      "trainer/Q Targets Max                                 45.8096\n",
      "trainer/Q Targets Min                                -15.5752\n",
      "trainer/Log Pis Mean                                   6.38902\n",
      "trainer/Log Pis Std                                    5.34713\n",
      "trainer/Log Pis Max                                   27.1354\n",
      "trainer/Log Pis Min                                   -4.30058\n",
      "trainer/Policy mu Mean                                 0.206705\n",
      "trainer/Policy mu Std                                  1.63268\n",
      "trainer/Policy mu Max                                  5.06413\n",
      "trainer/Policy mu Min                                 -5.48985\n",
      "trainer/Policy log std Mean                           -0.773155\n",
      "trainer/Policy log std Std                             0.280599\n",
      "trainer/Policy log std Max                             0.110482\n",
      "trainer/Policy log std Min                            -2.20569\n",
      "trainer/Alpha                                          0.0127978\n",
      "trainer/Alpha Loss                                     1.69561\n",
      "exploration/num steps total                        89000\n",
      "exploration/num paths total                           89\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.0593308\n",
      "exploration/Rewards Std                                1.31797\n",
      "exploration/Rewards Max                                6.47254\n",
      "exploration/Rewards Min                               -5.56896\n",
      "exploration/Returns Mean                              59.3308\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                               59.3308\n",
      "exploration/Returns Min                               59.3308\n",
      "exploration/Actions Mean                               0.0130397\n",
      "exploration/Actions Std                                0.777639\n",
      "exploration/Actions Max                                0.99999\n",
      "exploration/Actions Min                               -0.999996\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                           59.3308\n",
      "exploration/env_infos/final/reward_run Mean            0.799231\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.799231\n",
      "exploration/env_infos/final/reward_run Min             0.799231\n",
      "exploration/env_infos/initial/reward_run Mean          0.963459\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.963459\n",
      "exploration/env_infos/initial/reward_run Min           0.963459\n",
      "exploration/env_infos/reward_run Mean                  0.343257\n",
      "exploration/env_infos/reward_run Std                   0.67135\n",
      "exploration/env_infos/reward_run Max                   2.72844\n",
      "exploration/env_infos/reward_run Min                  -1.59905\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.410546\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.410546\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.410546\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.301971\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.301971\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.301971\n",
      "exploration/env_infos/reward_ctrl Mean                -0.362936\n",
      "exploration/env_infos/reward_ctrl Std                  0.0890746\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0930024\n",
      "exploration/env_infos/reward_ctrl Min                 -0.573283\n",
      "exploration/env_infos/final/height Mean               -0.286584\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.286584\n",
      "exploration/env_infos/final/height Min                -0.286584\n",
      "exploration/env_infos/initial/height Mean             -0.0617137\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0617137\n",
      "exploration/env_infos/initial/height Min              -0.0617137\n",
      "exploration/env_infos/height Mean                     -0.301589\n",
      "exploration/env_infos/height Std                       0.240045\n",
      "exploration/env_infos/height Max                       0.333025\n",
      "exploration/env_infos/height Min                      -0.587621\n",
      "exploration/env_infos/final/reward_angular Mean       -1.3023\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.3023\n",
      "exploration/env_infos/final/reward_angular Min        -1.3023\n",
      "exploration/env_infos/initial/reward_angular Mean     -0.377457\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -0.377457\n",
      "exploration/env_infos/initial/reward_angular Min      -0.377457\n",
      "exploration/env_infos/reward_angular Mean              0.0574438\n",
      "exploration/env_infos/reward_angular Std               1.55408\n",
      "exploration/env_infos/reward_angular Max               8.04332\n",
      "exploration/env_infos/reward_angular Min              -6.22937\n",
      "evaluation/num steps total                             2.2e+06\n",
      "evaluation/num paths total                          2200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0651037\n",
      "evaluation/Rewards Std                                 1.11933\n",
      "evaluation/Rewards Max                                 7.26241\n",
      "evaluation/Rewards Min                                -6.36446\n",
      "evaluation/Returns Mean                               65.1037\n",
      "evaluation/Returns Std                               464.453\n",
      "evaluation/Returns Max                              1713.93\n",
      "evaluation/Returns Min                              -447.391\n",
      "evaluation/Actions Mean                                0.0538016\n",
      "evaluation/Actions Std                                 0.749989\n",
      "evaluation/Actions Max                                 0.999973\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            65.1037\n",
      "evaluation/env_infos/final/reward_run Mean             0.600287\n",
      "evaluation/env_infos/final/reward_run Std              1.16592\n",
      "evaluation/env_infos/final/reward_run Max              2.46489\n",
      "evaluation/env_infos/final/reward_run Min             -1.54461\n",
      "evaluation/env_infos/initial/reward_run Mean           0.531834\n",
      "evaluation/env_infos/initial/reward_run Std            0.386058\n",
      "evaluation/env_infos/initial/reward_run Max            1.01665\n",
      "evaluation/env_infos/initial/reward_run Min           -0.254446\n",
      "evaluation/env_infos/reward_run Mean                   0.598655\n",
      "evaluation/env_infos/reward_run Std                    1.39137\n",
      "evaluation/env_infos/reward_run Max                    4.9383\n",
      "evaluation/env_infos/reward_run Min                   -5.38225\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.330874\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.113097\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0858445\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.533873\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.321526\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0467783\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.250567\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.415057\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.339227\n",
      "evaluation/env_infos/reward_ctrl Std                   0.12317\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00264985\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.591911\n",
      "evaluation/env_infos/final/height Mean                -0.350774\n",
      "evaluation/env_infos/final/height Std                  0.185052\n",
      "evaluation/env_infos/final/height Max                  0.0810543\n",
      "evaluation/env_infos/final/height Min                 -0.576621\n",
      "evaluation/env_infos/initial/height Mean              -0.00830327\n",
      "evaluation/env_infos/initial/height Std                0.052267\n",
      "evaluation/env_infos/initial/height Max                0.0880562\n",
      "evaluation/env_infos/initial/height Min               -0.0927712\n",
      "evaluation/env_infos/height Mean                      -0.309409\n",
      "evaluation/env_infos/height Std                        0.220896\n",
      "evaluation/env_infos/height Max                        0.472929\n",
      "evaluation/env_infos/height Min                       -0.596925\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.204757\n",
      "evaluation/env_infos/final/reward_angular Std          1.72046\n",
      "evaluation/env_infos/final/reward_angular Max          3.65611\n",
      "evaluation/env_infos/final/reward_angular Min         -3.15506\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.833143\n",
      "evaluation/env_infos/initial/reward_angular Std        0.842463\n",
      "evaluation/env_infos/initial/reward_angular Max        1.00137\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.34533\n",
      "evaluation/env_infos/reward_angular Mean              -0.015801\n",
      "evaluation/env_infos/reward_angular Std                1.66188\n",
      "evaluation/env_infos/reward_angular Max                8.63178\n",
      "evaluation/env_infos/reward_angular Min               -7.52093\n",
      "time/data storing (s)                                  0.0153446\n",
      "time/evaluation sampling (s)                          27.7613\n",
      "time/exploration sampling (s)                          1.05192\n",
      "time/logging (s)                                       0.247275\n",
      "time/saving (s)                                        0.03017\n",
      "time/training (s)                                      3.97037\n",
      "time/epoch (s)                                        33.0763\n",
      "time/total (s)                                      2566.84\n",
      "Epoch                                                 87\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:55:22.034954 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 88 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 90000\n",
      "trainer/QF1 Loss                                       0.809508\n",
      "trainer/QF2 Loss                                       0.833593\n",
      "trainer/Policy Loss                                    2.31095\n",
      "trainer/Q1 Predictions Mean                            3.97069\n",
      "trainer/Q1 Predictions Std                            11.1391\n",
      "trainer/Q1 Predictions Max                            51.8679\n",
      "trainer/Q1 Predictions Min                           -10.5851\n",
      "trainer/Q2 Predictions Mean                            4.14002\n",
      "trainer/Q2 Predictions Std                            11.2198\n",
      "trainer/Q2 Predictions Max                            53.6401\n",
      "trainer/Q2 Predictions Min                           -10.3561\n",
      "trainer/Q Targets Mean                                 4.02865\n",
      "trainer/Q Targets Std                                 11.2311\n",
      "trainer/Q Targets Max                                 51.0981\n",
      "trainer/Q Targets Min                                -10.3875\n",
      "trainer/Log Pis Mean                                   6.68041\n",
      "trainer/Log Pis Std                                    5.51937\n",
      "trainer/Log Pis Max                                   24.8382\n",
      "trainer/Log Pis Min                                   -4.08098\n",
      "trainer/Policy mu Mean                                 0.157437\n",
      "trainer/Policy mu Std                                  1.65756\n",
      "trainer/Policy mu Max                                  4.67507\n",
      "trainer/Policy mu Min                                 -6.27616\n",
      "trainer/Policy log std Mean                           -0.732587\n",
      "trainer/Policy log std Std                             0.265177\n",
      "trainer/Policy log std Max                             0.371698\n",
      "trainer/Policy log std Min                            -1.90998\n",
      "trainer/Alpha                                          0.0134667\n",
      "trainer/Alpha Loss                                     2.93181\n",
      "exploration/num steps total                        90000\n",
      "exploration/num paths total                           90\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.260564\n",
      "exploration/Rewards Std                                0.417807\n",
      "exploration/Rewards Max                                1.36855\n",
      "exploration/Rewards Min                               -1.34668\n",
      "exploration/Returns Mean                             260.564\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              260.564\n",
      "exploration/Returns Min                              260.564\n",
      "exploration/Actions Mean                               0.075223\n",
      "exploration/Actions Std                                0.770853\n",
      "exploration/Actions Max                                0.999991\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          260.564\n",
      "exploration/env_infos/final/reward_run Mean            1.93198\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.93198\n",
      "exploration/env_infos/final/reward_run Min             1.93198\n",
      "exploration/env_infos/initial/reward_run Mean          1.0355\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.0355\n",
      "exploration/env_infos/initial/reward_run Min           1.0355\n",
      "exploration/env_infos/reward_run Mean                  1.75433\n",
      "exploration/env_infos/reward_run Std                   0.820316\n",
      "exploration/env_infos/reward_run Max                   3.96347\n",
      "exploration/env_infos/reward_run Min                  -0.547919\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.374083\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.374083\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.374083\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.30403\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.30403\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.30403\n",
      "exploration/env_infos/reward_ctrl Mean                -0.359924\n",
      "exploration/env_infos/reward_ctrl Std                  0.101019\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0307623\n",
      "exploration/env_infos/reward_ctrl Min                 -0.587185\n",
      "exploration/env_infos/final/height Mean               -0.0526807\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.0526807\n",
      "exploration/env_infos/final/height Min                -0.0526807\n",
      "exploration/env_infos/initial/height Mean             -0.0713918\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0713918\n",
      "exploration/env_infos/initial/height Min              -0.0713918\n",
      "exploration/env_infos/height Mean                     -0.105876\n",
      "exploration/env_infos/height Std                       0.120381\n",
      "exploration/env_infos/height Max                       0.313505\n",
      "exploration/env_infos/height Min                      -0.417474\n",
      "exploration/env_infos/final/reward_angular Mean        0.394678\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.394678\n",
      "exploration/env_infos/final/reward_angular Min         0.394678\n",
      "exploration/env_infos/initial/reward_angular Mean      0.635451\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.635451\n",
      "exploration/env_infos/initial/reward_angular Min       0.635451\n",
      "exploration/env_infos/reward_angular Mean             -0.0516804\n",
      "exploration/env_infos/reward_angular Std               1.88711\n",
      "exploration/env_infos/reward_angular Max               5.89379\n",
      "exploration/env_infos/reward_angular Min              -5.83491\n",
      "evaluation/num steps total                             2.225e+06\n",
      "evaluation/num paths total                          2225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0777405\n",
      "evaluation/Rewards Std                                 1.13029\n",
      "evaluation/Rewards Max                                 6.92463\n",
      "evaluation/Rewards Min                                -6.70207\n",
      "evaluation/Returns Mean                               77.7405\n",
      "evaluation/Returns Std                               468.36\n",
      "evaluation/Returns Max                              1719.04\n",
      "evaluation/Returns Min                              -428.827\n",
      "evaluation/Actions Mean                               -0.0205537\n",
      "evaluation/Actions Std                                 0.735281\n",
      "evaluation/Actions Max                                 0.999991\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            77.7405\n",
      "evaluation/env_infos/final/reward_run Mean             0.584581\n",
      "evaluation/env_infos/final/reward_run Std              1.2345\n",
      "evaluation/env_infos/final/reward_run Max              4.66538\n",
      "evaluation/env_infos/final/reward_run Min             -1.01093\n",
      "evaluation/env_infos/initial/reward_run Mean           0.475202\n",
      "evaluation/env_infos/initial/reward_run Std            0.476673\n",
      "evaluation/env_infos/initial/reward_run Max            1.04674\n",
      "evaluation/env_infos/initial/reward_run Min           -0.266163\n",
      "evaluation/env_infos/reward_run Mean                   0.508684\n",
      "evaluation/env_infos/reward_run Std                    1.46279\n",
      "evaluation/env_infos/reward_run Max                    5.21062\n",
      "evaluation/env_infos/reward_run Min                   -5.55992\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.331571\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.137014\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0720629\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.569857\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.326979\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.090521\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.151501\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.457922\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.324636\n",
      "evaluation/env_infos/reward_ctrl Std                   0.132737\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00912276\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596895\n",
      "evaluation/env_infos/final/height Mean                -0.340128\n",
      "evaluation/env_infos/final/height Std                  0.222811\n",
      "evaluation/env_infos/final/height Max                  0.196906\n",
      "evaluation/env_infos/final/height Min                 -0.552908\n",
      "evaluation/env_infos/initial/height Mean              -0.00414833\n",
      "evaluation/env_infos/initial/height Std                0.049169\n",
      "evaluation/env_infos/initial/height Max                0.0767769\n",
      "evaluation/env_infos/initial/height Min               -0.106175\n",
      "evaluation/env_infos/height Mean                      -0.319532\n",
      "evaluation/env_infos/height Std                        0.221331\n",
      "evaluation/env_infos/height Max                        0.41629\n",
      "evaluation/env_infos/height Min                       -0.594271\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.621786\n",
      "evaluation/env_infos/final/reward_angular Std          1.63061\n",
      "evaluation/env_infos/final/reward_angular Max          3.28038\n",
      "evaluation/env_infos/final/reward_angular Min         -4.02697\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.716204\n",
      "evaluation/env_infos/initial/reward_angular Std        0.923151\n",
      "evaluation/env_infos/initial/reward_angular Max        1.22762\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.99644\n",
      "evaluation/env_infos/reward_angular Mean              -0.0111545\n",
      "evaluation/env_infos/reward_angular Std                1.67221\n",
      "evaluation/env_infos/reward_angular Max                8.77095\n",
      "evaluation/env_infos/reward_angular Min               -6.77427\n",
      "time/data storing (s)                                  0.0163256\n",
      "time/evaluation sampling (s)                          25.7111\n",
      "time/exploration sampling (s)                          1.10243\n",
      "time/logging (s)                                       0.260024\n",
      "time/saving (s)                                        0.028896\n",
      "time/training (s)                                      4.65185\n",
      "time/epoch (s)                                        31.7706\n",
      "time/total (s)                                      2599.39\n",
      "Epoch                                                 88\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:55:51.140417 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 89 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 91000\n",
      "trainer/QF1 Loss                                       0.876979\n",
      "trainer/QF2 Loss                                       0.610547\n",
      "trainer/Policy Loss                                    3.09533\n",
      "trainer/Q1 Predictions Mean                            2.78319\n",
      "trainer/Q1 Predictions Std                            10.343\n",
      "trainer/Q1 Predictions Max                            50.2544\n",
      "trainer/Q1 Predictions Min                           -15.6211\n",
      "trainer/Q2 Predictions Mean                            2.57495\n",
      "trainer/Q2 Predictions Std                            10.2569\n",
      "trainer/Q2 Predictions Max                            50.1506\n",
      "trainer/Q2 Predictions Min                           -14.4009\n",
      "trainer/Q Targets Mean                                 2.61859\n",
      "trainer/Q Targets Std                                 10.4254\n",
      "trainer/Q Targets Max                                 50.8401\n",
      "trainer/Q Targets Min                                -16.1795\n",
      "trainer/Log Pis Mean                                   6.01087\n",
      "trainer/Log Pis Std                                    5.43005\n",
      "trainer/Log Pis Max                                   24.4288\n",
      "trainer/Log Pis Min                                   -3.74266\n",
      "trainer/Policy mu Mean                                 0.263408\n",
      "trainer/Policy mu Std                                  1.52477\n",
      "trainer/Policy mu Max                                  4.33\n",
      "trainer/Policy mu Min                                 -4.94913\n",
      "trainer/Policy log std Mean                           -0.802028\n",
      "trainer/Policy log std Std                             0.272582\n",
      "trainer/Policy log std Max                             0.41953\n",
      "trainer/Policy log std Min                            -1.80152\n",
      "trainer/Alpha                                          0.0129228\n",
      "trainer/Alpha Loss                                     0.0472865\n",
      "exploration/num steps total                        91000\n",
      "exploration/num paths total                           91\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.287914\n",
      "exploration/Rewards Std                                1.35341\n",
      "exploration/Rewards Max                                4.54486\n",
      "exploration/Rewards Min                               -3.87441\n",
      "exploration/Returns Mean                             287.914\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              287.914\n",
      "exploration/Returns Min                              287.914\n",
      "exploration/Actions Mean                               0.176459\n",
      "exploration/Actions Std                                0.81893\n",
      "exploration/Actions Max                                0.999976\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          287.914\n",
      "exploration/env_infos/final/reward_run Mean            1.63923\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.63923\n",
      "exploration/env_infos/final/reward_run Min             1.63923\n",
      "exploration/env_infos/initial/reward_run Mean          0.692065\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.692065\n",
      "exploration/env_infos/initial/reward_run Min           0.692065\n",
      "exploration/env_infos/reward_run Mean                  2.18592\n",
      "exploration/env_infos/reward_run Std                   0.997036\n",
      "exploration/env_infos/reward_run Max                   4.71659\n",
      "exploration/env_infos/reward_run Min                  -1.11414\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.377942\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.377942\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.377942\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.307668\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.307668\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.307668\n",
      "exploration/env_infos/reward_ctrl Mean                -0.42107\n",
      "exploration/env_infos/reward_ctrl Std                  0.0806677\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0754557\n",
      "exploration/env_infos/reward_ctrl Min                 -0.584853\n",
      "exploration/env_infos/final/height Mean               -0.210618\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.210618\n",
      "exploration/env_infos/final/height Min                -0.210618\n",
      "exploration/env_infos/initial/height Mean              0.0365665\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0365665\n",
      "exploration/env_infos/initial/height Min               0.0365665\n",
      "exploration/env_infos/height Mean                     -0.124342\n",
      "exploration/env_infos/height Std                       0.103187\n",
      "exploration/env_infos/height Max                       0.233549\n",
      "exploration/env_infos/height Min                      -0.365983\n",
      "exploration/env_infos/final/reward_angular Mean        3.12598\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         3.12598\n",
      "exploration/env_infos/final/reward_angular Min         3.12598\n",
      "exploration/env_infos/initial/reward_angular Mean     -1.92807\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -1.92807\n",
      "exploration/env_infos/initial/reward_angular Min      -1.92807\n",
      "exploration/env_infos/reward_angular Mean             -0.0994553\n",
      "exploration/env_infos/reward_angular Std               1.69714\n",
      "exploration/env_infos/reward_angular Max               4.76894\n",
      "exploration/env_infos/reward_angular Min              -5.47625\n",
      "evaluation/num steps total                             2.25e+06\n",
      "evaluation/num paths total                          2250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0801151\n",
      "evaluation/Rewards Std                                 1.10614\n",
      "evaluation/Rewards Max                                 8.07308\n",
      "evaluation/Rewards Min                                -6.1857\n",
      "evaluation/Returns Mean                               80.1151\n",
      "evaluation/Returns Std                               413.885\n",
      "evaluation/Returns Max                              1585.79\n",
      "evaluation/Returns Min                              -485.038\n",
      "evaluation/Actions Mean                                0.10054\n",
      "evaluation/Actions Std                                 0.735968\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            80.1151\n",
      "evaluation/env_infos/final/reward_run Mean             0.801134\n",
      "evaluation/env_infos/final/reward_run Std              1.33947\n",
      "evaluation/env_infos/final/reward_run Max              3.67928\n",
      "evaluation/env_infos/final/reward_run Min             -1.07166\n",
      "evaluation/env_infos/initial/reward_run Mean           0.43607\n",
      "evaluation/env_infos/initial/reward_run Std            0.452753\n",
      "evaluation/env_infos/initial/reward_run Max            1.1516\n",
      "evaluation/env_infos/initial/reward_run Min           -0.462555\n",
      "evaluation/env_infos/reward_run Mean                   0.574692\n",
      "evaluation/env_infos/reward_run Std                    1.37571\n",
      "evaluation/env_infos/reward_run Max                    4.77155\n",
      "evaluation/env_infos/reward_run Min                   -5.34485\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.306755\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.152016\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0794264\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.567657\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.305803\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0669782\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.131049\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.445755\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.331055\n",
      "evaluation/env_infos/reward_ctrl Std                   0.130163\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00583705\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.599031\n",
      "evaluation/env_infos/final/height Mean                -0.311255\n",
      "evaluation/env_infos/final/height Std                  0.191076\n",
      "evaluation/env_infos/final/height Max                  0.0776042\n",
      "evaluation/env_infos/final/height Min                 -0.577281\n",
      "evaluation/env_infos/initial/height Mean               0.0104977\n",
      "evaluation/env_infos/initial/height Std                0.0613843\n",
      "evaluation/env_infos/initial/height Max                0.0859273\n",
      "evaluation/env_infos/initial/height Min               -0.100804\n",
      "evaluation/env_infos/height Mean                      -0.282512\n",
      "evaluation/env_infos/height Std                        0.231763\n",
      "evaluation/env_infos/height Max                        0.412231\n",
      "evaluation/env_infos/height Min                       -0.591652\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.0760117\n",
      "evaluation/env_infos/final/reward_angular Std          1.2331\n",
      "evaluation/env_infos/final/reward_angular Max          3.28948\n",
      "evaluation/env_infos/final/reward_angular Min         -3.08765\n",
      "evaluation/env_infos/initial/reward_angular Mean      -1.08329\n",
      "evaluation/env_infos/initial/reward_angular Std        0.90253\n",
      "evaluation/env_infos/initial/reward_angular Max        1.63171\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.85644\n",
      "evaluation/env_infos/reward_angular Mean               7.53702e-06\n",
      "evaluation/env_infos/reward_angular Std                1.69708\n",
      "evaluation/env_infos/reward_angular Max                8.53391\n",
      "evaluation/env_infos/reward_angular Min               -8.09273\n",
      "time/data storing (s)                                  0.0156048\n",
      "time/evaluation sampling (s)                          22.4412\n",
      "time/exploration sampling (s)                          1.12937\n",
      "time/logging (s)                                       0.261209\n",
      "time/saving (s)                                        0.0335597\n",
      "time/training (s)                                      4.4522\n",
      "time/epoch (s)                                        28.3331\n",
      "time/total (s)                                      2628.5\n",
      "Epoch                                                 89\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:56:22.107357 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 90 finished\n",
      "-------------------------------------------------  -------------\n",
      "replay_buffer/size                                 92000\n",
      "trainer/QF1 Loss                                       0.801149\n",
      "trainer/QF2 Loss                                       0.892478\n",
      "trainer/Policy Loss                                    1.67125\n",
      "trainer/Q1 Predictions Mean                            4.52683\n",
      "trainer/Q1 Predictions Std                            11.5154\n",
      "trainer/Q1 Predictions Max                            48.8017\n",
      "trainer/Q1 Predictions Min                           -11.7024\n",
      "trainer/Q2 Predictions Mean                            4.36417\n",
      "trainer/Q2 Predictions Std                            11.4558\n",
      "trainer/Q2 Predictions Max                            47.8063\n",
      "trainer/Q2 Predictions Min                           -11.7042\n",
      "trainer/Q Targets Mean                                 4.49673\n",
      "trainer/Q Targets Std                                 11.4588\n",
      "trainer/Q Targets Max                                 47.6441\n",
      "trainer/Q Targets Min                                -11.8951\n",
      "trainer/Log Pis Mean                                   6.33045\n",
      "trainer/Log Pis Std                                    5.40638\n",
      "trainer/Log Pis Max                                   25.3105\n",
      "trainer/Log Pis Min                                   -4.49144\n",
      "trainer/Policy mu Mean                                 0.263312\n",
      "trainer/Policy mu Std                                  1.62552\n",
      "trainer/Policy mu Max                                  4.52227\n",
      "trainer/Policy mu Min                                 -8.54528\n",
      "trainer/Policy log std Mean                           -0.766728\n",
      "trainer/Policy log std Std                             0.278896\n",
      "trainer/Policy log std Max                             0.310335\n",
      "trainer/Policy log std Min                            -1.95142\n",
      "trainer/Alpha                                          0.0131658\n",
      "trainer/Alpha Loss                                     1.43114\n",
      "exploration/num steps total                        92000\n",
      "exploration/num paths total                           92\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.127591\n",
      "exploration/Rewards Std                                1.60848\n",
      "exploration/Rewards Max                                5.79723\n",
      "exploration/Rewards Min                               -4.94533\n",
      "exploration/Returns Mean                             127.591\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              127.591\n",
      "exploration/Returns Min                              127.591\n",
      "exploration/Actions Mean                              -0.24977\n",
      "exploration/Actions Std                                0.750905\n",
      "exploration/Actions Max                                0.999989\n",
      "exploration/Actions Min                               -0.999995\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          127.591\n",
      "exploration/env_infos/final/reward_run Mean            1.78051\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             1.78051\n",
      "exploration/env_infos/final/reward_run Min             1.78051\n",
      "exploration/env_infos/initial/reward_run Mean          0.96295\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.96295\n",
      "exploration/env_infos/initial/reward_run Min           0.96295\n",
      "exploration/env_infos/reward_run Mean                  0.303035\n",
      "exploration/env_infos/reward_run Std                   0.622694\n",
      "exploration/env_infos/reward_run Max                   2.22403\n",
      "exploration/env_infos/reward_run Min                  -1.5537\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.481212\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.481212\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.481212\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.369658\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.369658\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.369658\n",
      "exploration/env_infos/reward_ctrl Mean                -0.375746\n",
      "exploration/env_infos/reward_ctrl Std                  0.101823\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0956328\n",
      "exploration/env_infos/reward_ctrl Min                 -0.588652\n",
      "exploration/env_infos/final/height Mean               -0.405484\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.405484\n",
      "exploration/env_infos/final/height Min                -0.405484\n",
      "exploration/env_infos/initial/height Mean             -0.0414678\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0414678\n",
      "exploration/env_infos/initial/height Min              -0.0414678\n",
      "exploration/env_infos/height Mean                     -0.455952\n",
      "exploration/env_infos/height Std                       0.110096\n",
      "exploration/env_infos/height Max                       0.213013\n",
      "exploration/env_infos/height Min                      -0.59243\n",
      "exploration/env_infos/final/reward_angular Mean        2.18656\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         2.18656\n",
      "exploration/env_infos/final/reward_angular Min         2.18656\n",
      "exploration/env_infos/initial/reward_angular Mean      0.408707\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.408707\n",
      "exploration/env_infos/initial/reward_angular Min       0.408707\n",
      "exploration/env_infos/reward_angular Mean              0.0867686\n",
      "exploration/env_infos/reward_angular Std               1.72795\n",
      "exploration/env_infos/reward_angular Max               6.74155\n",
      "exploration/env_infos/reward_angular Min              -4.88343\n",
      "evaluation/num steps total                             2.275e+06\n",
      "evaluation/num paths total                          2275\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.0492103\n",
      "evaluation/Rewards Std                                 1.18059\n",
      "evaluation/Rewards Max                                 6.48861\n",
      "evaluation/Rewards Min                                -5.7355\n",
      "evaluation/Returns Mean                               49.2103\n",
      "evaluation/Returns Std                               401.713\n",
      "evaluation/Returns Max                              1628.87\n",
      "evaluation/Returns Min                              -345.943\n",
      "evaluation/Actions Mean                                0.0404867\n",
      "evaluation/Actions Std                                 0.75941\n",
      "evaluation/Actions Max                                 0.999992\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                            49.2103\n",
      "evaluation/env_infos/final/reward_run Mean             0.536059\n",
      "evaluation/env_infos/final/reward_run Std              1.18837\n",
      "evaluation/env_infos/final/reward_run Max              3.61933\n",
      "evaluation/env_infos/final/reward_run Min             -1.19045\n",
      "evaluation/env_infos/initial/reward_run Mean           0.492852\n",
      "evaluation/env_infos/initial/reward_run Std            0.419\n",
      "evaluation/env_infos/initial/reward_run Max            1.1081\n",
      "evaluation/env_infos/initial/reward_run Min           -0.264151\n",
      "evaluation/env_infos/reward_run Mean                   0.345665\n",
      "evaluation/env_infos/reward_run Std                    1.29618\n",
      "evaluation/env_infos/reward_run Max                    4.73195\n",
      "evaluation/env_infos/reward_run Min                   -5.1041\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.33437\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.138011\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0795167\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.587008\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.329197\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0756488\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.186207\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.495644\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.347005\n",
      "evaluation/env_infos/reward_ctrl Std                   0.118051\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0157967\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.595472\n",
      "evaluation/env_infos/final/height Mean                -0.322801\n",
      "evaluation/env_infos/final/height Std                  0.21715\n",
      "evaluation/env_infos/final/height Max                  0.100619\n",
      "evaluation/env_infos/final/height Min                 -0.577266\n",
      "evaluation/env_infos/initial/height Mean              -0.0173566\n",
      "evaluation/env_infos/initial/height Std                0.0513022\n",
      "evaluation/env_infos/initial/height Max                0.0806605\n",
      "evaluation/env_infos/initial/height Min               -0.088586\n",
      "evaluation/env_infos/height Mean                      -0.274619\n",
      "evaluation/env_infos/height Std                        0.210801\n",
      "evaluation/env_infos/height Max                        0.498861\n",
      "evaluation/env_infos/height Min                       -0.593676\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.520335\n",
      "evaluation/env_infos/final/reward_angular Std          1.17729\n",
      "evaluation/env_infos/final/reward_angular Max          1.00603\n",
      "evaluation/env_infos/final/reward_angular Min         -3.8063\n",
      "evaluation/env_infos/initial/reward_angular Mean      -1.29588\n",
      "evaluation/env_infos/initial/reward_angular Std        0.793568\n",
      "evaluation/env_infos/initial/reward_angular Max        0.472946\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.65266\n",
      "evaluation/env_infos/reward_angular Mean              -0.0111358\n",
      "evaluation/env_infos/reward_angular Std                1.77659\n",
      "evaluation/env_infos/reward_angular Max                9.09258\n",
      "evaluation/env_infos/reward_angular Min               -9.26501\n",
      "time/data storing (s)                                  0.0167235\n",
      "time/evaluation sampling (s)                          24.3358\n",
      "time/exploration sampling (s)                          1.05841\n",
      "time/logging (s)                                       0.285167\n",
      "time/saving (s)                                        0.0291796\n",
      "time/training (s)                                      4.39656\n",
      "time/epoch (s)                                        30.1218\n",
      "time/total (s)                                      2659.49\n",
      "Epoch                                                 90\n",
      "-------------------------------------------------  -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:56:53.513240 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 91 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 93000\n",
      "trainer/QF1 Loss                                       0.917441\n",
      "trainer/QF2 Loss                                       0.841611\n",
      "trainer/Policy Loss                                    4.12475\n",
      "trainer/Q1 Predictions Mean                            2.17892\n",
      "trainer/Q1 Predictions Std                             9.97513\n",
      "trainer/Q1 Predictions Max                            49.131\n",
      "trainer/Q1 Predictions Min                           -10.6332\n",
      "trainer/Q2 Predictions Mean                            2.23614\n",
      "trainer/Q2 Predictions Std                             9.98644\n",
      "trainer/Q2 Predictions Max                            47.9431\n",
      "trainer/Q2 Predictions Min                           -11.6457\n",
      "trainer/Q Targets Mean                                 2.09902\n",
      "trainer/Q Targets Std                                  9.97305\n",
      "trainer/Q Targets Max                                 45.4867\n",
      "trainer/Q Targets Min                                -10.4892\n",
      "trainer/Log Pis Mean                                   6.54452\n",
      "trainer/Log Pis Std                                    6.07109\n",
      "trainer/Log Pis Max                                   34.1207\n",
      "trainer/Log Pis Min                                   -4.71486\n",
      "trainer/Policy mu Mean                                 0.19792\n",
      "trainer/Policy mu Std                                  1.66262\n",
      "trainer/Policy mu Max                                  6.66667\n",
      "trainer/Policy mu Min                                 -7.87851\n",
      "trainer/Policy log std Mean                           -0.736602\n",
      "trainer/Policy log std Std                             0.28856\n",
      "trainer/Policy log std Max                             0.602389\n",
      "trainer/Policy log std Min                            -2.85627\n",
      "trainer/Alpha                                          0.0138175\n",
      "trainer/Alpha Loss                                     2.33293\n",
      "exploration/num steps total                        93000\n",
      "exploration/num paths total                           93\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.248971\n",
      "exploration/Rewards Std                                1.06437\n",
      "exploration/Rewards Max                                3.99385\n",
      "exploration/Rewards Min                               -3.4802\n",
      "exploration/Returns Mean                            -248.971\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -248.971\n",
      "exploration/Returns Min                             -248.971\n",
      "exploration/Actions Mean                               0.00388527\n",
      "exploration/Actions Std                                0.707161\n",
      "exploration/Actions Max                                0.999709\n",
      "exploration/Actions Min                               -0.999998\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -248.971\n",
      "exploration/env_infos/final/reward_run Mean           -0.153787\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.153787\n",
      "exploration/env_infos/final/reward_run Min            -0.153787\n",
      "exploration/env_infos/initial/reward_run Mean          1.00608\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           1.00608\n",
      "exploration/env_infos/initial/reward_run Min           1.00608\n",
      "exploration/env_infos/reward_run Mean                  0.202472\n",
      "exploration/env_infos/reward_run Std                   0.553276\n",
      "exploration/env_infos/reward_run Max                   1.58187\n",
      "exploration/env_infos/reward_run Min                  -1.19345\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.119297\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.119297\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.119297\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.424574\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.424574\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.424574\n",
      "exploration/env_infos/reward_ctrl Mean                -0.300055\n",
      "exploration/env_infos/reward_ctrl Std                  0.086211\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0860008\n",
      "exploration/env_infos/reward_ctrl Min                 -0.571276\n",
      "exploration/env_infos/final/height Mean               -0.436912\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.436912\n",
      "exploration/env_infos/final/height Min                -0.436912\n",
      "exploration/env_infos/initial/height Mean             -0.0174739\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0174739\n",
      "exploration/env_infos/initial/height Min              -0.0174739\n",
      "exploration/env_infos/height Mean                     -0.487128\n",
      "exploration/env_infos/height Std                       0.115024\n",
      "exploration/env_infos/height Max                       0.342756\n",
      "exploration/env_infos/height Min                      -0.58711\n",
      "exploration/env_infos/final/reward_angular Mean       -1.55578\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -1.55578\n",
      "exploration/env_infos/final/reward_angular Min        -1.55578\n",
      "exploration/env_infos/initial/reward_angular Mean      0.442399\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.442399\n",
      "exploration/env_infos/initial/reward_angular Min       0.442399\n",
      "exploration/env_infos/reward_angular Mean              0.115009\n",
      "exploration/env_infos/reward_angular Std               1.46622\n",
      "exploration/env_infos/reward_angular Max               6.08016\n",
      "exploration/env_infos/reward_angular Min              -4.33321\n",
      "evaluation/num steps total                             2.3e+06\n",
      "evaluation/num paths total                          2300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.289173\n",
      "evaluation/Rewards Std                                 1.15873\n",
      "evaluation/Rewards Max                                 6.38303\n",
      "evaluation/Rewards Min                                -4.80815\n",
      "evaluation/Returns Mean                              289.173\n",
      "evaluation/Returns Std                               498.743\n",
      "evaluation/Returns Max                              1760.83\n",
      "evaluation/Returns Min                              -274.964\n",
      "evaluation/Actions Mean                                0.0587906\n",
      "evaluation/Actions Std                                 0.757431\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           289.173\n",
      "evaluation/env_infos/final/reward_run Mean             0.0318951\n",
      "evaluation/env_infos/final/reward_run Std              1.51656\n",
      "evaluation/env_infos/final/reward_run Max              3.20006\n",
      "evaluation/env_infos/final/reward_run Min             -3.39294\n",
      "evaluation/env_infos/initial/reward_run Mean           0.554379\n",
      "evaluation/env_infos/initial/reward_run Std            0.444434\n",
      "evaluation/env_infos/initial/reward_run Max            1.13985\n",
      "evaluation/env_infos/initial/reward_run Min           -0.126345\n",
      "evaluation/env_infos/reward_run Mean                   0.0263539\n",
      "evaluation/env_infos/reward_run Std                    1.75038\n",
      "evaluation/env_infos/reward_run Max                    4.98326\n",
      "evaluation/env_infos/reward_run Min                   -5.0738\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.337771\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0894921\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.160793\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.537676\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.39802\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0653289\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.268483\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.531768\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.346295\n",
      "evaluation/env_infos/reward_ctrl Std                   0.107296\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0160349\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.594361\n",
      "evaluation/env_infos/final/height Mean                -0.199089\n",
      "evaluation/env_infos/final/height Std                  0.220994\n",
      "evaluation/env_infos/final/height Max                  0.0728673\n",
      "evaluation/env_infos/final/height Min                 -0.572216\n",
      "evaluation/env_infos/initial/height Mean               0.00693481\n",
      "evaluation/env_infos/initial/height Std                0.0424077\n",
      "evaluation/env_infos/initial/height Max                0.0782414\n",
      "evaluation/env_infos/initial/height Min               -0.0788011\n",
      "evaluation/env_infos/height Mean                      -0.149497\n",
      "evaluation/env_infos/height Std                        0.19693\n",
      "evaluation/env_infos/height Max                        0.408573\n",
      "evaluation/env_infos/height Min                       -0.595273\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.338414\n",
      "evaluation/env_infos/final/reward_angular Std          1.37986\n",
      "evaluation/env_infos/final/reward_angular Max          2.6087\n",
      "evaluation/env_infos/final/reward_angular Min         -3.14073\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.51228\n",
      "evaluation/env_infos/initial/reward_angular Std        1.13073\n",
      "evaluation/env_infos/initial/reward_angular Max        1.67351\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.12755\n",
      "evaluation/env_infos/reward_angular Mean               0.0130271\n",
      "evaluation/env_infos/reward_angular Std                1.75991\n",
      "evaluation/env_infos/reward_angular Max                7.31141\n",
      "evaluation/env_infos/reward_angular Min               -7.36073\n",
      "time/data storing (s)                                  0.0152615\n",
      "time/evaluation sampling (s)                          24.8958\n",
      "time/exploration sampling (s)                          1.02167\n",
      "time/logging (s)                                       0.243008\n",
      "time/saving (s)                                        0.0296728\n",
      "time/training (s)                                      4.12342\n",
      "time/epoch (s)                                        30.3288\n",
      "time/total (s)                                      2690.85\n",
      "Epoch                                                 91\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:57:22.637459 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 92 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 94000\n",
      "trainer/QF1 Loss                                       0.789519\n",
      "trainer/QF2 Loss                                       0.971478\n",
      "trainer/Policy Loss                                    2.59033\n",
      "trainer/Q1 Predictions Mean                            3.47455\n",
      "trainer/Q1 Predictions Std                            10.8276\n",
      "trainer/Q1 Predictions Max                            45.4831\n",
      "trainer/Q1 Predictions Min                           -20.1992\n",
      "trainer/Q2 Predictions Mean                            3.51596\n",
      "trainer/Q2 Predictions Std                            10.9019\n",
      "trainer/Q2 Predictions Max                            45.0623\n",
      "trainer/Q2 Predictions Min                           -20.5442\n",
      "trainer/Q Targets Mean                                 3.60033\n",
      "trainer/Q Targets Std                                 10.9032\n",
      "trainer/Q Targets Max                                 44.6444\n",
      "trainer/Q Targets Min                                -19.9944\n",
      "trainer/Log Pis Mean                                   6.30411\n",
      "trainer/Log Pis Std                                    5.2687\n",
      "trainer/Log Pis Max                                   23.6601\n",
      "trainer/Log Pis Min                                   -3.43934\n",
      "trainer/Policy mu Mean                                 0.19181\n",
      "trainer/Policy mu Std                                  1.6304\n",
      "trainer/Policy mu Max                                  4.51104\n",
      "trainer/Policy mu Min                                 -6.80271\n",
      "trainer/Policy log std Mean                           -0.741006\n",
      "trainer/Policy log std Std                             0.267404\n",
      "trainer/Policy log std Max                             0.270872\n",
      "trainer/Policy log std Min                            -1.66696\n",
      "trainer/Alpha                                          0.013805\n",
      "trainer/Alpha Loss                                     1.30222\n",
      "exploration/num steps total                        94000\n",
      "exploration/num paths total                           94\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.160834\n",
      "exploration/Rewards Std                                1.0292\n",
      "exploration/Rewards Max                                3.84632\n",
      "exploration/Rewards Min                               -1.8468\n",
      "exploration/Returns Mean                             160.834\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              160.834\n",
      "exploration/Returns Min                              160.834\n",
      "exploration/Actions Mean                               0.150932\n",
      "exploration/Actions Std                                0.697621\n",
      "exploration/Actions Max                                0.999997\n",
      "exploration/Actions Min                               -0.999991\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          160.834\n",
      "exploration/env_infos/final/reward_run Mean            0.271272\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.271272\n",
      "exploration/env_infos/final/reward_run Min             0.271272\n",
      "exploration/env_infos/initial/reward_run Mean         -0.532519\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.532519\n",
      "exploration/env_infos/initial/reward_run Min          -0.532519\n",
      "exploration/env_infos/reward_run Mean                 -0.658402\n",
      "exploration/env_infos/reward_run Std                   1.1972\n",
      "exploration/env_infos/reward_run Max                   1.42493\n",
      "exploration/env_infos/reward_run Min                  -5.14991\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.269836\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.269836\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.269836\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.353344\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.353344\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.353344\n",
      "exploration/env_infos/reward_ctrl Mean                -0.305673\n",
      "exploration/env_infos/reward_ctrl Std                  0.125759\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0396742\n",
      "exploration/env_infos/reward_ctrl Min                 -0.593299\n",
      "exploration/env_infos/final/height Mean               -0.349037\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.349037\n",
      "exploration/env_infos/final/height Min                -0.349037\n",
      "exploration/env_infos/initial/height Mean              0.00937118\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.00937118\n",
      "exploration/env_infos/initial/height Min               0.00937118\n",
      "exploration/env_infos/height Mean                     -0.354289\n",
      "exploration/env_infos/height Std                       0.24524\n",
      "exploration/env_infos/height Max                       0.517254\n",
      "exploration/env_infos/height Min                      -0.584967\n",
      "exploration/env_infos/final/reward_angular Mean        1.42408\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.42408\n",
      "exploration/env_infos/final/reward_angular Min         1.42408\n",
      "exploration/env_infos/initial/reward_angular Mean      1.78765\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.78765\n",
      "exploration/env_infos/initial/reward_angular Min       1.78765\n",
      "exploration/env_infos/reward_angular Mean             -0.0502017\n",
      "exploration/env_infos/reward_angular Std               2.25668\n",
      "exploration/env_infos/reward_angular Max               7.63722\n",
      "exploration/env_infos/reward_angular Min              -7.60813\n",
      "evaluation/num steps total                             2.325e+06\n",
      "evaluation/num paths total                          2325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.126359\n",
      "evaluation/Rewards Std                                 1.18837\n",
      "evaluation/Rewards Max                                 7.08777\n",
      "evaluation/Rewards Min                                -5.93028\n",
      "evaluation/Returns Mean                              126.359\n",
      "evaluation/Returns Std                               469.117\n",
      "evaluation/Returns Max                              1758.87\n",
      "evaluation/Returns Min                              -322.181\n",
      "evaluation/Actions Mean                               -0.00147025\n",
      "evaluation/Actions Std                                 0.721037\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           126.359\n",
      "evaluation/env_infos/final/reward_run Mean             0.613283\n",
      "evaluation/env_infos/final/reward_run Std              1.40453\n",
      "evaluation/env_infos/final/reward_run Max              4.07956\n",
      "evaluation/env_infos/final/reward_run Min             -1.39692\n",
      "evaluation/env_infos/initial/reward_run Mean           0.492931\n",
      "evaluation/env_infos/initial/reward_run Std            0.475388\n",
      "evaluation/env_infos/initial/reward_run Max            1.05599\n",
      "evaluation/env_infos/initial/reward_run Min           -0.478337\n",
      "evaluation/env_infos/reward_run Mean                   0.264876\n",
      "evaluation/env_infos/reward_run Std                    1.48689\n",
      "evaluation/env_infos/reward_run Max                    5.6133\n",
      "evaluation/env_infos/reward_run Min                   -5.22765\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.291947\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.107217\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.110786\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.476741\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.328024\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0742837\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.166819\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.437498\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.311938\n",
      "evaluation/env_infos/reward_ctrl Std                   0.118787\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00926934\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.595668\n",
      "evaluation/env_infos/final/height Mean                -0.287525\n",
      "evaluation/env_infos/final/height Std                  0.219471\n",
      "evaluation/env_infos/final/height Max                  0.0738714\n",
      "evaluation/env_infos/final/height Min                 -0.57592\n",
      "evaluation/env_infos/initial/height Mean              -0.0119986\n",
      "evaluation/env_infos/initial/height Std                0.052643\n",
      "evaluation/env_infos/initial/height Max                0.0890965\n",
      "evaluation/env_infos/initial/height Min               -0.102867\n",
      "evaluation/env_infos/height Mean                      -0.223352\n",
      "evaluation/env_infos/height Std                        0.234008\n",
      "evaluation/env_infos/height Max                        0.505848\n",
      "evaluation/env_infos/height Min                       -0.596253\n",
      "evaluation/env_infos/final/reward_angular Mean         0.114731\n",
      "evaluation/env_infos/final/reward_angular Std          1.505\n",
      "evaluation/env_infos/final/reward_angular Max          3.33715\n",
      "evaluation/env_infos/final/reward_angular Min         -3.07501\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.365316\n",
      "evaluation/env_infos/initial/reward_angular Std        1.29773\n",
      "evaluation/env_infos/initial/reward_angular Max        3.65322\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.26169\n",
      "evaluation/env_infos/reward_angular Mean              -0.014071\n",
      "evaluation/env_infos/reward_angular Std                1.7653\n",
      "evaluation/env_infos/reward_angular Max                7.89598\n",
      "evaluation/env_infos/reward_angular Min               -8.63906\n",
      "time/data storing (s)                                  0.0167664\n",
      "time/evaluation sampling (s)                          22.6638\n",
      "time/exploration sampling (s)                          1.10753\n",
      "time/logging (s)                                       0.253401\n",
      "time/saving (s)                                        0.0304152\n",
      "time/training (s)                                      4.30505\n",
      "time/epoch (s)                                        28.377\n",
      "time/total (s)                                      2719.98\n",
      "Epoch                                                 92\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:57:49.268200 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 93 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 95000\n",
      "trainer/QF1 Loss                                       0.915617\n",
      "trainer/QF2 Loss                                       0.887341\n",
      "trainer/Policy Loss                                    1.8331\n",
      "trainer/Q1 Predictions Mean                            3.76716\n",
      "trainer/Q1 Predictions Std                            11.6978\n",
      "trainer/Q1 Predictions Max                            51.8492\n",
      "trainer/Q1 Predictions Min                           -22.1927\n",
      "trainer/Q2 Predictions Mean                            3.82751\n",
      "trainer/Q2 Predictions Std                            11.8377\n",
      "trainer/Q2 Predictions Max                            52.2457\n",
      "trainer/Q2 Predictions Min                           -22.5367\n",
      "trainer/Q Targets Mean                                 3.87987\n",
      "trainer/Q Targets Std                                 11.642\n",
      "trainer/Q Targets Max                                 49.9244\n",
      "trainer/Q Targets Min                                -21.6248\n",
      "trainer/Log Pis Mean                                   5.85025\n",
      "trainer/Log Pis Std                                    5.35042\n",
      "trainer/Log Pis Max                                   29.1503\n",
      "trainer/Log Pis Min                                   -5.75515\n",
      "trainer/Policy mu Mean                                 0.208722\n",
      "trainer/Policy mu Std                                  1.58417\n",
      "trainer/Policy mu Max                                  5.37527\n",
      "trainer/Policy mu Min                                 -6.27035\n",
      "trainer/Policy log std Mean                           -0.741978\n",
      "trainer/Policy log std Std                             0.281428\n",
      "trainer/Policy log std Max                             0.930437\n",
      "trainer/Policy log std Min                            -1.78487\n",
      "trainer/Alpha                                          0.013437\n",
      "trainer/Alpha Loss                                    -0.645139\n",
      "exploration/num steps total                        95000\n",
      "exploration/num paths total                           95\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0257372\n",
      "exploration/Rewards Std                                0.704882\n",
      "exploration/Rewards Max                                5.03339\n",
      "exploration/Rewards Min                               -2.12866\n",
      "exploration/Returns Mean                             -25.7372\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -25.7372\n",
      "exploration/Returns Min                              -25.7372\n",
      "exploration/Actions Mean                               0.0223763\n",
      "exploration/Actions Std                                0.752247\n",
      "exploration/Actions Max                                0.99995\n",
      "exploration/Actions Min                               -0.999999\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -25.7372\n",
      "exploration/env_infos/final/reward_run Mean           -0.101328\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.101328\n",
      "exploration/env_infos/final/reward_run Min            -0.101328\n",
      "exploration/env_infos/initial/reward_run Mean         -0.579813\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.579813\n",
      "exploration/env_infos/initial/reward_run Min          -0.579813\n",
      "exploration/env_infos/reward_run Mean                 -0.214135\n",
      "exploration/env_infos/reward_run Std                   0.563253\n",
      "exploration/env_infos/reward_run Max                   1.12535\n",
      "exploration/env_infos/reward_run Min                  -4.60193\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.364569\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.364569\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.364569\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.444085\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.444085\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.444085\n",
      "exploration/env_infos/reward_ctrl Mean                -0.339826\n",
      "exploration/env_infos/reward_ctrl Std                  0.0792254\n",
      "exploration/env_infos/reward_ctrl Max                 -0.117084\n",
      "exploration/env_infos/reward_ctrl Min                 -0.579419\n",
      "exploration/env_infos/final/height Mean               -0.533346\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.533346\n",
      "exploration/env_infos/final/height Min                -0.533346\n",
      "exploration/env_infos/initial/height Mean             -0.0108065\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0108065\n",
      "exploration/env_infos/initial/height Min              -0.0108065\n",
      "exploration/env_infos/height Mean                     -0.51122\n",
      "exploration/env_infos/height Std                       0.1516\n",
      "exploration/env_infos/height Max                       0.324744\n",
      "exploration/env_infos/height Min                      -0.590582\n",
      "exploration/env_infos/final/reward_angular Mean        0.18699\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.18699\n",
      "exploration/env_infos/final/reward_angular Min         0.18699\n",
      "exploration/env_infos/initial/reward_angular Mean      2.39082\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       2.39082\n",
      "exploration/env_infos/initial/reward_angular Min       2.39082\n",
      "exploration/env_infos/reward_angular Mean             -0.0535435\n",
      "exploration/env_infos/reward_angular Std               1.32676\n",
      "exploration/env_infos/reward_angular Max               6.79548\n",
      "exploration/env_infos/reward_angular Min              -7.99556\n",
      "evaluation/num steps total                             2.35e+06\n",
      "evaluation/num paths total                          2350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.11361\n",
      "evaluation/Rewards Std                                 1.16138\n",
      "evaluation/Rewards Max                                 6.37728\n",
      "evaluation/Rewards Min                                -6.03244\n",
      "evaluation/Returns Mean                              113.61\n",
      "evaluation/Returns Std                               439.618\n",
      "evaluation/Returns Max                              1825.04\n",
      "evaluation/Returns Min                              -307.959\n",
      "evaluation/Actions Mean                               -0.0162716\n",
      "evaluation/Actions Std                                 0.723683\n",
      "evaluation/Actions Max                                 0.99998\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           113.61\n",
      "evaluation/env_infos/final/reward_run Mean             0.743974\n",
      "evaluation/env_infos/final/reward_run Std              1.46829\n",
      "evaluation/env_infos/final/reward_run Max              3.65747\n",
      "evaluation/env_infos/final/reward_run Min             -1.38635\n",
      "evaluation/env_infos/initial/reward_run Mean           0.435262\n",
      "evaluation/env_infos/initial/reward_run Std            0.523666\n",
      "evaluation/env_infos/initial/reward_run Max            1.13114\n",
      "evaluation/env_infos/initial/reward_run Min           -0.894091\n",
      "evaluation/env_infos/reward_run Mean                   0.366716\n",
      "evaluation/env_infos/reward_run Std                    1.55102\n",
      "evaluation/env_infos/reward_run Max                    5.64382\n",
      "evaluation/env_infos/reward_run Min                   -5.19948\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.329921\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.121923\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.122673\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.557162\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.354563\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.092769\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.154419\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.509244\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.314389\n",
      "evaluation/env_infos/reward_ctrl Std                   0.122688\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00611939\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.588251\n",
      "evaluation/env_infos/final/height Mean                -0.309355\n",
      "evaluation/env_infos/final/height Std                  0.169436\n",
      "evaluation/env_infos/final/height Max                 -0.022052\n",
      "evaluation/env_infos/final/height Min                 -0.573164\n",
      "evaluation/env_infos/initial/height Mean               0.00613569\n",
      "evaluation/env_infos/initial/height Std                0.0514398\n",
      "evaluation/env_infos/initial/height Max                0.0951446\n",
      "evaluation/env_infos/initial/height Min               -0.0669019\n",
      "evaluation/env_infos/height Mean                      -0.275915\n",
      "evaluation/env_infos/height Std                        0.202555\n",
      "evaluation/env_infos/height Max                        0.449702\n",
      "evaluation/env_infos/height Min                       -0.593507\n",
      "evaluation/env_infos/final/reward_angular Mean         0.0895796\n",
      "evaluation/env_infos/final/reward_angular Std          1.36489\n",
      "evaluation/env_infos/final/reward_angular Max          3.49389\n",
      "evaluation/env_infos/final/reward_angular Min         -2.55123\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.21544\n",
      "evaluation/env_infos/initial/reward_angular Std        1.28431\n",
      "evaluation/env_infos/initial/reward_angular Max        2.53389\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.41111\n",
      "evaluation/env_infos/reward_angular Mean              -0.0231826\n",
      "evaluation/env_infos/reward_angular Std                1.7814\n",
      "evaluation/env_infos/reward_angular Max                7.84115\n",
      "evaluation/env_infos/reward_angular Min               -8.59206\n",
      "time/data storing (s)                                  0.0137725\n",
      "time/evaluation sampling (s)                          20.6365\n",
      "time/exploration sampling (s)                          0.988405\n",
      "time/logging (s)                                       0.235688\n",
      "time/saving (s)                                        0.0273974\n",
      "time/training (s)                                      3.88618\n",
      "time/epoch (s)                                        25.7879\n",
      "time/total (s)                                      2746.59\n",
      "Epoch                                                 93\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:58:17.728949 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 94 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 96000\n",
      "trainer/QF1 Loss                                       0.773278\n",
      "trainer/QF2 Loss                                       0.632831\n",
      "trainer/Policy Loss                                    2.18452\n",
      "trainer/Q1 Predictions Mean                            3.46121\n",
      "trainer/Q1 Predictions Std                            10.5205\n",
      "trainer/Q1 Predictions Max                            42.6349\n",
      "trainer/Q1 Predictions Min                           -11.872\n",
      "trainer/Q2 Predictions Mean                            3.36587\n",
      "trainer/Q2 Predictions Std                            10.4735\n",
      "trainer/Q2 Predictions Max                            45.1874\n",
      "trainer/Q2 Predictions Min                           -10.8689\n",
      "trainer/Q Targets Mean                                 3.34248\n",
      "trainer/Q Targets Std                                 10.5087\n",
      "trainer/Q Targets Max                                 43.6918\n",
      "trainer/Q Targets Min                                -11.5637\n",
      "trainer/Log Pis Mean                                   5.72303\n",
      "trainer/Log Pis Std                                    5.28269\n",
      "trainer/Log Pis Max                                   25.4426\n",
      "trainer/Log Pis Min                                   -5.03491\n",
      "trainer/Policy mu Mean                                 0.255538\n",
      "trainer/Policy mu Std                                  1.54929\n",
      "trainer/Policy mu Max                                  4.99167\n",
      "trainer/Policy mu Min                                 -5.09002\n",
      "trainer/Policy log std Mean                           -0.682197\n",
      "trainer/Policy log std Std                             0.278896\n",
      "trainer/Policy log std Max                             0.500683\n",
      "trainer/Policy log std Min                            -1.89961\n",
      "trainer/Alpha                                          0.0141904\n",
      "trainer/Alpha Loss                                    -1.17888\n",
      "exploration/num steps total                        96000\n",
      "exploration/num paths total                           96\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.283986\n",
      "exploration/Rewards Std                                0.447238\n",
      "exploration/Rewards Max                                2.57998\n",
      "exploration/Rewards Min                               -1.46834\n",
      "exploration/Returns Mean                            -283.986\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -283.986\n",
      "exploration/Returns Min                             -283.986\n",
      "exploration/Actions Mean                               0.152458\n",
      "exploration/Actions Std                                0.711449\n",
      "exploration/Actions Max                                0.999976\n",
      "exploration/Actions Min                               -0.999981\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -283.986\n",
      "exploration/env_infos/final/reward_run Mean            0.272196\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.272196\n",
      "exploration/env_infos/final/reward_run Min             0.272196\n",
      "exploration/env_infos/initial/reward_run Mean         -0.466607\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max          -0.466607\n",
      "exploration/env_infos/initial/reward_run Min          -0.466607\n",
      "exploration/env_infos/reward_run Mean                 -0.274435\n",
      "exploration/env_infos/reward_run Std                   0.64951\n",
      "exploration/env_infos/reward_run Max                   2.12339\n",
      "exploration/env_infos/reward_run Min                  -4.50526\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.24593\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.24593\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.24593\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.302451\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.302451\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.302451\n",
      "exploration/env_infos/reward_ctrl Mean                -0.317641\n",
      "exploration/env_infos/reward_ctrl Std                  0.0770721\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0984735\n",
      "exploration/env_infos/reward_ctrl Min                 -0.588794\n",
      "exploration/env_infos/final/height Mean               -0.561972\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.561972\n",
      "exploration/env_infos/final/height Min                -0.561972\n",
      "exploration/env_infos/initial/height Mean             -0.0025365\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0025365\n",
      "exploration/env_infos/initial/height Min              -0.0025365\n",
      "exploration/env_infos/height Mean                     -0.49309\n",
      "exploration/env_infos/height Std                       0.184572\n",
      "exploration/env_infos/height Max                       0.265247\n",
      "exploration/env_infos/height Min                      -0.58241\n",
      "exploration/env_infos/final/reward_angular Mean        1.0351\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.0351\n",
      "exploration/env_infos/final/reward_angular Min         1.0351\n",
      "exploration/env_infos/initial/reward_angular Mean      0.817727\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       0.817727\n",
      "exploration/env_infos/initial/reward_angular Min       0.817727\n",
      "exploration/env_infos/reward_angular Mean             -0.0550314\n",
      "exploration/env_infos/reward_angular Std               1.30302\n",
      "exploration/env_infos/reward_angular Max               7.02831\n",
      "exploration/env_infos/reward_angular Min              -6.42159\n",
      "evaluation/num steps total                             2.375e+06\n",
      "evaluation/num paths total                          2375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.153917\n",
      "evaluation/Rewards Std                                 1.20435\n",
      "evaluation/Rewards Max                                 6.48314\n",
      "evaluation/Rewards Min                                -5.98396\n",
      "evaluation/Returns Mean                              153.917\n",
      "evaluation/Returns Std                               498.15\n",
      "evaluation/Returns Max                              1830.94\n",
      "evaluation/Returns Min                              -466.869\n",
      "evaluation/Actions Mean                                0.0906955\n",
      "evaluation/Actions Std                                 0.725254\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -0.999998\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           153.917\n",
      "evaluation/env_infos/final/reward_run Mean             0.648893\n",
      "evaluation/env_infos/final/reward_run Std              1.53109\n",
      "evaluation/env_infos/final/reward_run Max              4.22388\n",
      "evaluation/env_infos/final/reward_run Min             -2.81482\n",
      "evaluation/env_infos/initial/reward_run Mean           0.430101\n",
      "evaluation/env_infos/initial/reward_run Std            0.416179\n",
      "evaluation/env_infos/initial/reward_run Max            0.950705\n",
      "evaluation/env_infos/initial/reward_run Min           -0.631707\n",
      "evaluation/env_infos/reward_run Mean                   0.158816\n",
      "evaluation/env_infos/reward_run Std                    1.65219\n",
      "evaluation/env_infos/reward_run Max                    5.35595\n",
      "evaluation/env_infos/reward_run Min                   -5.46558\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.325632\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.115966\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0622378\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.56912\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.345893\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0916073\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.185461\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.501978\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.320532\n",
      "evaluation/env_infos/reward_ctrl Std                   0.115397\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00736395\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.596102\n",
      "evaluation/env_infos/final/height Mean                -0.293576\n",
      "evaluation/env_infos/final/height Std                  0.258262\n",
      "evaluation/env_infos/final/height Max                  0.185007\n",
      "evaluation/env_infos/final/height Min                 -0.580631\n",
      "evaluation/env_infos/initial/height Mean              -0.0150625\n",
      "evaluation/env_infos/initial/height Std                0.0523947\n",
      "evaluation/env_infos/initial/height Max                0.074939\n",
      "evaluation/env_infos/initial/height Min               -0.0920252\n",
      "evaluation/env_infos/height Mean                      -0.266493\n",
      "evaluation/env_infos/height Std                        0.227429\n",
      "evaluation/env_infos/height Max                        0.377142\n",
      "evaluation/env_infos/height Min                       -0.603531\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.434869\n",
      "evaluation/env_infos/final/reward_angular Std          1.68937\n",
      "evaluation/env_infos/final/reward_angular Max          4.51008\n",
      "evaluation/env_infos/final/reward_angular Min         -3.75578\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.144006\n",
      "evaluation/env_infos/initial/reward_angular Std        1.36351\n",
      "evaluation/env_infos/initial/reward_angular Max        4.43473\n",
      "evaluation/env_infos/initial/reward_angular Min       -1.71119\n",
      "evaluation/env_infos/reward_angular Mean               0.0129843\n",
      "evaluation/env_infos/reward_angular Std                1.79495\n",
      "evaluation/env_infos/reward_angular Max                8.71862\n",
      "evaluation/env_infos/reward_angular Min               -8.21142\n",
      "time/data storing (s)                                  0.0171545\n",
      "time/evaluation sampling (s)                          21.6545\n",
      "time/exploration sampling (s)                          1.211\n",
      "time/logging (s)                                       0.247826\n",
      "time/saving (s)                                        0.0290287\n",
      "time/training (s)                                      4.60263\n",
      "time/epoch (s)                                        27.7622\n",
      "time/total (s)                                      2775.06\n",
      "Epoch                                                 94\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:58:49.326585 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 95 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 97000\n",
      "trainer/QF1 Loss                                       0.82501\n",
      "trainer/QF2 Loss                                       0.759989\n",
      "trainer/Policy Loss                                    2.135\n",
      "trainer/Q1 Predictions Mean                            4.46993\n",
      "trainer/Q1 Predictions Std                            11.4268\n",
      "trainer/Q1 Predictions Max                            51.6641\n",
      "trainer/Q1 Predictions Min                            -9.28064\n",
      "trainer/Q2 Predictions Mean                            4.34034\n",
      "trainer/Q2 Predictions Std                            11.4298\n",
      "trainer/Q2 Predictions Max                            52.5811\n",
      "trainer/Q2 Predictions Min                            -9.10542\n",
      "trainer/Q Targets Mean                                 4.47446\n",
      "trainer/Q Targets Std                                 11.5505\n",
      "trainer/Q Targets Max                                 51.6317\n",
      "trainer/Q Targets Min                                -10.0892\n",
      "trainer/Log Pis Mean                                   6.74465\n",
      "trainer/Log Pis Std                                    5.56428\n",
      "trainer/Log Pis Max                                   27.2651\n",
      "trainer/Log Pis Min                                   -3.80831\n",
      "trainer/Policy mu Mean                                 0.385866\n",
      "trainer/Policy mu Std                                  1.64621\n",
      "trainer/Policy mu Max                                  5.18526\n",
      "trainer/Policy mu Min                                 -6.99596\n",
      "trainer/Policy log std Mean                           -0.740865\n",
      "trainer/Policy log std Std                             0.289131\n",
      "trainer/Policy log std Max                             0.203125\n",
      "trainer/Policy log std Min                            -2.38308\n",
      "trainer/Alpha                                          0.0136765\n",
      "trainer/Alpha Loss                                     3.19811\n",
      "exploration/num steps total                        97000\n",
      "exploration/num paths total                           97\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.0382886\n",
      "exploration/Rewards Std                                1.65312\n",
      "exploration/Rewards Max                                6.33032\n",
      "exploration/Rewards Min                               -4.21816\n",
      "exploration/Returns Mean                              38.2886\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                               38.2886\n",
      "exploration/Returns Min                               38.2886\n",
      "exploration/Actions Mean                               0.273528\n",
      "exploration/Actions Std                                0.801295\n",
      "exploration/Actions Max                                0.999994\n",
      "exploration/Actions Min                               -0.999999\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                           38.2886\n",
      "exploration/env_infos/final/reward_run Mean           -0.963417\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.963417\n",
      "exploration/env_infos/final/reward_run Min            -0.963417\n",
      "exploration/env_infos/initial/reward_run Mean          0.725766\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.725766\n",
      "exploration/env_infos/initial/reward_run Min           0.725766\n",
      "exploration/env_infos/reward_run Mean                 -0.130494\n",
      "exploration/env_infos/reward_run Std                   0.699989\n",
      "exploration/env_infos/reward_run Max                   1.79369\n",
      "exploration/env_infos/reward_run Min                  -1.89891\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.385352\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.385352\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.385352\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.493629\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.493629\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.493629\n",
      "exploration/env_infos/reward_ctrl Mean                -0.430135\n",
      "exploration/env_infos/reward_ctrl Std                  0.0704821\n",
      "exploration/env_infos/reward_ctrl Max                 -0.126006\n",
      "exploration/env_infos/reward_ctrl Min                 -0.581498\n",
      "exploration/env_infos/final/height Mean               -0.549902\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.549902\n",
      "exploration/env_infos/final/height Min                -0.549902\n",
      "exploration/env_infos/initial/height Mean             -0.0397444\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0397444\n",
      "exploration/env_infos/initial/height Min              -0.0397444\n",
      "exploration/env_infos/height Mean                     -0.471031\n",
      "exploration/env_infos/height Std                       0.115413\n",
      "exploration/env_infos/height Max                       0.118496\n",
      "exploration/env_infos/height Min                      -0.589599\n",
      "exploration/env_infos/final/reward_angular Mean        0.168185\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         0.168185\n",
      "exploration/env_infos/final/reward_angular Min         0.168185\n",
      "exploration/env_infos/initial/reward_angular Mean      1.90251\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.90251\n",
      "exploration/env_infos/initial/reward_angular Min       1.90251\n",
      "exploration/env_infos/reward_angular Mean              0.129702\n",
      "exploration/env_infos/reward_angular Std               1.71596\n",
      "exploration/env_infos/reward_angular Max               6.5096\n",
      "exploration/env_infos/reward_angular Min              -4.28025\n",
      "evaluation/num steps total                             2.4e+06\n",
      "evaluation/num paths total                          2400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.237405\n",
      "evaluation/Rewards Std                                 1.21111\n",
      "evaluation/Rewards Max                                 5.89914\n",
      "evaluation/Rewards Min                                -5.10962\n",
      "evaluation/Returns Mean                              237.405\n",
      "evaluation/Returns Std                               508.06\n",
      "evaluation/Returns Max                              1724.26\n",
      "evaluation/Returns Min                              -493.752\n",
      "evaluation/Actions Mean                                0.223039\n",
      "evaluation/Actions Std                                 0.753852\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           237.405\n",
      "evaluation/env_infos/final/reward_run Mean             0.173642\n",
      "evaluation/env_infos/final/reward_run Std              1.26616\n",
      "evaluation/env_infos/final/reward_run Max              2.44485\n",
      "evaluation/env_infos/final/reward_run Min             -3.09293\n",
      "evaluation/env_infos/initial/reward_run Mean           0.564549\n",
      "evaluation/env_infos/initial/reward_run Std            0.42212\n",
      "evaluation/env_infos/initial/reward_run Max            1.18288\n",
      "evaluation/env_infos/initial/reward_run Min           -0.480917\n",
      "evaluation/env_infos/reward_run Mean                   0.0506623\n",
      "evaluation/env_infos/reward_run Std                    1.75681\n",
      "evaluation/env_infos/reward_run Max                    5.27794\n",
      "evaluation/env_infos/reward_run Min                   -5.02537\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.355733\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0998052\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.171756\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.516306\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.31536\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0963528\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.116829\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.523841\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.370823\n",
      "evaluation/env_infos/reward_ctrl Std                   0.112224\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0148866\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.598754\n",
      "evaluation/env_infos/final/height Mean                -0.262256\n",
      "evaluation/env_infos/final/height Std                  0.22264\n",
      "evaluation/env_infos/final/height Max                  0.0829352\n",
      "evaluation/env_infos/final/height Min                 -0.577282\n",
      "evaluation/env_infos/initial/height Mean               0.000196424\n",
      "evaluation/env_infos/initial/height Std                0.0545168\n",
      "evaluation/env_infos/initial/height Max                0.078579\n",
      "evaluation/env_infos/initial/height Min               -0.0926558\n",
      "evaluation/env_infos/height Mean                      -0.220315\n",
      "evaluation/env_infos/height Std                        0.214279\n",
      "evaluation/env_infos/height Max                        0.433184\n",
      "evaluation/env_infos/height Min                       -0.600134\n",
      "evaluation/env_infos/final/reward_angular Mean         0.258103\n",
      "evaluation/env_infos/final/reward_angular Std          1.31929\n",
      "evaluation/env_infos/final/reward_angular Max          2.9554\n",
      "evaluation/env_infos/final/reward_angular Min         -2.54322\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.6261\n",
      "evaluation/env_infos/initial/reward_angular Std        1.05623\n",
      "evaluation/env_infos/initial/reward_angular Max        1.48776\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.09819\n",
      "evaluation/env_infos/reward_angular Mean               0.00853945\n",
      "evaluation/env_infos/reward_angular Std                1.79492\n",
      "evaluation/env_infos/reward_angular Max                8.58771\n",
      "evaluation/env_infos/reward_angular Min               -7.00433\n",
      "time/data storing (s)                                  0.0161191\n",
      "time/evaluation sampling (s)                          24.9968\n",
      "time/exploration sampling (s)                          1.13843\n",
      "time/logging (s)                                       0.276408\n",
      "time/saving (s)                                        0.0354782\n",
      "time/training (s)                                      4.37056\n",
      "time/epoch (s)                                        30.8338\n",
      "time/total (s)                                      2806.68\n",
      "Epoch                                                 95\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:59:22.716010 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 96 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 98000\n",
      "trainer/QF1 Loss                                       0.847393\n",
      "trainer/QF2 Loss                                       0.800178\n",
      "trainer/Policy Loss                                    2.78983\n",
      "trainer/Q1 Predictions Mean                            2.80066\n",
      "trainer/Q1 Predictions Std                            11.742\n",
      "trainer/Q1 Predictions Max                            51.3362\n",
      "trainer/Q1 Predictions Min                           -24.4581\n",
      "trainer/Q2 Predictions Mean                            2.86183\n",
      "trainer/Q2 Predictions Std                            11.8153\n",
      "trainer/Q2 Predictions Max                            52.8761\n",
      "trainer/Q2 Predictions Min                           -24.45\n",
      "trainer/Q Targets Mean                                 2.98259\n",
      "trainer/Q Targets Std                                 11.7586\n",
      "trainer/Q Targets Max                                 52.243\n",
      "trainer/Q Targets Min                                -23.2027\n",
      "trainer/Log Pis Mean                                   5.81637\n",
      "trainer/Log Pis Std                                    5.68475\n",
      "trainer/Log Pis Max                                   30.9194\n",
      "trainer/Log Pis Min                                   -5.46042\n",
      "trainer/Policy mu Mean                                 0.241379\n",
      "trainer/Policy mu Std                                  1.57888\n",
      "trainer/Policy mu Max                                  5.60006\n",
      "trainer/Policy mu Min                                 -7.4138\n",
      "trainer/Policy log std Mean                           -0.749265\n",
      "trainer/Policy log std Std                             0.276702\n",
      "trainer/Policy log std Max                             0.331854\n",
      "trainer/Policy log std Min                            -1.82123\n",
      "trainer/Alpha                                          0.0131346\n",
      "trainer/Alpha Loss                                    -0.795237\n",
      "exploration/num steps total                        98000\n",
      "exploration/num paths total                           98\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.233583\n",
      "exploration/Rewards Std                                1.341\n",
      "exploration/Rewards Max                                4.35087\n",
      "exploration/Rewards Min                               -3.58138\n",
      "exploration/Returns Mean                             233.583\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              233.583\n",
      "exploration/Returns Min                              233.583\n",
      "exploration/Actions Mean                               0.253837\n",
      "exploration/Actions Std                                0.742726\n",
      "exploration/Actions Max                                0.999999\n",
      "exploration/Actions Min                               -0.999996\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          233.583\n",
      "exploration/env_infos/final/reward_run Mean           -0.07706\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max            -0.07706\n",
      "exploration/env_infos/final/reward_run Min            -0.07706\n",
      "exploration/env_infos/initial/reward_run Mean          0.0196076\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.0196076\n",
      "exploration/env_infos/initial/reward_run Min           0.0196076\n",
      "exploration/env_infos/reward_run Mean                 -0.509323\n",
      "exploration/env_infos/reward_run Std                   0.81014\n",
      "exploration/env_infos/reward_run Max                   1.60697\n",
      "exploration/env_infos/reward_run Min                  -3.39133\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.346588\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.346588\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.346588\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.284677\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.284677\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.284677\n",
      "exploration/env_infos/reward_ctrl Mean                -0.369645\n",
      "exploration/env_infos/reward_ctrl Std                  0.10845\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0631669\n",
      "exploration/env_infos/reward_ctrl Min                 -0.580692\n",
      "exploration/env_infos/final/height Mean               -0.402442\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                -0.402442\n",
      "exploration/env_infos/final/height Min                -0.402442\n",
      "exploration/env_infos/initial/height Mean             -0.0189492\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max              -0.0189492\n",
      "exploration/env_infos/initial/height Min              -0.0189492\n",
      "exploration/env_infos/height Mean                     -0.271804\n",
      "exploration/env_infos/height Std                       0.22679\n",
      "exploration/env_infos/height Max                       0.327098\n",
      "exploration/env_infos/height Min                      -0.590321\n",
      "exploration/env_infos/final/reward_angular Mean        1.7227\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max         1.7227\n",
      "exploration/env_infos/final/reward_angular Min         1.7227\n",
      "exploration/env_infos/initial/reward_angular Mean      1.6349\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max       1.6349\n",
      "exploration/env_infos/initial/reward_angular Min       1.6349\n",
      "exploration/env_infos/reward_angular Mean              0.0999809\n",
      "exploration/env_infos/reward_angular Std               1.99812\n",
      "exploration/env_infos/reward_angular Max               5.49634\n",
      "exploration/env_infos/reward_angular Min              -5.88938\n",
      "evaluation/num steps total                             2.425e+06\n",
      "evaluation/num paths total                          2425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.198893\n",
      "evaluation/Rewards Std                                 1.28321\n",
      "evaluation/Rewards Max                                 8.18427\n",
      "evaluation/Rewards Min                                -6.09534\n",
      "evaluation/Returns Mean                              198.893\n",
      "evaluation/Returns Std                               543.093\n",
      "evaluation/Returns Max                              1877.55\n",
      "evaluation/Returns Min                              -388.081\n",
      "evaluation/Actions Mean                                0.0810228\n",
      "evaluation/Actions Std                                 0.745756\n",
      "evaluation/Actions Max                                 0.999996\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           198.893\n",
      "evaluation/env_infos/final/reward_run Mean             0.437666\n",
      "evaluation/env_infos/final/reward_run Std              1.24913\n",
      "evaluation/env_infos/final/reward_run Max              3.30418\n",
      "evaluation/env_infos/final/reward_run Min             -2.10064\n",
      "evaluation/env_infos/initial/reward_run Mean           0.493253\n",
      "evaluation/env_infos/initial/reward_run Std            0.421727\n",
      "evaluation/env_infos/initial/reward_run Max            1.13006\n",
      "evaluation/env_infos/initial/reward_run Min           -0.451788\n",
      "evaluation/env_infos/reward_run Mean                   0.27602\n",
      "evaluation/env_infos/reward_run Std                    1.71794\n",
      "evaluation/env_infos/reward_run Max                    5.63965\n",
      "evaluation/env_infos/reward_run Min                   -5.0378\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.327517\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.110352\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0676015\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.485773\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.302835\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0722843\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.178041\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.434065\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.33763\n",
      "evaluation/env_infos/reward_ctrl Std                   0.123309\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00825163\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.593899\n",
      "evaluation/env_infos/final/height Mean                -0.251486\n",
      "evaluation/env_infos/final/height Std                  0.245435\n",
      "evaluation/env_infos/final/height Max                  0.371191\n",
      "evaluation/env_infos/final/height Min                 -0.577291\n",
      "evaluation/env_infos/initial/height Mean              -0.00158766\n",
      "evaluation/env_infos/initial/height Std                0.0521433\n",
      "evaluation/env_infos/initial/height Max                0.0838255\n",
      "evaluation/env_infos/initial/height Min               -0.0931849\n",
      "evaluation/env_infos/height Mean                      -0.219959\n",
      "evaluation/env_infos/height Std                        0.226174\n",
      "evaluation/env_infos/height Max                        0.537245\n",
      "evaluation/env_infos/height Min                       -0.589132\n",
      "evaluation/env_infos/final/reward_angular Mean         0.060944\n",
      "evaluation/env_infos/final/reward_angular Std          1.68117\n",
      "evaluation/env_infos/final/reward_angular Max          3.2944\n",
      "evaluation/env_infos/final/reward_angular Min         -5.17246\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.330367\n",
      "evaluation/env_infos/initial/reward_angular Std        1.09678\n",
      "evaluation/env_infos/initial/reward_angular Max        1.90753\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.46379\n",
      "evaluation/env_infos/reward_angular Mean               0.00203696\n",
      "evaluation/env_infos/reward_angular Std                1.88933\n",
      "evaluation/env_infos/reward_angular Max                8.53089\n",
      "evaluation/env_infos/reward_angular Min               -8.37095\n",
      "time/data storing (s)                                  0.0167235\n",
      "time/evaluation sampling (s)                          25.6239\n",
      "time/exploration sampling (s)                          1.33015\n",
      "time/logging (s)                                       0.247561\n",
      "time/saving (s)                                        0.0268369\n",
      "time/training (s)                                      5.08652\n",
      "time/epoch (s)                                        32.3317\n",
      "time/total (s)                                      2840.04\n",
      "Epoch                                                 96\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 19:59:51.955369 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 97 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 99000\n",
      "trainer/QF1 Loss                                       0.766405\n",
      "trainer/QF2 Loss                                       0.648301\n",
      "trainer/Policy Loss                                    3.19084\n",
      "trainer/Q1 Predictions Mean                            3.14605\n",
      "trainer/Q1 Predictions Std                            11.9739\n",
      "trainer/Q1 Predictions Max                            54.6492\n",
      "trainer/Q1 Predictions Min                           -19.2474\n",
      "trainer/Q2 Predictions Mean                            3.2777\n",
      "trainer/Q2 Predictions Std                            12.0373\n",
      "trainer/Q2 Predictions Max                            55.2713\n",
      "trainer/Q2 Predictions Min                           -17.9179\n",
      "trainer/Q Targets Mean                                 3.16242\n",
      "trainer/Q Targets Std                                 12.1069\n",
      "trainer/Q Targets Max                                 57.9367\n",
      "trainer/Q Targets Min                                -18.7293\n",
      "trainer/Log Pis Mean                                   6.56839\n",
      "trainer/Log Pis Std                                    6.38631\n",
      "trainer/Log Pis Max                                   34.0182\n",
      "trainer/Log Pis Min                                   -8.07596\n",
      "trainer/Policy mu Mean                                 0.254954\n",
      "trainer/Policy mu Std                                  1.67195\n",
      "trainer/Policy mu Max                                  5.00034\n",
      "trainer/Policy mu Min                                 -5.97906\n",
      "trainer/Policy log std Mean                           -0.702582\n",
      "trainer/Policy log std Std                             0.293755\n",
      "trainer/Policy log std Max                             0.377983\n",
      "trainer/Policy log std Min                            -1.91148\n",
      "trainer/Alpha                                          0.0134959\n",
      "trainer/Alpha Loss                                     2.44805\n",
      "exploration/num steps total                        99000\n",
      "exploration/num paths total                           99\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.160154\n",
      "exploration/Rewards Std                                0.470248\n",
      "exploration/Rewards Max                                1.06757\n",
      "exploration/Rewards Min                               -1.57007\n",
      "exploration/Returns Mean                            -160.154\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -160.154\n",
      "exploration/Returns Min                             -160.154\n",
      "exploration/Actions Mean                               0.135902\n",
      "exploration/Actions Std                                0.778199\n",
      "exploration/Actions Max                                0.999988\n",
      "exploration/Actions Min                               -0.999963\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -160.154\n",
      "exploration/env_infos/final/reward_run Mean            0.347825\n",
      "exploration/env_infos/final/reward_run Std             0\n",
      "exploration/env_infos/final/reward_run Max             0.347825\n",
      "exploration/env_infos/final/reward_run Min             0.347825\n",
      "exploration/env_infos/initial/reward_run Mean          0.963266\n",
      "exploration/env_infos/initial/reward_run Std           0\n",
      "exploration/env_infos/initial/reward_run Max           0.963266\n",
      "exploration/env_infos/initial/reward_run Min           0.963266\n",
      "exploration/env_infos/reward_run Mean                  1.51627\n",
      "exploration/env_infos/reward_run Std                   0.840642\n",
      "exploration/env_infos/reward_run Max                   3.65032\n",
      "exploration/env_infos/reward_run Min                  -0.55676\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.284484\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.284484\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.284484\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.422046\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.422046\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.422046\n",
      "exploration/env_infos/reward_ctrl Mean                -0.374437\n",
      "exploration/env_infos/reward_ctrl Std                  0.0819103\n",
      "exploration/env_infos/reward_ctrl Max                 -0.109228\n",
      "exploration/env_infos/reward_ctrl Min                 -0.57082\n",
      "exploration/env_infos/final/height Mean                0.00983486\n",
      "exploration/env_infos/final/height Std                 0\n",
      "exploration/env_infos/final/height Max                 0.00983486\n",
      "exploration/env_infos/final/height Min                 0.00983486\n",
      "exploration/env_infos/initial/height Mean              0.0228268\n",
      "exploration/env_infos/initial/height Std               0\n",
      "exploration/env_infos/initial/height Max               0.0228268\n",
      "exploration/env_infos/initial/height Min               0.0228268\n",
      "exploration/env_infos/height Mean                     -0.218421\n",
      "exploration/env_infos/height Std                       0.078405\n",
      "exploration/env_infos/height Max                       0.115984\n",
      "exploration/env_infos/height Min                      -0.433002\n",
      "exploration/env_infos/final/reward_angular Mean       -3.31007\n",
      "exploration/env_infos/final/reward_angular Std         0\n",
      "exploration/env_infos/final/reward_angular Max        -3.31007\n",
      "exploration/env_infos/final/reward_angular Min        -3.31007\n",
      "exploration/env_infos/initial/reward_angular Mean     -2.0259\n",
      "exploration/env_infos/initial/reward_angular Std       0\n",
      "exploration/env_infos/initial/reward_angular Max      -2.0259\n",
      "exploration/env_infos/initial/reward_angular Min      -2.0259\n",
      "exploration/env_infos/reward_angular Mean             -0.076025\n",
      "exploration/env_infos/reward_angular Std               2.15785\n",
      "exploration/env_infos/reward_angular Max               6.27201\n",
      "exploration/env_infos/reward_angular Min              -5.79958\n",
      "evaluation/num steps total                             2.45e+06\n",
      "evaluation/num paths total                          2450\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.151067\n",
      "evaluation/Rewards Std                                 1.2892\n",
      "evaluation/Rewards Max                                 6.08531\n",
      "evaluation/Rewards Min                                -5.12496\n",
      "evaluation/Returns Mean                              151.067\n",
      "evaluation/Returns Std                               510.402\n",
      "evaluation/Returns Max                              1701.42\n",
      "evaluation/Returns Min                              -501.219\n",
      "evaluation/Actions Mean                                0.157331\n",
      "evaluation/Actions Std                                 0.762352\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           151.067\n",
      "evaluation/env_infos/final/reward_run Mean            -0.0837533\n",
      "evaluation/env_infos/final/reward_run Std              1.6328\n",
      "evaluation/env_infos/final/reward_run Max              2.7767\n",
      "evaluation/env_infos/final/reward_run Min             -3.28899\n",
      "evaluation/env_infos/initial/reward_run Mean           0.52581\n",
      "evaluation/env_infos/initial/reward_run Std            0.459571\n",
      "evaluation/env_infos/initial/reward_run Max            1.18604\n",
      "evaluation/env_infos/initial/reward_run Min           -0.292977\n",
      "evaluation/env_infos/reward_run Mean                   0.0979198\n",
      "evaluation/env_infos/reward_run Std                    1.48278\n",
      "evaluation/env_infos/reward_run Max                    5.17267\n",
      "evaluation/env_infos/reward_run Min                   -3.88583\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.368821\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.118847\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.176142\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.563071\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.37168\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.100711\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.199455\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.529967\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.36356\n",
      "evaluation/env_infos/reward_ctrl Std                   0.120637\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.017571\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.594281\n",
      "evaluation/env_infos/final/height Mean                -0.22769\n",
      "evaluation/env_infos/final/height Std                  0.201463\n",
      "evaluation/env_infos/final/height Max                  0.250642\n",
      "evaluation/env_infos/final/height Min                 -0.577283\n",
      "evaluation/env_infos/initial/height Mean               0.00408193\n",
      "evaluation/env_infos/initial/height Std                0.0528765\n",
      "evaluation/env_infos/initial/height Max                0.089453\n",
      "evaluation/env_infos/initial/height Min               -0.0830829\n",
      "evaluation/env_infos/height Mean                      -0.226727\n",
      "evaluation/env_infos/height Std                        0.20636\n",
      "evaluation/env_infos/height Max                        0.452766\n",
      "evaluation/env_infos/height Min                       -0.599023\n",
      "evaluation/env_infos/final/reward_angular Mean        -0.123104\n",
      "evaluation/env_infos/final/reward_angular Std          1.43121\n",
      "evaluation/env_infos/final/reward_angular Max          4.31568\n",
      "evaluation/env_infos/final/reward_angular Min         -2.6125\n",
      "evaluation/env_infos/initial/reward_angular Mean      -0.750012\n",
      "evaluation/env_infos/initial/reward_angular Std        0.929054\n",
      "evaluation/env_infos/initial/reward_angular Max        1.52615\n",
      "evaluation/env_infos/initial/reward_angular Min       -2.93893\n",
      "evaluation/env_infos/reward_angular Mean               0.00596897\n",
      "evaluation/env_infos/reward_angular Std                1.97195\n",
      "evaluation/env_infos/reward_angular Max                7.34393\n",
      "evaluation/env_infos/reward_angular Min               -7.95052\n",
      "time/data storing (s)                                  0.0167752\n",
      "time/evaluation sampling (s)                          22.029\n",
      "time/exploration sampling (s)                          1.19563\n",
      "time/logging (s)                                       0.259545\n",
      "time/saving (s)                                        0.0313442\n",
      "time/training (s)                                      4.93372\n",
      "time/epoch (s)                                        28.466\n",
      "time/total (s)                                      2869.29\n",
      "Epoch                                                 97\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:00:22.319954 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 98 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 100000\n",
      "trainer/QF1 Loss                                        0.924631\n",
      "trainer/QF2 Loss                                        1.19739\n",
      "trainer/Policy Loss                                     2.17594\n",
      "trainer/Q1 Predictions Mean                             4.11237\n",
      "trainer/Q1 Predictions Std                             12.4132\n",
      "trainer/Q1 Predictions Max                             56.3652\n",
      "trainer/Q1 Predictions Min                            -11.9112\n",
      "trainer/Q2 Predictions Mean                             4.07695\n",
      "trainer/Q2 Predictions Std                             12.5321\n",
      "trainer/Q2 Predictions Max                             57.9365\n",
      "trainer/Q2 Predictions Min                            -11.9185\n",
      "trainer/Q Targets Mean                                  3.9584\n",
      "trainer/Q Targets Std                                  12.3447\n",
      "trainer/Q Targets Max                                  57.42\n",
      "trainer/Q Targets Min                                 -12.1544\n",
      "trainer/Log Pis Mean                                    6.49436\n",
      "trainer/Log Pis Std                                     5.22726\n",
      "trainer/Log Pis Max                                    21.759\n",
      "trainer/Log Pis Min                                    -3.36202\n",
      "trainer/Policy mu Mean                                  0.331043\n",
      "trainer/Policy mu Std                                   1.59057\n",
      "trainer/Policy mu Max                                   4.85445\n",
      "trainer/Policy mu Min                                  -6.00022\n",
      "trainer/Policy log std Mean                            -0.779201\n",
      "trainer/Policy log std Std                              0.268438\n",
      "trainer/Policy log std Max                              0.477547\n",
      "trainer/Policy log std Min                             -1.87248\n",
      "trainer/Alpha                                           0.0136463\n",
      "trainer/Alpha Loss                                      2.12371\n",
      "exploration/num steps total                        100000\n",
      "exploration/num paths total                           100\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.310994\n",
      "exploration/Rewards Std                                 0.334048\n",
      "exploration/Rewards Max                                 1.49988\n",
      "exploration/Rewards Min                                -1.69383\n",
      "exploration/Returns Mean                             -310.994\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -310.994\n",
      "exploration/Returns Min                              -310.994\n",
      "exploration/Actions Mean                                0.195279\n",
      "exploration/Actions Std                                 0.597159\n",
      "exploration/Actions Max                                 0.999325\n",
      "exploration/Actions Min                                -0.999767\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -310.994\n",
      "exploration/env_infos/final/reward_run Mean            -0.558734\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.558734\n",
      "exploration/env_infos/final/reward_run Min             -0.558734\n",
      "exploration/env_infos/initial/reward_run Mean           0.781306\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.781306\n",
      "exploration/env_infos/initial/reward_run Min            0.781306\n",
      "exploration/env_infos/reward_run Mean                   0.207144\n",
      "exploration/env_infos/reward_run Std                    0.582533\n",
      "exploration/env_infos/reward_run Max                    2.37447\n",
      "exploration/env_infos/reward_run Min                   -1.39806\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270098\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270098\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270098\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.164808\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.164808\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.164808\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.23684\n",
      "exploration/env_infos/reward_ctrl Std                   0.0902318\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0294261\n",
      "exploration/env_infos/reward_ctrl Min                  -0.558708\n",
      "exploration/env_infos/final/height Mean                -0.528944\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.528944\n",
      "exploration/env_infos/final/height Min                 -0.528944\n",
      "exploration/env_infos/initial/height Mean              -0.00517382\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00517382\n",
      "exploration/env_infos/initial/height Min               -0.00517382\n",
      "exploration/env_infos/height Mean                      -0.502401\n",
      "exploration/env_infos/height Std                        0.165398\n",
      "exploration/env_infos/height Max                        0.233451\n",
      "exploration/env_infos/height Min                       -0.581733\n",
      "exploration/env_infos/final/reward_angular Mean        -0.198672\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.198672\n",
      "exploration/env_infos/final/reward_angular Min         -0.198672\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.754066\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.754066\n",
      "exploration/env_infos/initial/reward_angular Min       -0.754066\n",
      "exploration/env_infos/reward_angular Mean               0.0653696\n",
      "exploration/env_infos/reward_angular Std                0.909725\n",
      "exploration/env_infos/reward_angular Max                4.69397\n",
      "exploration/env_infos/reward_angular Min               -4.75013\n",
      "evaluation/num steps total                              2.475e+06\n",
      "evaluation/num paths total                           2475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.307475\n",
      "evaluation/Rewards Std                                  1.29288\n",
      "evaluation/Rewards Max                                  6.23702\n",
      "evaluation/Rewards Min                                 -5.93525\n",
      "evaluation/Returns Mean                               307.475\n",
      "evaluation/Returns Std                                637.161\n",
      "evaluation/Returns Max                               1677.6\n",
      "evaluation/Returns Min                               -466.095\n",
      "evaluation/Actions Mean                                 0.154005\n",
      "evaluation/Actions Std                                  0.761401\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            307.475\n",
      "evaluation/env_infos/final/reward_run Mean             -0.507451\n",
      "evaluation/env_infos/final/reward_run Std               2.20154\n",
      "evaluation/env_infos/final/reward_run Max               2.73661\n",
      "evaluation/env_infos/final/reward_run Min              -5.18216\n",
      "evaluation/env_infos/initial/reward_run Mean            0.593997\n",
      "evaluation/env_infos/initial/reward_run Std             0.372761\n",
      "evaluation/env_infos/initial/reward_run Max             1.0318\n",
      "evaluation/env_infos/initial/reward_run Min            -0.270842\n",
      "evaluation/env_infos/reward_run Mean                   -0.211646\n",
      "evaluation/env_infos/reward_run Std                     1.8229\n",
      "evaluation/env_infos/reward_run Max                     4.58843\n",
      "evaluation/env_infos/reward_run Min                    -5.51687\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.366321\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.107177\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.149486\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.54836\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.311154\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0764956\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.185879\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.486394\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.36207\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111791\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0230585\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595689\n",
      "evaluation/env_infos/final/height Mean                 -0.23185\n",
      "evaluation/env_infos/final/height Std                   0.207066\n",
      "evaluation/env_infos/final/height Max                   0.0736603\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0105964\n",
      "evaluation/env_infos/initial/height Std                 0.0506752\n",
      "evaluation/env_infos/initial/height Max                 0.0783118\n",
      "evaluation/env_infos/initial/height Min                -0.0871012\n",
      "evaluation/env_infos/height Mean                       -0.236028\n",
      "evaluation/env_infos/height Std                         0.18897\n",
      "evaluation/env_infos/height Max                         0.392917\n",
      "evaluation/env_infos/height Min                        -0.591354\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.448113\n",
      "evaluation/env_infos/final/reward_angular Std           1.77392\n",
      "evaluation/env_infos/final/reward_angular Max           3.0855\n",
      "evaluation/env_infos/final/reward_angular Min          -4.14349\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.706674\n",
      "evaluation/env_infos/initial/reward_angular Std         0.919861\n",
      "evaluation/env_infos/initial/reward_angular Max         1.36134\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.96105\n",
      "evaluation/env_infos/reward_angular Mean                0.0379084\n",
      "evaluation/env_infos/reward_angular Std                 1.87137\n",
      "evaluation/env_infos/reward_angular Max                 7.63477\n",
      "evaluation/env_infos/reward_angular Min                -7.19201\n",
      "time/data storing (s)                                   0.0154382\n",
      "time/evaluation sampling (s)                           23.5402\n",
      "time/exploration sampling (s)                           1.05213\n",
      "time/logging (s)                                        0.258029\n",
      "time/saving (s)                                         0.0287844\n",
      "time/training (s)                                       4.64225\n",
      "time/epoch (s)                                         29.5368\n",
      "time/total (s)                                       2899.65\n",
      "Epoch                                                  98\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:00:55.366680 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 99 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 101000\n",
      "trainer/QF1 Loss                                        1.24906\n",
      "trainer/QF2 Loss                                        1.12056\n",
      "trainer/Policy Loss                                     2.70305\n",
      "trainer/Q1 Predictions Mean                             3.35051\n",
      "trainer/Q1 Predictions Std                             12.3426\n",
      "trainer/Q1 Predictions Max                             57.0216\n",
      "trainer/Q1 Predictions Min                            -19.0514\n",
      "trainer/Q2 Predictions Mean                             3.16425\n",
      "trainer/Q2 Predictions Std                             12.1791\n",
      "trainer/Q2 Predictions Max                             57.3404\n",
      "trainer/Q2 Predictions Min                            -17.6352\n",
      "trainer/Q Targets Mean                                  3.07663\n",
      "trainer/Q Targets Std                                  12.2073\n",
      "trainer/Q Targets Max                                  53.7887\n",
      "trainer/Q Targets Min                                 -21.7911\n",
      "trainer/Log Pis Mean                                    6.12856\n",
      "trainer/Log Pis Std                                     5.2118\n",
      "trainer/Log Pis Max                                    29.795\n",
      "trainer/Log Pis Min                                    -5.4756\n",
      "trainer/Policy mu Mean                                  0.16033\n",
      "trainer/Policy mu Std                                   1.6026\n",
      "trainer/Policy mu Max                                   5.02067\n",
      "trainer/Policy mu Min                                  -5.87539\n",
      "trainer/Policy log std Mean                            -0.688137\n",
      "trainer/Policy log std Std                              0.288783\n",
      "trainer/Policy log std Max                              0.449622\n",
      "trainer/Policy log std Min                             -1.92027\n",
      "trainer/Alpha                                           0.0133089\n",
      "trainer/Alpha Loss                                      0.555232\n",
      "exploration/num steps total                        101000\n",
      "exploration/num paths total                           101\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.0749\n",
      "exploration/Rewards Std                                 0.59284\n",
      "exploration/Rewards Max                                 2.76719\n",
      "exploration/Rewards Min                                -1.05396\n",
      "exploration/Returns Mean                             1074.9\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1074.9\n",
      "exploration/Returns Min                              1074.9\n",
      "exploration/Actions Mean                                0.00616258\n",
      "exploration/Actions Std                                 0.852179\n",
      "exploration/Actions Max                                 0.999965\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1074.9\n",
      "exploration/env_infos/final/reward_run Mean             0.0292216\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0292216\n",
      "exploration/env_infos/final/reward_run Min              0.0292216\n",
      "exploration/env_infos/initial/reward_run Mean          -0.376735\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.376735\n",
      "exploration/env_infos/initial/reward_run Min           -0.376735\n",
      "exploration/env_infos/reward_run Mean                  -2.54734\n",
      "exploration/env_infos/reward_run Std                    1.08475\n",
      "exploration/env_infos/reward_run Max                    1.11057\n",
      "exploration/env_infos/reward_run Min                   -5.74201\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.32715\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.32715\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.32715\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.261883\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.261883\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.261883\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.435748\n",
      "exploration/env_infos/reward_ctrl Std                   0.0935563\n",
      "exploration/env_infos/reward_ctrl Max                  -0.107124\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596604\n",
      "exploration/env_infos/final/height Mean                -0.461629\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.461629\n",
      "exploration/env_infos/final/height Min                 -0.461629\n",
      "exploration/env_infos/initial/height Mean              -0.00335721\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00335721\n",
      "exploration/env_infos/initial/height Min               -0.00335721\n",
      "exploration/env_infos/height Mean                      -0.0695732\n",
      "exploration/env_infos/height Std                        0.13301\n",
      "exploration/env_infos/height Max                        0.535826\n",
      "exploration/env_infos/height Min                       -0.583901\n",
      "exploration/env_infos/final/reward_angular Mean        -0.143284\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.143284\n",
      "exploration/env_infos/final/reward_angular Min         -0.143284\n",
      "exploration/env_infos/initial/reward_angular Mean       0.37051\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.37051\n",
      "exploration/env_infos/initial/reward_angular Min        0.37051\n",
      "exploration/env_infos/reward_angular Mean               0.0192174\n",
      "exploration/env_infos/reward_angular Std                2.75696\n",
      "exploration/env_infos/reward_angular Max                9.37176\n",
      "exploration/env_infos/reward_angular Min               -6.3369\n",
      "evaluation/num steps total                              2.5e+06\n",
      "evaluation/num paths total                           2500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.370404\n",
      "evaluation/Rewards Std                                  1.36244\n",
      "evaluation/Rewards Max                                  6.8562\n",
      "evaluation/Rewards Min                                 -6.30204\n",
      "evaluation/Returns Mean                               370.404\n",
      "evaluation/Returns Std                                610.793\n",
      "evaluation/Returns Max                               1960.22\n",
      "evaluation/Returns Min                               -338.683\n",
      "evaluation/Actions Mean                                 0.0601129\n",
      "evaluation/Actions Std                                  0.760642\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            370.404\n",
      "evaluation/env_infos/final/reward_run Mean             -0.133505\n",
      "evaluation/env_infos/final/reward_run Std               1.61648\n",
      "evaluation/env_infos/final/reward_run Max               2.72014\n",
      "evaluation/env_infos/final/reward_run Min              -4.55925\n",
      "evaluation/env_infos/initial/reward_run Mean            0.552189\n",
      "evaluation/env_infos/initial/reward_run Std             0.50257\n",
      "evaluation/env_infos/initial/reward_run Max             1.06855\n",
      "evaluation/env_infos/initial/reward_run Min            -0.971005\n",
      "evaluation/env_infos/reward_run Mean                   -0.369838\n",
      "evaluation/env_infos/reward_run Std                     2.13279\n",
      "evaluation/env_infos/reward_run Max                     5.58797\n",
      "evaluation/env_infos/reward_run Min                    -6.20968\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.310418\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.118779\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.103401\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.563324\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.34199\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0867515\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.149657\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.468822\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.349314\n",
      "evaluation/env_infos/reward_ctrl Std                    0.12071\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.010504\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598956\n",
      "evaluation/env_infos/final/height Mean                 -0.298743\n",
      "evaluation/env_infos/final/height Std                   0.227198\n",
      "evaluation/env_infos/final/height Max                   0.0780596\n",
      "evaluation/env_infos/final/height Min                  -0.582165\n",
      "evaluation/env_infos/initial/height Mean               -0.00697028\n",
      "evaluation/env_infos/initial/height Std                 0.0468394\n",
      "evaluation/env_infos/initial/height Max                 0.0815783\n",
      "evaluation/env_infos/initial/height Min                -0.0962473\n",
      "evaluation/env_infos/height Mean                       -0.196101\n",
      "evaluation/env_infos/height Std                         0.203916\n",
      "evaluation/env_infos/height Max                         0.476206\n",
      "evaluation/env_infos/height Min                        -0.594003\n",
      "evaluation/env_infos/final/reward_angular Mean          0.126274\n",
      "evaluation/env_infos/final/reward_angular Std           1.29335\n",
      "evaluation/env_infos/final/reward_angular Max           2.55363\n",
      "evaluation/env_infos/final/reward_angular Min          -3.65814\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.672604\n",
      "evaluation/env_infos/initial/reward_angular Std         1.05068\n",
      "evaluation/env_infos/initial/reward_angular Max         2.20702\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.32809\n",
      "evaluation/env_infos/reward_angular Mean                0.0231092\n",
      "evaluation/env_infos/reward_angular Std                 1.99703\n",
      "evaluation/env_infos/reward_angular Max                 8.63215\n",
      "evaluation/env_infos/reward_angular Min                -7.11366\n",
      "time/data storing (s)                                   0.017009\n",
      "time/evaluation sampling (s)                           24.932\n",
      "time/exploration sampling (s)                           1.37638\n",
      "time/logging (s)                                        0.247647\n",
      "time/saving (s)                                         0.0300905\n",
      "time/training (s)                                       5.60986\n",
      "time/epoch (s)                                         32.213\n",
      "time/total (s)                                       2932.68\n",
      "Epoch                                                  99\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:01:25.608213 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 100 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 102000\n",
      "trainer/QF1 Loss                                        1.13244\n",
      "trainer/QF2 Loss                                        1.09918\n",
      "trainer/Policy Loss                                     0.763916\n",
      "trainer/Q1 Predictions Mean                             4.94391\n",
      "trainer/Q1 Predictions Std                             12.8541\n",
      "trainer/Q1 Predictions Max                             56.2834\n",
      "trainer/Q1 Predictions Min                            -12.1684\n",
      "trainer/Q2 Predictions Mean                             4.84333\n",
      "trainer/Q2 Predictions Std                             12.9258\n",
      "trainer/Q2 Predictions Max                             57.4595\n",
      "trainer/Q2 Predictions Min                            -11.6595\n",
      "trainer/Q Targets Mean                                  4.92684\n",
      "trainer/Q Targets Std                                  13.002\n",
      "trainer/Q Targets Max                                  56.5185\n",
      "trainer/Q Targets Min                                 -11.9949\n",
      "trainer/Log Pis Mean                                    5.93173\n",
      "trainer/Log Pis Std                                     5.46269\n",
      "trainer/Log Pis Max                                    28.6663\n",
      "trainer/Log Pis Min                                    -4.34789\n",
      "trainer/Policy mu Mean                                  0.259815\n",
      "trainer/Policy mu Std                                   1.5956\n",
      "trainer/Policy mu Max                                   4.788\n",
      "trainer/Policy mu Min                                  -5.49223\n",
      "trainer/Policy log std Mean                            -0.741896\n",
      "trainer/Policy log std Std                              0.284553\n",
      "trainer/Policy log std Max                              0.320194\n",
      "trainer/Policy log std Min                             -1.79686\n",
      "trainer/Alpha                                           0.0141639\n",
      "trainer/Alpha Loss                                     -0.29059\n",
      "exploration/num steps total                        102000\n",
      "exploration/num paths total                           102\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.211399\n",
      "exploration/Rewards Std                                 1.081\n",
      "exploration/Rewards Max                                 2.89554\n",
      "exploration/Rewards Min                                -3.10744\n",
      "exploration/Returns Mean                             -211.399\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -211.399\n",
      "exploration/Returns Min                              -211.399\n",
      "exploration/Actions Mean                                0.229914\n",
      "exploration/Actions Std                                 0.70392\n",
      "exploration/Actions Max                                 0.99989\n",
      "exploration/Actions Min                                -0.999985\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -211.399\n",
      "exploration/env_infos/final/reward_run Mean             1.10166\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.10166\n",
      "exploration/env_infos/final/reward_run Min              1.10166\n",
      "exploration/env_infos/initial/reward_run Mean           0.0259753\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0259753\n",
      "exploration/env_infos/initial/reward_run Min            0.0259753\n",
      "exploration/env_infos/reward_run Mean                   1.54293\n",
      "exploration/env_infos/reward_run Std                    0.938981\n",
      "exploration/env_infos/reward_run Max                    4.08759\n",
      "exploration/env_infos/reward_run Min                   -0.710609\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.341425\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.341425\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.341425\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.315977\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.315977\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.315977\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.329018\n",
      "exploration/env_infos/reward_ctrl Std                   0.0875023\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0668986\n",
      "exploration/env_infos/reward_ctrl Min                  -0.563484\n",
      "exploration/env_infos/final/height Mean                -0.0786877\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0786877\n",
      "exploration/env_infos/final/height Min                 -0.0786877\n",
      "exploration/env_infos/initial/height Mean              -0.0345507\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0345507\n",
      "exploration/env_infos/initial/height Min               -0.0345507\n",
      "exploration/env_infos/height Mean                      -0.1216\n",
      "exploration/env_infos/height Std                        0.0846022\n",
      "exploration/env_infos/height Max                        0.185169\n",
      "exploration/env_infos/height Min                       -0.324901\n",
      "exploration/env_infos/final/reward_angular Mean         1.15526\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.15526\n",
      "exploration/env_infos/final/reward_angular Min          1.15526\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.39755\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.39755\n",
      "exploration/env_infos/initial/reward_angular Min       -1.39755\n",
      "exploration/env_infos/reward_angular Mean              -0.0706248\n",
      "exploration/env_infos/reward_angular Std                1.80923\n",
      "exploration/env_infos/reward_angular Max                4.87464\n",
      "exploration/env_infos/reward_angular Min               -5.29251\n",
      "evaluation/num steps total                              2.525e+06\n",
      "evaluation/num paths total                           2525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.319532\n",
      "evaluation/Rewards Std                                  1.3597\n",
      "evaluation/Rewards Max                                  6.68656\n",
      "evaluation/Rewards Min                                 -6.14516\n",
      "evaluation/Returns Mean                               319.532\n",
      "evaluation/Returns Std                                536.327\n",
      "evaluation/Returns Max                               1723.07\n",
      "evaluation/Returns Min                               -317.597\n",
      "evaluation/Actions Mean                                 0.18903\n",
      "evaluation/Actions Std                                  0.750559\n",
      "evaluation/Actions Max                                  0.999957\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            319.532\n",
      "evaluation/env_infos/final/reward_run Mean             -0.320551\n",
      "evaluation/env_infos/final/reward_run Std               1.43469\n",
      "evaluation/env_infos/final/reward_run Max               2.23734\n",
      "evaluation/env_infos/final/reward_run Min              -3.48475\n",
      "evaluation/env_infos/initial/reward_run Mean            0.543578\n",
      "evaluation/env_infos/initial/reward_run Std             0.497566\n",
      "evaluation/env_infos/initial/reward_run Max             1.12357\n",
      "evaluation/env_infos/initial/reward_run Min            -0.617632\n",
      "evaluation/env_infos/reward_run Mean                   -0.276588\n",
      "evaluation/env_infos/reward_run Std                     1.75155\n",
      "evaluation/env_infos/reward_run Max                     4.77612\n",
      "evaluation/env_infos/reward_run Min                    -5.11654\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.3585\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.116223\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0983018\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.545189\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.308588\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0770309\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.185371\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.471436\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.359442\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113028\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0124314\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594709\n",
      "evaluation/env_infos/final/height Mean                 -0.228824\n",
      "evaluation/env_infos/final/height Std                   0.2261\n",
      "evaluation/env_infos/final/height Max                   0.105578\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00661561\n",
      "evaluation/env_infos/initial/height Std                 0.0555805\n",
      "evaluation/env_infos/initial/height Max                 0.085332\n",
      "evaluation/env_infos/initial/height Min                -0.0662059\n",
      "evaluation/env_infos/height Mean                       -0.19557\n",
      "evaluation/env_infos/height Std                         0.208797\n",
      "evaluation/env_infos/height Max                         0.405957\n",
      "evaluation/env_infos/height Min                        -0.603437\n",
      "evaluation/env_infos/final/reward_angular Mean          0.390022\n",
      "evaluation/env_infos/final/reward_angular Std           1.52479\n",
      "evaluation/env_infos/final/reward_angular Max           3.37839\n",
      "evaluation/env_infos/final/reward_angular Min          -4.21154\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.651526\n",
      "evaluation/env_infos/initial/reward_angular Std         0.896727\n",
      "evaluation/env_infos/initial/reward_angular Max         1.69072\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.75599\n",
      "evaluation/env_infos/reward_angular Mean                0.062934\n",
      "evaluation/env_infos/reward_angular Std                 2.08812\n",
      "evaluation/env_infos/reward_angular Max                 8.10851\n",
      "evaluation/env_infos/reward_angular Min                -8.48178\n",
      "time/data storing (s)                                   0.0149725\n",
      "time/evaluation sampling (s)                           23.9758\n",
      "time/exploration sampling (s)                           1.04638\n",
      "time/logging (s)                                        0.250011\n",
      "time/saving (s)                                         0.0585569\n",
      "time/training (s)                                       3.97255\n",
      "time/epoch (s)                                         29.3182\n",
      "time/total (s)                                       2962.92\n",
      "Epoch                                                 100\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:01:52.697952 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 101 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 103000\n",
      "trainer/QF1 Loss                                        0.859924\n",
      "trainer/QF2 Loss                                        0.890831\n",
      "trainer/Policy Loss                                     1.56549\n",
      "trainer/Q1 Predictions Mean                             4.46912\n",
      "trainer/Q1 Predictions Std                             12.829\n",
      "trainer/Q1 Predictions Max                             60.224\n",
      "trainer/Q1 Predictions Min                            -11.6894\n",
      "trainer/Q2 Predictions Mean                             4.44973\n",
      "trainer/Q2 Predictions Std                             12.8109\n",
      "trainer/Q2 Predictions Max                             60.2616\n",
      "trainer/Q2 Predictions Min                            -11.8633\n",
      "trainer/Q Targets Mean                                  4.42584\n",
      "trainer/Q Targets Std                                  12.8442\n",
      "trainer/Q Targets Max                                  58.9758\n",
      "trainer/Q Targets Min                                 -11.9374\n",
      "trainer/Log Pis Mean                                    6.13974\n",
      "trainer/Log Pis Std                                     5.53441\n",
      "trainer/Log Pis Max                                    25.3201\n",
      "trainer/Log Pis Min                                    -5.63922\n",
      "trainer/Policy mu Mean                                  0.303121\n",
      "trainer/Policy mu Std                                   1.57147\n",
      "trainer/Policy mu Max                                   4.2769\n",
      "trainer/Policy mu Min                                  -6.2843\n",
      "trainer/Policy log std Mean                            -0.749676\n",
      "trainer/Policy log std Std                              0.262596\n",
      "trainer/Policy log std Max                              0.262794\n",
      "trainer/Policy log std Min                             -1.89088\n",
      "trainer/Alpha                                           0.0131135\n",
      "trainer/Alpha Loss                                      0.60563\n",
      "exploration/num steps total                        103000\n",
      "exploration/num paths total                           103\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.222144\n",
      "exploration/Rewards Std                                 0.616175\n",
      "exploration/Rewards Max                                 2.67212\n",
      "exploration/Rewards Min                                -1.30702\n",
      "exploration/Returns Mean                             -222.144\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -222.144\n",
      "exploration/Returns Min                              -222.144\n",
      "exploration/Actions Mean                                0.122893\n",
      "exploration/Actions Std                                 0.623482\n",
      "exploration/Actions Max                                 0.999988\n",
      "exploration/Actions Min                                -0.999985\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -222.144\n",
      "exploration/env_infos/final/reward_run Mean             0.218334\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.218334\n",
      "exploration/env_infos/final/reward_run Min              0.218334\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0507633\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0507633\n",
      "exploration/env_infos/initial/reward_run Min           -0.0507633\n",
      "exploration/env_infos/reward_run Mean                  -0.404364\n",
      "exploration/env_infos/reward_run Std                    0.964624\n",
      "exploration/env_infos/reward_run Max                    1.31394\n",
      "exploration/env_infos/reward_run Min                   -5.13054\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.334512\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.334512\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.334512\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.205198\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.205198\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.205198\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.2423\n",
      "exploration/env_infos/reward_ctrl Std                   0.106169\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0395859\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596119\n",
      "exploration/env_infos/final/height Mean                -0.490249\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.490249\n",
      "exploration/env_infos/final/height Min                 -0.490249\n",
      "exploration/env_infos/initial/height Mean              -0.0432892\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0432892\n",
      "exploration/env_infos/initial/height Min               -0.0432892\n",
      "exploration/env_infos/height Mean                      -0.475713\n",
      "exploration/env_infos/height Std                        0.162813\n",
      "exploration/env_infos/height Max                        0.240892\n",
      "exploration/env_infos/height Min                       -0.592431\n",
      "exploration/env_infos/final/reward_angular Mean        -1.14145\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.14145\n",
      "exploration/env_infos/final/reward_angular Min         -1.14145\n",
      "exploration/env_infos/initial/reward_angular Mean       1.84362\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.84362\n",
      "exploration/env_infos/initial/reward_angular Min        1.84362\n",
      "exploration/env_infos/reward_angular Mean              -0.0606603\n",
      "exploration/env_infos/reward_angular Std                1.48477\n",
      "exploration/env_infos/reward_angular Max                4.99269\n",
      "exploration/env_infos/reward_angular Min               -5.64908\n",
      "evaluation/num steps total                              2.55e+06\n",
      "evaluation/num paths total                           2550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.282417\n",
      "evaluation/Rewards Std                                  1.2222\n",
      "evaluation/Rewards Max                                  6.19672\n",
      "evaluation/Rewards Min                                 -5.60092\n",
      "evaluation/Returns Mean                               282.417\n",
      "evaluation/Returns Std                                527.363\n",
      "evaluation/Returns Max                               1852.25\n",
      "evaluation/Returns Min                               -311.846\n",
      "evaluation/Actions Mean                                 0.145355\n",
      "evaluation/Actions Std                                  0.753034\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            282.417\n",
      "evaluation/env_infos/final/reward_run Mean              0.13294\n",
      "evaluation/env_infos/final/reward_run Std               1.23735\n",
      "evaluation/env_infos/final/reward_run Max               2.42991\n",
      "evaluation/env_infos/final/reward_run Min              -2.16835\n",
      "evaluation/env_infos/initial/reward_run Mean            0.388998\n",
      "evaluation/env_infos/initial/reward_run Std             0.367842\n",
      "evaluation/env_infos/initial/reward_run Max             1.00879\n",
      "evaluation/env_infos/initial/reward_run Min            -0.258452\n",
      "evaluation/env_infos/reward_run Mean                    0.0325286\n",
      "evaluation/env_infos/reward_run Std                     1.92171\n",
      "evaluation/env_infos/reward_run Max                     5.35746\n",
      "evaluation/env_infos/reward_run Min                    -5.22941\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.340405\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.1251\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.103079\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.543122\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.274684\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0797891\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0976523\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.4601\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.352913\n",
      "evaluation/env_infos/reward_ctrl Std                    0.120913\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0209711\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59541\n",
      "evaluation/env_infos/final/height Mean                 -0.205955\n",
      "evaluation/env_infos/final/height Std                   0.269563\n",
      "evaluation/env_infos/final/height Max                   0.463647\n",
      "evaluation/env_infos/final/height Min                  -0.57425\n",
      "evaluation/env_infos/initial/height Mean                0.00159069\n",
      "evaluation/env_infos/initial/height Std                 0.048027\n",
      "evaluation/env_infos/initial/height Max                 0.063364\n",
      "evaluation/env_infos/initial/height Min                -0.0938727\n",
      "evaluation/env_infos/height Mean                       -0.17975\n",
      "evaluation/env_infos/height Std                         0.233471\n",
      "evaluation/env_infos/height Max                         0.463647\n",
      "evaluation/env_infos/height Min                        -0.593469\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0567224\n",
      "evaluation/env_infos/final/reward_angular Std           1.37472\n",
      "evaluation/env_infos/final/reward_angular Max           2.57239\n",
      "evaluation/env_infos/final/reward_angular Min          -2.69369\n",
      "evaluation/env_infos/initial/reward_angular Mean       -1.20816\n",
      "evaluation/env_infos/initial/reward_angular Std         0.687469\n",
      "evaluation/env_infos/initial/reward_angular Max         0.39736\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.92811\n",
      "evaluation/env_infos/reward_angular Mean                0.031922\n",
      "evaluation/env_infos/reward_angular Std                 1.84595\n",
      "evaluation/env_infos/reward_angular Max                 7.67307\n",
      "evaluation/env_infos/reward_angular Min                -7.49021\n",
      "time/data storing (s)                                   0.0150356\n",
      "time/evaluation sampling (s)                           20.8743\n",
      "time/exploration sampling (s)                           1.01721\n",
      "time/logging (s)                                        0.241689\n",
      "time/saving (s)                                         0.0299654\n",
      "time/training (s)                                       4.0903\n",
      "time/epoch (s)                                         26.2685\n",
      "time/total (s)                                       2990\n",
      "Epoch                                                 101\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:02:22.399243 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 102 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 104000\n",
      "trainer/QF1 Loss                                        1.10717\n",
      "trainer/QF2 Loss                                        1.06194\n",
      "trainer/Policy Loss                                     1.45592\n",
      "trainer/Q1 Predictions Mean                             3.99348\n",
      "trainer/Q1 Predictions Std                             12.5626\n",
      "trainer/Q1 Predictions Max                             56.4503\n",
      "trainer/Q1 Predictions Min                            -12.6613\n",
      "trainer/Q2 Predictions Mean                             4.03652\n",
      "trainer/Q2 Predictions Std                             12.4975\n",
      "trainer/Q2 Predictions Max                             56.0518\n",
      "trainer/Q2 Predictions Min                            -13.3661\n",
      "trainer/Q Targets Mean                                  4.01047\n",
      "trainer/Q Targets Std                                  12.3844\n",
      "trainer/Q Targets Max                                  56.4011\n",
      "trainer/Q Targets Min                                 -12.4266\n",
      "trainer/Log Pis Mean                                    5.65558\n",
      "trainer/Log Pis Std                                     5.80327\n",
      "trainer/Log Pis Max                                    29.2886\n",
      "trainer/Log Pis Min                                    -6.35091\n",
      "trainer/Policy mu Mean                                  0.117073\n",
      "trainer/Policy mu Std                                   1.57884\n",
      "trainer/Policy mu Max                                   4.31817\n",
      "trainer/Policy mu Min                                  -6.96618\n",
      "trainer/Policy log std Mean                            -0.711667\n",
      "trainer/Policy log std Std                              0.30307\n",
      "trainer/Policy log std Max                              0.582655\n",
      "trainer/Policy log std Min                             -2.31555\n",
      "trainer/Alpha                                           0.0138865\n",
      "trainer/Alpha Loss                                     -1.47254\n",
      "exploration/num steps total                        104000\n",
      "exploration/num paths total                           104\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.734162\n",
      "exploration/Rewards Std                                 1.5209\n",
      "exploration/Rewards Max                                 5.76268\n",
      "exploration/Rewards Min                                -1.77095\n",
      "exploration/Returns Mean                              734.162\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               734.162\n",
      "exploration/Returns Min                               734.162\n",
      "exploration/Actions Mean                                0.164048\n",
      "exploration/Actions Std                                 0.776376\n",
      "exploration/Actions Max                                 0.999995\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           734.162\n",
      "exploration/env_infos/final/reward_run Mean            -0.221908\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.221908\n",
      "exploration/env_infos/final/reward_run Min             -0.221908\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0917981\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0917981\n",
      "exploration/env_infos/initial/reward_run Min           -0.0917981\n",
      "exploration/env_infos/reward_run Mean                  -1.28683\n",
      "exploration/env_infos/reward_run Std                    1.68914\n",
      "exploration/env_infos/reward_run Max                    1.04844\n",
      "exploration/env_infos/reward_run Min                   -5.7788\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.262428\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.262428\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.262428\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.247465\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.247465\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.247465\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.377803\n",
      "exploration/env_infos/reward_ctrl Std                   0.102604\n",
      "exploration/env_infos/reward_ctrl Max                  -0.134148\n",
      "exploration/env_infos/reward_ctrl Min                  -0.59738\n",
      "exploration/env_infos/final/height Mean                -0.544594\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.544594\n",
      "exploration/env_infos/final/height Min                 -0.544594\n",
      "exploration/env_infos/initial/height Mean               0.0395972\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0395972\n",
      "exploration/env_infos/initial/height Min                0.0395972\n",
      "exploration/env_infos/height Mean                      -0.319582\n",
      "exploration/env_infos/height Std                        0.289106\n",
      "exploration/env_infos/height Max                        0.358725\n",
      "exploration/env_infos/height Min                       -0.589737\n",
      "exploration/env_infos/final/reward_angular Mean         0.180564\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.180564\n",
      "exploration/env_infos/final/reward_angular Min          0.180564\n",
      "exploration/env_infos/initial/reward_angular Mean       0.888981\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.888981\n",
      "exploration/env_infos/initial/reward_angular Min        0.888981\n",
      "exploration/env_infos/reward_angular Mean              -0.0112382\n",
      "exploration/env_infos/reward_angular Std                2.09611\n",
      "exploration/env_infos/reward_angular Max                7.20241\n",
      "exploration/env_infos/reward_angular Min               -6.62969\n",
      "evaluation/num steps total                              2.575e+06\n",
      "evaluation/num paths total                           2575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.16255\n",
      "evaluation/Rewards Std                                  1.14577\n",
      "evaluation/Rewards Max                                  6.99951\n",
      "evaluation/Rewards Min                                 -5.86848\n",
      "evaluation/Returns Mean                               162.55\n",
      "evaluation/Returns Std                                458.648\n",
      "evaluation/Returns Max                               1583.01\n",
      "evaluation/Returns Min                               -286.947\n",
      "evaluation/Actions Mean                                 0.0615708\n",
      "evaluation/Actions Std                                  0.694424\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            162.55\n",
      "evaluation/env_infos/final/reward_run Mean              0.292822\n",
      "evaluation/env_infos/final/reward_run Std               1.26839\n",
      "evaluation/env_infos/final/reward_run Max               3.23131\n",
      "evaluation/env_infos/final/reward_run Min              -2.96818\n",
      "evaluation/env_infos/initial/reward_run Mean            0.386788\n",
      "evaluation/env_infos/initial/reward_run Std             0.339674\n",
      "evaluation/env_infos/initial/reward_run Max             1.04131\n",
      "evaluation/env_infos/initial/reward_run Min            -0.224131\n",
      "evaluation/env_infos/reward_run Mean                   -0.0131659\n",
      "evaluation/env_infos/reward_run Std                     1.60311\n",
      "evaluation/env_infos/reward_run Max                     5.52961\n",
      "evaluation/env_infos/reward_run Min                    -6.18736\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.283474\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.128804\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.114753\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.550445\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.26013\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0651698\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.112617\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.382222\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.291609\n",
      "evaluation/env_infos/reward_ctrl Std                    0.12094\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0117098\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598414\n",
      "evaluation/env_infos/final/height Mean                 -0.31307\n",
      "evaluation/env_infos/final/height Std                   0.235997\n",
      "evaluation/env_infos/final/height Max                   0.1555\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00807799\n",
      "evaluation/env_infos/initial/height Std                 0.0515584\n",
      "evaluation/env_infos/initial/height Max                 0.0730182\n",
      "evaluation/env_infos/initial/height Min                -0.08953\n",
      "evaluation/env_infos/height Mean                       -0.242047\n",
      "evaluation/env_infos/height Std                         0.236335\n",
      "evaluation/env_infos/height Max                         0.409848\n",
      "evaluation/env_infos/height Min                        -0.596074\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.341006\n",
      "evaluation/env_infos/final/reward_angular Std           1.09413\n",
      "evaluation/env_infos/final/reward_angular Max           1.78866\n",
      "evaluation/env_infos/final/reward_angular Min          -3.02422\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.606335\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07468\n",
      "evaluation/env_infos/initial/reward_angular Max         1.98455\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.20669\n",
      "evaluation/env_infos/reward_angular Mean                0.00178272\n",
      "evaluation/env_infos/reward_angular Std                 1.62897\n",
      "evaluation/env_infos/reward_angular Max                 8.11885\n",
      "evaluation/env_infos/reward_angular Min                -7.39744\n",
      "time/data storing (s)                                   0.0163783\n",
      "time/evaluation sampling (s)                           22.7234\n",
      "time/exploration sampling (s)                           1.18412\n",
      "time/logging (s)                                        0.255314\n",
      "time/saving (s)                                         0.0323318\n",
      "time/training (s)                                       4.64827\n",
      "time/epoch (s)                                         28.8599\n",
      "time/total (s)                                       3019.71\n",
      "Epoch                                                 102\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:02:54.474957 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 103 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 105000\n",
      "trainer/QF1 Loss                                        1.43965\n",
      "trainer/QF2 Loss                                        1.62151\n",
      "trainer/Policy Loss                                     0.677626\n",
      "trainer/Q1 Predictions Mean                             4.90799\n",
      "trainer/Q1 Predictions Std                             14.047\n",
      "trainer/Q1 Predictions Max                             63.6698\n",
      "trainer/Q1 Predictions Min                            -12.5444\n",
      "trainer/Q2 Predictions Mean                             4.88822\n",
      "trainer/Q2 Predictions Std                             14.032\n",
      "trainer/Q2 Predictions Max                             63.0605\n",
      "trainer/Q2 Predictions Min                            -12.8243\n",
      "trainer/Q Targets Mean                                  4.94634\n",
      "trainer/Q Targets Std                                  14.156\n",
      "trainer/Q Targets Max                                  61.3293\n",
      "trainer/Q Targets Min                                 -13.6443\n",
      "trainer/Log Pis Mean                                    5.7541\n",
      "trainer/Log Pis Std                                     6.10012\n",
      "trainer/Log Pis Max                                    23.469\n",
      "trainer/Log Pis Min                                    -7.20179\n",
      "trainer/Policy mu Mean                                  0.0646279\n",
      "trainer/Policy mu Std                                   1.63087\n",
      "trainer/Policy mu Max                                   4.57793\n",
      "trainer/Policy mu Min                                  -5.37008\n",
      "trainer/Policy log std Mean                            -0.675527\n",
      "trainer/Policy log std Std                              0.288938\n",
      "trainer/Policy log std Max                              0.456935\n",
      "trainer/Policy log std Min                             -2.16847\n",
      "trainer/Alpha                                           0.0145429\n",
      "trainer/Alpha Loss                                     -1.04053\n",
      "exploration/num steps total                        105000\n",
      "exploration/num paths total                           105\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0936407\n",
      "exploration/Rewards Std                                 0.788931\n",
      "exploration/Rewards Max                                 2.44776\n",
      "exploration/Rewards Min                                -1.26241\n",
      "exploration/Returns Mean                              -93.6407\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -93.6407\n",
      "exploration/Returns Min                               -93.6407\n",
      "exploration/Actions Mean                                0.134871\n",
      "exploration/Actions Std                                 0.661427\n",
      "exploration/Actions Max                                 0.999842\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -93.6407\n",
      "exploration/env_infos/final/reward_run Mean            -0.00484496\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.00484496\n",
      "exploration/env_infos/final/reward_run Min             -0.00484496\n",
      "exploration/env_infos/initial/reward_run Mean           0.00310641\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.00310641\n",
      "exploration/env_infos/initial/reward_run Min            0.00310641\n",
      "exploration/env_infos/reward_run Mean                  -0.595231\n",
      "exploration/env_infos/reward_run Std                    1.16407\n",
      "exploration/env_infos/reward_run Max                    1.20282\n",
      "exploration/env_infos/reward_run Min                   -4.89656\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.247238\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.247238\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.247238\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.255382\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.255382\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.255382\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.273405\n",
      "exploration/env_infos/reward_ctrl Std                   0.112648\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0384727\n",
      "exploration/env_infos/reward_ctrl Min                  -0.583944\n",
      "exploration/env_infos/final/height Mean                -0.576417\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576417\n",
      "exploration/env_infos/final/height Min                 -0.576417\n",
      "exploration/env_infos/initial/height Mean               0.0564906\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0564906\n",
      "exploration/env_infos/initial/height Min                0.0564906\n",
      "exploration/env_infos/height Mean                      -0.445986\n",
      "exploration/env_infos/height Std                        0.229697\n",
      "exploration/env_infos/height Max                        0.321564\n",
      "exploration/env_infos/height Min                       -0.584326\n",
      "exploration/env_infos/final/reward_angular Mean         0.0220072\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0220072\n",
      "exploration/env_infos/final/reward_angular Min          0.0220072\n",
      "exploration/env_infos/initial/reward_angular Mean       0.446042\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.446042\n",
      "exploration/env_infos/initial/reward_angular Min        0.446042\n",
      "exploration/env_infos/reward_angular Mean              -0.0629151\n",
      "exploration/env_infos/reward_angular Std                1.40353\n",
      "exploration/env_infos/reward_angular Max                5.73106\n",
      "exploration/env_infos/reward_angular Min               -5.84424\n",
      "evaluation/num steps total                              2.6e+06\n",
      "evaluation/num paths total                           2600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.197859\n",
      "evaluation/Rewards Std                                  1.16124\n",
      "evaluation/Rewards Max                                  6.75985\n",
      "evaluation/Rewards Min                                 -5.33316\n",
      "evaluation/Returns Mean                               197.859\n",
      "evaluation/Returns Std                                545.43\n",
      "evaluation/Returns Max                               1947.72\n",
      "evaluation/Returns Min                               -302.212\n",
      "evaluation/Actions Mean                                 0.0623061\n",
      "evaluation/Actions Std                                  0.73757\n",
      "evaluation/Actions Max                                  0.999986\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            197.859\n",
      "evaluation/env_infos/final/reward_run Mean              0.0797423\n",
      "evaluation/env_infos/final/reward_run Std               1.54085\n",
      "evaluation/env_infos/final/reward_run Max               3.14183\n",
      "evaluation/env_infos/final/reward_run Min              -3.45094\n",
      "evaluation/env_infos/initial/reward_run Mean            0.322986\n",
      "evaluation/env_infos/initial/reward_run Std             0.37935\n",
      "evaluation/env_infos/initial/reward_run Max             0.938785\n",
      "evaluation/env_infos/initial/reward_run Min            -0.684111\n",
      "evaluation/env_infos/reward_run Mean                   -0.0818727\n",
      "evaluation/env_infos/reward_run Std                     1.60535\n",
      "evaluation/env_infos/reward_run Max                     5.02383\n",
      "evaluation/env_infos/reward_run Min                    -5.68426\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.317232\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.122072\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0651431\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.567361\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277724\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0932593\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0803336\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.457219\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.328735\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113867\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00371949\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598593\n",
      "evaluation/env_infos/final/height Mean                 -0.235747\n",
      "evaluation/env_infos/final/height Std                   0.221338\n",
      "evaluation/env_infos/final/height Max                   0.0750051\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0124766\n",
      "evaluation/env_infos/initial/height Std                 0.0504041\n",
      "evaluation/env_infos/initial/height Max                 0.0766495\n",
      "evaluation/env_infos/initial/height Min                -0.106352\n",
      "evaluation/env_infos/height Mean                       -0.20279\n",
      "evaluation/env_infos/height Std                         0.221456\n",
      "evaluation/env_infos/height Max                         0.442757\n",
      "evaluation/env_infos/height Min                        -0.596126\n",
      "evaluation/env_infos/final/reward_angular Mean          0.238725\n",
      "evaluation/env_infos/final/reward_angular Std           1.33514\n",
      "evaluation/env_infos/final/reward_angular Max           3.37874\n",
      "evaluation/env_infos/final/reward_angular Min          -2.41405\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.696845\n",
      "evaluation/env_infos/initial/reward_angular Std         0.836586\n",
      "evaluation/env_infos/initial/reward_angular Max         1.71006\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.02778\n",
      "evaluation/env_infos/reward_angular Mean                0.0328194\n",
      "evaluation/env_infos/reward_angular Std                 1.70859\n",
      "evaluation/env_infos/reward_angular Max                 8.40167\n",
      "evaluation/env_infos/reward_angular Min                -7.42189\n",
      "time/data storing (s)                                   0.0167869\n",
      "time/evaluation sampling (s)                           25.045\n",
      "time/exploration sampling (s)                           1.05646\n",
      "time/logging (s)                                        0.246817\n",
      "time/saving (s)                                         0.0300489\n",
      "time/training (s)                                       4.84424\n",
      "time/epoch (s)                                         31.2394\n",
      "time/total (s)                                       3051.78\n",
      "Epoch                                                 103\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:03:24.948150 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 104 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 106000\n",
      "trainer/QF1 Loss                                        1.89009\n",
      "trainer/QF2 Loss                                        1.71277\n",
      "trainer/Policy Loss                                     1.24433\n",
      "trainer/Q1 Predictions Mean                             4.36284\n",
      "trainer/Q1 Predictions Std                             12.6977\n",
      "trainer/Q1 Predictions Max                             58.0659\n",
      "trainer/Q1 Predictions Min                            -12.0395\n",
      "trainer/Q2 Predictions Mean                             4.53667\n",
      "trainer/Q2 Predictions Std                             12.6714\n",
      "trainer/Q2 Predictions Max                             55.6545\n",
      "trainer/Q2 Predictions Min                            -11.9488\n",
      "trainer/Q Targets Mean                                  4.45\n",
      "trainer/Q Targets Std                                  13.0039\n",
      "trainer/Q Targets Max                                  57.375\n",
      "trainer/Q Targets Min                                 -12.4028\n",
      "trainer/Log Pis Mean                                    5.89119\n",
      "trainer/Log Pis Std                                     5.33916\n",
      "trainer/Log Pis Max                                    34.79\n",
      "trainer/Log Pis Min                                    -3.5172\n",
      "trainer/Policy mu Mean                                  0.321137\n",
      "trainer/Policy mu Std                                   1.58632\n",
      "trainer/Policy mu Max                                   4.22848\n",
      "trainer/Policy mu Min                                  -7.22344\n",
      "trainer/Policy log std Mean                            -0.707133\n",
      "trainer/Policy log std Std                              0.29877\n",
      "trainer/Policy log std Max                              0.398212\n",
      "trainer/Policy log std Min                             -2.2918\n",
      "trainer/Alpha                                           0.0139209\n",
      "trainer/Alpha Loss                                     -0.464801\n",
      "exploration/num steps total                        106000\n",
      "exploration/num paths total                           106\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.279226\n",
      "exploration/Rewards Std                                 0.721195\n",
      "exploration/Rewards Max                                 1.91249\n",
      "exploration/Rewards Min                                -2.61816\n",
      "exploration/Returns Mean                             -279.226\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -279.226\n",
      "exploration/Returns Min                              -279.226\n",
      "exploration/Actions Mean                                0.299054\n",
      "exploration/Actions Std                                 0.641619\n",
      "exploration/Actions Max                                 0.999574\n",
      "exploration/Actions Min                                -0.999993\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -279.226\n",
      "exploration/env_infos/final/reward_run Mean             0.773767\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.773767\n",
      "exploration/env_infos/final/reward_run Min              0.773767\n",
      "exploration/env_infos/initial/reward_run Mean          -0.252106\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.252106\n",
      "exploration/env_infos/initial/reward_run Min           -0.252106\n",
      "exploration/env_infos/reward_run Mean                   1.25011\n",
      "exploration/env_infos/reward_run Std                    0.925133\n",
      "exploration/env_infos/reward_run Max                    4.18038\n",
      "exploration/env_infos/reward_run Min                   -0.913977\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.275517\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.275517\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.275517\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.380263\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.380263\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.380263\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.300665\n",
      "exploration/env_infos/reward_ctrl Std                   0.0868593\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0315845\n",
      "exploration/env_infos/reward_ctrl Min                  -0.542027\n",
      "exploration/env_infos/final/height Mean                -0.103102\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.103102\n",
      "exploration/env_infos/final/height Min                 -0.103102\n",
      "exploration/env_infos/initial/height Mean               0.0189232\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0189232\n",
      "exploration/env_infos/initial/height Min                0.0189232\n",
      "exploration/env_infos/height Mean                      -0.10463\n",
      "exploration/env_infos/height Std                        0.0902825\n",
      "exploration/env_infos/height Max                        0.223242\n",
      "exploration/env_infos/height Min                       -0.337443\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0543835\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0543835\n",
      "exploration/env_infos/final/reward_angular Min         -0.0543835\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.49057\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.49057\n",
      "exploration/env_infos/initial/reward_angular Min       -1.49057\n",
      "exploration/env_infos/reward_angular Mean              -0.0532428\n",
      "exploration/env_infos/reward_angular Std                1.61609\n",
      "exploration/env_infos/reward_angular Max                4.96323\n",
      "exploration/env_infos/reward_angular Min               -4.88114\n",
      "evaluation/num steps total                              2.625e+06\n",
      "evaluation/num paths total                           2625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.365831\n",
      "evaluation/Rewards Std                                  1.17294\n",
      "evaluation/Rewards Max                                  6.58405\n",
      "evaluation/Rewards Min                                 -4.99436\n",
      "evaluation/Returns Mean                               365.831\n",
      "evaluation/Returns Std                                597.975\n",
      "evaluation/Returns Max                               1586.51\n",
      "evaluation/Returns Min                               -378.048\n",
      "evaluation/Actions Mean                                 0.214985\n",
      "evaluation/Actions Std                                  0.753398\n",
      "evaluation/Actions Max                                  0.999989\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            365.831\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0154479\n",
      "evaluation/env_infos/final/reward_run Std               1.32986\n",
      "evaluation/env_infos/final/reward_run Max               2.75029\n",
      "evaluation/env_infos/final/reward_run Min              -2.53974\n",
      "evaluation/env_infos/initial/reward_run Mean            0.432377\n",
      "evaluation/env_infos/initial/reward_run Std             0.418091\n",
      "evaluation/env_infos/initial/reward_run Max             1.16753\n",
      "evaluation/env_infos/initial/reward_run Min            -0.404593\n",
      "evaluation/env_infos/reward_run Mean                   -0.395189\n",
      "evaluation/env_infos/reward_run Std                     1.87043\n",
      "evaluation/env_infos/reward_run Max                     4.99348\n",
      "evaluation/env_infos/reward_run Min                    -5.06443\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.398901\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10517\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.100102\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.547196\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.314674\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0928902\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0516262\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.439999\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.368297\n",
      "evaluation/env_infos/reward_ctrl Std                    0.102607\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0147913\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59301\n",
      "evaluation/env_infos/final/height Mean                 -0.209972\n",
      "evaluation/env_infos/final/height Std                   0.173282\n",
      "evaluation/env_infos/final/height Max                   0.180098\n",
      "evaluation/env_infos/final/height Min                  -0.572263\n",
      "evaluation/env_infos/initial/height Mean               -0.00931319\n",
      "evaluation/env_infos/initial/height Std                 0.0465633\n",
      "evaluation/env_infos/initial/height Max                 0.0691893\n",
      "evaluation/env_infos/initial/height Min                -0.0913535\n",
      "evaluation/env_infos/height Mean                       -0.173233\n",
      "evaluation/env_infos/height Std                         0.190888\n",
      "evaluation/env_infos/height Max                         0.377995\n",
      "evaluation/env_infos/height Min                        -0.601139\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.191169\n",
      "evaluation/env_infos/final/reward_angular Std           1.39613\n",
      "evaluation/env_infos/final/reward_angular Max           2.28612\n",
      "evaluation/env_infos/final/reward_angular Min          -3.69548\n",
      "evaluation/env_infos/initial/reward_angular Mean       -1.10251\n",
      "evaluation/env_infos/initial/reward_angular Std         0.619549\n",
      "evaluation/env_infos/initial/reward_angular Max         0.428359\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.21975\n",
      "evaluation/env_infos/reward_angular Mean                0.0684921\n",
      "evaluation/env_infos/reward_angular Std                 1.78413\n",
      "evaluation/env_infos/reward_angular Max                 8.20296\n",
      "evaluation/env_infos/reward_angular Min                -7.57489\n",
      "time/data storing (s)                                   0.0162536\n",
      "time/evaluation sampling (s)                           23.6636\n",
      "time/exploration sampling (s)                           1.11299\n",
      "time/logging (s)                                        0.237767\n",
      "time/saving (s)                                         0.0287872\n",
      "time/training (s)                                       4.52991\n",
      "time/epoch (s)                                         29.5893\n",
      "time/total (s)                                       3082.24\n",
      "Epoch                                                 104\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:03:54.360968 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 105 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 107000\n",
      "trainer/QF1 Loss                                        0.866743\n",
      "trainer/QF2 Loss                                        0.874358\n",
      "trainer/Policy Loss                                     1.65925\n",
      "trainer/Q1 Predictions Mean                             4.09696\n",
      "trainer/Q1 Predictions Std                             13.0834\n",
      "trainer/Q1 Predictions Max                             63.3972\n",
      "trainer/Q1 Predictions Min                            -18.4502\n",
      "trainer/Q2 Predictions Mean                             4.03753\n",
      "trainer/Q2 Predictions Std                             13.1414\n",
      "trainer/Q2 Predictions Max                             63.2039\n",
      "trainer/Q2 Predictions Min                            -18.8318\n",
      "trainer/Q Targets Mean                                  4.12397\n",
      "trainer/Q Targets Std                                  12.9901\n",
      "trainer/Q Targets Max                                  62.7903\n",
      "trainer/Q Targets Min                                 -16.5721\n",
      "trainer/Log Pis Mean                                    5.91253\n",
      "trainer/Log Pis Std                                     5.12563\n",
      "trainer/Log Pis Max                                    21.9451\n",
      "trainer/Log Pis Min                                    -5.5222\n",
      "trainer/Policy mu Mean                                  0.196477\n",
      "trainer/Policy mu Std                                   1.61308\n",
      "trainer/Policy mu Max                                   4.0914\n",
      "trainer/Policy mu Min                                  -6.31936\n",
      "trainer/Policy log std Mean                            -0.705507\n",
      "trainer/Policy log std Std                              0.271569\n",
      "trainer/Policy log std Max                              0.120053\n",
      "trainer/Policy log std Min                             -2.05907\n",
      "trainer/Alpha                                           0.0132383\n",
      "trainer/Alpha Loss                                     -0.378304\n",
      "exploration/num steps total                        107000\n",
      "exploration/num paths total                           107\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.427762\n",
      "exploration/Rewards Std                                 1.8638\n",
      "exploration/Rewards Max                                 5.60319\n",
      "exploration/Rewards Min                                -5.03952\n",
      "exploration/Returns Mean                              427.762\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               427.762\n",
      "exploration/Returns Min                               427.762\n",
      "exploration/Actions Mean                                0.247219\n",
      "exploration/Actions Std                                 0.733585\n",
      "exploration/Actions Max                                 0.999996\n",
      "exploration/Actions Min                                -0.999978\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           427.762\n",
      "exploration/env_infos/final/reward_run Mean            -0.0180992\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0180992\n",
      "exploration/env_infos/final/reward_run Min             -0.0180992\n",
      "exploration/env_infos/initial/reward_run Mean           0.52535\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.52535\n",
      "exploration/env_infos/initial/reward_run Min            0.52535\n",
      "exploration/env_infos/reward_run Mean                  -0.863986\n",
      "exploration/env_infos/reward_run Std                    1.27227\n",
      "exploration/env_infos/reward_run Max                    1.62224\n",
      "exploration/env_infos/reward_run Min                   -5.42108\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.457456\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.457456\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.457456\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.228555\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.228555\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.228555\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.359559\n",
      "exploration/env_infos/reward_ctrl Std                   0.0976128\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0764789\n",
      "exploration/env_infos/reward_ctrl Min                  -0.587559\n",
      "exploration/env_infos/final/height Mean                -0.00584389\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.00584389\n",
      "exploration/env_infos/final/height Min                 -0.00584389\n",
      "exploration/env_infos/initial/height Mean              -0.085716\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.085716\n",
      "exploration/env_infos/initial/height Min               -0.085716\n",
      "exploration/env_infos/height Mean                      -0.278082\n",
      "exploration/env_infos/height Std                        0.226096\n",
      "exploration/env_infos/height Max                        0.363499\n",
      "exploration/env_infos/height Min                       -0.589219\n",
      "exploration/env_infos/final/reward_angular Mean         1.27029\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.27029\n",
      "exploration/env_infos/final/reward_angular Min          1.27029\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.71722\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.71722\n",
      "exploration/env_infos/initial/reward_angular Min       -1.71722\n",
      "exploration/env_infos/reward_angular Mean              -0.0689713\n",
      "exploration/env_infos/reward_angular Std                2.16142\n",
      "exploration/env_infos/reward_angular Max                6.48815\n",
      "exploration/env_infos/reward_angular Min               -6.78643\n",
      "evaluation/num steps total                              2.65e+06\n",
      "evaluation/num paths total                           2650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.0823277\n",
      "evaluation/Rewards Std                                  1.14402\n",
      "evaluation/Rewards Max                                  6.80131\n",
      "evaluation/Rewards Min                                 -5.09758\n",
      "evaluation/Returns Mean                                82.3277\n",
      "evaluation/Returns Std                                459.166\n",
      "evaluation/Returns Max                               1700.54\n",
      "evaluation/Returns Min                               -414.976\n",
      "evaluation/Actions Mean                                 0.103777\n",
      "evaluation/Actions Std                                  0.726372\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                             82.3277\n",
      "evaluation/env_infos/final/reward_run Mean              0.352806\n",
      "evaluation/env_infos/final/reward_run Std               1.08787\n",
      "evaluation/env_infos/final/reward_run Max               3.69737\n",
      "evaluation/env_infos/final/reward_run Min              -1.15536\n",
      "evaluation/env_infos/initial/reward_run Mean            0.446469\n",
      "evaluation/env_infos/initial/reward_run Std             0.325822\n",
      "evaluation/env_infos/initial/reward_run Max             1.05775\n",
      "evaluation/env_infos/initial/reward_run Min            -0.0991097\n",
      "evaluation/env_infos/reward_run Mean                    0.0844933\n",
      "evaluation/env_infos/reward_run Std                     1.42158\n",
      "evaluation/env_infos/reward_run Max                     4.8409\n",
      "evaluation/env_infos/reward_run Min                    -5.40091\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.296474\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104013\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.103419\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.573024\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.308812\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0996675\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.146848\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.534192\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.323031\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106579\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0175194\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596116\n",
      "evaluation/env_infos/final/height Mean                 -0.347376\n",
      "evaluation/env_infos/final/height Std                   0.175116\n",
      "evaluation/env_infos/final/height Max                  -0.0874592\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0257773\n",
      "evaluation/env_infos/initial/height Std                 0.049311\n",
      "evaluation/env_infos/initial/height Max                 0.0503758\n",
      "evaluation/env_infos/initial/height Min                -0.112798\n",
      "evaluation/env_infos/height Mean                       -0.266942\n",
      "evaluation/env_infos/height Std                         0.218491\n",
      "evaluation/env_infos/height Max                         0.410039\n",
      "evaluation/env_infos/height Min                        -0.59424\n",
      "evaluation/env_infos/final/reward_angular Mean          0.411404\n",
      "evaluation/env_infos/final/reward_angular Std           1.30126\n",
      "evaluation/env_infos/final/reward_angular Max           3.79165\n",
      "evaluation/env_infos/final/reward_angular Min          -1.9696\n",
      "evaluation/env_infos/initial/reward_angular Mean       -1.0563\n",
      "evaluation/env_infos/initial/reward_angular Std         0.808507\n",
      "evaluation/env_infos/initial/reward_angular Max         0.984824\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.15843\n",
      "evaluation/env_infos/reward_angular Mean                0.0260604\n",
      "evaluation/env_infos/reward_angular Std                 1.66833\n",
      "evaluation/env_infos/reward_angular Max                 8.4742\n",
      "evaluation/env_infos/reward_angular Min                -9.07103\n",
      "time/data storing (s)                                   0.0183101\n",
      "time/evaluation sampling (s)                           21.9433\n",
      "time/exploration sampling (s)                           1.13015\n",
      "time/logging (s)                                        0.277612\n",
      "time/saving (s)                                         0.0299818\n",
      "time/training (s)                                       5.18114\n",
      "time/epoch (s)                                         28.5805\n",
      "time/total (s)                                       3111.69\n",
      "Epoch                                                 105\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:04:23.209468 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 106 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 108000\n",
      "trainer/QF1 Loss                                        0.621077\n",
      "trainer/QF2 Loss                                        0.740777\n",
      "trainer/Policy Loss                                     3.02029\n",
      "trainer/Q1 Predictions Mean                             2.91307\n",
      "trainer/Q1 Predictions Std                             12.1762\n",
      "trainer/Q1 Predictions Max                             54.4534\n",
      "trainer/Q1 Predictions Min                            -20.3609\n",
      "trainer/Q2 Predictions Mean                             2.77576\n",
      "trainer/Q2 Predictions Std                             12.2903\n",
      "trainer/Q2 Predictions Max                             55.4787\n",
      "trainer/Q2 Predictions Min                            -20.1571\n",
      "trainer/Q Targets Mean                                  2.86354\n",
      "trainer/Q Targets Std                                  12.1514\n",
      "trainer/Q Targets Max                                  53.2961\n",
      "trainer/Q Targets Min                                 -19.7793\n",
      "trainer/Log Pis Mean                                    6.06041\n",
      "trainer/Log Pis Std                                     6.01466\n",
      "trainer/Log Pis Max                                    25.457\n",
      "trainer/Log Pis Min                                    -6.10798\n",
      "trainer/Policy mu Mean                                  0.217602\n",
      "trainer/Policy mu Std                                   1.6378\n",
      "trainer/Policy mu Max                                   5.33026\n",
      "trainer/Policy mu Min                                  -6.14171\n",
      "trainer/Policy log std Mean                            -0.746967\n",
      "trainer/Policy log std Std                              0.276253\n",
      "trainer/Policy log std Max                              0.421213\n",
      "trainer/Policy log std Min                             -2.06383\n",
      "trainer/Alpha                                           0.0135201\n",
      "trainer/Alpha Loss                                      0.260067\n",
      "exploration/num steps total                        108000\n",
      "exploration/num paths total                           108\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.320931\n",
      "exploration/Rewards Std                                 0.623545\n",
      "exploration/Rewards Max                                 1.95291\n",
      "exploration/Rewards Min                                -1.57198\n",
      "exploration/Returns Mean                              320.931\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               320.931\n",
      "exploration/Returns Min                               320.931\n",
      "exploration/Actions Mean                                0.0379298\n",
      "exploration/Actions Std                                 0.794518\n",
      "exploration/Actions Max                                 0.999931\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           320.931\n",
      "exploration/env_infos/final/reward_run Mean             1.22246\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.22246\n",
      "exploration/env_infos/final/reward_run Min              1.22246\n",
      "exploration/env_infos/initial/reward_run Mean           1.15873\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.15873\n",
      "exploration/env_infos/initial/reward_run Min            1.15873\n",
      "exploration/env_infos/reward_run Mean                   1.98685\n",
      "exploration/env_infos/reward_run Std                    1.08309\n",
      "exploration/env_infos/reward_run Max                    4.84653\n",
      "exploration/env_infos/reward_run Min                   -1.18794\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.474839\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.474839\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.474839\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.393536\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.393536\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.393536\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.379618\n",
      "exploration/env_infos/reward_ctrl Std                   0.0802035\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0926484\n",
      "exploration/env_infos/reward_ctrl Min                  -0.569189\n",
      "exploration/env_infos/final/height Mean                 0.0470692\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0470692\n",
      "exploration/env_infos/final/height Min                  0.0470692\n",
      "exploration/env_infos/initial/height Mean               0.033416\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.033416\n",
      "exploration/env_infos/initial/height Min                0.033416\n",
      "exploration/env_infos/height Mean                      -0.0637595\n",
      "exploration/env_infos/height Std                        0.120845\n",
      "exploration/env_infos/height Max                        0.380299\n",
      "exploration/env_infos/height Min                       -0.373056\n",
      "exploration/env_infos/final/reward_angular Mean         3.1054\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.1054\n",
      "exploration/env_infos/final/reward_angular Min          3.1054\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.09373\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.09373\n",
      "exploration/env_infos/initial/reward_angular Min       -1.09373\n",
      "exploration/env_infos/reward_angular Mean              -0.0680202\n",
      "exploration/env_infos/reward_angular Std                2.18563\n",
      "exploration/env_infos/reward_angular Max                6.70768\n",
      "exploration/env_infos/reward_angular Min               -5.55466\n",
      "evaluation/num steps total                              2.675e+06\n",
      "evaluation/num paths total                           2675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.0823727\n",
      "evaluation/Rewards Std                                  1.167\n",
      "evaluation/Rewards Max                                  7.69925\n",
      "evaluation/Rewards Min                                 -6.33363\n",
      "evaluation/Returns Mean                                82.3727\n",
      "evaluation/Returns Std                                418.178\n",
      "evaluation/Returns Max                               1604.66\n",
      "evaluation/Returns Min                               -342.952\n",
      "evaluation/Actions Mean                                 0.0151486\n",
      "evaluation/Actions Std                                  0.753613\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                             82.3727\n",
      "evaluation/env_infos/final/reward_run Mean              0.320121\n",
      "evaluation/env_infos/final/reward_run Std               0.950834\n",
      "evaluation/env_infos/final/reward_run Max               2.56826\n",
      "evaluation/env_infos/final/reward_run Min              -2.44931\n",
      "evaluation/env_infos/initial/reward_run Mean            0.492188\n",
      "evaluation/env_infos/initial/reward_run Std             0.499216\n",
      "evaluation/env_infos/initial/reward_run Max             1.27\n",
      "evaluation/env_infos/initial/reward_run Min            -0.227482\n",
      "evaluation/env_infos/reward_run Mean                    0.342909\n",
      "evaluation/env_infos/reward_run Std                     1.3406\n",
      "evaluation/env_infos/reward_run Max                     5.27086\n",
      "evaluation/env_infos/reward_run Min                    -6.00948\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.35697\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.113362\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.100904\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.560232\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.326461\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0893932\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.147935\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.522584\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.340898\n",
      "evaluation/env_infos/reward_ctrl Std                    0.105665\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0149992\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598212\n",
      "evaluation/env_infos/final/height Mean                 -0.296178\n",
      "evaluation/env_infos/final/height Std                   0.16814\n",
      "evaluation/env_infos/final/height Max                  -0.0962987\n",
      "evaluation/env_infos/final/height Min                  -0.579727\n",
      "evaluation/env_infos/initial/height Mean               -0.006538\n",
      "evaluation/env_infos/initial/height Std                 0.0565724\n",
      "evaluation/env_infos/initial/height Max                 0.0953867\n",
      "evaluation/env_infos/initial/height Min                -0.11526\n",
      "evaluation/env_infos/height Mean                       -0.262411\n",
      "evaluation/env_infos/height Std                         0.201598\n",
      "evaluation/env_infos/height Max                         0.470266\n",
      "evaluation/env_infos/height Min                        -0.601248\n",
      "evaluation/env_infos/final/reward_angular Mean          0.140992\n",
      "evaluation/env_infos/final/reward_angular Std           1.48306\n",
      "evaluation/env_infos/final/reward_angular Max           3.90622\n",
      "evaluation/env_infos/final/reward_angular Min          -2.19265\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.403335\n",
      "evaluation/env_infos/initial/reward_angular Std         0.833469\n",
      "evaluation/env_infos/initial/reward_angular Max         1.51526\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.6317\n",
      "evaluation/env_infos/reward_angular Mean                0.011789\n",
      "evaluation/env_infos/reward_angular Std                 1.87161\n",
      "evaluation/env_infos/reward_angular Max                 9.32963\n",
      "evaluation/env_infos/reward_angular Min                -7.77703\n",
      "time/data storing (s)                                   0.0148921\n",
      "time/evaluation sampling (s)                           22.6437\n",
      "time/exploration sampling (s)                           1.01472\n",
      "time/logging (s)                                        0.248406\n",
      "time/saving (s)                                         0.0291064\n",
      "time/training (s)                                       3.95036\n",
      "time/epoch (s)                                         27.9012\n",
      "time/total (s)                                       3140.5\n",
      "Epoch                                                 106\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:04:52.103645 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 107 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 109000\n",
      "trainer/QF1 Loss                                        0.814333\n",
      "trainer/QF2 Loss                                        0.976397\n",
      "trainer/Policy Loss                                     1.33875\n",
      "trainer/Q1 Predictions Mean                             5.14011\n",
      "trainer/Q1 Predictions Std                             14.1436\n",
      "trainer/Q1 Predictions Max                             64.4557\n",
      "trainer/Q1 Predictions Min                            -18.3275\n",
      "trainer/Q2 Predictions Mean                             5.08274\n",
      "trainer/Q2 Predictions Std                             14.0989\n",
      "trainer/Q2 Predictions Max                             64.3846\n",
      "trainer/Q2 Predictions Min                            -17.3893\n",
      "trainer/Q Targets Mean                                  5.16012\n",
      "trainer/Q Targets Std                                  14.1678\n",
      "trainer/Q Targets Max                                  65.995\n",
      "trainer/Q Targets Min                                 -15.1442\n",
      "trainer/Log Pis Mean                                    6.64908\n",
      "trainer/Log Pis Std                                     6.3262\n",
      "trainer/Log Pis Max                                    33.2645\n",
      "trainer/Log Pis Min                                    -5.92509\n",
      "trainer/Policy mu Mean                                  0.217435\n",
      "trainer/Policy mu Std                                   1.64297\n",
      "trainer/Policy mu Max                                   5.67333\n",
      "trainer/Policy mu Min                                  -8.13453\n",
      "trainer/Policy log std Mean                            -0.75635\n",
      "trainer/Policy log std Std                              0.270718\n",
      "trainer/Policy log std Max                              0.678693\n",
      "trainer/Policy log std Min                             -1.71388\n",
      "trainer/Alpha                                           0.0131955\n",
      "trainer/Alpha Loss                                      2.80953\n",
      "exploration/num steps total                        109000\n",
      "exploration/num paths total                           109\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0922337\n",
      "exploration/Rewards Std                                 0.98903\n",
      "exploration/Rewards Max                                 5.56514\n",
      "exploration/Rewards Min                                -2.89964\n",
      "exploration/Returns Mean                               92.2337\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                92.2337\n",
      "exploration/Returns Min                                92.2337\n",
      "exploration/Actions Mean                               -0.153127\n",
      "exploration/Actions Std                                 0.763995\n",
      "exploration/Actions Max                                 0.999993\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            92.2337\n",
      "exploration/env_infos/final/reward_run Mean             0.0268909\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0268909\n",
      "exploration/env_infos/final/reward_run Min              0.0268909\n",
      "exploration/env_infos/initial/reward_run Mean           0.350715\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.350715\n",
      "exploration/env_infos/initial/reward_run Min            0.350715\n",
      "exploration/env_infos/reward_run Mean                  -0.105868\n",
      "exploration/env_infos/reward_run Std                    0.390887\n",
      "exploration/env_infos/reward_run Max                    1.48131\n",
      "exploration/env_infos/reward_run Min                   -1.69916\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.32559\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.32559\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.32559\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.391412\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.391412\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.391412\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.364281\n",
      "exploration/env_infos/reward_ctrl Std                   0.0627681\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0929985\n",
      "exploration/env_infos/reward_ctrl Min                  -0.571007\n",
      "exploration/env_infos/final/height Mean                -0.180701\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.180701\n",
      "exploration/env_infos/final/height Min                 -0.180701\n",
      "exploration/env_infos/initial/height Mean              -0.0612698\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0612698\n",
      "exploration/env_infos/initial/height Min               -0.0612698\n",
      "exploration/env_infos/height Mean                      -0.235885\n",
      "exploration/env_infos/height Std                        0.140765\n",
      "exploration/env_infos/height Max                        0.329474\n",
      "exploration/env_infos/height Min                       -0.591187\n",
      "exploration/env_infos/final/reward_angular Mean        -0.184614\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.184614\n",
      "exploration/env_infos/final/reward_angular Min         -0.184614\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.636799\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.636799\n",
      "exploration/env_infos/initial/reward_angular Min       -0.636799\n",
      "exploration/env_infos/reward_angular Mean               0.136239\n",
      "exploration/env_infos/reward_angular Std                1.24634\n",
      "exploration/env_infos/reward_angular Max                6.04894\n",
      "exploration/env_infos/reward_angular Min               -4.07393\n",
      "evaluation/num steps total                              2.7e+06\n",
      "evaluation/num paths total                           2700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.158683\n",
      "evaluation/Rewards Std                                  1.17837\n",
      "evaluation/Rewards Max                                  7.2686\n",
      "evaluation/Rewards Min                                 -6.53892\n",
      "evaluation/Returns Mean                               158.683\n",
      "evaluation/Returns Std                                448.947\n",
      "evaluation/Returns Max                               1698.37\n",
      "evaluation/Returns Min                               -351.292\n",
      "evaluation/Actions Mean                                 0.0464144\n",
      "evaluation/Actions Std                                  0.755989\n",
      "evaluation/Actions Max                                  0.999991\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            158.683\n",
      "evaluation/env_infos/final/reward_run Mean              0.566748\n",
      "evaluation/env_infos/final/reward_run Std               1.12813\n",
      "evaluation/env_infos/final/reward_run Max               3.65486\n",
      "evaluation/env_infos/final/reward_run Min              -2.0123\n",
      "evaluation/env_infos/initial/reward_run Mean            0.314013\n",
      "evaluation/env_infos/initial/reward_run Std             0.4654\n",
      "evaluation/env_infos/initial/reward_run Max             1.07189\n",
      "evaluation/env_infos/initial/reward_run Min            -0.627682\n",
      "evaluation/env_infos/reward_run Mean                    0.121326\n",
      "evaluation/env_infos/reward_run Std                     1.47004\n",
      "evaluation/env_infos/reward_run Max                     5.37408\n",
      "evaluation/env_infos/reward_run Min                    -5.09615\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.313541\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10513\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0974516\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.561923\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.273938\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0642431\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.121249\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.418217\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.344205\n",
      "evaluation/env_infos/reward_ctrl Std                    0.103278\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00798315\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596252\n",
      "evaluation/env_infos/final/height Mean                 -0.258398\n",
      "evaluation/env_infos/final/height Std                   0.192387\n",
      "evaluation/env_infos/final/height Max                   0.126316\n",
      "evaluation/env_infos/final/height Min                  -0.577739\n",
      "evaluation/env_infos/initial/height Mean               -0.0200354\n",
      "evaluation/env_infos/initial/height Std                 0.0486974\n",
      "evaluation/env_infos/initial/height Max                 0.0730436\n",
      "evaluation/env_infos/initial/height Min                -0.0939471\n",
      "evaluation/env_infos/height Mean                       -0.215894\n",
      "evaluation/env_infos/height Std                         0.208288\n",
      "evaluation/env_infos/height Max                         0.520674\n",
      "evaluation/env_infos/height Min                        -0.593633\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.144267\n",
      "evaluation/env_infos/final/reward_angular Std           1.59274\n",
      "evaluation/env_infos/final/reward_angular Max           3.98645\n",
      "evaluation/env_infos/final/reward_angular Min          -3.27104\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.610889\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07518\n",
      "evaluation/env_infos/initial/reward_angular Max         2.09809\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.98838\n",
      "evaluation/env_infos/reward_angular Mean                0.018836\n",
      "evaluation/env_infos/reward_angular Std                 1.84214\n",
      "evaluation/env_infos/reward_angular Max                 8.50656\n",
      "evaluation/env_infos/reward_angular Min                -8.09417\n",
      "time/data storing (s)                                   0.0159936\n",
      "time/evaluation sampling (s)                           22.2126\n",
      "time/exploration sampling (s)                           1.00528\n",
      "time/logging (s)                                        0.244499\n",
      "time/saving (s)                                         0.0273097\n",
      "time/training (s)                                       4.46103\n",
      "time/epoch (s)                                         27.9667\n",
      "time/total (s)                                       3169.39\n",
      "Epoch                                                 107\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:05:22.713888 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 108 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 110000\n",
      "trainer/QF1 Loss                                        1.11651\n",
      "trainer/QF2 Loss                                        1.16166\n",
      "trainer/Policy Loss                                     2.24566\n",
      "trainer/Q1 Predictions Mean                             3.85952\n",
      "trainer/Q1 Predictions Std                             12.8934\n",
      "trainer/Q1 Predictions Max                             56.856\n",
      "trainer/Q1 Predictions Min                            -12.3435\n",
      "trainer/Q2 Predictions Mean                             3.59751\n",
      "trainer/Q2 Predictions Std                             12.8749\n",
      "trainer/Q2 Predictions Max                             57.4503\n",
      "trainer/Q2 Predictions Min                            -12.8894\n",
      "trainer/Q Targets Mean                                  3.80295\n",
      "trainer/Q Targets Std                                  12.9135\n",
      "trainer/Q Targets Max                                  57.6983\n",
      "trainer/Q Targets Min                                 -13.1502\n",
      "trainer/Log Pis Mean                                    6.16673\n",
      "trainer/Log Pis Std                                     5.64574\n",
      "trainer/Log Pis Max                                    28.9676\n",
      "trainer/Log Pis Min                                    -4.02521\n",
      "trainer/Policy mu Mean                                  0.291489\n",
      "trainer/Policy mu Std                                   1.58733\n",
      "trainer/Policy mu Max                                   4.892\n",
      "trainer/Policy mu Min                                  -5.8652\n",
      "trainer/Policy log std Mean                            -0.725473\n",
      "trainer/Policy log std Std                              0.265633\n",
      "trainer/Policy log std Max                              0.169102\n",
      "trainer/Policy log std Min                             -1.73318\n",
      "trainer/Alpha                                           0.0142637\n",
      "trainer/Alpha Loss                                      0.708392\n",
      "exploration/num steps total                        110000\n",
      "exploration/num paths total                           110\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.116163\n",
      "exploration/Rewards Std                                 0.850112\n",
      "exploration/Rewards Max                                 8.21301\n",
      "exploration/Rewards Min                                -2.72109\n",
      "exploration/Returns Mean                              116.163\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               116.163\n",
      "exploration/Returns Min                               116.163\n",
      "exploration/Actions Mean                               -0.117485\n",
      "exploration/Actions Std                                 0.7649\n",
      "exploration/Actions Max                                 0.999908\n",
      "exploration/Actions Min                                -0.999995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           116.163\n",
      "exploration/env_infos/final/reward_run Mean            -0.0182141\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0182141\n",
      "exploration/env_infos/final/reward_run Min             -0.0182141\n",
      "exploration/env_infos/initial/reward_run Mean           0.767234\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.767234\n",
      "exploration/env_infos/initial/reward_run Min            0.767234\n",
      "exploration/env_infos/reward_run Mean                   0.0313596\n",
      "exploration/env_infos/reward_run Std                    0.350625\n",
      "exploration/env_infos/reward_run Max                    1.98973\n",
      "exploration/env_infos/reward_run Min                   -1.54655\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.346094\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.346094\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.346094\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.408266\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.408266\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.408266\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.359325\n",
      "exploration/env_infos/reward_ctrl Std                   0.0594652\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0970335\n",
      "exploration/env_infos/reward_ctrl Min                  -0.58114\n",
      "exploration/env_infos/final/height Mean                 0.0763931\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0763931\n",
      "exploration/env_infos/final/height Min                  0.0763931\n",
      "exploration/env_infos/initial/height Mean              -0.0192373\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0192373\n",
      "exploration/env_infos/initial/height Min               -0.0192373\n",
      "exploration/env_infos/height Mean                       0.0582086\n",
      "exploration/env_infos/height Std                        0.0615763\n",
      "exploration/env_infos/height Max                        0.368464\n",
      "exploration/env_infos/height Min                       -0.316088\n",
      "exploration/env_infos/final/reward_angular Mean        -0.44677\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.44677\n",
      "exploration/env_infos/final/reward_angular Min         -0.44677\n",
      "exploration/env_infos/initial/reward_angular Mean       1.19447\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.19447\n",
      "exploration/env_infos/initial/reward_angular Min        1.19447\n",
      "exploration/env_infos/reward_angular Mean               0.164169\n",
      "exploration/env_infos/reward_angular Std                0.897762\n",
      "exploration/env_infos/reward_angular Max                8.75898\n",
      "exploration/env_infos/reward_angular Min               -2.84018\n",
      "evaluation/num steps total                              2.725e+06\n",
      "evaluation/num paths total                           2725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.257863\n",
      "evaluation/Rewards Std                                  1.21406\n",
      "evaluation/Rewards Max                                  7.22442\n",
      "evaluation/Rewards Min                                 -5.92981\n",
      "evaluation/Returns Mean                               257.863\n",
      "evaluation/Returns Std                                621.341\n",
      "evaluation/Returns Max                               2104.67\n",
      "evaluation/Returns Min                               -347.37\n",
      "evaluation/Actions Mean                                 0.1322\n",
      "evaluation/Actions Std                                  0.731723\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            257.863\n",
      "evaluation/env_infos/final/reward_run Mean             -0.128692\n",
      "evaluation/env_infos/final/reward_run Std               1.45837\n",
      "evaluation/env_infos/final/reward_run Max               2.67582\n",
      "evaluation/env_infos/final/reward_run Min              -3.67541\n",
      "evaluation/env_infos/initial/reward_run Mean            0.29498\n",
      "evaluation/env_infos/initial/reward_run Std             0.512882\n",
      "evaluation/env_infos/initial/reward_run Max             1.14225\n",
      "evaluation/env_infos/initial/reward_run Min            -0.553985\n",
      "evaluation/env_infos/reward_run Mean                   -0.334594\n",
      "evaluation/env_infos/reward_run Std                     1.80724\n",
      "evaluation/env_infos/reward_run Max                     5.28966\n",
      "evaluation/env_infos/reward_run Min                    -6.27171\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.316\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.137348\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0446786\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.582865\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.275748\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0736442\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.11897\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.492399\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.331737\n",
      "evaluation/env_infos/reward_ctrl Std                    0.122168\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00633449\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596749\n",
      "evaluation/env_infos/final/height Mean                 -0.319876\n",
      "evaluation/env_infos/final/height Std                   0.195934\n",
      "evaluation/env_infos/final/height Max                   0.043805\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0117375\n",
      "evaluation/env_infos/initial/height Std                 0.04978\n",
      "evaluation/env_infos/initial/height Max                 0.0747244\n",
      "evaluation/env_infos/initial/height Min                -0.0985961\n",
      "evaluation/env_infos/height Mean                       -0.21295\n",
      "evaluation/env_infos/height Std                         0.212781\n",
      "evaluation/env_infos/height Max                         0.542818\n",
      "evaluation/env_infos/height Min                        -0.596808\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0508699\n",
      "evaluation/env_infos/final/reward_angular Std           1.67359\n",
      "evaluation/env_infos/final/reward_angular Max           2.806\n",
      "evaluation/env_infos/final/reward_angular Min          -3.72693\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.748375\n",
      "evaluation/env_infos/initial/reward_angular Std         0.644826\n",
      "evaluation/env_infos/initial/reward_angular Max         0.789427\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.73087\n",
      "evaluation/env_infos/reward_angular Mean                0.0432752\n",
      "evaluation/env_infos/reward_angular Std                 1.83978\n",
      "evaluation/env_infos/reward_angular Max                 8.70131\n",
      "evaluation/env_infos/reward_angular Min                -7.78505\n",
      "time/data storing (s)                                   0.0162068\n",
      "time/evaluation sampling (s)                           23.0438\n",
      "time/exploration sampling (s)                           1.0799\n",
      "time/logging (s)                                        0.288981\n",
      "time/saving (s)                                         0.0607853\n",
      "time/training (s)                                       5.2932\n",
      "time/epoch (s)                                         29.7829\n",
      "time/total (s)                                       3200.04\n",
      "Epoch                                                 108\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:05:56.064861 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 109 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 111000\n",
      "trainer/QF1 Loss                                        1.11766\n",
      "trainer/QF2 Loss                                        1.10294\n",
      "trainer/Policy Loss                                     1.06187\n",
      "trainer/Q1 Predictions Mean                             4.31128\n",
      "trainer/Q1 Predictions Std                             14.4969\n",
      "trainer/Q1 Predictions Max                             64.8824\n",
      "trainer/Q1 Predictions Min                            -19.1492\n",
      "trainer/Q2 Predictions Mean                             4.47235\n",
      "trainer/Q2 Predictions Std                             14.4207\n",
      "trainer/Q2 Predictions Max                             64.1827\n",
      "trainer/Q2 Predictions Min                            -18.671\n",
      "trainer/Q Targets Mean                                  4.39958\n",
      "trainer/Q Targets Std                                  14.5793\n",
      "trainer/Q Targets Max                                  66.9419\n",
      "trainer/Q Targets Min                                 -19.3888\n",
      "trainer/Log Pis Mean                                    5.59549\n",
      "trainer/Log Pis Std                                     5.18767\n",
      "trainer/Log Pis Max                                    24.2152\n",
      "trainer/Log Pis Min                                    -7.93288\n",
      "trainer/Policy mu Mean                                  0.0587026\n",
      "trainer/Policy mu Std                                   1.552\n",
      "trainer/Policy mu Max                                   6.33765\n",
      "trainer/Policy mu Min                                  -5.3276\n",
      "trainer/Policy log std Mean                            -0.71725\n",
      "trainer/Policy log std Std                              0.278317\n",
      "trainer/Policy log std Max                              0.126147\n",
      "trainer/Policy log std Min                             -2.04924\n",
      "trainer/Alpha                                           0.0144503\n",
      "trainer/Alpha Loss                                     -1.71343\n",
      "exploration/num steps total                        111000\n",
      "exploration/num paths total                           111\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.146625\n",
      "exploration/Rewards Std                                 0.611126\n",
      "exploration/Rewards Max                                 2.67471\n",
      "exploration/Rewards Min                                -2.4464\n",
      "exploration/Returns Mean                             -146.625\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -146.625\n",
      "exploration/Returns Min                              -146.625\n",
      "exploration/Actions Mean                               -0.0247277\n",
      "exploration/Actions Std                                 0.607838\n",
      "exploration/Actions Max                                 0.999596\n",
      "exploration/Actions Min                                -0.999978\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -146.625\n",
      "exploration/env_infos/final/reward_run Mean             0.408471\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.408471\n",
      "exploration/env_infos/final/reward_run Min              0.408471\n",
      "exploration/env_infos/initial/reward_run Mean           0.742086\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.742086\n",
      "exploration/env_infos/initial/reward_run Min            0.742086\n",
      "exploration/env_infos/reward_run Mean                   0.146679\n",
      "exploration/env_infos/reward_run Std                    0.47028\n",
      "exploration/env_infos/reward_run Max                    2.02191\n",
      "exploration/env_infos/reward_run Min                   -1.24787\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.197143\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.197143\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.197143\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.330762\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.330762\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.330762\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.222047\n",
      "exploration/env_infos/reward_ctrl Std                   0.0865129\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0290397\n",
      "exploration/env_infos/reward_ctrl Min                  -0.569602\n",
      "exploration/env_infos/final/height Mean                -0.141748\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.141748\n",
      "exploration/env_infos/final/height Min                 -0.141748\n",
      "exploration/env_infos/initial/height Mean               0.0439633\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0439633\n",
      "exploration/env_infos/initial/height Min                0.0439633\n",
      "exploration/env_infos/height Mean                      -0.154587\n",
      "exploration/env_infos/height Std                        0.0949129\n",
      "exploration/env_infos/height Max                        0.219101\n",
      "exploration/env_infos/height Min                       -0.582413\n",
      "exploration/env_infos/final/reward_angular Mean         0.0290798\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0290798\n",
      "exploration/env_infos/final/reward_angular Min          0.0290798\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.17852\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.17852\n",
      "exploration/env_infos/initial/reward_angular Min       -1.17852\n",
      "exploration/env_infos/reward_angular Mean               0.126171\n",
      "exploration/env_infos/reward_angular Std                1.22034\n",
      "exploration/env_infos/reward_angular Max                6.28163\n",
      "exploration/env_infos/reward_angular Min               -4.38483\n",
      "evaluation/num steps total                              2.75e+06\n",
      "evaluation/num paths total                           2750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.310006\n",
      "evaluation/Rewards Std                                  1.18854\n",
      "evaluation/Rewards Max                                  6.6939\n",
      "evaluation/Rewards Min                                 -6.7704\n",
      "evaluation/Returns Mean                               310.006\n",
      "evaluation/Returns Std                                591.987\n",
      "evaluation/Returns Max                               1776.05\n",
      "evaluation/Returns Min                               -402.338\n",
      "evaluation/Actions Mean                                 0.10074\n",
      "evaluation/Actions Std                                  0.731501\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            310.006\n",
      "evaluation/env_infos/final/reward_run Mean              0.110329\n",
      "evaluation/env_infos/final/reward_run Std               1.13833\n",
      "evaluation/env_infos/final/reward_run Max               2.56283\n",
      "evaluation/env_infos/final/reward_run Min              -3.16424\n",
      "evaluation/env_infos/initial/reward_run Mean            0.337557\n",
      "evaluation/env_infos/initial/reward_run Std             0.475215\n",
      "evaluation/env_infos/initial/reward_run Max             0.940509\n",
      "evaluation/env_infos/initial/reward_run Min            -0.509789\n",
      "evaluation/env_infos/reward_run Mean                   -0.231736\n",
      "evaluation/env_infos/reward_run Std                     1.67376\n",
      "evaluation/env_infos/reward_run Max                     4.19987\n",
      "evaluation/env_infos/reward_run Min                    -6.77307\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.32039\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.113251\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.184425\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.574086\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.265737\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0941619\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0650492\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.40633\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.327145\n",
      "evaluation/env_infos/reward_ctrl Std                    0.12269\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00701546\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598521\n",
      "evaluation/env_infos/final/height Mean                 -0.291145\n",
      "evaluation/env_infos/final/height Std                   0.203521\n",
      "evaluation/env_infos/final/height Max                   0.0446593\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0110238\n",
      "evaluation/env_infos/initial/height Std                 0.0477204\n",
      "evaluation/env_infos/initial/height Max                 0.0723357\n",
      "evaluation/env_infos/initial/height Min                -0.0901876\n",
      "evaluation/env_infos/height Mean                       -0.165888\n",
      "evaluation/env_infos/height Std                         0.211321\n",
      "evaluation/env_infos/height Max                         0.442686\n",
      "evaluation/env_infos/height Min                        -0.589635\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.389571\n",
      "evaluation/env_infos/final/reward_angular Std           2.08197\n",
      "evaluation/env_infos/final/reward_angular Max           6.35367\n",
      "evaluation/env_infos/final/reward_angular Min          -7.22189\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.516025\n",
      "evaluation/env_infos/initial/reward_angular Std         0.814105\n",
      "evaluation/env_infos/initial/reward_angular Max         1.23683\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.70071\n",
      "evaluation/env_infos/reward_angular Mean                0.0214105\n",
      "evaluation/env_infos/reward_angular Std                 1.89845\n",
      "evaluation/env_infos/reward_angular Max                 8.94397\n",
      "evaluation/env_infos/reward_angular Min                -8.04858\n",
      "time/data storing (s)                                   0.016443\n",
      "time/evaluation sampling (s)                           24.0217\n",
      "time/exploration sampling (s)                           1.18177\n",
      "time/logging (s)                                        0.287192\n",
      "time/saving (s)                                         0.0288597\n",
      "time/training (s)                                       6.80196\n",
      "time/epoch (s)                                         32.3379\n",
      "time/total (s)                                       3233.39\n",
      "Epoch                                                 109\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:06:27.298045 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 110 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 112000\n",
      "trainer/QF1 Loss                                        0.994299\n",
      "trainer/QF2 Loss                                        1.07832\n",
      "trainer/Policy Loss                                     0.296377\n",
      "trainer/Q1 Predictions Mean                             5.33182\n",
      "trainer/Q1 Predictions Std                             13.9334\n",
      "trainer/Q1 Predictions Max                             63.4858\n",
      "trainer/Q1 Predictions Min                            -19.5641\n",
      "trainer/Q2 Predictions Mean                             5.43131\n",
      "trainer/Q2 Predictions Std                             13.9826\n",
      "trainer/Q2 Predictions Max                             64.3095\n",
      "trainer/Q2 Predictions Min                            -19.2666\n",
      "trainer/Q Targets Mean                                  5.35138\n",
      "trainer/Q Targets Std                                  13.8168\n",
      "trainer/Q Targets Max                                  64.0026\n",
      "trainer/Q Targets Min                                 -20.0484\n",
      "trainer/Log Pis Mean                                    5.86317\n",
      "trainer/Log Pis Std                                     5.42677\n",
      "trainer/Log Pis Max                                    31.0482\n",
      "trainer/Log Pis Min                                    -4.04434\n",
      "trainer/Policy mu Mean                                  0.149403\n",
      "trainer/Policy mu Std                                   1.57143\n",
      "trainer/Policy mu Max                                   4.25579\n",
      "trainer/Policy mu Min                                  -5.68273\n",
      "trainer/Policy log std Mean                            -0.778811\n",
      "trainer/Policy log std Std                              0.285144\n",
      "trainer/Policy log std Max                              0.305665\n",
      "trainer/Policy log std Min                             -1.92087\n",
      "trainer/Alpha                                           0.0146456\n",
      "trainer/Alpha Loss                                     -0.577965\n",
      "exploration/num steps total                        112000\n",
      "exploration/num paths total                           112\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.741138\n",
      "exploration/Rewards Std                                 1.81482\n",
      "exploration/Rewards Max                                 6.63162\n",
      "exploration/Rewards Min                                -5.88483\n",
      "exploration/Returns Mean                              741.138\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               741.138\n",
      "exploration/Returns Min                               741.138\n",
      "exploration/Actions Mean                                0.319492\n",
      "exploration/Actions Std                                 0.776761\n",
      "exploration/Actions Max                                 0.999975\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           741.138\n",
      "exploration/env_infos/final/reward_run Mean            -2.37549\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.37549\n",
      "exploration/env_infos/final/reward_run Min             -2.37549\n",
      "exploration/env_infos/initial/reward_run Mean          -0.241854\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.241854\n",
      "exploration/env_infos/initial/reward_run Min           -0.241854\n",
      "exploration/env_infos/reward_run Mean                  -2.61366\n",
      "exploration/env_infos/reward_run Std                    0.886567\n",
      "exploration/env_infos/reward_run Max                    0.604433\n",
      "exploration/env_infos/reward_run Min                   -5.05236\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.501642\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.501642\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.501642\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.26669\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.26669\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.26669\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.423259\n",
      "exploration/env_infos/reward_ctrl Std                   0.0993732\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0785019\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595336\n",
      "exploration/env_infos/final/height Mean                -0.0161662\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0161662\n",
      "exploration/env_infos/final/height Min                 -0.0161662\n",
      "exploration/env_infos/initial/height Mean               0.079754\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.079754\n",
      "exploration/env_infos/initial/height Min                0.079754\n",
      "exploration/env_infos/height Mean                       0.00945817\n",
      "exploration/env_infos/height Std                        0.111663\n",
      "exploration/env_infos/height Max                        0.380847\n",
      "exploration/env_infos/height Min                       -0.314907\n",
      "exploration/env_infos/final/reward_angular Mean         1.25743\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.25743\n",
      "exploration/env_infos/final/reward_angular Min          1.25743\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.749698\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.749698\n",
      "exploration/env_infos/initial/reward_angular Min       -0.749698\n",
      "exploration/env_infos/reward_angular Mean               0.112019\n",
      "exploration/env_infos/reward_angular Std                2.36774\n",
      "exploration/env_infos/reward_angular Max                7.64018\n",
      "exploration/env_infos/reward_angular Min               -7.11798\n",
      "evaluation/num steps total                              2.775e+06\n",
      "evaluation/num paths total                           2775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.453971\n",
      "evaluation/Rewards Std                                  1.25409\n",
      "evaluation/Rewards Max                                  7.31861\n",
      "evaluation/Rewards Min                                 -5.76613\n",
      "evaluation/Returns Mean                               453.971\n",
      "evaluation/Returns Std                                640.307\n",
      "evaluation/Returns Max                               1812.85\n",
      "evaluation/Returns Min                               -184.631\n",
      "evaluation/Actions Mean                                 0.102968\n",
      "evaluation/Actions Std                                  0.790758\n",
      "evaluation/Actions Max                                  0.999995\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            453.971\n",
      "evaluation/env_infos/final/reward_run Mean             -0.512017\n",
      "evaluation/env_infos/final/reward_run Std               1.71139\n",
      "evaluation/env_infos/final/reward_run Max               2.77001\n",
      "evaluation/env_infos/final/reward_run Min              -3.38104\n",
      "evaluation/env_infos/initial/reward_run Mean            0.371846\n",
      "evaluation/env_infos/initial/reward_run Std             0.478376\n",
      "evaluation/env_infos/initial/reward_run Max             1.08923\n",
      "evaluation/env_infos/initial/reward_run Min            -0.495964\n",
      "evaluation/env_infos/reward_run Mean                   -0.408616\n",
      "evaluation/env_infos/reward_run Std                     1.94012\n",
      "evaluation/env_infos/reward_run Max                     4.86634\n",
      "evaluation/env_infos/reward_run Min                    -5.79298\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.384868\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101067\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.161864\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.552536\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.264934\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0825093\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.101488\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.419154\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.38154\n",
      "evaluation/env_infos/reward_ctrl Std                    0.11205\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0183427\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597334\n",
      "evaluation/env_infos/final/height Mean                 -0.0761309\n",
      "evaluation/env_infos/final/height Std                   0.124118\n",
      "evaluation/env_infos/final/height Max                   0.147791\n",
      "evaluation/env_infos/final/height Min                  -0.354662\n",
      "evaluation/env_infos/initial/height Mean               -0.0383462\n",
      "evaluation/env_infos/initial/height Std                 0.0483931\n",
      "evaluation/env_infos/initial/height Max                 0.0601773\n",
      "evaluation/env_infos/initial/height Min                -0.0922221\n",
      "evaluation/env_infos/height Mean                       -0.0894518\n",
      "evaluation/env_infos/height Std                         0.15776\n",
      "evaluation/env_infos/height Max                         0.481784\n",
      "evaluation/env_infos/height Min                        -0.592879\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.128085\n",
      "evaluation/env_infos/final/reward_angular Std           1.74468\n",
      "evaluation/env_infos/final/reward_angular Max           5.17506\n",
      "evaluation/env_infos/final/reward_angular Min          -3.21684\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.543382\n",
      "evaluation/env_infos/initial/reward_angular Std         0.920603\n",
      "evaluation/env_infos/initial/reward_angular Max         1.82188\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.27255\n",
      "evaluation/env_infos/reward_angular Mean                0.0536599\n",
      "evaluation/env_infos/reward_angular Std                 2.03725\n",
      "evaluation/env_infos/reward_angular Max                 8.87292\n",
      "evaluation/env_infos/reward_angular Min                -7.2101\n",
      "time/data storing (s)                                   0.0163613\n",
      "time/evaluation sampling (s)                           24.4171\n",
      "time/exploration sampling (s)                           1.10676\n",
      "time/logging (s)                                        0.257611\n",
      "time/saving (s)                                         0.0309138\n",
      "time/training (s)                                       4.3539\n",
      "time/epoch (s)                                         30.1827\n",
      "time/total (s)                                       3264.59\n",
      "Epoch                                                 110\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:07:00.025771 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 111 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 113000\n",
      "trainer/QF1 Loss                                        1.02414\n",
      "trainer/QF2 Loss                                        1.16052\n",
      "trainer/Policy Loss                                     1.23486\n",
      "trainer/Q1 Predictions Mean                             4.39714\n",
      "trainer/Q1 Predictions Std                             13.1262\n",
      "trainer/Q1 Predictions Max                             68.042\n",
      "trainer/Q1 Predictions Min                            -16.522\n",
      "trainer/Q2 Predictions Mean                             4.38288\n",
      "trainer/Q2 Predictions Std                             13.1889\n",
      "trainer/Q2 Predictions Max                             68.0606\n",
      "trainer/Q2 Predictions Min                            -17.552\n",
      "trainer/Q Targets Mean                                  4.26721\n",
      "trainer/Q Targets Std                                  13.2084\n",
      "trainer/Q Targets Max                                  65.8125\n",
      "trainer/Q Targets Min                                 -19.9174\n",
      "trainer/Log Pis Mean                                    5.87449\n",
      "trainer/Log Pis Std                                     5.43437\n",
      "trainer/Log Pis Max                                    22.2629\n",
      "trainer/Log Pis Min                                    -9.09005\n",
      "trainer/Policy mu Mean                                  0.180161\n",
      "trainer/Policy mu Std                                   1.57925\n",
      "trainer/Policy mu Max                                   4.18738\n",
      "trainer/Policy mu Min                                  -5.85912\n",
      "trainer/Policy log std Mean                            -0.744993\n",
      "trainer/Policy log std Std                              0.295157\n",
      "trainer/Policy log std Max                              0.652661\n",
      "trainer/Policy log std Min                             -1.69755\n",
      "trainer/Alpha                                           0.0146228\n",
      "trainer/Alpha Loss                                     -0.530314\n",
      "exploration/num steps total                        113000\n",
      "exploration/num paths total                           113\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.154259\n",
      "exploration/Rewards Std                                 0.956979\n",
      "exploration/Rewards Max                                 3.12366\n",
      "exploration/Rewards Min                                -4.46078\n",
      "exploration/Returns Mean                             -154.259\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -154.259\n",
      "exploration/Returns Min                              -154.259\n",
      "exploration/Actions Mean                                0.263732\n",
      "exploration/Actions Std                                 0.591132\n",
      "exploration/Actions Max                                 0.997735\n",
      "exploration/Actions Min                                -0.999116\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -154.259\n",
      "exploration/env_infos/final/reward_run Mean            -0.770674\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.770674\n",
      "exploration/env_infos/final/reward_run Min             -0.770674\n",
      "exploration/env_infos/initial/reward_run Mean          -0.245022\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.245022\n",
      "exploration/env_infos/initial/reward_run Min           -0.245022\n",
      "exploration/env_infos/reward_run Mean                  -0.897784\n",
      "exploration/env_infos/reward_run Std                    0.636197\n",
      "exploration/env_infos/reward_run Max                    1.08456\n",
      "exploration/env_infos/reward_run Min                   -2.99846\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.337899\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.337899\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.337899\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.230966\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.230966\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.230966\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.251395\n",
      "exploration/env_infos/reward_ctrl Std                   0.0746845\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0547111\n",
      "exploration/env_infos/reward_ctrl Min                  -0.465108\n",
      "exploration/env_infos/final/height Mean                -0.126219\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.126219\n",
      "exploration/env_infos/final/height Min                 -0.126219\n",
      "exploration/env_infos/initial/height Mean               0.0316671\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0316671\n",
      "exploration/env_infos/initial/height Min                0.0316671\n",
      "exploration/env_infos/height Mean                      -0.151333\n",
      "exploration/env_infos/height Std                        0.0636286\n",
      "exploration/env_infos/height Max                        0.0805135\n",
      "exploration/env_infos/height Min                       -0.342722\n",
      "exploration/env_infos/final/reward_angular Mean        -0.8026\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.8026\n",
      "exploration/env_infos/final/reward_angular Min         -0.8026\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.854582\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.854582\n",
      "exploration/env_infos/initial/reward_angular Min       -0.854582\n",
      "exploration/env_infos/reward_angular Mean               0.0148726\n",
      "exploration/env_infos/reward_angular Std                1.11527\n",
      "exploration/env_infos/reward_angular Max                4.9647\n",
      "exploration/env_infos/reward_angular Min               -3.65303\n",
      "evaluation/num steps total                              2.8e+06\n",
      "evaluation/num paths total                           2800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.416716\n",
      "evaluation/Rewards Std                                  1.13316\n",
      "evaluation/Rewards Max                                  7.40216\n",
      "evaluation/Rewards Min                                 -5.51319\n",
      "evaluation/Returns Mean                               416.716\n",
      "evaluation/Returns Std                                589.768\n",
      "evaluation/Returns Max                               1751.89\n",
      "evaluation/Returns Min                               -220.551\n",
      "evaluation/Actions Mean                                 0.13401\n",
      "evaluation/Actions Std                                  0.760587\n",
      "evaluation/Actions Max                                  0.999892\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            416.716\n",
      "evaluation/env_infos/final/reward_run Mean             -0.571728\n",
      "evaluation/env_infos/final/reward_run Std               1.88948\n",
      "evaluation/env_infos/final/reward_run Max               2.70037\n",
      "evaluation/env_infos/final/reward_run Min              -3.97539\n",
      "evaluation/env_infos/initial/reward_run Mean            0.28281\n",
      "evaluation/env_infos/initial/reward_run Std             0.446782\n",
      "evaluation/env_infos/initial/reward_run Max             1.1389\n",
      "evaluation/env_infos/initial/reward_run Min            -0.470032\n",
      "evaluation/env_infos/reward_run Mean                   -0.490458\n",
      "evaluation/env_infos/reward_run Std                     1.85864\n",
      "evaluation/env_infos/reward_run Max                     4.98275\n",
      "evaluation/env_infos/reward_run Min                    -5.63052\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.372486\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.112593\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.102263\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.551603\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.278705\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0797478\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.137078\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.461345\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.357871\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112933\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0149123\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59744\n",
      "evaluation/env_infos/final/height Mean                 -0.151759\n",
      "evaluation/env_infos/final/height Std                   0.155546\n",
      "evaluation/env_infos/final/height Max                   0.0867639\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0244033\n",
      "evaluation/env_infos/initial/height Std                 0.043669\n",
      "evaluation/env_infos/initial/height Max                 0.0653961\n",
      "evaluation/env_infos/initial/height Min                -0.115388\n",
      "evaluation/env_infos/height Mean                       -0.155006\n",
      "evaluation/env_infos/height Std                         0.170513\n",
      "evaluation/env_infos/height Max                         0.388037\n",
      "evaluation/env_infos/height Min                        -0.59408\n",
      "evaluation/env_infos/final/reward_angular Mean          0.10516\n",
      "evaluation/env_infos/final/reward_angular Std           1.56182\n",
      "evaluation/env_infos/final/reward_angular Max           4.73723\n",
      "evaluation/env_infos/final/reward_angular Min          -2.25025\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.439454\n",
      "evaluation/env_infos/initial/reward_angular Std         1.17069\n",
      "evaluation/env_infos/initial/reward_angular Max         2.16298\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.90018\n",
      "evaluation/env_infos/reward_angular Mean                0.0637944\n",
      "evaluation/env_infos/reward_angular Std                 1.73378\n",
      "evaluation/env_infos/reward_angular Max                 8.47145\n",
      "evaluation/env_infos/reward_angular Min                -7.34581\n",
      "time/data storing (s)                                   0.0174026\n",
      "time/evaluation sampling (s)                           25.3525\n",
      "time/exploration sampling (s)                           1.28583\n",
      "time/logging (s)                                        0.273182\n",
      "time/saving (s)                                         0.0310665\n",
      "time/training (s)                                       4.8461\n",
      "time/epoch (s)                                         31.8061\n",
      "time/total (s)                                       3297.33\n",
      "Epoch                                                 111\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:07:30.779938 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 112 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 114000\n",
      "trainer/QF1 Loss                                        1.33353\n",
      "trainer/QF2 Loss                                        1.27814\n",
      "trainer/Policy Loss                                     0.534556\n",
      "trainer/Q1 Predictions Mean                             4.83158\n",
      "trainer/Q1 Predictions Std                             15.6791\n",
      "trainer/Q1 Predictions Max                             65.1104\n",
      "trainer/Q1 Predictions Min                            -17.2507\n",
      "trainer/Q2 Predictions Mean                             4.8283\n",
      "trainer/Q2 Predictions Std                             15.5434\n",
      "trainer/Q2 Predictions Max                             65.5091\n",
      "trainer/Q2 Predictions Min                            -17.178\n",
      "trainer/Q Targets Mean                                  4.43931\n",
      "trainer/Q Targets Std                                  15.4017\n",
      "trainer/Q Targets Max                                  62.4968\n",
      "trainer/Q Targets Min                                 -17.3778\n",
      "trainer/Log Pis Mean                                    5.58376\n",
      "trainer/Log Pis Std                                     5.6523\n",
      "trainer/Log Pis Max                                    30.3935\n",
      "trainer/Log Pis Min                                    -4.44319\n",
      "trainer/Policy mu Mean                                  0.148252\n",
      "trainer/Policy mu Std                                   1.56737\n",
      "trainer/Policy mu Max                                   4.82578\n",
      "trainer/Policy mu Min                                  -7.03157\n",
      "trainer/Policy log std Mean                            -0.714973\n",
      "trainer/Policy log std Std                              0.300731\n",
      "trainer/Policy log std Max                              0.317694\n",
      "trainer/Policy log std Min                             -1.73841\n",
      "trainer/Alpha                                           0.014853\n",
      "trainer/Alpha Loss                                     -1.75185\n",
      "exploration/num steps total                        114000\n",
      "exploration/num paths total                           114\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.984518\n",
      "exploration/Rewards Std                                 1.47285\n",
      "exploration/Rewards Max                                 5.05717\n",
      "exploration/Rewards Min                                -4.77038\n",
      "exploration/Returns Mean                              984.518\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               984.518\n",
      "exploration/Returns Min                               984.518\n",
      "exploration/Actions Mean                                0.299494\n",
      "exploration/Actions Std                                 0.807845\n",
      "exploration/Actions Max                                 0.999988\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           984.518\n",
      "exploration/env_infos/final/reward_run Mean            -2.86235\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.86235\n",
      "exploration/env_infos/final/reward_run Min             -2.86235\n",
      "exploration/env_infos/initial/reward_run Mean           0.23892\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.23892\n",
      "exploration/env_infos/initial/reward_run Min            0.23892\n",
      "exploration/env_infos/reward_run Mean                  -2.84402\n",
      "exploration/env_infos/reward_run Std                    0.83864\n",
      "exploration/env_infos/reward_run Max                    0.792892\n",
      "exploration/env_infos/reward_run Min                   -5.09666\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.49086\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.49086\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.49086\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.35223\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.35223\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.35223\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.445386\n",
      "exploration/env_infos/reward_ctrl Std                   0.0939888\n",
      "exploration/env_infos/reward_ctrl Max                  -0.177701\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593008\n",
      "exploration/env_infos/final/height Mean                -0.192383\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.192383\n",
      "exploration/env_infos/final/height Min                 -0.192383\n",
      "exploration/env_infos/initial/height Mean               0.061055\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.061055\n",
      "exploration/env_infos/initial/height Min                0.061055\n",
      "exploration/env_infos/height Mean                      -0.0269166\n",
      "exploration/env_infos/height Std                        0.119392\n",
      "exploration/env_infos/height Max                        0.353022\n",
      "exploration/env_infos/height Min                       -0.333232\n",
      "exploration/env_infos/final/reward_angular Mean         4.0243\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          4.0243\n",
      "exploration/env_infos/final/reward_angular Min          4.0243\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.39279\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.39279\n",
      "exploration/env_infos/initial/reward_angular Min       -1.39279\n",
      "exploration/env_infos/reward_angular Mean               0.0873595\n",
      "exploration/env_infos/reward_angular Std                2.36571\n",
      "exploration/env_infos/reward_angular Max                9.15672\n",
      "exploration/env_infos/reward_angular Min               -6.54609\n",
      "evaluation/num steps total                              2.825e+06\n",
      "evaluation/num paths total                           2825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.45829\n",
      "evaluation/Rewards Std                                  1.2226\n",
      "evaluation/Rewards Max                                  7.86864\n",
      "evaluation/Rewards Min                                 -6.6208\n",
      "evaluation/Returns Mean                               458.29\n",
      "evaluation/Returns Std                                644.564\n",
      "evaluation/Returns Max                               1788.98\n",
      "evaluation/Returns Min                               -320.977\n",
      "evaluation/Actions Mean                                 0.145325\n",
      "evaluation/Actions Std                                  0.759059\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            458.29\n",
      "evaluation/env_infos/final/reward_run Mean             -0.38141\n",
      "evaluation/env_infos/final/reward_run Std               1.9156\n",
      "evaluation/env_infos/final/reward_run Max               2.97317\n",
      "evaluation/env_infos/final/reward_run Min              -4.14962\n",
      "evaluation/env_infos/initial/reward_run Mean            0.250711\n",
      "evaluation/env_infos/initial/reward_run Std             0.415131\n",
      "evaluation/env_infos/initial/reward_run Max             0.983086\n",
      "evaluation/env_infos/initial/reward_run Min            -0.559083\n",
      "evaluation/env_infos/reward_run Mean                   -0.446289\n",
      "evaluation/env_infos/reward_run Std                     1.99187\n",
      "evaluation/env_infos/reward_run Max                     5.04795\n",
      "evaluation/env_infos/reward_run Min                    -5.62915\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.334686\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.131324\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.118286\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.564691\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.261403\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0726049\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.138055\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.419974\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.358374\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112762\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0166621\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598449\n",
      "evaluation/env_infos/final/height Mean                 -0.134039\n",
      "evaluation/env_infos/final/height Std                   0.170693\n",
      "evaluation/env_infos/final/height Max                   0.167576\n",
      "evaluation/env_infos/final/height Min                  -0.577281\n",
      "evaluation/env_infos/initial/height Mean               -0.0254521\n",
      "evaluation/env_infos/initial/height Std                 0.0527383\n",
      "evaluation/env_infos/initial/height Max                 0.0969331\n",
      "evaluation/env_infos/initial/height Min                -0.11413\n",
      "evaluation/env_infos/height Mean                       -0.131254\n",
      "evaluation/env_infos/height Std                         0.165073\n",
      "evaluation/env_infos/height Max                         0.458169\n",
      "evaluation/env_infos/height Min                        -0.593596\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.146533\n",
      "evaluation/env_infos/final/reward_angular Std           1.77196\n",
      "evaluation/env_infos/final/reward_angular Max           3.31308\n",
      "evaluation/env_infos/final/reward_angular Min          -4.57163\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.342989\n",
      "evaluation/env_infos/initial/reward_angular Std         0.940164\n",
      "evaluation/env_infos/initial/reward_angular Max         1.86728\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.00815\n",
      "evaluation/env_infos/reward_angular Mean                0.0498132\n",
      "evaluation/env_infos/reward_angular Std                 1.9241\n",
      "evaluation/env_infos/reward_angular Max                 9.26285\n",
      "evaluation/env_infos/reward_angular Min                -8.97277\n",
      "time/data storing (s)                                   0.0158379\n",
      "time/evaluation sampling (s)                           24.1225\n",
      "time/exploration sampling (s)                           1.1292\n",
      "time/logging (s)                                        0.248712\n",
      "time/saving (s)                                         0.0282791\n",
      "time/training (s)                                       4.22093\n",
      "time/epoch (s)                                         29.7654\n",
      "time/total (s)                                       3328.05\n",
      "Epoch                                                 112\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:08:02.802657 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 113 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 115000\n",
      "trainer/QF1 Loss                                        1.00156\n",
      "trainer/QF2 Loss                                        1.14321\n",
      "trainer/Policy Loss                                     1.10478\n",
      "trainer/Q1 Predictions Mean                             5.08951\n",
      "trainer/Q1 Predictions Std                             14.5449\n",
      "trainer/Q1 Predictions Max                             66.6356\n",
      "trainer/Q1 Predictions Min                            -18.5653\n",
      "trainer/Q2 Predictions Mean                             5.06689\n",
      "trainer/Q2 Predictions Std                             14.489\n",
      "trainer/Q2 Predictions Max                             66.3141\n",
      "trainer/Q2 Predictions Min                            -19.3688\n",
      "trainer/Q Targets Mean                                  5.12528\n",
      "trainer/Q Targets Std                                  14.7286\n",
      "trainer/Q Targets Max                                  67.1955\n",
      "trainer/Q Targets Min                                 -20.5693\n",
      "trainer/Log Pis Mean                                    6.37646\n",
      "trainer/Log Pis Std                                     5.65414\n",
      "trainer/Log Pis Max                                    35.8581\n",
      "trainer/Log Pis Min                                    -7.2408\n",
      "trainer/Policy mu Mean                                  0.105661\n",
      "trainer/Policy mu Std                                   1.61608\n",
      "trainer/Policy mu Max                                   5.04909\n",
      "trainer/Policy mu Min                                  -6.4228\n",
      "trainer/Policy log std Mean                            -0.779528\n",
      "trainer/Policy log std Std                              0.275473\n",
      "trainer/Policy log std Max                              0.00628847\n",
      "trainer/Policy log std Min                             -1.72755\n",
      "trainer/Alpha                                           0.0139634\n",
      "trainer/Alpha Loss                                      1.60782\n",
      "exploration/num steps total                        115000\n",
      "exploration/num paths total                           115\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.00262447\n",
      "exploration/Rewards Std                                 0.56884\n",
      "exploration/Rewards Max                                 1.64793\n",
      "exploration/Rewards Min                                -1.37654\n",
      "exploration/Returns Mean                               -2.62447\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                -2.62447\n",
      "exploration/Returns Min                                -2.62447\n",
      "exploration/Actions Mean                                0.0378396\n",
      "exploration/Actions Std                                 0.741123\n",
      "exploration/Actions Max                                 0.999979\n",
      "exploration/Actions Min                                -0.999995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            -2.62447\n",
      "exploration/env_infos/final/reward_run Mean            -0.880668\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.880668\n",
      "exploration/env_infos/final/reward_run Min             -0.880668\n",
      "exploration/env_infos/initial/reward_run Mean          -0.211601\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.211601\n",
      "exploration/env_infos/initial/reward_run Min           -0.211601\n",
      "exploration/env_infos/reward_run Mean                  -1.69925\n",
      "exploration/env_infos/reward_run Std                    1.73738\n",
      "exploration/env_infos/reward_run Max                    1.45472\n",
      "exploration/env_infos/reward_run Min                   -5.85288\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.345968\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.345968\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.345968\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.27432\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.27432\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.27432\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.330417\n",
      "exploration/env_infos/reward_ctrl Std                   0.118706\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0150231\n",
      "exploration/env_infos/reward_ctrl Min                  -0.572425\n",
      "exploration/env_infos/final/height Mean                -0.369088\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.369088\n",
      "exploration/env_infos/final/height Min                 -0.369088\n",
      "exploration/env_infos/initial/height Mean              -0.0438621\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0438621\n",
      "exploration/env_infos/initial/height Min               -0.0438621\n",
      "exploration/env_infos/height Mean                      -0.180902\n",
      "exploration/env_infos/height Std                        0.272494\n",
      "exploration/env_infos/height Max                        0.441547\n",
      "exploration/env_infos/height Min                       -0.592784\n",
      "exploration/env_infos/final/reward_angular Mean        -4.25928\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -4.25928\n",
      "exploration/env_infos/final/reward_angular Min         -4.25928\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.519443\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.519443\n",
      "exploration/env_infos/initial/reward_angular Min       -0.519443\n",
      "exploration/env_infos/reward_angular Mean              -0.0199677\n",
      "exploration/env_infos/reward_angular Std                2.78527\n",
      "exploration/env_infos/reward_angular Max                8.25978\n",
      "exploration/env_infos/reward_angular Min               -6.37623\n",
      "evaluation/num steps total                              2.85e+06\n",
      "evaluation/num paths total                           2850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.360248\n",
      "evaluation/Rewards Std                                  1.37277\n",
      "evaluation/Rewards Max                                  7.59654\n",
      "evaluation/Rewards Min                                 -8.73136\n",
      "evaluation/Returns Mean                               360.248\n",
      "evaluation/Returns Std                                668.105\n",
      "evaluation/Returns Max                               2228.9\n",
      "evaluation/Returns Min                               -315.04\n",
      "evaluation/Actions Mean                                 0.0254324\n",
      "evaluation/Actions Std                                  0.754924\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            360.248\n",
      "evaluation/env_infos/final/reward_run Mean              0.0269079\n",
      "evaluation/env_infos/final/reward_run Std               1.58587\n",
      "evaluation/env_infos/final/reward_run Max               2.36514\n",
      "evaluation/env_infos/final/reward_run Min              -4.06785\n",
      "evaluation/env_infos/initial/reward_run Mean            0.128792\n",
      "evaluation/env_infos/initial/reward_run Std             0.425077\n",
      "evaluation/env_infos/initial/reward_run Max             1.00192\n",
      "evaluation/env_infos/initial/reward_run Min            -0.566971\n",
      "evaluation/env_infos/reward_run Mean                   -0.254677\n",
      "evaluation/env_infos/reward_run Std                     1.92069\n",
      "evaluation/env_infos/reward_run Max                     4.36884\n",
      "evaluation/env_infos/reward_run Min                    -6.62324\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.315651\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.103095\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0915721\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.56314\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.299135\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0998413\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.14349\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.51531\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.342334\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112883\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0101376\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598363\n",
      "evaluation/env_infos/final/height Mean                 -0.2016\n",
      "evaluation/env_infos/final/height Std                   0.226238\n",
      "evaluation/env_infos/final/height Max                   0.18476\n",
      "evaluation/env_infos/final/height Min                  -0.577275\n",
      "evaluation/env_infos/initial/height Mean               -0.0140668\n",
      "evaluation/env_infos/initial/height Std                 0.0554425\n",
      "evaluation/env_infos/initial/height Max                 0.0935916\n",
      "evaluation/env_infos/initial/height Min                -0.086109\n",
      "evaluation/env_infos/height Mean                       -0.160878\n",
      "evaluation/env_infos/height Std                         0.199285\n",
      "evaluation/env_infos/height Max                         0.578192\n",
      "evaluation/env_infos/height Min                        -0.592919\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.349394\n",
      "evaluation/env_infos/final/reward_angular Std           1.45414\n",
      "evaluation/env_infos/final/reward_angular Max           3.03978\n",
      "evaluation/env_infos/final/reward_angular Min          -2.51363\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.587025\n",
      "evaluation/env_infos/initial/reward_angular Std         0.682391\n",
      "evaluation/env_infos/initial/reward_angular Max         1.27357\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.701\n",
      "evaluation/env_infos/reward_angular Mean                0.0131444\n",
      "evaluation/env_infos/reward_angular Std                 2.05773\n",
      "evaluation/env_infos/reward_angular Max                10.6664\n",
      "evaluation/env_infos/reward_angular Min                -7.31361\n",
      "time/data storing (s)                                   0.0163338\n",
      "time/evaluation sampling (s)                           24.8125\n",
      "time/exploration sampling (s)                           1.14413\n",
      "time/logging (s)                                        0.250352\n",
      "time/saving (s)                                         0.028322\n",
      "time/training (s)                                       4.82789\n",
      "time/epoch (s)                                         31.0795\n",
      "time/total (s)                                       3360.07\n",
      "Epoch                                                 113\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:08:33.775031 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 114 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 116000\n",
      "trainer/QF1 Loss                                        1.00159\n",
      "trainer/QF2 Loss                                        1.14147\n",
      "trainer/Policy Loss                                     2.20509\n",
      "trainer/Q1 Predictions Mean                             3.72606\n",
      "trainer/Q1 Predictions Std                             14.1112\n",
      "trainer/Q1 Predictions Max                             63.5278\n",
      "trainer/Q1 Predictions Min                            -16.7619\n",
      "trainer/Q2 Predictions Mean                             3.88431\n",
      "trainer/Q2 Predictions Std                             14.2549\n",
      "trainer/Q2 Predictions Max                             67.6725\n",
      "trainer/Q2 Predictions Min                            -17.0543\n",
      "trainer/Q Targets Mean                                  3.73443\n",
      "trainer/Q Targets Std                                  14.1777\n",
      "trainer/Q Targets Max                                  65.2043\n",
      "trainer/Q Targets Min                                 -17.6419\n",
      "trainer/Log Pis Mean                                    6.23796\n",
      "trainer/Log Pis Std                                     5.96295\n",
      "trainer/Log Pis Max                                    29.1133\n",
      "trainer/Log Pis Min                                    -4.46828\n",
      "trainer/Policy mu Mean                                  0.261517\n",
      "trainer/Policy mu Std                                   1.63818\n",
      "trainer/Policy mu Max                                   4.69202\n",
      "trainer/Policy mu Min                                  -5.80843\n",
      "trainer/Policy log std Mean                            -0.727165\n",
      "trainer/Policy log std Std                              0.272014\n",
      "trainer/Policy log std Max                              0.256535\n",
      "trainer/Policy log std Min                             -1.66161\n",
      "trainer/Alpha                                           0.0140884\n",
      "trainer/Alpha Loss                                      1.01437\n",
      "exploration/num steps total                        116000\n",
      "exploration/num paths total                           116\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.744172\n",
      "exploration/Rewards Std                                 1.48092\n",
      "exploration/Rewards Max                                 4.47309\n",
      "exploration/Rewards Min                                -5.54216\n",
      "exploration/Returns Mean                              744.172\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               744.172\n",
      "exploration/Returns Min                               744.172\n",
      "exploration/Actions Mean                                0.277808\n",
      "exploration/Actions Std                                 0.817739\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           744.172\n",
      "exploration/env_infos/final/reward_run Mean            -3.28868\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.28868\n",
      "exploration/env_infos/final/reward_run Min             -3.28868\n",
      "exploration/env_infos/initial/reward_run Mean          -0.326351\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.326351\n",
      "exploration/env_infos/initial/reward_run Min           -0.326351\n",
      "exploration/env_infos/reward_run Mean                  -2.56469\n",
      "exploration/env_infos/reward_run Std                    0.837255\n",
      "exploration/env_infos/reward_run Max                    0.439133\n",
      "exploration/env_infos/reward_run Min                   -4.62585\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.56857\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.56857\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.56857\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.286366\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.286366\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.286366\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.447524\n",
      "exploration/env_infos/reward_ctrl Std                   0.0772138\n",
      "exploration/env_infos/reward_ctrl Max                  -0.227004\n",
      "exploration/env_infos/reward_ctrl Min                  -0.59282\n",
      "exploration/env_infos/final/height Mean                -0.0639603\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0639603\n",
      "exploration/env_infos/final/height Min                 -0.0639603\n",
      "exploration/env_infos/initial/height Mean              -0.101176\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.101176\n",
      "exploration/env_infos/initial/height Min               -0.101176\n",
      "exploration/env_infos/height Mean                      -0.096601\n",
      "exploration/env_infos/height Std                        0.112751\n",
      "exploration/env_infos/height Max                        0.230054\n",
      "exploration/env_infos/height Min                       -0.394377\n",
      "exploration/env_infos/final/reward_angular Mean        -3.11235\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.11235\n",
      "exploration/env_infos/final/reward_angular Min         -3.11235\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.676293\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.676293\n",
      "exploration/env_infos/initial/reward_angular Min       -0.676293\n",
      "exploration/env_infos/reward_angular Mean               0.112758\n",
      "exploration/env_infos/reward_angular Std                1.78138\n",
      "exploration/env_infos/reward_angular Max                6.76018\n",
      "exploration/env_infos/reward_angular Min               -4.44682\n",
      "evaluation/num steps total                              2.875e+06\n",
      "evaluation/num paths total                           2875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.188583\n",
      "evaluation/Rewards Std                                  1.12261\n",
      "evaluation/Rewards Max                                  7.04807\n",
      "evaluation/Rewards Min                                 -6.57399\n",
      "evaluation/Returns Mean                               188.583\n",
      "evaluation/Returns Std                                608.905\n",
      "evaluation/Returns Max                               1783.77\n",
      "evaluation/Returns Min                               -484.216\n",
      "evaluation/Actions Mean                                 0.124129\n",
      "evaluation/Actions Std                                  0.752252\n",
      "evaluation/Actions Max                                  0.99999\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            188.583\n",
      "evaluation/env_infos/final/reward_run Mean             -0.196887\n",
      "evaluation/env_infos/final/reward_run Std               1.68365\n",
      "evaluation/env_infos/final/reward_run Max               2.8792\n",
      "evaluation/env_infos/final/reward_run Min              -3.88521\n",
      "evaluation/env_infos/initial/reward_run Mean            0.139384\n",
      "evaluation/env_infos/initial/reward_run Std             0.46689\n",
      "evaluation/env_infos/initial/reward_run Max             1.1366\n",
      "evaluation/env_infos/initial/reward_run Min            -0.658197\n",
      "evaluation/env_infos/reward_run Mean                   -0.351873\n",
      "evaluation/env_infos/reward_run Std                     1.63539\n",
      "evaluation/env_infos/reward_run Max                     4.1446\n",
      "evaluation/env_infos/reward_run Min                    -5.58712\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.357296\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0897805\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.138599\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.524621\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.310893\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.111293\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.074252\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.503091\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.348775\n",
      "evaluation/env_infos/reward_ctrl Std                    0.107393\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0145822\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598132\n",
      "evaluation/env_infos/final/height Mean                 -0.215161\n",
      "evaluation/env_infos/final/height Std                   0.203778\n",
      "evaluation/env_infos/final/height Max                   0.0173187\n",
      "evaluation/env_infos/final/height Min                  -0.577279\n",
      "evaluation/env_infos/initial/height Mean               -0.0340927\n",
      "evaluation/env_infos/initial/height Std                 0.0525599\n",
      "evaluation/env_infos/initial/height Max                 0.074136\n",
      "evaluation/env_infos/initial/height Min                -0.101841\n",
      "evaluation/env_infos/height Mean                       -0.199556\n",
      "evaluation/env_infos/height Std                         0.197681\n",
      "evaluation/env_infos/height Max                         0.418641\n",
      "evaluation/env_infos/height Min                        -0.595554\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.073321\n",
      "evaluation/env_infos/final/reward_angular Std           0.960933\n",
      "evaluation/env_infos/final/reward_angular Max           2.2886\n",
      "evaluation/env_infos/final/reward_angular Min          -1.95132\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.578118\n",
      "evaluation/env_infos/initial/reward_angular Std         0.992916\n",
      "evaluation/env_infos/initial/reward_angular Max         2.18062\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.05138\n",
      "evaluation/env_infos/reward_angular Mean                0.031233\n",
      "evaluation/env_infos/reward_angular Std                 1.53898\n",
      "evaluation/env_infos/reward_angular Max                 8.76061\n",
      "evaluation/env_infos/reward_angular Min                -6.19016\n",
      "time/data storing (s)                                   0.0161678\n",
      "time/evaluation sampling (s)                           24.1152\n",
      "time/exploration sampling (s)                           1.14294\n",
      "time/logging (s)                                        0.264477\n",
      "time/saving (s)                                         0.032872\n",
      "time/training (s)                                       4.44429\n",
      "time/epoch (s)                                         30.0159\n",
      "time/total (s)                                       3391.05\n",
      "Epoch                                                 114\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:09:08.668929 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 115 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 117000\n",
      "trainer/QF1 Loss                                        1.44248\n",
      "trainer/QF2 Loss                                        1.15655\n",
      "trainer/Policy Loss                                     1.20145\n",
      "trainer/Q1 Predictions Mean                             5.34896\n",
      "trainer/Q1 Predictions Std                             14.5404\n",
      "trainer/Q1 Predictions Max                             68.7575\n",
      "trainer/Q1 Predictions Min                            -12.5935\n",
      "trainer/Q2 Predictions Mean                             5.57237\n",
      "trainer/Q2 Predictions Std                             14.6802\n",
      "trainer/Q2 Predictions Max                             68.5752\n",
      "trainer/Q2 Predictions Min                            -12.3243\n",
      "trainer/Q Targets Mean                                  5.32775\n",
      "trainer/Q Targets Std                                  14.5906\n",
      "trainer/Q Targets Max                                  67.2408\n",
      "trainer/Q Targets Min                                 -13.1835\n",
      "trainer/Log Pis Mean                                    6.86287\n",
      "trainer/Log Pis Std                                     5.66028\n",
      "trainer/Log Pis Max                                    24.5393\n",
      "trainer/Log Pis Min                                    -6.63382\n",
      "trainer/Policy mu Mean                                  0.100328\n",
      "trainer/Policy mu Std                                   1.68574\n",
      "trainer/Policy mu Max                                   4.70522\n",
      "trainer/Policy mu Min                                  -4.81156\n",
      "trainer/Policy log std Mean                            -0.757167\n",
      "trainer/Policy log std Std                              0.263434\n",
      "trainer/Policy log std Max                              0.0850244\n",
      "trainer/Policy log std Min                             -1.88434\n",
      "trainer/Alpha                                           0.014553\n",
      "trainer/Alpha Loss                                      3.65263\n",
      "exploration/num steps total                        117000\n",
      "exploration/num paths total                           117\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.375\n",
      "exploration/Rewards Std                                 0.575843\n",
      "exploration/Rewards Max                                 3.0356\n",
      "exploration/Rewards Min                                -0.890722\n",
      "exploration/Returns Mean                             1375\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1375\n",
      "exploration/Returns Min                              1375\n",
      "exploration/Actions Mean                                0.224049\n",
      "exploration/Actions Std                                 0.832279\n",
      "exploration/Actions Max                                 0.999965\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1375\n",
      "exploration/env_infos/final/reward_run Mean            -2.77013\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.77013\n",
      "exploration/env_infos/final/reward_run Min             -2.77013\n",
      "exploration/env_infos/initial/reward_run Mean          -0.242121\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.242121\n",
      "exploration/env_infos/initial/reward_run Min           -0.242121\n",
      "exploration/env_infos/reward_run Mean                  -3.31708\n",
      "exploration/env_infos/reward_run Std                    0.989997\n",
      "exploration/env_infos/reward_run Max                    0.558228\n",
      "exploration/env_infos/reward_run Min                   -6.10373\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.584886\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.584886\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.584886\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.129541\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.129541\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.129541\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.445731\n",
      "exploration/env_infos/reward_ctrl Std                   0.0787943\n",
      "exploration/env_infos/reward_ctrl Max                  -0.120396\n",
      "exploration/env_infos/reward_ctrl Min                  -0.589644\n",
      "exploration/env_infos/final/height Mean                -0.00536484\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.00536484\n",
      "exploration/env_infos/final/height Min                 -0.00536484\n",
      "exploration/env_infos/initial/height Mean              -0.00777931\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00777931\n",
      "exploration/env_infos/initial/height Min               -0.00777931\n",
      "exploration/env_infos/height Mean                      -0.0161251\n",
      "exploration/env_infos/height Std                        0.128957\n",
      "exploration/env_infos/height Max                        0.416979\n",
      "exploration/env_infos/height Min                       -0.358717\n",
      "exploration/env_infos/final/reward_angular Mean         3.50188\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.50188\n",
      "exploration/env_infos/final/reward_angular Min          3.50188\n",
      "exploration/env_infos/initial/reward_angular Mean       0.772256\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.772256\n",
      "exploration/env_infos/initial/reward_angular Min        0.772256\n",
      "exploration/env_infos/reward_angular Mean               0.134579\n",
      "exploration/env_infos/reward_angular Std                2.75544\n",
      "exploration/env_infos/reward_angular Max                8.16397\n",
      "exploration/env_infos/reward_angular Min               -6.61929\n",
      "evaluation/num steps total                              2.9e+06\n",
      "evaluation/num paths total                           2900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.46387\n",
      "evaluation/Rewards Std                                  1.20761\n",
      "evaluation/Rewards Max                                  6.84023\n",
      "evaluation/Rewards Min                                 -6.58188\n",
      "evaluation/Returns Mean                               463.87\n",
      "evaluation/Returns Std                                694.933\n",
      "evaluation/Returns Max                               1896.33\n",
      "evaluation/Returns Min                               -599.397\n",
      "evaluation/Actions Mean                                 0.136722\n",
      "evaluation/Actions Std                                  0.787872\n",
      "evaluation/Actions Max                                  0.999988\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            463.87\n",
      "evaluation/env_infos/final/reward_run Mean             -0.817714\n",
      "evaluation/env_infos/final/reward_run Std               2.18916\n",
      "evaluation/env_infos/final/reward_run Max               2.82957\n",
      "evaluation/env_infos/final/reward_run Min              -4.4054\n",
      "evaluation/env_infos/initial/reward_run Mean            0.215602\n",
      "evaluation/env_infos/initial/reward_run Std             0.434611\n",
      "evaluation/env_infos/initial/reward_run Max             1.03833\n",
      "evaluation/env_infos/initial/reward_run Min            -0.614257\n",
      "evaluation/env_infos/reward_run Mean                   -0.887505\n",
      "evaluation/env_infos/reward_run Std                     2.17973\n",
      "evaluation/env_infos/reward_run Max                     5.0341\n",
      "evaluation/env_infos/reward_run Min                    -6.09879\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.363402\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0937685\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.159878\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.530057\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.28707\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.115389\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0948934\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.48298\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.383661\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106504\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0165281\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596903\n",
      "evaluation/env_infos/final/height Mean                 -0.125395\n",
      "evaluation/env_infos/final/height Std                   0.181054\n",
      "evaluation/env_infos/final/height Max                   0.207808\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0135049\n",
      "evaluation/env_infos/initial/height Std                 0.0424025\n",
      "evaluation/env_infos/initial/height Max                 0.0511206\n",
      "evaluation/env_infos/initial/height Min                -0.113247\n",
      "evaluation/env_infos/height Mean                       -0.123151\n",
      "evaluation/env_infos/height Std                         0.172814\n",
      "evaluation/env_infos/height Max                         0.514763\n",
      "evaluation/env_infos/height Min                        -0.592594\n",
      "evaluation/env_infos/final/reward_angular Mean          0.223604\n",
      "evaluation/env_infos/final/reward_angular Std           1.78808\n",
      "evaluation/env_infos/final/reward_angular Max           4.20487\n",
      "evaluation/env_infos/final/reward_angular Min          -3.39128\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.1765\n",
      "evaluation/env_infos/initial/reward_angular Std         0.953489\n",
      "evaluation/env_infos/initial/reward_angular Max         2.45066\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.35095\n",
      "evaluation/env_infos/reward_angular Mean                0.0649041\n",
      "evaluation/env_infos/reward_angular Std                 1.92587\n",
      "evaluation/env_infos/reward_angular Max                 9.167\n",
      "evaluation/env_infos/reward_angular Min                -7.19421\n",
      "time/data storing (s)                                   0.0187976\n",
      "time/evaluation sampling (s)                           27.1258\n",
      "time/exploration sampling (s)                           1.30415\n",
      "time/logging (s)                                        0.243747\n",
      "time/saving (s)                                         0.0843848\n",
      "time/training (s)                                       4.97124\n",
      "time/epoch (s)                                         33.7481\n",
      "time/total (s)                                       3425.92\n",
      "Epoch                                                 115\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:09:40.723247 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 116 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 118000\n",
      "trainer/QF1 Loss                                        1.47866\n",
      "trainer/QF2 Loss                                        1.59534\n",
      "trainer/Policy Loss                                     1.07741\n",
      "trainer/Q1 Predictions Mean                             5.36539\n",
      "trainer/Q1 Predictions Std                             15.2369\n",
      "trainer/Q1 Predictions Max                             66.3147\n",
      "trainer/Q1 Predictions Min                            -17.3822\n",
      "trainer/Q2 Predictions Mean                             5.33955\n",
      "trainer/Q2 Predictions Std                             15.2246\n",
      "trainer/Q2 Predictions Max                             63.4114\n",
      "trainer/Q2 Predictions Min                            -17.623\n",
      "trainer/Q Targets Mean                                  5.16085\n",
      "trainer/Q Targets Std                                  15.1344\n",
      "trainer/Q Targets Max                                  59.7522\n",
      "trainer/Q Targets Min                                 -18.2463\n",
      "trainer/Log Pis Mean                                    6.66244\n",
      "trainer/Log Pis Std                                     5.5272\n",
      "trainer/Log Pis Max                                    26.1486\n",
      "trainer/Log Pis Min                                    -4.41013\n",
      "trainer/Policy mu Mean                                  0.135027\n",
      "trainer/Policy mu Std                                   1.67048\n",
      "trainer/Policy mu Max                                   5.08785\n",
      "trainer/Policy mu Min                                  -6.94584\n",
      "trainer/Policy log std Mean                            -0.713364\n",
      "trainer/Policy log std Std                              0.281052\n",
      "trainer/Policy log std Max                              0.319458\n",
      "trainer/Policy log std Min                             -1.90095\n",
      "trainer/Alpha                                           0.0162687\n",
      "trainer/Alpha Loss                                      2.72917\n",
      "exploration/num steps total                        118000\n",
      "exploration/num paths total                           118\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.138538\n",
      "exploration/Rewards Std                                 1.10667\n",
      "exploration/Rewards Max                                 5.02591\n",
      "exploration/Rewards Min                                -4.46579\n",
      "exploration/Returns Mean                             -138.538\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -138.538\n",
      "exploration/Returns Min                              -138.538\n",
      "exploration/Actions Mean                               -0.072181\n",
      "exploration/Actions Std                                 0.760151\n",
      "exploration/Actions Max                                 0.999997\n",
      "exploration/Actions Min                                -0.999984\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -138.538\n",
      "exploration/env_infos/final/reward_run Mean            -0.494827\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.494827\n",
      "exploration/env_infos/final/reward_run Min             -0.494827\n",
      "exploration/env_infos/initial/reward_run Mean           0.266084\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.266084\n",
      "exploration/env_infos/initial/reward_run Min            0.266084\n",
      "exploration/env_infos/reward_run Mean                  -0.126103\n",
      "exploration/env_infos/reward_run Std                    0.617464\n",
      "exploration/env_infos/reward_run Max                    2.61896\n",
      "exploration/env_infos/reward_run Min                   -1.77688\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.379028\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.379028\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.379028\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.328407\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.328407\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.328407\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.349823\n",
      "exploration/env_infos/reward_ctrl Std                   0.0825367\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0951069\n",
      "exploration/env_infos/reward_ctrl Min                  -0.592907\n",
      "exploration/env_infos/final/height Mean                -0.0831595\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0831595\n",
      "exploration/env_infos/final/height Min                 -0.0831595\n",
      "exploration/env_infos/initial/height Mean              -0.0779419\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0779419\n",
      "exploration/env_infos/initial/height Min               -0.0779419\n",
      "exploration/env_infos/height Mean                      -0.105526\n",
      "exploration/env_infos/height Std                        0.12034\n",
      "exploration/env_infos/height Max                        0.310624\n",
      "exploration/env_infos/height Min                       -0.578886\n",
      "exploration/env_infos/final/reward_angular Mean        -0.689129\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.689129\n",
      "exploration/env_infos/final/reward_angular Min         -0.689129\n",
      "exploration/env_infos/initial/reward_angular Mean       0.808631\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.808631\n",
      "exploration/env_infos/initial/reward_angular Min        0.808631\n",
      "exploration/env_infos/reward_angular Mean               0.14401\n",
      "exploration/env_infos/reward_angular Std                1.47685\n",
      "exploration/env_infos/reward_angular Max                7.34261\n",
      "exploration/env_infos/reward_angular Min               -5.12559\n",
      "evaluation/num steps total                              2.925e+06\n",
      "evaluation/num paths total                           2925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.451608\n",
      "evaluation/Rewards Std                                  1.19013\n",
      "evaluation/Rewards Max                                  7.07031\n",
      "evaluation/Rewards Min                                 -6.08548\n",
      "evaluation/Returns Mean                               451.608\n",
      "evaluation/Returns Std                                633.579\n",
      "evaluation/Returns Max                               1882\n",
      "evaluation/Returns Min                               -191.415\n",
      "evaluation/Actions Mean                                 0.145336\n",
      "evaluation/Actions Std                                  0.751497\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            451.608\n",
      "evaluation/env_infos/final/reward_run Mean             -0.409684\n",
      "evaluation/env_infos/final/reward_run Std               1.80869\n",
      "evaluation/env_infos/final/reward_run Max               3.71738\n",
      "evaluation/env_infos/final/reward_run Min              -3.53716\n",
      "evaluation/env_infos/initial/reward_run Mean            0.260083\n",
      "evaluation/env_infos/initial/reward_run Std             0.390576\n",
      "evaluation/env_infos/initial/reward_run Max             1.01148\n",
      "evaluation/env_infos/initial/reward_run Min            -0.464757\n",
      "evaluation/env_infos/reward_run Mean                   -0.595403\n",
      "evaluation/env_infos/reward_run Std                     2.04808\n",
      "evaluation/env_infos/reward_run Max                     5.46528\n",
      "evaluation/env_infos/reward_run Min                    -6.3764\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.364261\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109042\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.122668\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.542062\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.280477\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0977995\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.109253\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.483959\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.351522\n",
      "evaluation/env_infos/reward_ctrl Std                    0.102284\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00704724\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.5995\n",
      "evaluation/env_infos/final/height Mean                 -0.131513\n",
      "evaluation/env_infos/final/height Std                   0.166135\n",
      "evaluation/env_infos/final/height Max                   0.0684244\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0106309\n",
      "evaluation/env_infos/initial/height Std                 0.0429325\n",
      "evaluation/env_infos/initial/height Max                 0.0714616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/height Min                -0.0797104\n",
      "evaluation/env_infos/height Mean                       -0.118218\n",
      "evaluation/env_infos/height Std                         0.155367\n",
      "evaluation/env_infos/height Max                         0.491332\n",
      "evaluation/env_infos/height Min                        -0.591687\n",
      "evaluation/env_infos/final/reward_angular Mean          0.313706\n",
      "evaluation/env_infos/final/reward_angular Std           1.00334\n",
      "evaluation/env_infos/final/reward_angular Max           2.71774\n",
      "evaluation/env_infos/final/reward_angular Min          -1.54139\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.372149\n",
      "evaluation/env_infos/initial/reward_angular Std         1.10312\n",
      "evaluation/env_infos/initial/reward_angular Max         2.22915\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.72846\n",
      "evaluation/env_infos/reward_angular Mean                0.0585461\n",
      "evaluation/env_infos/reward_angular Std                 1.86698\n",
      "evaluation/env_infos/reward_angular Max                 9.2272\n",
      "evaluation/env_infos/reward_angular Min                -7.79549\n",
      "time/data storing (s)                                   0.016765\n",
      "time/evaluation sampling (s)                           25.1872\n",
      "time/exploration sampling (s)                           1.1999\n",
      "time/logging (s)                                        0.269342\n",
      "time/saving (s)                                         0.0300614\n",
      "time/training (s)                                       4.44126\n",
      "time/epoch (s)                                         31.1445\n",
      "time/total (s)                                       3458\n",
      "Epoch                                                 116\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 20:10:14.892641 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 117 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 119000\n",
      "trainer/QF1 Loss                                        1.16775\n",
      "trainer/QF2 Loss                                        1.16165\n",
      "trainer/Policy Loss                                     0.64669\n",
      "trainer/Q1 Predictions Mean                             5.81863\n",
      "trainer/Q1 Predictions Std                             15.7988\n",
      "trainer/Q1 Predictions Max                             61.6276\n",
      "trainer/Q1 Predictions Min                            -15.4864\n",
      "trainer/Q2 Predictions Mean                             5.88112\n",
      "trainer/Q2 Predictions Std                             15.8398\n",
      "trainer/Q2 Predictions Max                             61.875\n",
      "trainer/Q2 Predictions Min                            -16.4576\n",
      "trainer/Q Targets Mean                                  5.75771\n",
      "trainer/Q Targets Std                                  15.7486\n",
      "trainer/Q Targets Max                                  62.8126\n",
      "trainer/Q Targets Min                                 -18.6485\n",
      "trainer/Log Pis Mean                                    6.68614\n",
      "trainer/Log Pis Std                                     6.1445\n",
      "trainer/Log Pis Max                                    30.6497\n",
      "trainer/Log Pis Min                                    -4.89997\n",
      "trainer/Policy mu Mean                                  0.262693\n",
      "trainer/Policy mu Std                                   1.69355\n",
      "trainer/Policy mu Max                                   5.11208\n",
      "trainer/Policy mu Min                                  -5.09436\n",
      "trainer/Policy log std Mean                            -0.717107\n",
      "trainer/Policy log std Std                              0.304174\n",
      "trainer/Policy log std Max                              0.456661\n",
      "trainer/Policy log std Min                             -1.8358\n",
      "trainer/Alpha                                           0.0151954\n",
      "trainer/Alpha Loss                                      2.87441\n",
      "exploration/num steps total                        119000\n",
      "exploration/num paths total                           119\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.53833\n",
      "exploration/Rewards Std                                 0.887684\n",
      "exploration/Rewards Max                                 4.76175\n",
      "exploration/Rewards Min                                -0.951723\n",
      "exploration/Returns Mean                             2538.33\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2538.33\n",
      "exploration/Returns Min                              2538.33\n",
      "exploration/Actions Mean                                0.0866164\n",
      "exploration/Actions Std                                 0.890944\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2538.33\n",
      "exploration/env_infos/final/reward_run Mean            -5.19782\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -5.19782\n",
      "exploration/env_infos/final/reward_run Min             -5.19782\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0626476\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0626476\n",
      "exploration/env_infos/initial/reward_run Min           -0.0626476\n",
      "exploration/env_infos/reward_run Mean                  -3.71409\n",
      "exploration/env_infos/reward_run Std                    1.18793\n",
      "exploration/env_infos/reward_run Max                    0.838406\n",
      "exploration/env_infos/reward_run Min                   -6.73812\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.574704\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.574704\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.574704\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.179205\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.179205\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.179205\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.48077\n",
      "exploration/env_infos/reward_ctrl Std                   0.0784933\n",
      "exploration/env_infos/reward_ctrl Max                  -0.179205\n",
      "exploration/env_infos/reward_ctrl Min                  -0.598129\n",
      "exploration/env_infos/final/height Mean                 0.0384709\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0384709\n",
      "exploration/env_infos/final/height Min                  0.0384709\n",
      "exploration/env_infos/initial/height Mean               0.0179221\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0179221\n",
      "exploration/env_infos/initial/height Min                0.0179221\n",
      "exploration/env_infos/height Mean                      -0.0188936\n",
      "exploration/env_infos/height Std                        0.121559\n",
      "exploration/env_infos/height Max                        0.416456\n",
      "exploration/env_infos/height Min                       -0.319631\n",
      "exploration/env_infos/final/reward_angular Mean        -1.08185\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.08185\n",
      "exploration/env_infos/final/reward_angular Min         -1.08185\n",
      "exploration/env_infos/initial/reward_angular Mean       0.707054\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.707054\n",
      "exploration/env_infos/initial/reward_angular Min        0.707054\n",
      "exploration/env_infos/reward_angular Mean               0.1206\n",
      "exploration/env_infos/reward_angular Std                2.73449\n",
      "exploration/env_infos/reward_angular Max                8.97524\n",
      "exploration/env_infos/reward_angular Min               -5.82121\n",
      "evaluation/num steps total                              2.95e+06\n",
      "evaluation/num paths total                           2950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.158744\n",
      "evaluation/Rewards Std                                  1.11724\n",
      "evaluation/Rewards Max                                  6.14363\n",
      "evaluation/Rewards Min                                 -5.92453\n",
      "evaluation/Returns Mean                               158.744\n",
      "evaluation/Returns Std                                542.961\n",
      "evaluation/Returns Max                               1889.51\n",
      "evaluation/Returns Min                               -453.097\n",
      "evaluation/Actions Mean                                 0.112461\n",
      "evaluation/Actions Std                                  0.741382\n",
      "evaluation/Actions Max                                  0.999995\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            158.744\n",
      "evaluation/env_infos/final/reward_run Mean              0.0975691\n",
      "evaluation/env_infos/final/reward_run Std               1.74485\n",
      "evaluation/env_infos/final/reward_run Max               4.2003\n",
      "evaluation/env_infos/final/reward_run Min              -3.47246\n",
      "evaluation/env_infos/initial/reward_run Mean            0.220506\n",
      "evaluation/env_infos/initial/reward_run Std             0.442502\n",
      "evaluation/env_infos/initial/reward_run Max             1.11546\n",
      "evaluation/env_infos/initial/reward_run Min            -0.504155\n",
      "evaluation/env_infos/reward_run Mean                   -0.216745\n",
      "evaluation/env_infos/reward_run Std                     1.8211\n",
      "evaluation/env_infos/reward_run Max                     5.30835\n",
      "evaluation/env_infos/reward_run Min                    -6.37763\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.327822\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.107804\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.15272\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.573497\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.307719\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.123511\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.102828\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.536298\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.337376\n",
      "evaluation/env_infos/reward_ctrl Std                    0.114516\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00800658\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598553\n",
      "evaluation/env_infos/final/height Mean                 -0.235964\n",
      "evaluation/env_infos/final/height Std                   0.210119\n",
      "evaluation/env_infos/final/height Max                   0.128263\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00102391\n",
      "evaluation/env_infos/initial/height Std                 0.0532298\n",
      "evaluation/env_infos/initial/height Max                 0.0737494\n",
      "evaluation/env_infos/initial/height Min                -0.102884\n",
      "evaluation/env_infos/height Mean                       -0.19336\n",
      "evaluation/env_infos/height Std                         0.201592\n",
      "evaluation/env_infos/height Max                         0.395194\n",
      "evaluation/env_infos/height Min                        -0.591983\n",
      "evaluation/env_infos/final/reward_angular Mean          0.406273\n",
      "evaluation/env_infos/final/reward_angular Std           1.31948\n",
      "evaluation/env_infos/final/reward_angular Max           3.82659\n",
      "evaluation/env_infos/final/reward_angular Min          -2.20221\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.486098\n",
      "evaluation/env_infos/initial/reward_angular Std         0.854689\n",
      "evaluation/env_infos/initial/reward_angular Max         1.15735\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.40899\n",
      "evaluation/env_infos/reward_angular Mean                0.0090945\n",
      "evaluation/env_infos/reward_angular Std                 1.59283\n",
      "evaluation/env_infos/reward_angular Max                 9.06091\n",
      "evaluation/env_infos/reward_angular Min                -7.54789\n",
      "time/data storing (s)                                   0.0179027\n",
      "time/evaluation sampling (s)                           24.4605\n",
      "time/exploration sampling (s)                           1.27906\n",
      "time/logging (s)                                        0.252585\n",
      "time/saving (s)                                         0.0302632\n",
      "time/training (s)                                       7.19485\n",
      "time/epoch (s)                                         33.2352\n",
      "time/total (s)                                       3492.15\n",
      "Epoch                                                 117\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:10:45.553321 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 118 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 120000\n",
      "trainer/QF1 Loss                                        1.57561\n",
      "trainer/QF2 Loss                                        1.39324\n",
      "trainer/Policy Loss                                    -0.669424\n",
      "trainer/Q1 Predictions Mean                             6.30945\n",
      "trainer/Q1 Predictions Std                             17.2611\n",
      "trainer/Q1 Predictions Max                             71.6309\n",
      "trainer/Q1 Predictions Min                            -16.2035\n",
      "trainer/Q2 Predictions Mean                             6.37116\n",
      "trainer/Q2 Predictions Std                             17.2365\n",
      "trainer/Q2 Predictions Max                             71.0321\n",
      "trainer/Q2 Predictions Min                            -15.8767\n",
      "trainer/Q Targets Mean                                  6.11368\n",
      "trainer/Q Targets Std                                  17.1688\n",
      "trainer/Q Targets Max                                  69.7327\n",
      "trainer/Q Targets Min                                 -16.307\n",
      "trainer/Log Pis Mean                                    5.89093\n",
      "trainer/Log Pis Std                                     5.72882\n",
      "trainer/Log Pis Max                                    29.6875\n",
      "trainer/Log Pis Min                                    -5.6569\n",
      "trainer/Policy mu Mean                                  0.201454\n",
      "trainer/Policy mu Std                                   1.54333\n",
      "trainer/Policy mu Max                                   4.5013\n",
      "trainer/Policy mu Min                                  -6.75066\n",
      "trainer/Policy log std Mean                            -0.752703\n",
      "trainer/Policy log std Std                              0.252914\n",
      "trainer/Policy log std Max                              0.0473995\n",
      "trainer/Policy log std Min                             -1.90227\n",
      "trainer/Alpha                                           0.016027\n",
      "trainer/Alpha Loss                                     -0.450715\n",
      "exploration/num steps total                        120000\n",
      "exploration/num paths total                           120\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.713452\n",
      "exploration/Rewards Std                                 1.31681\n",
      "exploration/Rewards Max                                 4.4827\n",
      "exploration/Rewards Min                                -2.82957\n",
      "exploration/Returns Mean                              713.452\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               713.452\n",
      "exploration/Returns Min                               713.452\n",
      "exploration/Actions Mean                                0.0596444\n",
      "exploration/Actions Std                                 0.792184\n",
      "exploration/Actions Max                                 0.999985\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           713.452\n",
      "exploration/env_infos/final/reward_run Mean             0.0956801\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0956801\n",
      "exploration/env_infos/final/reward_run Min              0.0956801\n",
      "exploration/env_infos/initial/reward_run Mean           0.144035\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.144035\n",
      "exploration/env_infos/initial/reward_run Min            0.144035\n",
      "exploration/env_infos/reward_run Mean                  -1.50194\n",
      "exploration/env_infos/reward_run Std                    1.50137\n",
      "exploration/env_infos/reward_run Max                    0.744638\n",
      "exploration/env_infos/reward_run Min                   -5.68395\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.288909\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.288909\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.288909\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.323243\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.323243\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.323243\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.378668\n",
      "exploration/env_infos/reward_ctrl Std                   0.0962714\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0816669\n",
      "exploration/env_infos/reward_ctrl Min                  -0.583341\n",
      "exploration/env_infos/final/height Mean                -0.576974\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576974\n",
      "exploration/env_infos/final/height Min                 -0.576974\n",
      "exploration/env_infos/initial/height Mean              -0.108176\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.108176\n",
      "exploration/env_infos/initial/height Min               -0.108176\n",
      "exploration/env_infos/height Mean                      -0.292065\n",
      "exploration/env_infos/height Std                        0.298859\n",
      "exploration/env_infos/height Max                        0.337896\n",
      "exploration/env_infos/height Min                       -0.58402\n",
      "exploration/env_infos/final/reward_angular Mean         0.0196448\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0196448\n",
      "exploration/env_infos/final/reward_angular Min          0.0196448\n",
      "exploration/env_infos/initial/reward_angular Mean       0.760062\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.760062\n",
      "exploration/env_infos/initial/reward_angular Min        0.760062\n",
      "exploration/env_infos/reward_angular Mean               0.00353197\n",
      "exploration/env_infos/reward_angular Std                2.14655\n",
      "exploration/env_infos/reward_angular Max                7.17719\n",
      "exploration/env_infos/reward_angular Min               -8.36518\n",
      "evaluation/num steps total                              2.975e+06\n",
      "evaluation/num paths total                           2975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.357301\n",
      "evaluation/Rewards Std                                  1.1134\n",
      "evaluation/Rewards Max                                  7.52159\n",
      "evaluation/Rewards Min                                 -6.29575\n",
      "evaluation/Returns Mean                               357.301\n",
      "evaluation/Returns Std                                577.379\n",
      "evaluation/Returns Max                               1665.91\n",
      "evaluation/Returns Min                               -322.069\n",
      "evaluation/Actions Mean                                 0.095338\n",
      "evaluation/Actions Std                                  0.775446\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            357.301\n",
      "evaluation/env_infos/final/reward_run Mean             -0.396857\n",
      "evaluation/env_infos/final/reward_run Std               1.68686\n",
      "evaluation/env_infos/final/reward_run Max               2.57079\n",
      "evaluation/env_infos/final/reward_run Min              -3.67886\n",
      "evaluation/env_infos/initial/reward_run Mean            0.389081\n",
      "evaluation/env_infos/initial/reward_run Std             0.369329\n",
      "evaluation/env_infos/initial/reward_run Max             1.00398\n",
      "evaluation/env_infos/initial/reward_run Min            -0.24589\n",
      "evaluation/env_infos/reward_run Mean                   -0.450011\n",
      "evaluation/env_infos/reward_run Std                     1.83032\n",
      "evaluation/env_infos/reward_run Max                     4.73327\n",
      "evaluation/env_infos/reward_run Min                    -5.90526\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.359725\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.114843\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.108309\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.548197\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.30255\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.119886\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.129181\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.525563\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.366244\n",
      "evaluation/env_infos/reward_ctrl Std                    0.10001\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.021455\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598682\n",
      "evaluation/env_infos/final/height Mean                 -0.153697\n",
      "evaluation/env_infos/final/height Std                   0.219988\n",
      "evaluation/env_infos/final/height Max                   0.293723\n",
      "evaluation/env_infos/final/height Min                  -0.57728\n",
      "evaluation/env_infos/initial/height Mean               -0.0257149\n",
      "evaluation/env_infos/initial/height Std                 0.0523584\n",
      "evaluation/env_infos/initial/height Max                 0.0610498\n",
      "evaluation/env_infos/initial/height Min                -0.102371\n",
      "evaluation/env_infos/height Mean                       -0.140966\n",
      "evaluation/env_infos/height Std                         0.189112\n",
      "evaluation/env_infos/height Max                         0.544587\n",
      "evaluation/env_infos/height Min                        -0.593788\n",
      "evaluation/env_infos/final/reward_angular Mean          0.384102\n",
      "evaluation/env_infos/final/reward_angular Std           1.50401\n",
      "evaluation/env_infos/final/reward_angular Max           3.58366\n",
      "evaluation/env_infos/final/reward_angular Min          -2.39606\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0404116\n",
      "evaluation/env_infos/initial/reward_angular Std         1.14349\n",
      "evaluation/env_infos/initial/reward_angular Max         2.9077\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.46157\n",
      "evaluation/env_infos/reward_angular Mean                0.0641243\n",
      "evaluation/env_infos/reward_angular Std                 1.85172\n",
      "evaluation/env_infos/reward_angular Max                 8.9281\n",
      "evaluation/env_infos/reward_angular Min                -7.22425\n",
      "time/data storing (s)                                   0.01734\n",
      "time/evaluation sampling (s)                           22.9594\n",
      "time/exploration sampling (s)                           1.3993\n",
      "time/logging (s)                                        0.276064\n",
      "time/saving (s)                                         0.0374661\n",
      "time/training (s)                                       5.03323\n",
      "time/epoch (s)                                         29.7228\n",
      "time/total (s)                                       3522.83\n",
      "Epoch                                                 118\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:11:18.169610 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 119 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 121000\n",
      "trainer/QF1 Loss                                        1.27144\n",
      "trainer/QF2 Loss                                        1.24567\n",
      "trainer/Policy Loss                                    -1.08504\n",
      "trainer/Q1 Predictions Mean                             6.30874\n",
      "trainer/Q1 Predictions Std                             16.0062\n",
      "trainer/Q1 Predictions Max                             65.2043\n",
      "trainer/Q1 Predictions Min                            -15.4432\n",
      "trainer/Q2 Predictions Mean                             6.1906\n",
      "trainer/Q2 Predictions Std                             15.9966\n",
      "trainer/Q2 Predictions Max                             66.8257\n",
      "trainer/Q2 Predictions Min                            -15.0912\n",
      "trainer/Q Targets Mean                                  6.15964\n",
      "trainer/Q Targets Std                                  16.0251\n",
      "trainer/Q Targets Max                                  66.7643\n",
      "trainer/Q Targets Min                                 -17.1352\n",
      "trainer/Log Pis Mean                                    5.33971\n",
      "trainer/Log Pis Std                                     5.6616\n",
      "trainer/Log Pis Max                                    25.1661\n",
      "trainer/Log Pis Min                                    -5.35149\n",
      "trainer/Policy mu Mean                                  0.0252627\n",
      "trainer/Policy mu Std                                   1.52416\n",
      "trainer/Policy mu Max                                   4.07271\n",
      "trainer/Policy mu Min                                  -6.28139\n",
      "trainer/Policy log std Mean                            -0.780564\n",
      "trainer/Policy log std Std                              0.27405\n",
      "trainer/Policy log std Max                              0.146498\n",
      "trainer/Policy log std Min                             -1.94285\n",
      "trainer/Alpha                                           0.0162635\n",
      "trainer/Alpha Loss                                     -2.71866\n",
      "exploration/num steps total                        121000\n",
      "exploration/num paths total                           121\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938046\n",
      "exploration/Rewards Std                                 0.513179\n",
      "exploration/Rewards Max                                 2.391\n",
      "exploration/Rewards Min                                -1.37453\n",
      "exploration/Returns Mean                              938.046\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.046\n",
      "exploration/Returns Min                               938.046\n",
      "exploration/Actions Mean                                0.211346\n",
      "exploration/Actions Std                                 0.746662\n",
      "exploration/Actions Max                                 0.999896\n",
      "exploration/Actions Min                                -0.99969\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.046\n",
      "exploration/env_infos/final/reward_run Mean             0.979471\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.979471\n",
      "exploration/env_infos/final/reward_run Min              0.979471\n",
      "exploration/env_infos/initial/reward_run Mean           0.316186\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.316186\n",
      "exploration/env_infos/initial/reward_run Min            0.316186\n",
      "exploration/env_infos/reward_run Mean                   1.44228\n",
      "exploration/env_infos/reward_run Std                    0.672746\n",
      "exploration/env_infos/reward_run Max                    3.25502\n",
      "exploration/env_infos/reward_run Min                   -1.07624\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.399653\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.399653\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.399653\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.394111\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.394111\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.394111\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.361303\n",
      "exploration/env_infos/reward_ctrl Std                   0.0894408\n",
      "exploration/env_infos/reward_ctrl Max                  -0.084458\n",
      "exploration/env_infos/reward_ctrl Min                  -0.572455\n",
      "exploration/env_infos/final/height Mean                -0.0911958\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0911958\n",
      "exploration/env_infos/final/height Min                 -0.0911958\n",
      "exploration/env_infos/initial/height Mean               0.0607507\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0607507\n",
      "exploration/env_infos/initial/height Min                0.0607507\n",
      "exploration/env_infos/height Mean                      -0.227346\n",
      "exploration/env_infos/height Std                        0.0723027\n",
      "exploration/env_infos/height Max                        0.0607507\n",
      "exploration/env_infos/height Min                       -0.409698\n",
      "exploration/env_infos/final/reward_angular Mean         0.891207\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.891207\n",
      "exploration/env_infos/final/reward_angular Min          0.891207\n",
      "exploration/env_infos/initial/reward_angular Mean       0.173535\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.173535\n",
      "exploration/env_infos/initial/reward_angular Min        0.173535\n",
      "exploration/env_infos/reward_angular Mean              -0.0673047\n",
      "exploration/env_infos/reward_angular Std                2.05766\n",
      "exploration/env_infos/reward_angular Max                5.01531\n",
      "exploration/env_infos/reward_angular Min               -4.66961\n",
      "evaluation/num steps total                              3e+06\n",
      "evaluation/num paths total                           3000\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.370455\n",
      "evaluation/Rewards Std                                  1.32113\n",
      "evaluation/Rewards Max                                  7.20558\n",
      "evaluation/Rewards Min                                 -6.67895\n",
      "evaluation/Returns Mean                               370.455\n",
      "evaluation/Returns Std                                658.798\n",
      "evaluation/Returns Max                               1744.97\n",
      "evaluation/Returns Min                               -614.772\n",
      "evaluation/Actions Mean                                 0.0443053\n",
      "evaluation/Actions Std                                  0.75427\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            370.455\n",
      "evaluation/env_infos/final/reward_run Mean             -0.819702\n",
      "evaluation/env_infos/final/reward_run Std               2.14261\n",
      "evaluation/env_infos/final/reward_run Max               2.73221\n",
      "evaluation/env_infos/final/reward_run Min              -5.31239\n",
      "evaluation/env_infos/initial/reward_run Mean            0.380572\n",
      "evaluation/env_infos/initial/reward_run Std             0.348684\n",
      "evaluation/env_infos/initial/reward_run Max             1.05014\n",
      "evaluation/env_infos/initial/reward_run Min            -0.413364\n",
      "evaluation/env_infos/reward_run Mean                   -0.90489\n",
      "evaluation/env_infos/reward_run Std                     2.08551\n",
      "evaluation/env_infos/reward_run Max                     4.7377\n",
      "evaluation/env_infos/reward_run Min                    -6.81289\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.373748\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0946312\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.128302\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.527215\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.264949\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.118416\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0546932\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.477519\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.342532\n",
      "evaluation/env_infos/reward_ctrl Std                    0.1169\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00799593\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599577\n",
      "evaluation/env_infos/final/height Mean                 -0.0749013\n",
      "evaluation/env_infos/final/height Std                   0.175581\n",
      "evaluation/env_infos/final/height Max                   0.293194\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0105818\n",
      "evaluation/env_infos/initial/height Std                 0.0512019\n",
      "evaluation/env_infos/initial/height Max                 0.07563\n",
      "evaluation/env_infos/initial/height Min                -0.099565\n",
      "evaluation/env_infos/height Mean                       -0.102359\n",
      "evaluation/env_infos/height Std                         0.143297\n",
      "evaluation/env_infos/height Max                         0.483413\n",
      "evaluation/env_infos/height Min                        -0.577317\n",
      "evaluation/env_infos/final/reward_angular Mean          0.212655\n",
      "evaluation/env_infos/final/reward_angular Std           1.6517\n",
      "evaluation/env_infos/final/reward_angular Max           4.2669\n",
      "evaluation/env_infos/final/reward_angular Min          -2.91839\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.305608\n",
      "evaluation/env_infos/initial/reward_angular Std         0.820844\n",
      "evaluation/env_infos/initial/reward_angular Max         1.54005\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.79636\n",
      "evaluation/env_infos/reward_angular Mean                0.0424045\n",
      "evaluation/env_infos/reward_angular Std                 2.01871\n",
      "evaluation/env_infos/reward_angular Max                 9.09767\n",
      "evaluation/env_infos/reward_angular Min                -7.05183\n",
      "time/data storing (s)                                   0.0167331\n",
      "time/evaluation sampling (s)                           25.5603\n",
      "time/exploration sampling (s)                           1.12123\n",
      "time/logging (s)                                        0.2528\n",
      "time/saving (s)                                         0.0301929\n",
      "time/training (s)                                       4.59084\n",
      "time/epoch (s)                                         31.5721\n",
      "time/total (s)                                       3555.42\n",
      "Epoch                                                 119\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:11:48.530034 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 120 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 122000\n",
      "trainer/QF1 Loss                                        1.45164\n",
      "trainer/QF2 Loss                                        1.54299\n",
      "trainer/Policy Loss                                    -0.0304847\n",
      "trainer/Q1 Predictions Mean                             6.08182\n",
      "trainer/Q1 Predictions Std                             15.7042\n",
      "trainer/Q1 Predictions Max                             77.2441\n",
      "trainer/Q1 Predictions Min                            -14.9969\n",
      "trainer/Q2 Predictions Mean                             6.04332\n",
      "trainer/Q2 Predictions Std                             15.6644\n",
      "trainer/Q2 Predictions Max                             77.0869\n",
      "trainer/Q2 Predictions Min                            -13.9991\n",
      "trainer/Q Targets Mean                                  5.83434\n",
      "trainer/Q Targets Std                                  15.7143\n",
      "trainer/Q Targets Max                                  76.4102\n",
      "trainer/Q Targets Min                                 -14.0905\n",
      "trainer/Log Pis Mean                                    6.30774\n",
      "trainer/Log Pis Std                                     5.81199\n",
      "trainer/Log Pis Max                                    31.1753\n",
      "trainer/Log Pis Min                                    -6.2775\n",
      "trainer/Policy mu Mean                                  0.119682\n",
      "trainer/Policy mu Std                                   1.6316\n",
      "trainer/Policy mu Max                                   4.50425\n",
      "trainer/Policy mu Min                                  -7.05861\n",
      "trainer/Policy log std Mean                            -0.706102\n",
      "trainer/Policy log std Std                              0.272925\n",
      "trainer/Policy log std Max                              0.427676\n",
      "trainer/Policy log std Min                             -1.85489\n",
      "trainer/Alpha                                           0.0154473\n",
      "trainer/Alpha Loss                                      1.28358\n",
      "exploration/num steps total                        122000\n",
      "exploration/num paths total                           122\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0183868\n",
      "exploration/Rewards Std                                 0.895127\n",
      "exploration/Rewards Max                                 3.10615\n",
      "exploration/Rewards Min                                -2.9628\n",
      "exploration/Returns Mean                              -18.3868\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -18.3868\n",
      "exploration/Returns Min                               -18.3868\n",
      "exploration/Actions Mean                                0.118339\n",
      "exploration/Actions Std                                 0.686376\n",
      "exploration/Actions Max                                 0.999922\n",
      "exploration/Actions Min                                -0.99982\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -18.3868\n",
      "exploration/env_infos/final/reward_run Mean             0.209415\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.209415\n",
      "exploration/env_infos/final/reward_run Min              0.209415\n",
      "exploration/env_infos/initial/reward_run Mean           0.216795\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.216795\n",
      "exploration/env_infos/initial/reward_run Min            0.216795\n",
      "exploration/env_infos/reward_run Mean                   0.189581\n",
      "exploration/env_infos/reward_run Std                    0.2518\n",
      "exploration/env_infos/reward_run Max                    1.10842\n",
      "exploration/env_infos/reward_run Min                   -0.684084\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.240815\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.240815\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.240815\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.439657\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.439657\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.439657\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.29107\n",
      "exploration/env_infos/reward_ctrl Std                   0.0606682\n",
      "exploration/env_infos/reward_ctrl Max                  -0.110618\n",
      "exploration/env_infos/reward_ctrl Min                  -0.513492\n",
      "exploration/env_infos/final/height Mean                -0.138986\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.138986\n",
      "exploration/env_infos/final/height Min                 -0.138986\n",
      "exploration/env_infos/initial/height Mean              -0.112491\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.112491\n",
      "exploration/env_infos/initial/height Min               -0.112491\n",
      "exploration/env_infos/height Mean                      -0.19694\n",
      "exploration/env_infos/height Std                        0.0596258\n",
      "exploration/env_infos/height Max                        0.0141668\n",
      "exploration/env_infos/height Min                       -0.416175\n",
      "exploration/env_infos/final/reward_angular Mean        -0.209864\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.209864\n",
      "exploration/env_infos/final/reward_angular Min         -0.209864\n",
      "exploration/env_infos/initial/reward_angular Mean      -2.7336\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -2.7336\n",
      "exploration/env_infos/initial/reward_angular Min       -2.7336\n",
      "exploration/env_infos/reward_angular Mean               0.0192524\n",
      "exploration/env_infos/reward_angular Std                0.953552\n",
      "exploration/env_infos/reward_angular Max                3.4522\n",
      "exploration/env_infos/reward_angular Min               -3.01882\n",
      "evaluation/num steps total                              3.025e+06\n",
      "evaluation/num paths total                           3025\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.394143\n",
      "evaluation/Rewards Std                                  1.16821\n",
      "evaluation/Rewards Max                                  7.30213\n",
      "evaluation/Rewards Min                                 -5.58516\n",
      "evaluation/Returns Mean                               394.143\n",
      "evaluation/Returns Std                                553.073\n",
      "evaluation/Returns Max                               1728.45\n",
      "evaluation/Returns Min                               -373.547\n",
      "evaluation/Actions Mean                                 0.068002\n",
      "evaluation/Actions Std                                  0.76872\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            394.143\n",
      "evaluation/env_infos/final/reward_run Mean             -0.478041\n",
      "evaluation/env_infos/final/reward_run Std               2.01114\n",
      "evaluation/env_infos/final/reward_run Max               2.97535\n",
      "evaluation/env_infos/final/reward_run Min              -4.23125\n",
      "evaluation/env_infos/initial/reward_run Mean            0.19155\n",
      "evaluation/env_infos/initial/reward_run Std             0.382118\n",
      "evaluation/env_infos/initial/reward_run Max             0.906356\n",
      "evaluation/env_infos/initial/reward_run Min            -0.537475\n",
      "evaluation/env_infos/reward_run Mean                   -0.561356\n",
      "evaluation/env_infos/reward_run Std                     2.10089\n",
      "evaluation/env_infos/reward_run Max                     5.31804\n",
      "evaluation/env_infos/reward_run Min                    -6.52018\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.352781\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0952523\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.167845\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.522344\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.276079\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.111337\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0487086\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.494766\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.357332\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0980144\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0267509\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599885\n",
      "evaluation/env_infos/final/height Mean                 -0.186663\n",
      "evaluation/env_infos/final/height Std                   0.188707\n",
      "evaluation/env_infos/final/height Max                   0.271519\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0236816\n",
      "evaluation/env_infos/initial/height Std                 0.0513274\n",
      "evaluation/env_infos/initial/height Max                 0.0829498\n",
      "evaluation/env_infos/initial/height Min                -0.102985\n",
      "evaluation/env_infos/height Mean                       -0.141108\n",
      "evaluation/env_infos/height Std                         0.171284\n",
      "evaluation/env_infos/height Max                         0.45661\n",
      "evaluation/env_infos/height Min                        -0.591898\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0426218\n",
      "evaluation/env_infos/final/reward_angular Std           1.87061\n",
      "evaluation/env_infos/final/reward_angular Max           4.55965\n",
      "evaluation/env_infos/final/reward_angular Min          -3.56631\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.495247\n",
      "evaluation/env_infos/initial/reward_angular Std         1.28755\n",
      "evaluation/env_infos/initial/reward_angular Max         3.50056\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.04714\n",
      "evaluation/env_infos/reward_angular Mean                0.0481809\n",
      "evaluation/env_infos/reward_angular Std                 1.91853\n",
      "evaluation/env_infos/reward_angular Max                 9.14114\n",
      "evaluation/env_infos/reward_angular Min                -6.31185\n",
      "time/data storing (s)                                   0.0156531\n",
      "time/evaluation sampling (s)                           23.9491\n",
      "time/exploration sampling (s)                           1.05562\n",
      "time/logging (s)                                        0.238595\n",
      "time/saving (s)                                         0.0273753\n",
      "time/training (s)                                       3.99159\n",
      "time/epoch (s)                                         29.2779\n",
      "time/total (s)                                       3585.76\n",
      "Epoch                                                 120\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:12:18.346761 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 121 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 123000\n",
      "trainer/QF1 Loss                                        2.28823\n",
      "trainer/QF2 Loss                                        1.43682\n",
      "trainer/Policy Loss                                    -0.613443\n",
      "trainer/Q1 Predictions Mean                             6.92016\n",
      "trainer/Q1 Predictions Std                             17.3401\n",
      "trainer/Q1 Predictions Max                             74.9775\n",
      "trainer/Q1 Predictions Min                            -15.848\n",
      "trainer/Q2 Predictions Mean                             7.06031\n",
      "trainer/Q2 Predictions Std                             17.5742\n",
      "trainer/Q2 Predictions Max                             74.953\n",
      "trainer/Q2 Predictions Min                            -15.2062\n",
      "trainer/Q Targets Mean                                  7.25941\n",
      "trainer/Q Targets Std                                  17.825\n",
      "trainer/Q Targets Max                                  75.8984\n",
      "trainer/Q Targets Min                                 -15.6761\n",
      "trainer/Log Pis Mean                                    6.62091\n",
      "trainer/Log Pis Std                                     6.02656\n",
      "trainer/Log Pis Max                                    32.072\n",
      "trainer/Log Pis Min                                    -4.39394\n",
      "trainer/Policy mu Mean                                 -0.0675882\n",
      "trainer/Policy mu Std                                   1.69567\n",
      "trainer/Policy mu Max                                   6.61638\n",
      "trainer/Policy mu Min                                  -5.45112\n",
      "trainer/Policy log std Mean                            -0.720942\n",
      "trainer/Policy log std Std                              0.276489\n",
      "trainer/Policy log std Max                              0.48851\n",
      "trainer/Policy log std Min                             -1.70347\n",
      "trainer/Alpha                                           0.0171958\n",
      "trainer/Alpha Loss                                      2.52348\n",
      "exploration/num steps total                        123000\n",
      "exploration/num paths total                           123\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.35582\n",
      "exploration/Rewards Std                                 0.827561\n",
      "exploration/Rewards Max                                 4.62342\n",
      "exploration/Rewards Min                                -0.42784\n",
      "exploration/Returns Mean                             2355.82\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2355.82\n",
      "exploration/Returns Min                              2355.82\n",
      "exploration/Actions Mean                                0.112331\n",
      "exploration/Actions Std                                 0.875836\n",
      "exploration/Actions Max                                 0.999974\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2355.82\n",
      "exploration/env_infos/final/reward_run Mean            -3.25085\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.25085\n",
      "exploration/env_infos/final/reward_run Min             -3.25085\n",
      "exploration/env_infos/initial/reward_run Mean           0.352519\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.352519\n",
      "exploration/env_infos/initial/reward_run Min            0.352519\n",
      "exploration/env_infos/reward_run Mean                  -3.43764\n",
      "exploration/env_infos/reward_run Std                    1.12034\n",
      "exploration/env_infos/reward_run Max                    0.352519\n",
      "exploration/env_infos/reward_run Min                   -6.59931\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.445658\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.445658\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.445658\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.299104\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.299104\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.299104\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.467824\n",
      "exploration/env_infos/reward_ctrl Std                   0.0711716\n",
      "exploration/env_infos/reward_ctrl Max                  -0.181989\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593697\n",
      "exploration/env_infos/final/height Mean                -0.026564\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.026564\n",
      "exploration/env_infos/final/height Min                 -0.026564\n",
      "exploration/env_infos/initial/height Mean              -0.0432052\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0432052\n",
      "exploration/env_infos/initial/height Min               -0.0432052\n",
      "exploration/env_infos/height Mean                      -0.0152112\n",
      "exploration/env_infos/height Std                        0.122443\n",
      "exploration/env_infos/height Max                        0.368299\n",
      "exploration/env_infos/height Min                       -0.317765\n",
      "exploration/env_infos/final/reward_angular Mean         5.56842\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          5.56842\n",
      "exploration/env_infos/final/reward_angular Min          5.56842\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.349691\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.349691\n",
      "exploration/env_infos/initial/reward_angular Min       -0.349691\n",
      "exploration/env_infos/reward_angular Mean               0.13733\n",
      "exploration/env_infos/reward_angular Std                2.94704\n",
      "exploration/env_infos/reward_angular Max                9.62575\n",
      "exploration/env_infos/reward_angular Min               -6.20263\n",
      "evaluation/num steps total                              3.05e+06\n",
      "evaluation/num paths total                           3050\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.501512\n",
      "evaluation/Rewards Std                                  1.28356\n",
      "evaluation/Rewards Max                                  5.92542\n",
      "evaluation/Rewards Min                                 -7.71928\n",
      "evaluation/Returns Mean                               501.512\n",
      "evaluation/Returns Std                                761.33\n",
      "evaluation/Returns Max                               2121.79\n",
      "evaluation/Returns Min                               -671.063\n",
      "evaluation/Actions Mean                                 0.0454116\n",
      "evaluation/Actions Std                                  0.761433\n",
      "evaluation/Actions Max                                  0.999946\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            501.512\n",
      "evaluation/env_infos/final/reward_run Mean             -1.21528\n",
      "evaluation/env_infos/final/reward_run Std               2.42391\n",
      "evaluation/env_infos/final/reward_run Max               2.70337\n",
      "evaluation/env_infos/final/reward_run Min              -5.13122\n",
      "evaluation/env_infos/initial/reward_run Mean            0.296391\n",
      "evaluation/env_infos/initial/reward_run Std             0.473662\n",
      "evaluation/env_infos/initial/reward_run Max             1.2046\n",
      "evaluation/env_infos/initial/reward_run Min            -0.594772\n",
      "evaluation/env_infos/reward_run Mean                   -1.19623\n",
      "evaluation/env_infos/reward_run Std                     2.24737\n",
      "evaluation/env_infos/reward_run Max                     4.9366\n",
      "evaluation/env_infos/reward_run Min                    -6.54721\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.363302\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.110354\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.169216\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.564511\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.272803\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.086488\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0810571\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.403389\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.349105\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112649\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0121186\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.591102\n",
      "evaluation/env_infos/final/height Mean                 -0.07576\n",
      "evaluation/env_infos/final/height Std                   0.107982\n",
      "evaluation/env_infos/final/height Max                   0.0820935\n",
      "evaluation/env_infos/final/height Min                  -0.274323\n",
      "evaluation/env_infos/initial/height Mean               -0.0212769\n",
      "evaluation/env_infos/initial/height Std                 0.0615436\n",
      "evaluation/env_infos/initial/height Max                 0.0726917\n",
      "evaluation/env_infos/initial/height Min                -0.141111\n",
      "evaluation/env_infos/height Mean                       -0.0497693\n",
      "evaluation/env_infos/height Std                         0.112392\n",
      "evaluation/env_infos/height Max                         0.520951\n",
      "evaluation/env_infos/height Min                        -0.592785\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0762415\n",
      "evaluation/env_infos/final/reward_angular Std           2.33714\n",
      "evaluation/env_infos/final/reward_angular Max           6.05135\n",
      "evaluation/env_infos/final/reward_angular Min          -4.65872\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.28593\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11704\n",
      "evaluation/env_infos/initial/reward_angular Max         3.02639\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.86741\n",
      "evaluation/env_infos/reward_angular Mean                0.0600504\n",
      "evaluation/env_infos/reward_angular Std                 1.97992\n",
      "evaluation/env_infos/reward_angular Max                 9.62704\n",
      "evaluation/env_infos/reward_angular Min                -6.65139\n",
      "time/data storing (s)                                   0.0156642\n",
      "time/evaluation sampling (s)                           23.4702\n",
      "time/exploration sampling (s)                           1.00256\n",
      "time/logging (s)                                        0.255485\n",
      "time/saving (s)                                         0.0297828\n",
      "time/training (s)                                       4.14684\n",
      "time/epoch (s)                                         28.9205\n",
      "time/total (s)                                       3615.59\n",
      "Epoch                                                 121\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:12:50.161987 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 122 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 124000\n",
      "trainer/QF1 Loss                                        1.18697\n",
      "trainer/QF2 Loss                                        1.08598\n",
      "trainer/Policy Loss                                    -0.66915\n",
      "trainer/Q1 Predictions Mean                             6.91249\n",
      "trainer/Q1 Predictions Std                             18.7514\n",
      "trainer/Q1 Predictions Max                             73.9422\n",
      "trainer/Q1 Predictions Min                            -15.9404\n",
      "trainer/Q2 Predictions Mean                             6.76106\n",
      "trainer/Q2 Predictions Std                             18.4613\n",
      "trainer/Q2 Predictions Max                             74.558\n",
      "trainer/Q2 Predictions Min                            -16.8131\n",
      "trainer/Q Targets Mean                                  6.76435\n",
      "trainer/Q Targets Std                                  18.6631\n",
      "trainer/Q Targets Max                                  75.9432\n",
      "trainer/Q Targets Min                                 -16.2279\n",
      "trainer/Log Pis Mean                                    6.4422\n",
      "trainer/Log Pis Std                                     5.84971\n",
      "trainer/Log Pis Max                                    25.1712\n",
      "trainer/Log Pis Min                                    -7.36406\n",
      "trainer/Policy mu Mean                                  0.0731908\n",
      "trainer/Policy mu Std                                   1.66327\n",
      "trainer/Policy mu Max                                   4.66002\n",
      "trainer/Policy mu Min                                  -5.33772\n",
      "trainer/Policy log std Mean                            -0.720262\n",
      "trainer/Policy log std Std                              0.267155\n",
      "trainer/Policy log std Max                              0.518917\n",
      "trainer/Policy log std Min                             -1.82001\n",
      "trainer/Alpha                                           0.0179541\n",
      "trainer/Alpha Loss                                      1.77858\n",
      "exploration/num steps total                        124000\n",
      "exploration/num paths total                           124\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.67121\n",
      "exploration/Rewards Std                                 1.47591\n",
      "exploration/Rewards Max                                 5.11481\n",
      "exploration/Rewards Min                                -1.44025\n",
      "exploration/Returns Mean                             1671.21\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1671.21\n",
      "exploration/Returns Min                              1671.21\n",
      "exploration/Actions Mean                               -0.069113\n",
      "exploration/Actions Std                                 0.884321\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1671.21\n",
      "exploration/env_infos/final/reward_run Mean             0.110832\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.110832\n",
      "exploration/env_infos/final/reward_run Min              0.110832\n",
      "exploration/env_infos/initial/reward_run Mean           0.125771\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.125771\n",
      "exploration/env_infos/initial/reward_run Min            0.125771\n",
      "exploration/env_infos/reward_run Mean                  -2.40853\n",
      "exploration/env_infos/reward_run Std                    1.69584\n",
      "exploration/env_infos/reward_run Max                    0.946141\n",
      "exploration/env_infos/reward_run Min                   -6.19296\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.424308\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.424308\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.424308\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.329821\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.329821\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.329821\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.47208\n",
      "exploration/env_infos/reward_ctrl Std                   0.0800881\n",
      "exploration/env_infos/reward_ctrl Max                  -0.142752\n",
      "exploration/env_infos/reward_ctrl Min                  -0.597215\n",
      "exploration/env_infos/final/height Mean                -0.57704\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.57704\n",
      "exploration/env_infos/final/height Min                 -0.57704\n",
      "exploration/env_infos/initial/height Mean              -0.087064\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.087064\n",
      "exploration/env_infos/initial/height Min               -0.087064\n",
      "exploration/env_infos/height Mean                      -0.122897\n",
      "exploration/env_infos/height Std                        0.261221\n",
      "exploration/env_infos/height Max                        0.43732\n",
      "exploration/env_infos/height Min                       -0.579228\n",
      "exploration/env_infos/final/reward_angular Mean        -0.00148183\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.00148183\n",
      "exploration/env_infos/final/reward_angular Min         -0.00148183\n",
      "exploration/env_infos/initial/reward_angular Mean       2.25849\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.25849\n",
      "exploration/env_infos/initial/reward_angular Min        2.25849\n",
      "exploration/env_infos/reward_angular Mean               0.0523404\n",
      "exploration/env_infos/reward_angular Std                3.03484\n",
      "exploration/env_infos/reward_angular Max                8.30903\n",
      "exploration/env_infos/reward_angular Min               -7.31414\n",
      "evaluation/num steps total                              3.075e+06\n",
      "evaluation/num paths total                           3075\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.449044\n",
      "evaluation/Rewards Std                                  1.39533\n",
      "evaluation/Rewards Max                                  8.57\n",
      "evaluation/Rewards Min                                 -8.46352\n",
      "evaluation/Returns Mean                               449.044\n",
      "evaluation/Returns Std                                715.041\n",
      "evaluation/Returns Max                               2326.86\n",
      "evaluation/Returns Min                               -440.044\n",
      "evaluation/Actions Mean                                -0.0340844\n",
      "evaluation/Actions Std                                  0.782583\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            449.044\n",
      "evaluation/env_infos/final/reward_run Mean             -0.605655\n",
      "evaluation/env_infos/final/reward_run Std               2.44089\n",
      "evaluation/env_infos/final/reward_run Max               3.66969\n",
      "evaluation/env_infos/final/reward_run Min              -5.50684\n",
      "evaluation/env_infos/initial/reward_run Mean            0.338652\n",
      "evaluation/env_infos/initial/reward_run Std             0.409748\n",
      "evaluation/env_infos/initial/reward_run Max             1.06524\n",
      "evaluation/env_infos/initial/reward_run Min            -0.401524\n",
      "evaluation/env_infos/reward_run Mean                   -0.692736\n",
      "evaluation/env_infos/reward_run Std                     2.39387\n",
      "evaluation/env_infos/reward_run Max                     5.47886\n",
      "evaluation/env_infos/reward_run Min                    -6.97451\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.368996\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101334\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.179524\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.558049\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.292142\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0798425\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0851226\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.384427\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.368159\n",
      "evaluation/env_infos/reward_ctrl Std                    0.110251\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0253902\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598965\n",
      "evaluation/env_infos/final/height Mean                 -0.208785\n",
      "evaluation/env_infos/final/height Std                   0.204051\n",
      "evaluation/env_infos/final/height Max                   0.0206922\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00550855\n",
      "evaluation/env_infos/initial/height Std                 0.0507708\n",
      "evaluation/env_infos/initial/height Max                 0.0762213\n",
      "evaluation/env_infos/initial/height Min                -0.0908208\n",
      "evaluation/env_infos/height Mean                       -0.152928\n",
      "evaluation/env_infos/height Std                         0.191388\n",
      "evaluation/env_infos/height Max                         0.461757\n",
      "evaluation/env_infos/height Min                        -0.590283\n",
      "evaluation/env_infos/final/reward_angular Mean          0.450364\n",
      "evaluation/env_infos/final/reward_angular Std           1.694\n",
      "evaluation/env_infos/final/reward_angular Max           4.60149\n",
      "evaluation/env_infos/final/reward_angular Min          -2.42105\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.635941\n",
      "evaluation/env_infos/initial/reward_angular Std         0.956661\n",
      "evaluation/env_infos/initial/reward_angular Max         1.53693\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.82594\n",
      "evaluation/env_infos/reward_angular Mean                0.0510629\n",
      "evaluation/env_infos/reward_angular Std                 2.04454\n",
      "evaluation/env_infos/reward_angular Max                10.7124\n",
      "evaluation/env_infos/reward_angular Min                -7.51016\n",
      "time/data storing (s)                                   0.0175524\n",
      "time/evaluation sampling (s)                           24.5953\n",
      "time/exploration sampling (s)                           1.24443\n",
      "time/logging (s)                                        0.250237\n",
      "time/saving (s)                                         0.0292021\n",
      "time/training (s)                                       4.51066\n",
      "time/epoch (s)                                         30.6474\n",
      "time/total (s)                                       3647.4\n",
      "Epoch                                                 122\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:13:22.483176 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 123 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 125000\n",
      "trainer/QF1 Loss                                        1.54886\n",
      "trainer/QF2 Loss                                        1.77929\n",
      "trainer/Policy Loss                                     0.908356\n",
      "trainer/Q1 Predictions Mean                             5.03268\n",
      "trainer/Q1 Predictions Std                             16.2873\n",
      "trainer/Q1 Predictions Max                             72.4922\n",
      "trainer/Q1 Predictions Min                            -15.3559\n",
      "trainer/Q2 Predictions Mean                             4.97693\n",
      "trainer/Q2 Predictions Std                             16.2449\n",
      "trainer/Q2 Predictions Max                             71.2753\n",
      "trainer/Q2 Predictions Min                            -16.1177\n",
      "trainer/Q Targets Mean                                  5.15782\n",
      "trainer/Q Targets Std                                  16.4933\n",
      "trainer/Q Targets Max                                  70.9681\n",
      "trainer/Q Targets Min                                 -15.1297\n",
      "trainer/Log Pis Mean                                    6.24351\n",
      "trainer/Log Pis Std                                     6.9754\n",
      "trainer/Log Pis Max                                    42.5605\n",
      "trainer/Log Pis Min                                    -4.42827\n",
      "trainer/Policy mu Mean                                  0.152365\n",
      "trainer/Policy mu Std                                   1.67972\n",
      "trainer/Policy mu Max                                   8.19304\n",
      "trainer/Policy mu Min                                  -6.96251\n",
      "trainer/Policy log std Mean                            -0.689885\n",
      "trainer/Policy log std Std                              0.272528\n",
      "trainer/Policy log std Max                              0.583565\n",
      "trainer/Policy log std Min                             -1.81886\n",
      "trainer/Alpha                                           0.0183724\n",
      "trainer/Alpha Loss                                      0.973218\n",
      "exploration/num steps total                        125000\n",
      "exploration/num paths total                           125\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.111108\n",
      "exploration/Rewards Std                                 0.81202\n",
      "exploration/Rewards Max                                 5.8762\n",
      "exploration/Rewards Min                                -4.62789\n",
      "exploration/Returns Mean                             -111.108\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -111.108\n",
      "exploration/Returns Min                              -111.108\n",
      "exploration/Actions Mean                               -0.30132\n",
      "exploration/Actions Std                                 0.667343\n",
      "exploration/Actions Max                                 0.999978\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -111.108\n",
      "exploration/env_infos/final/reward_run Mean            -0.0916034\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0916034\n",
      "exploration/env_infos/final/reward_run Min             -0.0916034\n",
      "exploration/env_infos/initial/reward_run Mean           0.0780116\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0780116\n",
      "exploration/env_infos/initial/reward_run Min            0.0780116\n",
      "exploration/env_infos/reward_run Mean                  -0.0164678\n",
      "exploration/env_infos/reward_run Std                    0.393357\n",
      "exploration/env_infos/reward_run Max                    2.22497\n",
      "exploration/env_infos/reward_run Min                   -2.14107\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.366077\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.366077\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.366077\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.277394\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.277394\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.277394\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.321684\n",
      "exploration/env_infos/reward_ctrl Std                   0.067461\n",
      "exploration/env_infos/reward_ctrl Max                  -0.115921\n",
      "exploration/env_infos/reward_ctrl Min                  -0.5999\n",
      "exploration/env_infos/final/height Mean                -0.191246\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.191246\n",
      "exploration/env_infos/final/height Min                 -0.191246\n",
      "exploration/env_infos/initial/height Mean               0.0342899\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0342899\n",
      "exploration/env_infos/initial/height Min                0.0342899\n",
      "exploration/env_infos/height Mean                      -0.189255\n",
      "exploration/env_infos/height Std                        0.104606\n",
      "exploration/env_infos/height Max                        0.366844\n",
      "exploration/env_infos/height Min                       -0.576142\n",
      "exploration/env_infos/final/reward_angular Mean         0.82898\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.82898\n",
      "exploration/env_infos/final/reward_angular Min          0.82898\n",
      "exploration/env_infos/initial/reward_angular Mean       0.138409\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.138409\n",
      "exploration/env_infos/initial/reward_angular Min        0.138409\n",
      "exploration/env_infos/reward_angular Mean               0.131354\n",
      "exploration/env_infos/reward_angular Std                1.15521\n",
      "exploration/env_infos/reward_angular Max                7.50665\n",
      "exploration/env_infos/reward_angular Min               -6.065\n",
      "evaluation/num steps total                              3.1e+06\n",
      "evaluation/num paths total                           3100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.165894\n",
      "evaluation/Rewards Std                                  1.21083\n",
      "evaluation/Rewards Max                                  9.48215\n",
      "evaluation/Rewards Min                                 -7.45652\n",
      "evaluation/Returns Mean                               165.894\n",
      "evaluation/Returns Std                                553.34\n",
      "evaluation/Returns Max                               1668.87\n",
      "evaluation/Returns Min                               -551.775\n",
      "evaluation/Actions Mean                                 0.0505793\n",
      "evaluation/Actions Std                                  0.712773\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            165.894\n",
      "evaluation/env_infos/final/reward_run Mean              0.1345\n",
      "evaluation/env_infos/final/reward_run Std               0.823525\n",
      "evaluation/env_infos/final/reward_run Max               2.27623\n",
      "evaluation/env_infos/final/reward_run Min              -1.57914\n",
      "evaluation/env_infos/initial/reward_run Mean            0.176875\n",
      "evaluation/env_infos/initial/reward_run Std             0.441278\n",
      "evaluation/env_infos/initial/reward_run Max             0.953779\n",
      "evaluation/env_infos/initial/reward_run Min            -0.710243\n",
      "evaluation/env_infos/reward_run Mean                   -0.280353\n",
      "evaluation/env_infos/reward_run Std                     1.71655\n",
      "evaluation/env_infos/reward_run Max                     4.20623\n",
      "evaluation/env_infos/reward_run Min                    -6.86504\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.282858\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0952747\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.119492\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.5144\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.244156\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0851145\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.101405\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.389588\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.306362\n",
      "evaluation/env_infos/reward_ctrl Std                    0.118482\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0235123\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599761\n",
      "evaluation/env_infos/final/height Mean                 -0.404215\n",
      "evaluation/env_infos/final/height Std                   0.227158\n",
      "evaluation/env_infos/final/height Max                   0.153336\n",
      "evaluation/env_infos/final/height Min                  -0.580436\n",
      "evaluation/env_infos/initial/height Mean               -0.00685063\n",
      "evaluation/env_infos/initial/height Std                 0.0588533\n",
      "evaluation/env_infos/initial/height Max                 0.0857299\n",
      "evaluation/env_infos/initial/height Min                -0.136955\n",
      "evaluation/env_infos/height Mean                       -0.31157\n",
      "evaluation/env_infos/height Std                         0.257135\n",
      "evaluation/env_infos/height Max                         0.504528\n",
      "evaluation/env_infos/height Min                        -0.589503\n",
      "evaluation/env_infos/final/reward_angular Mean          0.469606\n",
      "evaluation/env_infos/final/reward_angular Std           1.68821\n",
      "evaluation/env_infos/final/reward_angular Max           6.17149\n",
      "evaluation/env_infos/final/reward_angular Min          -2.9232\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.376632\n",
      "evaluation/env_infos/initial/reward_angular Std         0.737061\n",
      "evaluation/env_infos/initial/reward_angular Max         1.62122\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.35284\n",
      "evaluation/env_infos/reward_angular Mean                0.0521419\n",
      "evaluation/env_infos/reward_angular Std                 1.884\n",
      "evaluation/env_infos/reward_angular Max                10.8989\n",
      "evaluation/env_infos/reward_angular Min                -9.13758\n",
      "time/data storing (s)                                   0.0170518\n",
      "time/evaluation sampling (s)                           25.0833\n",
      "time/exploration sampling (s)                           1.09172\n",
      "time/logging (s)                                        0.248937\n",
      "time/saving (s)                                         0.0286093\n",
      "time/training (s)                                       4.71736\n",
      "time/epoch (s)                                         31.1869\n",
      "time/total (s)                                       3679.71\n",
      "Epoch                                                 123\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:13:54.105207 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 124 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 126000\n",
      "trainer/QF1 Loss                                        1.18772\n",
      "trainer/QF2 Loss                                        1.52314\n",
      "trainer/Policy Loss                                    -2.21346\n",
      "trainer/Q1 Predictions Mean                             8.69501\n",
      "trainer/Q1 Predictions Std                             19.1511\n",
      "trainer/Q1 Predictions Max                             74.8694\n",
      "trainer/Q1 Predictions Min                            -14.9797\n",
      "trainer/Q2 Predictions Mean                             8.65148\n",
      "trainer/Q2 Predictions Std                             19.2422\n",
      "trainer/Q2 Predictions Max                             74.4365\n",
      "trainer/Q2 Predictions Min                            -15.0897\n",
      "trainer/Q Targets Mean                                  8.80221\n",
      "trainer/Q Targets Std                                  19.2177\n",
      "trainer/Q Targets Max                                  74.8606\n",
      "trainer/Q Targets Min                                 -16.5962\n",
      "trainer/Log Pis Mean                                    6.76536\n",
      "trainer/Log Pis Std                                     6.49609\n",
      "trainer/Log Pis Max                                    33.4547\n",
      "trainer/Log Pis Min                                    -3.99948\n",
      "trainer/Policy mu Mean                                  0.128675\n",
      "trainer/Policy mu Std                                   1.71907\n",
      "trainer/Policy mu Max                                   6.71929\n",
      "trainer/Policy mu Min                                  -5.99121\n",
      "trainer/Policy log std Mean                            -0.687869\n",
      "trainer/Policy log std Std                              0.257693\n",
      "trainer/Policy log std Max                              0.37352\n",
      "trainer/Policy log std Min                             -1.6269\n",
      "trainer/Alpha                                           0.0174196\n",
      "trainer/Alpha Loss                                      3.10103\n",
      "exploration/num steps total                        126000\n",
      "exploration/num paths total                           126\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.16337\n",
      "exploration/Rewards Std                                 1.04611\n",
      "exploration/Rewards Max                                 6.37093\n",
      "exploration/Rewards Min                                -3.95716\n",
      "exploration/Returns Mean                              163.37\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               163.37\n",
      "exploration/Returns Min                               163.37\n",
      "exploration/Actions Mean                               -0.146301\n",
      "exploration/Actions Std                                 0.667647\n",
      "exploration/Actions Max                                 0.999993\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           163.37\n",
      "exploration/env_infos/final/reward_run Mean             0.259842\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.259842\n",
      "exploration/env_infos/final/reward_run Min              0.259842\n",
      "exploration/env_infos/initial/reward_run Mean           0.614048\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.614048\n",
      "exploration/env_infos/initial/reward_run Min            0.614048\n",
      "exploration/env_infos/reward_run Mean                  -0.0830265\n",
      "exploration/env_infos/reward_run Std                    0.571237\n",
      "exploration/env_infos/reward_run Max                    2.00477\n",
      "exploration/env_infos/reward_run Min                   -2.68241\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.228894\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.228894\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.228894\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.405461\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.405461\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.405461\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.280294\n",
      "exploration/env_infos/reward_ctrl Std                   0.0824065\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0727059\n",
      "exploration/env_infos/reward_ctrl Min                  -0.573963\n",
      "exploration/env_infos/final/height Mean                -0.570707\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.570707\n",
      "exploration/env_infos/final/height Min                 -0.570707\n",
      "exploration/env_infos/initial/height Mean               0.0119032\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0119032\n",
      "exploration/env_infos/initial/height Min                0.0119032\n",
      "exploration/env_infos/height Mean                      -0.454488\n",
      "exploration/env_infos/height Std                        0.244883\n",
      "exploration/env_infos/height Max                        0.50879\n",
      "exploration/env_infos/height Min                       -0.579768\n",
      "exploration/env_infos/final/reward_angular Mean         0.0961598\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0961598\n",
      "exploration/env_infos/final/reward_angular Min          0.0961598\n",
      "exploration/env_infos/initial/reward_angular Mean       1.46448\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.46448\n",
      "exploration/env_infos/initial/reward_angular Min        1.46448\n",
      "exploration/env_infos/reward_angular Mean               0.200001\n",
      "exploration/env_infos/reward_angular Std                1.1642\n",
      "exploration/env_infos/reward_angular Max                6.37854\n",
      "exploration/env_infos/reward_angular Min               -4.30741\n",
      "evaluation/num steps total                              3.125e+06\n",
      "evaluation/num paths total                           3125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.429217\n",
      "evaluation/Rewards Std                                  1.26628\n",
      "evaluation/Rewards Max                                  8.19828\n",
      "evaluation/Rewards Min                                 -5.34796\n",
      "evaluation/Returns Mean                               429.217\n",
      "evaluation/Returns Std                                640.908\n",
      "evaluation/Returns Max                               1687.99\n",
      "evaluation/Returns Min                               -563.009\n",
      "evaluation/Actions Mean                                 0.0552573\n",
      "evaluation/Actions Std                                  0.782595\n",
      "evaluation/Actions Max                                  0.99997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            429.217\n",
      "evaluation/env_infos/final/reward_run Mean             -1.11059\n",
      "evaluation/env_infos/final/reward_run Std               1.92806\n",
      "evaluation/env_infos/final/reward_run Max               2.40188\n",
      "evaluation/env_infos/final/reward_run Min              -4.66013\n",
      "evaluation/env_infos/initial/reward_run Mean            0.232727\n",
      "evaluation/env_infos/initial/reward_run Std             0.423786\n",
      "evaluation/env_infos/initial/reward_run Max             1.0025\n",
      "evaluation/env_infos/initial/reward_run Min            -0.52078\n",
      "evaluation/env_infos/reward_run Mean                   -0.924504\n",
      "evaluation/env_infos/reward_run Std                     2.0813\n",
      "evaluation/env_infos/reward_run Max                     4.40636\n",
      "evaluation/env_infos/reward_run Min                    -6.27303\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.374136\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104886\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0897392\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.557871\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.272093\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.119599\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0809515\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.503283\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.369305\n",
      "evaluation/env_infos/reward_ctrl Std                    0.107233\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00863996\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595027\n",
      "evaluation/env_infos/final/height Mean                 -0.149033\n",
      "evaluation/env_infos/final/height Std                   0.18905\n",
      "evaluation/env_infos/final/height Max                   0.234748\n",
      "evaluation/env_infos/final/height Min                  -0.578096\n",
      "evaluation/env_infos/initial/height Mean               -0.0176023\n",
      "evaluation/env_infos/initial/height Std                 0.0469966\n",
      "evaluation/env_infos/initial/height Max                 0.0788974\n",
      "evaluation/env_infos/initial/height Min                -0.0740482\n",
      "evaluation/env_infos/height Mean                       -0.126713\n",
      "evaluation/env_infos/height Std                         0.172468\n",
      "evaluation/env_infos/height Max                         0.507871\n",
      "evaluation/env_infos/height Min                        -0.581848\n",
      "evaluation/env_infos/final/reward_angular Mean          0.276857\n",
      "evaluation/env_infos/final/reward_angular Std           1.68319\n",
      "evaluation/env_infos/final/reward_angular Max           4.48086\n",
      "evaluation/env_infos/final/reward_angular Min          -2.25428\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.215868\n",
      "evaluation/env_infos/initial/reward_angular Std         1.00858\n",
      "evaluation/env_infos/initial/reward_angular Max         2.72237\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.76075\n",
      "evaluation/env_infos/reward_angular Mean                0.0876541\n",
      "evaluation/env_infos/reward_angular Std                 1.95627\n",
      "evaluation/env_infos/reward_angular Max                10.6389\n",
      "evaluation/env_infos/reward_angular Min                -6.8045\n",
      "time/data storing (s)                                   0.0172517\n",
      "time/evaluation sampling (s)                           22.6932\n",
      "time/exploration sampling (s)                           1.62637\n",
      "time/logging (s)                                        0.251612\n",
      "time/saving (s)                                         0.0297302\n",
      "time/training (s)                                       5.96486\n",
      "time/epoch (s)                                         30.583\n",
      "time/total (s)                                       3711.33\n",
      "Epoch                                                 124\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:14:24.855322 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 125 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 127000\n",
      "trainer/QF1 Loss                                        1.1982\n",
      "trainer/QF2 Loss                                        1.04286\n",
      "trainer/Policy Loss                                    -2.27637\n",
      "trainer/Q1 Predictions Mean                             7.05995\n",
      "trainer/Q1 Predictions Std                             18.6121\n",
      "trainer/Q1 Predictions Max                             77.8071\n",
      "trainer/Q1 Predictions Min                            -14.2481\n",
      "trainer/Q2 Predictions Mean                             7.00696\n",
      "trainer/Q2 Predictions Std                             18.6439\n",
      "trainer/Q2 Predictions Max                             76.6132\n",
      "trainer/Q2 Predictions Min                            -14.1879\n",
      "trainer/Q Targets Mean                                  7.22972\n",
      "trainer/Q Targets Std                                  18.8863\n",
      "trainer/Q Targets Max                                  79.5232\n",
      "trainer/Q Targets Min                                 -14.8221\n",
      "trainer/Log Pis Mean                                    5.03375\n",
      "trainer/Log Pis Std                                     5.21988\n",
      "trainer/Log Pis Max                                    24.3022\n",
      "trainer/Log Pis Min                                    -5.8375\n",
      "trainer/Policy mu Mean                                  0.102781\n",
      "trainer/Policy mu Std                                   1.51651\n",
      "trainer/Policy mu Max                                   4.24335\n",
      "trainer/Policy mu Min                                  -5.29479\n",
      "trainer/Policy log std Mean                            -0.729939\n",
      "trainer/Policy log std Std                              0.255463\n",
      "trainer/Policy log std Max                              0.0918362\n",
      "trainer/Policy log std Min                             -1.75089\n",
      "trainer/Alpha                                           0.0169255\n",
      "trainer/Alpha Loss                                     -3.9398\n",
      "exploration/num steps total                        127000\n",
      "exploration/num paths total                           127\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.251608\n",
      "exploration/Rewards Std                                 0.10804\n",
      "exploration/Rewards Max                                 0.0892381\n",
      "exploration/Rewards Min                                -0.56307\n",
      "exploration/Returns Mean                             -251.608\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -251.608\n",
      "exploration/Returns Min                              -251.608\n",
      "exploration/Actions Mean                                0.191914\n",
      "exploration/Actions Std                                 0.570466\n",
      "exploration/Actions Max                                 0.997313\n",
      "exploration/Actions Min                                -0.999199\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -251.608\n",
      "exploration/env_infos/final/reward_run Mean             0.849453\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.849453\n",
      "exploration/env_infos/final/reward_run Min              0.849453\n",
      "exploration/env_infos/initial/reward_run Mean           0.173761\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.173761\n",
      "exploration/env_infos/initial/reward_run Min            0.173761\n",
      "exploration/env_infos/reward_run Mean                  -0.205903\n",
      "exploration/env_infos/reward_run Std                    0.782178\n",
      "exploration/env_infos/reward_run Max                    2.43169\n",
      "exploration/env_infos/reward_run Min                   -2.63235\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.311695\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.311695\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.311695\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.257528\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.257528\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.257528\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.217358\n",
      "exploration/env_infos/reward_ctrl Std                   0.0744483\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0306503\n",
      "exploration/env_infos/reward_ctrl Min                  -0.520508\n",
      "exploration/env_infos/final/height Mean                -0.17915\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.17915\n",
      "exploration/env_infos/final/height Min                 -0.17915\n",
      "exploration/env_infos/initial/height Mean               0.0170614\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0170614\n",
      "exploration/env_infos/initial/height Min                0.0170614\n",
      "exploration/env_infos/height Mean                      -0.093623\n",
      "exploration/env_infos/height Std                        0.0719581\n",
      "exploration/env_infos/height Max                        0.180178\n",
      "exploration/env_infos/height Min                       -0.314134\n",
      "exploration/env_infos/final/reward_angular Mean         2.7109\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.7109\n",
      "exploration/env_infos/final/reward_angular Min          2.7109\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.72688\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.72688\n",
      "exploration/env_infos/initial/reward_angular Min       -0.72688\n",
      "exploration/env_infos/reward_angular Mean              -0.000142785\n",
      "exploration/env_infos/reward_angular Std                1.40599\n",
      "exploration/env_infos/reward_angular Max                5.2152\n",
      "exploration/env_infos/reward_angular Min               -4.59267\n",
      "evaluation/num steps total                              3.15e+06\n",
      "evaluation/num paths total                           3150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.488593\n",
      "evaluation/Rewards Std                                  1.26884\n",
      "evaluation/Rewards Max                                  7.97924\n",
      "evaluation/Rewards Min                                 -6.09389\n",
      "evaluation/Returns Mean                               488.593\n",
      "evaluation/Returns Std                                703.604\n",
      "evaluation/Returns Max                               1937.65\n",
      "evaluation/Returns Min                               -638.581\n",
      "evaluation/Actions Mean                                 0.0989112\n",
      "evaluation/Actions Std                                  0.75582\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            488.593\n",
      "evaluation/env_infos/final/reward_run Mean             -1.27079\n",
      "evaluation/env_infos/final/reward_run Std               2.20992\n",
      "evaluation/env_infos/final/reward_run Max               2.63786\n",
      "evaluation/env_infos/final/reward_run Min              -4.50359\n",
      "evaluation/env_infos/initial/reward_run Mean            0.200498\n",
      "evaluation/env_infos/initial/reward_run Std             0.440566\n",
      "evaluation/env_infos/initial/reward_run Max             0.890688\n",
      "evaluation/env_infos/initial/reward_run Min            -0.65675\n",
      "evaluation/env_infos/reward_run Mean                   -1.07709\n",
      "evaluation/env_infos/reward_run Std                     2.20347\n",
      "evaluation/env_infos/reward_run Max                     4.94535\n",
      "evaluation/env_infos/reward_run Min                    -6.18675\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.364918\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.117904\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.08868\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.540188\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266076\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.076935\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.144496\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.393927\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.348629\n",
      "evaluation/env_infos/reward_ctrl Std                    0.117909\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0174028\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593591\n",
      "evaluation/env_infos/final/height Mean                 -0.101786\n",
      "evaluation/env_infos/final/height Std                   0.147732\n",
      "evaluation/env_infos/final/height Max                   0.245142\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.000556201\n",
      "evaluation/env_infos/initial/height Std                 0.0471021\n",
      "evaluation/env_infos/initial/height Max                 0.075245\n",
      "evaluation/env_infos/initial/height Min                -0.0921423\n",
      "evaluation/env_infos/height Mean                       -0.102291\n",
      "evaluation/env_infos/height Std                         0.149959\n",
      "evaluation/env_infos/height Max                         0.46908\n",
      "evaluation/env_infos/height Min                        -0.57988\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.470625\n",
      "evaluation/env_infos/final/reward_angular Std           1.91752\n",
      "evaluation/env_infos/final/reward_angular Max           2.89596\n",
      "evaluation/env_infos/final/reward_angular Min          -4.30719\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.548835\n",
      "evaluation/env_infos/initial/reward_angular Std         0.916335\n",
      "evaluation/env_infos/initial/reward_angular Max         2.21053\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.86237\n",
      "evaluation/env_infos/reward_angular Mean                0.0728236\n",
      "evaluation/env_infos/reward_angular Std                 2.00539\n",
      "evaluation/env_infos/reward_angular Max                 9.60977\n",
      "evaluation/env_infos/reward_angular Min                -6.6898\n",
      "time/data storing (s)                                   0.0174649\n",
      "time/evaluation sampling (s)                           23.6695\n",
      "time/exploration sampling (s)                           1.18088\n",
      "time/logging (s)                                        0.242081\n",
      "time/saving (s)                                         0.0268736\n",
      "time/training (s)                                       4.61836\n",
      "time/epoch (s)                                         29.7551\n",
      "time/total (s)                                       3742.07\n",
      "Epoch                                                 125\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:14:58.146421 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 126 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 128000\n",
      "trainer/QF1 Loss                                        1.63993\n",
      "trainer/QF2 Loss                                        1.3625\n",
      "trainer/Policy Loss                                    -3.5427\n",
      "trainer/Q1 Predictions Mean                             9.15813\n",
      "trainer/Q1 Predictions Std                             18.4378\n",
      "trainer/Q1 Predictions Max                             71.0795\n",
      "trainer/Q1 Predictions Min                            -14.1676\n",
      "trainer/Q2 Predictions Mean                             9.2193\n",
      "trainer/Q2 Predictions Std                             18.5896\n",
      "trainer/Q2 Predictions Max                             73.3806\n",
      "trainer/Q2 Predictions Min                            -15.4802\n",
      "trainer/Q Targets Mean                                  9.27841\n",
      "trainer/Q Targets Std                                  18.8069\n",
      "trainer/Q Targets Max                                  74.3395\n",
      "trainer/Q Targets Min                                 -15.4859\n",
      "trainer/Log Pis Mean                                    5.88207\n",
      "trainer/Log Pis Std                                     5.84169\n",
      "trainer/Log Pis Max                                    24.4843\n",
      "trainer/Log Pis Min                                    -5.66663\n",
      "trainer/Policy mu Mean                                  0.0968505\n",
      "trainer/Policy mu Std                                   1.58638\n",
      "trainer/Policy mu Max                                   5.12899\n",
      "trainer/Policy mu Min                                  -5.03073\n",
      "trainer/Policy log std Mean                            -0.730525\n",
      "trainer/Policy log std Std                              0.279446\n",
      "trainer/Policy log std Max                              0.153627\n",
      "trainer/Policy log std Min                             -2.34142\n",
      "trainer/Alpha                                           0.0166\n",
      "trainer/Alpha Loss                                     -0.483133\n",
      "exploration/num steps total                        128000\n",
      "exploration/num paths total                           128\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0844704\n",
      "exploration/Rewards Std                                 0.439934\n",
      "exploration/Rewards Max                                 2.17855\n",
      "exploration/Rewards Min                                -3.37206\n",
      "exploration/Returns Mean                              -84.4704\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -84.4704\n",
      "exploration/Returns Min                               -84.4704\n",
      "exploration/Actions Mean                               -0.259819\n",
      "exploration/Actions Std                                 0.756094\n",
      "exploration/Actions Max                                 0.999991\n",
      "exploration/Actions Min                                -0.999973\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -84.4704\n",
      "exploration/env_infos/final/reward_run Mean             0.475879\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.475879\n",
      "exploration/env_infos/final/reward_run Min              0.475879\n",
      "exploration/env_infos/initial/reward_run Mean           0.101648\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.101648\n",
      "exploration/env_infos/initial/reward_run Min            0.101648\n",
      "exploration/env_infos/reward_run Mean                   0.151749\n",
      "exploration/env_infos/reward_run Std                    0.408204\n",
      "exploration/env_infos/reward_run Max                    1.06473\n",
      "exploration/env_infos/reward_run Min                   -2.1068\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270639\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270639\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270639\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0422146\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0422146\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0422146\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.38351\n",
      "exploration/env_infos/reward_ctrl Std                   0.0852852\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0422146\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596123\n",
      "exploration/env_infos/final/height Mean                 0.0729251\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0729251\n",
      "exploration/env_infos/final/height Min                  0.0729251\n",
      "exploration/env_infos/initial/height Mean              -0.0281028\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0281028\n",
      "exploration/env_infos/initial/height Min               -0.0281028\n",
      "exploration/env_infos/height Mean                       0.064187\n",
      "exploration/env_infos/height Std                        0.0390336\n",
      "exploration/env_infos/height Max                        0.264873\n",
      "exploration/env_infos/height Min                       -0.149805\n",
      "exploration/env_infos/final/reward_angular Mean        -1.44464\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.44464\n",
      "exploration/env_infos/final/reward_angular Min         -1.44464\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.647226\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.647226\n",
      "exploration/env_infos/initial/reward_angular Min       -0.647226\n",
      "exploration/env_infos/reward_angular Mean               0.0376536\n",
      "exploration/env_infos/reward_angular Std                0.944442\n",
      "exploration/env_infos/reward_angular Max                5.16678\n",
      "exploration/env_infos/reward_angular Min               -6.58407\n",
      "evaluation/num steps total                              3.175e+06\n",
      "evaluation/num paths total                           3175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.545331\n",
      "evaluation/Rewards Std                                  1.56611\n",
      "evaluation/Rewards Max                                  8.34781\n",
      "evaluation/Rewards Min                                 -7.91225\n",
      "evaluation/Returns Mean                               545.331\n",
      "evaluation/Returns Std                                747.806\n",
      "evaluation/Returns Max                               2289.83\n",
      "evaluation/Returns Min                               -443.29\n",
      "evaluation/Actions Mean                                 0.0950297\n",
      "evaluation/Actions Std                                  0.785872\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            545.331\n",
      "evaluation/env_infos/final/reward_run Mean             -1.10962\n",
      "evaluation/env_infos/final/reward_run Std               2.27702\n",
      "evaluation/env_infos/final/reward_run Max               2.65781\n",
      "evaluation/env_infos/final/reward_run Min              -5.20794\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0251742\n",
      "evaluation/env_infos/initial/reward_run Std             0.503123\n",
      "evaluation/env_infos/initial/reward_run Max             1.12409\n",
      "evaluation/env_infos/initial/reward_run Min            -0.734547\n",
      "evaluation/env_infos/reward_run Mean                   -1.08074\n",
      "evaluation/env_infos/reward_run Std                     2.30978\n",
      "evaluation/env_infos/reward_run Max                     4.64755\n",
      "evaluation/env_infos/reward_run Min                    -7.03119\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.338802\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.121181\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.137067\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.559113\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.238536\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0993077\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0518393\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.389034\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.375975\n",
      "evaluation/env_infos/reward_ctrl Std                    0.123924\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0234637\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599551\n",
      "evaluation/env_infos/final/height Mean                 -0.161373\n",
      "evaluation/env_infos/final/height Std                   0.266474\n",
      "evaluation/env_infos/final/height Max                   0.390458\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00114364\n",
      "evaluation/env_infos/initial/height Std                 0.0454246\n",
      "evaluation/env_infos/initial/height Max                 0.0770957\n",
      "evaluation/env_infos/initial/height Min                -0.0872218\n",
      "evaluation/env_infos/height Mean                       -0.12061\n",
      "evaluation/env_infos/height Std                         0.222208\n",
      "evaluation/env_infos/height Max                         0.577555\n",
      "evaluation/env_infos/height Min                        -0.58682\n",
      "evaluation/env_infos/final/reward_angular Mean          0.377814\n",
      "evaluation/env_infos/final/reward_angular Std           1.90114\n",
      "evaluation/env_infos/final/reward_angular Max           5.37559\n",
      "evaluation/env_infos/final/reward_angular Min          -3.2949\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.186278\n",
      "evaluation/env_infos/initial/reward_angular Std         1.0566\n",
      "evaluation/env_infos/initial/reward_angular Max         1.66616\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.0417\n",
      "evaluation/env_infos/reward_angular Mean                0.0697063\n",
      "evaluation/env_infos/reward_angular Std                 2.38428\n",
      "evaluation/env_infos/reward_angular Max                 9.59662\n",
      "evaluation/env_infos/reward_angular Min                -9.85155\n",
      "time/data storing (s)                                   0.0214349\n",
      "time/evaluation sampling (s)                           26.0316\n",
      "time/exploration sampling (s)                           1.29053\n",
      "time/logging (s)                                        0.2413\n",
      "time/saving (s)                                         0.0279474\n",
      "time/training (s)                                       4.62947\n",
      "time/epoch (s)                                         32.2423\n",
      "time/total (s)                                       3775.36\n",
      "Epoch                                                 126\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:15:27.350816 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 127 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 129000\n",
      "trainer/QF1 Loss                                        1.08444\n",
      "trainer/QF2 Loss                                        1.25178\n",
      "trainer/Policy Loss                                    -0.320062\n",
      "trainer/Q1 Predictions Mean                             6.08533\n",
      "trainer/Q1 Predictions Std                             18.1315\n",
      "trainer/Q1 Predictions Max                             82.2429\n",
      "trainer/Q1 Predictions Min                            -15.7704\n",
      "trainer/Q2 Predictions Mean                             6.15662\n",
      "trainer/Q2 Predictions Std                             18.2641\n",
      "trainer/Q2 Predictions Max                             82.5716\n",
      "trainer/Q2 Predictions Min                            -16.4925\n",
      "trainer/Q Targets Mean                                  6.26574\n",
      "trainer/Q Targets Std                                  18.0455\n",
      "trainer/Q Targets Max                                  82.2065\n",
      "trainer/Q Targets Min                                 -16.1784\n",
      "trainer/Log Pis Mean                                    6.04377\n",
      "trainer/Log Pis Std                                     6.10465\n",
      "trainer/Log Pis Max                                    35.6703\n",
      "trainer/Log Pis Min                                    -4.54456\n",
      "trainer/Policy mu Mean                                  0.00484767\n",
      "trainer/Policy mu Std                                   1.64504\n",
      "trainer/Policy mu Max                                   5.58339\n",
      "trainer/Policy mu Min                                  -5.25764\n",
      "trainer/Policy log std Mean                            -0.722477\n",
      "trainer/Policy log std Std                              0.277163\n",
      "trainer/Policy log std Max                              0.444734\n",
      "trainer/Policy log std Min                             -1.90028\n",
      "trainer/Alpha                                           0.016413\n",
      "trainer/Alpha Loss                                      0.179861\n",
      "exploration/num steps total                        129000\n",
      "exploration/num paths total                           129\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0127012\n",
      "exploration/Rewards Std                                 0.801297\n",
      "exploration/Rewards Max                                 2.41748\n",
      "exploration/Rewards Min                                -2.69542\n",
      "exploration/Returns Mean                              -12.7012\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -12.7012\n",
      "exploration/Returns Min                               -12.7012\n",
      "exploration/Actions Mean                                0.17558\n",
      "exploration/Actions Std                                 0.665195\n",
      "exploration/Actions Max                                 0.999916\n",
      "exploration/Actions Min                                -0.999947\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -12.7012\n",
      "exploration/env_infos/final/reward_run Mean             1.23757\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.23757\n",
      "exploration/env_infos/final/reward_run Min              1.23757\n",
      "exploration/env_infos/initial/reward_run Mean           0.282364\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.282364\n",
      "exploration/env_infos/initial/reward_run Min            0.282364\n",
      "exploration/env_infos/reward_run Mean                   1.02085\n",
      "exploration/env_infos/reward_run Std                    0.716202\n",
      "exploration/env_infos/reward_run Max                    3.36221\n",
      "exploration/env_infos/reward_run Min                   -0.995171\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.306056\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.306056\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.306056\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.296623\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.296623\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.296623\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.283988\n",
      "exploration/env_infos/reward_ctrl Std                   0.0809792\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0280921\n",
      "exploration/env_infos/reward_ctrl Min                  -0.505501\n",
      "exploration/env_infos/final/height Mean                -0.170508\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.170508\n",
      "exploration/env_infos/final/height Min                 -0.170508\n",
      "exploration/env_infos/initial/height Mean               0.0626881\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0626881\n",
      "exploration/env_infos/initial/height Min                0.0626881\n",
      "exploration/env_infos/height Mean                      -0.109907\n",
      "exploration/env_infos/height Std                        0.0923856\n",
      "exploration/env_infos/height Max                        0.203018\n",
      "exploration/env_infos/height Min                       -0.368472\n",
      "exploration/env_infos/final/reward_angular Mean        -0.417603\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.417603\n",
      "exploration/env_infos/final/reward_angular Min         -0.417603\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.5686\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.5686\n",
      "exploration/env_infos/initial/reward_angular Min       -1.5686\n",
      "exploration/env_infos/reward_angular Mean              -0.0368044\n",
      "exploration/env_infos/reward_angular Std                1.48733\n",
      "exploration/env_infos/reward_angular Max                5.25262\n",
      "exploration/env_infos/reward_angular Min               -4.3437\n",
      "evaluation/num steps total                              3.2e+06\n",
      "evaluation/num paths total                           3200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.169165\n",
      "evaluation/Rewards Std                                  1.19981\n",
      "evaluation/Rewards Max                                  7.91777\n",
      "evaluation/Rewards Min                                 -7.51151\n",
      "evaluation/Returns Mean                               169.165\n",
      "evaluation/Returns Std                                449.512\n",
      "evaluation/Returns Max                               1593.94\n",
      "evaluation/Returns Min                               -388.001\n",
      "evaluation/Actions Mean                                 0.0488953\n",
      "evaluation/Actions Std                                  0.699426\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            169.165\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0771765\n",
      "evaluation/env_infos/final/reward_run Std               1.38692\n",
      "evaluation/env_infos/final/reward_run Max               2.40287\n",
      "evaluation/env_infos/final/reward_run Min              -4.00418\n",
      "evaluation/env_infos/initial/reward_run Mean            0.226549\n",
      "evaluation/env_infos/initial/reward_run Std             0.471724\n",
      "evaluation/env_infos/initial/reward_run Max             1.0424\n",
      "evaluation/env_infos/initial/reward_run Min            -0.597876\n",
      "evaluation/env_infos/reward_run Mean                   -0.326459\n",
      "evaluation/env_infos/reward_run Std                     1.72502\n",
      "evaluation/env_infos/reward_run Max                     3.71988\n",
      "evaluation/env_infos/reward_run Min                    -7.0169\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.262302\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0891078\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.111182\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.468181\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.272505\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0696345\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.173023\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.394828\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.294953\n",
      "evaluation/env_infos/reward_ctrl Std                    0.117208\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0345027\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599605\n",
      "evaluation/env_infos/final/height Mean                 -0.329757\n",
      "evaluation/env_infos/final/height Std                   0.262675\n",
      "evaluation/env_infos/final/height Max                   0.244977\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0248023\n",
      "evaluation/env_infos/initial/height Std                 0.0469607\n",
      "evaluation/env_infos/initial/height Max                 0.0768658\n",
      "evaluation/env_infos/initial/height Min                -0.0906056\n",
      "evaluation/env_infos/height Mean                       -0.253976\n",
      "evaluation/env_infos/height Std                         0.258767\n",
      "evaluation/env_infos/height Max                         0.555217\n",
      "evaluation/env_infos/height Min                        -0.590438\n",
      "evaluation/env_infos/final/reward_angular Mean          0.277871\n",
      "evaluation/env_infos/final/reward_angular Std           1.49338\n",
      "evaluation/env_infos/final/reward_angular Max           6.28694\n",
      "evaluation/env_infos/final/reward_angular Min          -1.89796\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.177642\n",
      "evaluation/env_infos/initial/reward_angular Std         1.22211\n",
      "evaluation/env_infos/initial/reward_angular Max         3.39607\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.78563\n",
      "evaluation/env_infos/reward_angular Mean                0.0137394\n",
      "evaluation/env_infos/reward_angular Std                 1.74857\n",
      "evaluation/env_infos/reward_angular Max                 9.55242\n",
      "evaluation/env_infos/reward_angular Min                -7.03861\n",
      "time/data storing (s)                                   0.0147781\n",
      "time/evaluation sampling (s)                           23.0057\n",
      "time/exploration sampling (s)                           0.974384\n",
      "time/logging (s)                                        0.24033\n",
      "time/saving (s)                                         0.0313536\n",
      "time/training (s)                                       3.95474\n",
      "time/epoch (s)                                         28.2213\n",
      "time/total (s)                                       3804.56\n",
      "Epoch                                                 127\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:15:58.665000 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 128 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 130000\n",
      "trainer/QF1 Loss                                        1.38062\n",
      "trainer/QF2 Loss                                        1.44359\n",
      "trainer/Policy Loss                                    -0.974731\n",
      "trainer/Q1 Predictions Mean                             7.48936\n",
      "trainer/Q1 Predictions Std                             19.3803\n",
      "trainer/Q1 Predictions Max                             81.3315\n",
      "trainer/Q1 Predictions Min                            -16.2639\n",
      "trainer/Q2 Predictions Mean                             7.53319\n",
      "trainer/Q2 Predictions Std                             19.531\n",
      "trainer/Q2 Predictions Max                             82.212\n",
      "trainer/Q2 Predictions Min                            -15.6031\n",
      "trainer/Q Targets Mean                                  7.64792\n",
      "trainer/Q Targets Std                                  19.4039\n",
      "trainer/Q Targets Max                                  76.7045\n",
      "trainer/Q Targets Min                                 -16.5694\n",
      "trainer/Log Pis Mean                                    6.76306\n",
      "trainer/Log Pis Std                                     6.31853\n",
      "trainer/Log Pis Max                                    28.8622\n",
      "trainer/Log Pis Min                                    -7.59854\n",
      "trainer/Policy mu Mean                                  0.0236699\n",
      "trainer/Policy mu Std                                   1.6977\n",
      "trainer/Policy mu Max                                   5.34142\n",
      "trainer/Policy mu Min                                  -6.14315\n",
      "trainer/Policy log std Mean                            -0.765639\n",
      "trainer/Policy log std Std                              0.284298\n",
      "trainer/Policy log std Max                              0.170056\n",
      "trainer/Policy log std Min                             -1.81201\n",
      "trainer/Alpha                                           0.0167246\n",
      "trainer/Alpha Loss                                      3.12247\n",
      "exploration/num steps total                        130000\n",
      "exploration/num paths total                           130\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.88047\n",
      "exploration/Rewards Std                                 1.53201\n",
      "exploration/Rewards Max                                 5.5983\n",
      "exploration/Rewards Min                                -2.05123\n",
      "exploration/Returns Mean                             1880.47\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1880.47\n",
      "exploration/Returns Min                              1880.47\n",
      "exploration/Actions Mean                                0.141547\n",
      "exploration/Actions Std                                 0.873425\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999942\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1880.47\n",
      "exploration/env_infos/final/reward_run Mean            -3.77266\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.77266\n",
      "exploration/env_infos/final/reward_run Min             -3.77266\n",
      "exploration/env_infos/initial/reward_run Mean           0.0672403\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0672403\n",
      "exploration/env_infos/initial/reward_run Min            0.0672403\n",
      "exploration/env_infos/reward_run Mean                  -2.50732\n",
      "exploration/env_infos/reward_run Std                    1.19469\n",
      "exploration/env_infos/reward_run Max                    0.744498\n",
      "exploration/env_infos/reward_run Min                   -5.59927\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.594362\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.594362\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.594362\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.276097\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.276097\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.276097\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.469744\n",
      "exploration/env_infos/reward_ctrl Std                   0.0906594\n",
      "exploration/env_infos/reward_ctrl Max                  -0.170352\n",
      "exploration/env_infos/reward_ctrl Min                  -0.598131\n",
      "exploration/env_infos/final/height Mean                -0.0732934\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0732934\n",
      "exploration/env_infos/final/height Min                 -0.0732934\n",
      "exploration/env_infos/initial/height Mean              -0.0678142\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0678142\n",
      "exploration/env_infos/initial/height Min               -0.0678142\n",
      "exploration/env_infos/height Mean                      -0.0287321\n",
      "exploration/env_infos/height Std                        0.108478\n",
      "exploration/env_infos/height Max                        0.269533\n",
      "exploration/env_infos/height Min                       -0.342716\n",
      "exploration/env_infos/final/reward_angular Mean        -2.85052\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.85052\n",
      "exploration/env_infos/final/reward_angular Min         -2.85052\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0350438\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0350438\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0350438\n",
      "exploration/env_infos/reward_angular Mean               0.133723\n",
      "exploration/env_infos/reward_angular Std                2.81986\n",
      "exploration/env_infos/reward_angular Max                7.72399\n",
      "exploration/env_infos/reward_angular Min               -5.6332\n",
      "evaluation/num steps total                              3.225e+06\n",
      "evaluation/num paths total                           3225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.431418\n",
      "evaluation/Rewards Std                                  1.23337\n",
      "evaluation/Rewards Max                                  7.81333\n",
      "evaluation/Rewards Min                                 -6.08911\n",
      "evaluation/Returns Mean                               431.418\n",
      "evaluation/Returns Std                                671.812\n",
      "evaluation/Returns Max                               2061.08\n",
      "evaluation/Returns Min                               -642.691\n",
      "evaluation/Actions Mean                                 0.110566\n",
      "evaluation/Actions Std                                  0.743648\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            431.418\n",
      "evaluation/env_infos/final/reward_run Mean             -1.24227\n",
      "evaluation/env_infos/final/reward_run Std               2.24569\n",
      "evaluation/env_infos/final/reward_run Max               3.30869\n",
      "evaluation/env_infos/final/reward_run Min              -5.03948\n",
      "evaluation/env_infos/initial/reward_run Mean            0.171478\n",
      "evaluation/env_infos/initial/reward_run Std             0.412967\n",
      "evaluation/env_infos/initial/reward_run Max             1.03783\n",
      "evaluation/env_infos/initial/reward_run Min            -0.516074\n",
      "evaluation/env_infos/reward_run Mean                   -1.14383\n",
      "evaluation/env_infos/reward_run Std                     2.05095\n",
      "evaluation/env_infos/reward_run Max                     4.10024\n",
      "evaluation/env_infos/reward_run Min                    -6.68585\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.336921\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.133862\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.106805\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.594653\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.263966\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0791903\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.148621\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.429541\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.339142\n",
      "evaluation/env_infos/reward_ctrl Std                    0.139154\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.043131\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596446\n",
      "evaluation/env_infos/final/height Mean                 -0.0793188\n",
      "evaluation/env_infos/final/height Std                   0.142461\n",
      "evaluation/env_infos/final/height Max                   0.0809854\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0127608\n",
      "evaluation/env_infos/initial/height Std                 0.0543941\n",
      "evaluation/env_infos/initial/height Max                 0.094532\n",
      "evaluation/env_infos/initial/height Min                -0.103508\n",
      "evaluation/env_infos/height Mean                       -0.0930881\n",
      "evaluation/env_infos/height Std                         0.144295\n",
      "evaluation/env_infos/height Max                         0.425523\n",
      "evaluation/env_infos/height Min                        -0.577312\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.30072\n",
      "evaluation/env_infos/final/reward_angular Std           1.94701\n",
      "evaluation/env_infos/final/reward_angular Max           2.91468\n",
      "evaluation/env_infos/final/reward_angular Min          -5.64951\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.406777\n",
      "evaluation/env_infos/initial/reward_angular Std         1.22794\n",
      "evaluation/env_infos/initial/reward_angular Max         2.20169\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.35219\n",
      "evaluation/env_infos/reward_angular Mean                0.0796078\n",
      "evaluation/env_infos/reward_angular Std                 1.88525\n",
      "evaluation/env_infos/reward_angular Max                 9.35918\n",
      "evaluation/env_infos/reward_angular Min                -7.95664\n",
      "time/data storing (s)                                   0.0149375\n",
      "time/evaluation sampling (s)                           24.606\n",
      "time/exploration sampling (s)                           1.17616\n",
      "time/logging (s)                                        0.249421\n",
      "time/saving (s)                                         0.0277474\n",
      "time/training (s)                                       4.29002\n",
      "time/epoch (s)                                         30.3643\n",
      "time/total (s)                                       3835.88\n",
      "Epoch                                                 128\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:16:26.228096 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 129 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 131000\n",
      "trainer/QF1 Loss                                        1.26805\n",
      "trainer/QF2 Loss                                        1.52056\n",
      "trainer/Policy Loss                                    -3.45894\n",
      "trainer/Q1 Predictions Mean                             9.84528\n",
      "trainer/Q1 Predictions Std                             21.3756\n",
      "trainer/Q1 Predictions Max                             75.9104\n",
      "trainer/Q1 Predictions Min                            -15.4149\n",
      "trainer/Q2 Predictions Mean                            10.0315\n",
      "trainer/Q2 Predictions Std                             21.59\n",
      "trainer/Q2 Predictions Max                             76.5149\n",
      "trainer/Q2 Predictions Min                            -15.8047\n",
      "trainer/Q Targets Mean                                  9.85687\n",
      "trainer/Q Targets Std                                  21.6105\n",
      "trainer/Q Targets Max                                  78.9182\n",
      "trainer/Q Targets Min                                 -14.7749\n",
      "trainer/Log Pis Mean                                    6.73675\n",
      "trainer/Log Pis Std                                     5.6075\n",
      "trainer/Log Pis Max                                    24.194\n",
      "trainer/Log Pis Min                                    -5.87915\n",
      "trainer/Policy mu Mean                                  0.167854\n",
      "trainer/Policy mu Std                                   1.66197\n",
      "trainer/Policy mu Max                                   4.14433\n",
      "trainer/Policy mu Min                                  -5.83592\n",
      "trainer/Policy log std Mean                            -0.738525\n",
      "trainer/Policy log std Std                              0.278395\n",
      "trainer/Policy log std Max                              0.242376\n",
      "trainer/Policy log std Min                             -1.65641\n",
      "trainer/Alpha                                           0.0173844\n",
      "trainer/Alpha Loss                                      2.9865\n",
      "exploration/num steps total                        131000\n",
      "exploration/num paths total                           131\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.480216\n",
      "exploration/Rewards Std                                 0.864556\n",
      "exploration/Rewards Max                                 2.77224\n",
      "exploration/Rewards Min                                -2.20249\n",
      "exploration/Returns Mean                              480.216\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               480.216\n",
      "exploration/Returns Min                               480.216\n",
      "exploration/Actions Mean                                0.118188\n",
      "exploration/Actions Std                                 0.793736\n",
      "exploration/Actions Max                                 0.999884\n",
      "exploration/Actions Min                                -0.999998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           480.216\n",
      "exploration/env_infos/final/reward_run Mean             3.07176\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              3.07176\n",
      "exploration/env_infos/final/reward_run Min              3.07176\n",
      "exploration/env_infos/initial/reward_run Mean           0.986787\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.986787\n",
      "exploration/env_infos/initial/reward_run Min            0.986787\n",
      "exploration/env_infos/reward_run Mean                   1.95245\n",
      "exploration/env_infos/reward_run Std                    0.831792\n",
      "exploration/env_infos/reward_run Max                    4.30016\n",
      "exploration/env_infos/reward_run Min                   -1.18487\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.407126\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.407126\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.407126\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.359255\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.359255\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.359255\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.386391\n",
      "exploration/env_infos/reward_ctrl Std                   0.0806355\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0880255\n",
      "exploration/env_infos/reward_ctrl Min                  -0.559256\n",
      "exploration/env_infos/final/height Mean                -0.151503\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.151503\n",
      "exploration/env_infos/final/height Min                 -0.151503\n",
      "exploration/env_infos/initial/height Mean               0.0162426\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0162426\n",
      "exploration/env_infos/initial/height Min                0.0162426\n",
      "exploration/env_infos/height Mean                      -0.134386\n",
      "exploration/env_infos/height Std                        0.0909706\n",
      "exploration/env_infos/height Max                        0.149409\n",
      "exploration/env_infos/height Min                       -0.358758\n",
      "exploration/env_infos/final/reward_angular Mean         1.73312\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.73312\n",
      "exploration/env_infos/final/reward_angular Min          1.73312\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.835786\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.835786\n",
      "exploration/env_infos/initial/reward_angular Min       -0.835786\n",
      "exploration/env_infos/reward_angular Mean              -0.0555334\n",
      "exploration/env_infos/reward_angular Std                1.71639\n",
      "exploration/env_infos/reward_angular Max                5.01323\n",
      "exploration/env_infos/reward_angular Min               -5.7847\n",
      "evaluation/num steps total                              3.25e+06\n",
      "evaluation/num paths total                           3250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.480932\n",
      "evaluation/Rewards Std                                  1.5398\n",
      "evaluation/Rewards Max                                  8.25332\n",
      "evaluation/Rewards Min                                 -7.00725\n",
      "evaluation/Returns Mean                               480.932\n",
      "evaluation/Returns Std                                698.733\n",
      "evaluation/Returns Max                               2050.05\n",
      "evaluation/Returns Min                               -705.908\n",
      "evaluation/Actions Mean                                 0.10248\n",
      "evaluation/Actions Std                                  0.776102\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            480.932\n",
      "evaluation/env_infos/final/reward_run Mean             -1.44814\n",
      "evaluation/env_infos/final/reward_run Std               1.97702\n",
      "evaluation/env_infos/final/reward_run Max               1.51132\n",
      "evaluation/env_infos/final/reward_run Min              -4.83459\n",
      "evaluation/env_infos/initial/reward_run Mean            0.152055\n",
      "evaluation/env_infos/initial/reward_run Std             0.463931\n",
      "evaluation/env_infos/initial/reward_run Max             1.00486\n",
      "evaluation/env_infos/initial/reward_run Min            -0.85171\n",
      "evaluation/env_infos/reward_run Mean                   -1.23733\n",
      "evaluation/env_infos/reward_run Std                     2.25355\n",
      "evaluation/env_infos/reward_run Max                     4.66905\n",
      "evaluation/env_infos/reward_run Min                    -6.39411\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.361914\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.118216\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.163512\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.548135\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.30793\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0814048\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.145608\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.510981\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.367701\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106952\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.037596\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594706\n",
      "evaluation/env_infos/final/height Mean                 -0.126318\n",
      "evaluation/env_infos/final/height Std                   0.23561\n",
      "evaluation/env_infos/final/height Max                   0.300821\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00870407\n",
      "evaluation/env_infos/initial/height Std                 0.0414156\n",
      "evaluation/env_infos/initial/height Max                 0.0561217\n",
      "evaluation/env_infos/initial/height Min                -0.077623\n",
      "evaluation/env_infos/height Mean                       -0.120986\n",
      "evaluation/env_infos/height Std                         0.190218\n",
      "evaluation/env_infos/height Max                         0.600598\n",
      "evaluation/env_infos/height Min                        -0.580263\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.50957\n",
      "evaluation/env_infos/final/reward_angular Std           2.06972\n",
      "evaluation/env_infos/final/reward_angular Max           4.23738\n",
      "evaluation/env_infos/final/reward_angular Min          -5.28263\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.276666\n",
      "evaluation/env_infos/initial/reward_angular Std         1.13598\n",
      "evaluation/env_infos/initial/reward_angular Max         1.62753\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.9016\n",
      "evaluation/env_infos/reward_angular Mean                0.0979475\n",
      "evaluation/env_infos/reward_angular Std                 2.32491\n",
      "evaluation/env_infos/reward_angular Max                 9.50232\n",
      "evaluation/env_infos/reward_angular Min                -8.34776\n",
      "time/data storing (s)                                   0.0151382\n",
      "time/evaluation sampling (s)                           21.0862\n",
      "time/exploration sampling (s)                           1.02242\n",
      "time/logging (s)                                        0.247726\n",
      "time/saving (s)                                         0.0282648\n",
      "time/training (s)                                       4.11944\n",
      "time/epoch (s)                                         26.5192\n",
      "time/total (s)                                       3863.43\n",
      "Epoch                                                 129\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:16:57.919846 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 130 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 132000\n",
      "trainer/QF1 Loss                                        1.36786\n",
      "trainer/QF2 Loss                                        1.17509\n",
      "trainer/Policy Loss                                    -2.20822\n",
      "trainer/Q1 Predictions Mean                             7.70868\n",
      "trainer/Q1 Predictions Std                             20.4312\n",
      "trainer/Q1 Predictions Max                             81.0538\n",
      "trainer/Q1 Predictions Min                            -15.7623\n",
      "trainer/Q2 Predictions Mean                             7.77853\n",
      "trainer/Q2 Predictions Std                             20.5227\n",
      "trainer/Q2 Predictions Max                             83.031\n",
      "trainer/Q2 Predictions Min                            -16.3581\n",
      "trainer/Q Targets Mean                                  7.73475\n",
      "trainer/Q Targets Std                                  20.573\n",
      "trainer/Q Targets Max                                  82.427\n",
      "trainer/Q Targets Min                                 -15.8719\n",
      "trainer/Log Pis Mean                                    5.80557\n",
      "trainer/Log Pis Std                                     5.50075\n",
      "trainer/Log Pis Max                                    24.3109\n",
      "trainer/Log Pis Min                                    -4.1192\n",
      "trainer/Policy mu Mean                                  0.102952\n",
      "trainer/Policy mu Std                                   1.58118\n",
      "trainer/Policy mu Max                                   4.76863\n",
      "trainer/Policy mu Min                                  -4.84688\n",
      "trainer/Policy log std Mean                            -0.693069\n",
      "trainer/Policy log std Std                              0.274849\n",
      "trainer/Policy log std Max                              0.234723\n",
      "trainer/Policy log std Min                             -1.9558\n",
      "trainer/Alpha                                           0.0170895\n",
      "trainer/Alpha Loss                                     -0.791031\n",
      "exploration/num steps total                        132000\n",
      "exploration/num paths total                           132\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.486401\n",
      "exploration/Rewards Std                                 0.964336\n",
      "exploration/Rewards Max                                 3.4125\n",
      "exploration/Rewards Min                                -1.85523\n",
      "exploration/Returns Mean                              486.401\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               486.401\n",
      "exploration/Returns Min                               486.401\n",
      "exploration/Actions Mean                                0.183072\n",
      "exploration/Actions Std                                 0.72424\n",
      "exploration/Actions Max                                 0.999924\n",
      "exploration/Actions Min                                -0.99995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           486.401\n",
      "exploration/env_infos/final/reward_run Mean             2.05492\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.05492\n",
      "exploration/env_infos/final/reward_run Min              2.05492\n",
      "exploration/env_infos/initial/reward_run Mean           0.90294\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.90294\n",
      "exploration/env_infos/initial/reward_run Min            0.90294\n",
      "exploration/env_infos/reward_run Mean                   1.71643\n",
      "exploration/env_infos/reward_run Std                    0.755354\n",
      "exploration/env_infos/reward_run Max                    3.40577\n",
      "exploration/env_infos/reward_run Min                   -1.16333\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.427154\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.427154\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.427154\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.26932\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.26932\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.26932\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.334823\n",
      "exploration/env_infos/reward_ctrl Std                   0.106628\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0263998\n",
      "exploration/env_infos/reward_ctrl Min                  -0.58498\n",
      "exploration/env_infos/final/height Mean                -0.0359272\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0359272\n",
      "exploration/env_infos/final/height Min                 -0.0359272\n",
      "exploration/env_infos/initial/height Mean               0.03171\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.03171\n",
      "exploration/env_infos/initial/height Min                0.03171\n",
      "exploration/env_infos/height Mean                      -0.0762993\n",
      "exploration/env_infos/height Std                        0.0739537\n",
      "exploration/env_infos/height Max                        0.169158\n",
      "exploration/env_infos/height Min                       -0.235173\n",
      "exploration/env_infos/final/reward_angular Mean         1.12145\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.12145\n",
      "exploration/env_infos/final/reward_angular Min          1.12145\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.09336\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.09336\n",
      "exploration/env_infos/initial/reward_angular Min       -1.09336\n",
      "exploration/env_infos/reward_angular Mean              -0.0523392\n",
      "exploration/env_infos/reward_angular Std                1.74593\n",
      "exploration/env_infos/reward_angular Max                4.68419\n",
      "exploration/env_infos/reward_angular Min               -5.08605\n",
      "evaluation/num steps total                              3.275e+06\n",
      "evaluation/num paths total                           3275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.524718\n",
      "evaluation/Rewards Std                                  1.38024\n",
      "evaluation/Rewards Max                                  8.5967\n",
      "evaluation/Rewards Min                                 -7.17222\n",
      "evaluation/Returns Mean                               524.718\n",
      "evaluation/Returns Std                                770.968\n",
      "evaluation/Returns Max                               2175.96\n",
      "evaluation/Returns Min                               -697.84\n",
      "evaluation/Actions Mean                                 0.174898\n",
      "evaluation/Actions Std                                  0.754246\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            524.718\n",
      "evaluation/env_infos/final/reward_run Mean             -1.12831\n",
      "evaluation/env_infos/final/reward_run Std               2.21904\n",
      "evaluation/env_infos/final/reward_run Max               3.44802\n",
      "evaluation/env_infos/final/reward_run Min              -4.33025\n",
      "evaluation/env_infos/initial/reward_run Mean            0.249296\n",
      "evaluation/env_infos/initial/reward_run Std             0.480994\n",
      "evaluation/env_infos/initial/reward_run Max             0.986871\n",
      "evaluation/env_infos/initial/reward_run Min            -0.501423\n",
      "evaluation/env_infos/reward_run Mean                   -1.12864\n",
      "evaluation/env_infos/reward_run Std                     2.30964\n",
      "evaluation/env_infos/reward_run Max                     4.01423\n",
      "evaluation/env_infos/reward_run Min                    -6.37549\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.331123\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0918332\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.135149\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.5296\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.310609\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0908726\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.136151\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.490302\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.359686\n",
      "evaluation/env_infos/reward_ctrl Std                    0.098415\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0335749\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599605\n",
      "evaluation/env_infos/final/height Mean                 -0.167725\n",
      "evaluation/env_infos/final/height Std                   0.237276\n",
      "evaluation/env_infos/final/height Max                   0.257776\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0224623\n",
      "evaluation/env_infos/initial/height Std                 0.0494334\n",
      "evaluation/env_infos/initial/height Max                 0.0666848\n",
      "evaluation/env_infos/initial/height Min                -0.0998872\n",
      "evaluation/env_infos/height Mean                       -0.160564\n",
      "evaluation/env_infos/height Std                         0.219537\n",
      "evaluation/env_infos/height Max                         0.422291\n",
      "evaluation/env_infos/height Min                        -0.587017\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.440908\n",
      "evaluation/env_infos/final/reward_angular Std           2.06098\n",
      "evaluation/env_infos/final/reward_angular Max           3.26811\n",
      "evaluation/env_infos/final/reward_angular Min          -4.03636\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.490732\n",
      "evaluation/env_infos/initial/reward_angular Std         1.06901\n",
      "evaluation/env_infos/initial/reward_angular Max         2.95369\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.8238\n",
      "evaluation/env_infos/reward_angular Mean                0.0865139\n",
      "evaluation/env_infos/reward_angular Std                 2.15391\n",
      "evaluation/env_infos/reward_angular Max                 9.88928\n",
      "evaluation/env_infos/reward_angular Min                -7.13773\n",
      "time/data storing (s)                                   0.0151609\n",
      "time/evaluation sampling (s)                           24.0452\n",
      "time/exploration sampling (s)                           1.24853\n",
      "time/logging (s)                                        0.248799\n",
      "time/saving (s)                                         0.0277191\n",
      "time/training (s)                                       5.10151\n",
      "time/epoch (s)                                         30.6869\n",
      "time/total (s)                                       3895.12\n",
      "Epoch                                                 130\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:17:30.140141 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 131 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 133000\n",
      "trainer/QF1 Loss                                        1.46379\n",
      "trainer/QF2 Loss                                        1.63106\n",
      "trainer/Policy Loss                                    -1.66693\n",
      "trainer/Q1 Predictions Mean                             7.37873\n",
      "trainer/Q1 Predictions Std                             20.1044\n",
      "trainer/Q1 Predictions Max                             81.0246\n",
      "trainer/Q1 Predictions Min                            -15.0164\n",
      "trainer/Q2 Predictions Mean                             7.3582\n",
      "trainer/Q2 Predictions Std                             19.9787\n",
      "trainer/Q2 Predictions Max                             81.0438\n",
      "trainer/Q2 Predictions Min                            -15.7035\n",
      "trainer/Q Targets Mean                                  7.11913\n",
      "trainer/Q Targets Std                                  20.2208\n",
      "trainer/Q Targets Max                                  81.9598\n",
      "trainer/Q Targets Min                                 -15.7131\n",
      "trainer/Log Pis Mean                                    5.91552\n",
      "trainer/Log Pis Std                                     5.90063\n",
      "trainer/Log Pis Max                                    29.712\n",
      "trainer/Log Pis Min                                   -11.2073\n",
      "trainer/Policy mu Mean                                  0.0253854\n",
      "trainer/Policy mu Std                                   1.59712\n",
      "trainer/Policy mu Max                                   5.22005\n",
      "trainer/Policy mu Min                                  -5.89645\n",
      "trainer/Policy log std Mean                            -0.753793\n",
      "trainer/Policy log std Std                              0.279683\n",
      "trainer/Policy log std Max                              0.429522\n",
      "trainer/Policy log std Min                             -1.7448\n",
      "trainer/Alpha                                           0.0163945\n",
      "trainer/Alpha Loss                                     -0.347348\n",
      "exploration/num steps total                        133000\n",
      "exploration/num paths total                           133\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.40057\n",
      "exploration/Rewards Std                                 1.00639\n",
      "exploration/Rewards Max                                 4.60504\n",
      "exploration/Rewards Min                                -1.42026\n",
      "exploration/Returns Mean                             2400.57\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2400.57\n",
      "exploration/Returns Min                              2400.57\n",
      "exploration/Actions Mean                                0.122733\n",
      "exploration/Actions Std                                 0.889898\n",
      "exploration/Actions Max                                 0.999994\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2400.57\n",
      "exploration/env_infos/final/reward_run Mean            -4.16098\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -4.16098\n",
      "exploration/env_infos/final/reward_run Min             -4.16098\n",
      "exploration/env_infos/initial/reward_run Mean           0.159524\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.159524\n",
      "exploration/env_infos/initial/reward_run Min            0.159524\n",
      "exploration/env_infos/reward_run Mean                  -3.3943\n",
      "exploration/env_infos/reward_run Std                    1.25686\n",
      "exploration/env_infos/reward_run Max                    1.08448\n",
      "exploration/env_infos/reward_run Min                   -6.62095\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.451553\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.451553\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.451553\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.462401\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.462401\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.462401\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.484189\n",
      "exploration/env_infos/reward_ctrl Std                   0.0748826\n",
      "exploration/env_infos/reward_ctrl Max                  -0.204886\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596808\n",
      "exploration/env_infos/final/height Mean                -0.0183218\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0183218\n",
      "exploration/env_infos/final/height Min                 -0.0183218\n",
      "exploration/env_infos/initial/height Mean              -0.0541142\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0541142\n",
      "exploration/env_infos/initial/height Min               -0.0541142\n",
      "exploration/env_infos/height Mean                       0.0217263\n",
      "exploration/env_infos/height Std                        0.146349\n",
      "exploration/env_infos/height Max                        0.468506\n",
      "exploration/env_infos/height Min                       -0.371462\n",
      "exploration/env_infos/final/reward_angular Mean        -1.62768\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.62768\n",
      "exploration/env_infos/final/reward_angular Min         -1.62768\n",
      "exploration/env_infos/initial/reward_angular Mean       2.31431\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.31431\n",
      "exploration/env_infos/initial/reward_angular Min        2.31431\n",
      "exploration/env_infos/reward_angular Mean               0.0704752\n",
      "exploration/env_infos/reward_angular Std                3.21971\n",
      "exploration/env_infos/reward_angular Max                9.59204\n",
      "exploration/env_infos/reward_angular Min               -7.80595\n",
      "evaluation/num steps total                              3.3e+06\n",
      "evaluation/num paths total                           3300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.367903\n",
      "evaluation/Rewards Std                                  1.27154\n",
      "evaluation/Rewards Max                                  7.68204\n",
      "evaluation/Rewards Min                                 -6.55098\n",
      "evaluation/Returns Mean                               367.903\n",
      "evaluation/Returns Std                                656.434\n",
      "evaluation/Returns Max                               2238.12\n",
      "evaluation/Returns Min                               -428.48\n",
      "evaluation/Actions Mean                                 0.0606173\n",
      "evaluation/Actions Std                                  0.728011\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            367.903\n",
      "evaluation/env_infos/final/reward_run Mean              0.0734917\n",
      "evaluation/env_infos/final/reward_run Std               1.52834\n",
      "evaluation/env_infos/final/reward_run Max               2.66419\n",
      "evaluation/env_infos/final/reward_run Min              -3.58564\n",
      "evaluation/env_infos/initial/reward_run Mean            0.135247\n",
      "evaluation/env_infos/initial/reward_run Std             0.463533\n",
      "evaluation/env_infos/initial/reward_run Max             1.1152\n",
      "evaluation/env_infos/initial/reward_run Min            -0.659534\n",
      "evaluation/env_infos/reward_run Mean                   -0.366287\n",
      "evaluation/env_infos/reward_run Std                     1.97519\n",
      "evaluation/env_infos/reward_run Max                     5.05546\n",
      "evaluation/env_infos/reward_run Min                    -6.73148\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.270278\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.122466\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0824576\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.559729\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.317968\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0873481\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.110795\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.448065\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.320205\n",
      "evaluation/env_infos/reward_ctrl Std                    0.1272\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0333793\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597073\n",
      "evaluation/env_infos/final/height Mean                 -0.280127\n",
      "evaluation/env_infos/final/height Std                   0.235215\n",
      "evaluation/env_infos/final/height Max                   0.090055\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00896022\n",
      "evaluation/env_infos/initial/height Std                 0.0457535\n",
      "evaluation/env_infos/initial/height Max                 0.0783736\n",
      "evaluation/env_infos/initial/height Min                -0.10802\n",
      "evaluation/env_infos/height Mean                       -0.182443\n",
      "evaluation/env_infos/height Std                         0.238187\n",
      "evaluation/env_infos/height Max                         0.561228\n",
      "evaluation/env_infos/height Min                        -0.584745\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.531748\n",
      "evaluation/env_infos/final/reward_angular Std           1.30877\n",
      "evaluation/env_infos/final/reward_angular Max           1.82774\n",
      "evaluation/env_infos/final/reward_angular Min          -3.64249\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.602554\n",
      "evaluation/env_infos/initial/reward_angular Std         0.912909\n",
      "evaluation/env_infos/initial/reward_angular Max         1.15393\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.23147\n",
      "evaluation/env_infos/reward_angular Mean                0.021106\n",
      "evaluation/env_infos/reward_angular Std                 1.98788\n",
      "evaluation/env_infos/reward_angular Max                10.083\n",
      "evaluation/env_infos/reward_angular Min                -7.73945\n",
      "time/data storing (s)                                   0.0156704\n",
      "time/evaluation sampling (s)                           25.7425\n",
      "time/exploration sampling (s)                           1.05518\n",
      "time/logging (s)                                        0.247865\n",
      "time/saving (s)                                         0.0291985\n",
      "time/training (s)                                       4.07294\n",
      "time/epoch (s)                                         31.1634\n",
      "time/total (s)                                       3927.34\n",
      "Epoch                                                 131\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:18:09.115535 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 132 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 134000\n",
      "trainer/QF1 Loss                                        1.75163\n",
      "trainer/QF2 Loss                                        1.59898\n",
      "trainer/Policy Loss                                    -3.29814\n",
      "trainer/Q1 Predictions Mean                             7.94119\n",
      "trainer/Q1 Predictions Std                             19.5471\n",
      "trainer/Q1 Predictions Max                             76.5421\n",
      "trainer/Q1 Predictions Min                            -15.0495\n",
      "trainer/Q2 Predictions Mean                             8.00148\n",
      "trainer/Q2 Predictions Std                             19.4271\n",
      "trainer/Q2 Predictions Max                             75.9142\n",
      "trainer/Q2 Predictions Min                            -14.6471\n",
      "trainer/Q Targets Mean                                  7.7446\n",
      "trainer/Q Targets Std                                  19.5172\n",
      "trainer/Q Targets Max                                  76.705\n",
      "trainer/Q Targets Min                                 -15.45\n",
      "trainer/Log Pis Mean                                    4.782\n",
      "trainer/Log Pis Std                                     5.22943\n",
      "trainer/Log Pis Max                                    25.2156\n",
      "trainer/Log Pis Min                                    -4.64981\n",
      "trainer/Policy mu Mean                                  0.0337524\n",
      "trainer/Policy mu Std                                   1.48588\n",
      "trainer/Policy mu Max                                   6.11491\n",
      "trainer/Policy mu Min                                  -5.73969\n",
      "trainer/Policy log std Mean                            -0.722012\n",
      "trainer/Policy log std Std                              0.241138\n",
      "trainer/Policy log std Max                              0.155256\n",
      "trainer/Policy log std Min                             -2.0529\n",
      "trainer/Alpha                                           0.0175163\n",
      "trainer/Alpha Loss                                     -4.92569\n",
      "exploration/num steps total                        134000\n",
      "exploration/num paths total                           134\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.10297\n",
      "exploration/Rewards Std                                 2.3656\n",
      "exploration/Rewards Max                                 7.03543\n",
      "exploration/Rewards Min                                -5.70731\n",
      "exploration/Returns Mean                             2102.97\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2102.97\n",
      "exploration/Returns Min                              2102.97\n",
      "exploration/Actions Mean                                0.0292304\n",
      "exploration/Actions Std                                 0.851105\n",
      "exploration/Actions Max                                 0.999928\n",
      "exploration/Actions Min                                -0.999995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2102.97\n",
      "exploration/env_infos/final/reward_run Mean            -1.83685\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -1.83685\n",
      "exploration/env_infos/final/reward_run Min             -1.83685\n",
      "exploration/env_infos/initial/reward_run Mean          -0.183737\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.183737\n",
      "exploration/env_infos/initial/reward_run Min           -0.183737\n",
      "exploration/env_infos/reward_run Mean                  -3.48318\n",
      "exploration/env_infos/reward_run Std                    1.23153\n",
      "exploration/env_infos/reward_run Max                    0.371858\n",
      "exploration/env_infos/reward_run Min                   -6.81594\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.469113\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.469113\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.469113\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.277909\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.277909\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.277909\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.43514\n",
      "exploration/env_infos/reward_ctrl Std                   0.0860033\n",
      "exploration/env_infos/reward_ctrl Max                  -0.080027\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590105\n",
      "exploration/env_infos/final/height Mean                 0.0224347\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0224347\n",
      "exploration/env_infos/final/height Min                  0.0224347\n",
      "exploration/env_infos/initial/height Mean               0.0419655\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0419655\n",
      "exploration/env_infos/initial/height Min                0.0419655\n",
      "exploration/env_infos/height Mean                      -0.038617\n",
      "exploration/env_infos/height Std                        0.12529\n",
      "exploration/env_infos/height Max                        0.375813\n",
      "exploration/env_infos/height Min                       -0.342225\n",
      "exploration/env_infos/final/reward_angular Mean         0.278497\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.278497\n",
      "exploration/env_infos/final/reward_angular Min          0.278497\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.778054\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.778054\n",
      "exploration/env_infos/initial/reward_angular Min       -0.778054\n",
      "exploration/env_infos/reward_angular Mean               0.101865\n",
      "exploration/env_infos/reward_angular Std                3.17059\n",
      "exploration/env_infos/reward_angular Max                9.63936\n",
      "exploration/env_infos/reward_angular Min               -5.66785\n",
      "evaluation/num steps total                              3.325e+06\n",
      "evaluation/num paths total                           3325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.512292\n",
      "evaluation/Rewards Std                                  1.40951\n",
      "evaluation/Rewards Max                                  7.53589\n",
      "evaluation/Rewards Min                                 -6.65071\n",
      "evaluation/Returns Mean                               512.292\n",
      "evaluation/Returns Std                                689.411\n",
      "evaluation/Returns Max                               2208.17\n",
      "evaluation/Returns Min                               -460.428\n",
      "evaluation/Actions Mean                                 0.0839064\n",
      "evaluation/Actions Std                                  0.753523\n",
      "evaluation/Actions Max                                  0.999947\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            512.292\n",
      "evaluation/env_infos/final/reward_run Mean             -0.703322\n",
      "evaluation/env_infos/final/reward_run Std               2.33581\n",
      "evaluation/env_infos/final/reward_run Max               2.40043\n",
      "evaluation/env_infos/final/reward_run Min              -5.77667\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0604616\n",
      "evaluation/env_infos/initial/reward_run Std             0.532032\n",
      "evaluation/env_infos/initial/reward_run Max             1.02886\n",
      "evaluation/env_infos/initial/reward_run Min            -1.12425\n",
      "evaluation/env_infos/reward_run Mean                   -0.660669\n",
      "evaluation/env_infos/reward_run Std                     2.31627\n",
      "evaluation/env_infos/reward_run Max                     5.13523\n",
      "evaluation/env_infos/reward_run Min                    -7.15706\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.318106\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.108701\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0947056\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.517537\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.283515\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.088382\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.102384\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.52636\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.344902\n",
      "evaluation/env_infos/reward_ctrl Std                    0.103731\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0268697\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595907\n",
      "evaluation/env_infos/final/height Mean                 -0.257458\n",
      "evaluation/env_infos/final/height Std                   0.262945\n",
      "evaluation/env_infos/final/height Max                   0.336282\n",
      "evaluation/env_infos/final/height Min                  -0.577277\n",
      "evaluation/env_infos/initial/height Mean               -0.0114906\n",
      "evaluation/env_infos/initial/height Std                 0.0480448\n",
      "evaluation/env_infos/initial/height Max                 0.0818266\n",
      "evaluation/env_infos/initial/height Min                -0.0834938\n",
      "evaluation/env_infos/height Mean                       -0.171501\n",
      "evaluation/env_infos/height Std                         0.226173\n",
      "evaluation/env_infos/height Max                         0.525622\n",
      "evaluation/env_infos/height Min                        -0.588485\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.531434\n",
      "evaluation/env_infos/final/reward_angular Std           1.47678\n",
      "evaluation/env_infos/final/reward_angular Max           2.46478\n",
      "evaluation/env_infos/final/reward_angular Min          -3.81034\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0856623\n",
      "evaluation/env_infos/initial/reward_angular Std         0.917803\n",
      "evaluation/env_infos/initial/reward_angular Max         2.55065\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.55377\n",
      "evaluation/env_infos/reward_angular Mean                0.041084\n",
      "evaluation/env_infos/reward_angular Std                 2.1588\n",
      "evaluation/env_infos/reward_angular Max                 9.53881\n",
      "evaluation/env_infos/reward_angular Min                -7.0844\n",
      "time/data storing (s)                                   0.0168462\n",
      "time/evaluation sampling (s)                           31.1473\n",
      "time/exploration sampling (s)                           1.20976\n",
      "time/logging (s)                                        0.251712\n",
      "time/saving (s)                                         0.028244\n",
      "time/training (s)                                       5.30124\n",
      "time/epoch (s)                                         37.9551\n",
      "time/total (s)                                       3966.31\n",
      "Epoch                                                 132\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:18:37.843269 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 133 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 135000\n",
      "trainer/QF1 Loss                                        1.35279\n",
      "trainer/QF2 Loss                                        1.26614\n",
      "trainer/Policy Loss                                    -1.30273\n",
      "trainer/Q1 Predictions Mean                             6.73937\n",
      "trainer/Q1 Predictions Std                             18.6669\n",
      "trainer/Q1 Predictions Max                             81.9649\n",
      "trainer/Q1 Predictions Min                            -15.8615\n",
      "trainer/Q2 Predictions Mean                             6.51294\n",
      "trainer/Q2 Predictions Std                             18.4699\n",
      "trainer/Q2 Predictions Max                             78.5974\n",
      "trainer/Q2 Predictions Min                            -15.3542\n",
      "trainer/Q Targets Mean                                  6.83959\n",
      "trainer/Q Targets Std                                  18.5853\n",
      "trainer/Q Targets Max                                  78.9599\n",
      "trainer/Q Targets Min                                 -15.3524\n",
      "trainer/Log Pis Mean                                    5.4781\n",
      "trainer/Log Pis Std                                     5.65161\n",
      "trainer/Log Pis Max                                    23.1576\n",
      "trainer/Log Pis Min                                    -6.1653\n",
      "trainer/Policy mu Mean                                  0.21258\n",
      "trainer/Policy mu Std                                   1.56899\n",
      "trainer/Policy mu Max                                   4.39309\n",
      "trainer/Policy mu Min                                  -4.73524\n",
      "trainer/Policy log std Mean                            -0.688651\n",
      "trainer/Policy log std Std                              0.262471\n",
      "trainer/Policy log std Max                              0.157316\n",
      "trainer/Policy log std Min                             -1.67651\n",
      "trainer/Alpha                                           0.0184621\n",
      "trainer/Alpha Loss                                     -2.08374\n",
      "exploration/num steps total                        135000\n",
      "exploration/num paths total                           135\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.231871\n",
      "exploration/Rewards Std                                 1.47132\n",
      "exploration/Rewards Max                                 3.94154\n",
      "exploration/Rewards Min                                -4.70136\n",
      "exploration/Returns Mean                              231.871\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               231.871\n",
      "exploration/Returns Min                               231.871\n",
      "exploration/Actions Mean                                0.130098\n",
      "exploration/Actions Std                                 0.766554\n",
      "exploration/Actions Max                                 0.999692\n",
      "exploration/Actions Min                                -0.999994\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           231.871\n",
      "exploration/env_infos/final/reward_run Mean             2.66387\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.66387\n",
      "exploration/env_infos/final/reward_run Min              2.66387\n",
      "exploration/env_infos/initial/reward_run Mean           0.307834\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.307834\n",
      "exploration/env_infos/initial/reward_run Min            0.307834\n",
      "exploration/env_infos/reward_run Mean                   1.88863\n",
      "exploration/env_infos/reward_run Std                    0.892451\n",
      "exploration/env_infos/reward_run Max                    4.75983\n",
      "exploration/env_infos/reward_run Min                   -0.910307\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.334443\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.334443\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.334443\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21867\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21867\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21867\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.362718\n",
      "exploration/env_infos/reward_ctrl Std                   0.0955826\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0464173\n",
      "exploration/env_infos/reward_ctrl Min                  -0.581323\n",
      "exploration/env_infos/final/height Mean                -0.197933\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.197933\n",
      "exploration/env_infos/final/height Min                 -0.197933\n",
      "exploration/env_infos/initial/height Mean              -0.0913336\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0913336\n",
      "exploration/env_infos/initial/height Min               -0.0913336\n",
      "exploration/env_infos/height Mean                      -0.14908\n",
      "exploration/env_infos/height Std                        0.0813267\n",
      "exploration/env_infos/height Max                        0.111715\n",
      "exploration/env_infos/height Min                       -0.329352\n",
      "exploration/env_infos/final/reward_angular Mean         1.24916\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.24916\n",
      "exploration/env_infos/final/reward_angular Min          1.24916\n",
      "exploration/env_infos/initial/reward_angular Mean      -2.41414\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -2.41414\n",
      "exploration/env_infos/initial/reward_angular Min       -2.41414\n",
      "exploration/env_infos/reward_angular Mean              -0.0878365\n",
      "exploration/env_infos/reward_angular Std                1.54794\n",
      "exploration/env_infos/reward_angular Max                5.17129\n",
      "exploration/env_infos/reward_angular Min               -3.85655\n",
      "evaluation/num steps total                              3.35e+06\n",
      "evaluation/num paths total                           3350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.410304\n",
      "evaluation/Rewards Std                                  1.26727\n",
      "evaluation/Rewards Max                                  7.74305\n",
      "evaluation/Rewards Min                                 -8.30244\n",
      "evaluation/Returns Mean                               410.304\n",
      "evaluation/Returns Std                                638.4\n",
      "evaluation/Returns Max                               2143.65\n",
      "evaluation/Returns Min                               -415.597\n",
      "evaluation/Actions Mean                                 0.0965217\n",
      "evaluation/Actions Std                                  0.779522\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            410.304\n",
      "evaluation/env_infos/final/reward_run Mean             -0.220202\n",
      "evaluation/env_infos/final/reward_run Std               1.87768\n",
      "evaluation/env_infos/final/reward_run Max               3.11003\n",
      "evaluation/env_infos/final/reward_run Min              -5.49147\n",
      "evaluation/env_infos/initial/reward_run Mean            0.157076\n",
      "evaluation/env_infos/initial/reward_run Std             0.509298\n",
      "evaluation/env_infos/initial/reward_run Max             1.01033\n",
      "evaluation/env_infos/initial/reward_run Min            -0.842851\n",
      "evaluation/env_infos/reward_run Mean                   -0.572181\n",
      "evaluation/env_infos/reward_run Std                     2.18506\n",
      "evaluation/env_infos/reward_run Max                     5.03016\n",
      "evaluation/env_infos/reward_run Min                    -7.1525\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.358389\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10675\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.181974\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.565148\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.325315\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0851462\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.156968\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.456167\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.370182\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109348\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0512386\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595803\n",
      "evaluation/env_infos/final/height Mean                 -0.199992\n",
      "evaluation/env_infos/final/height Std                   0.243632\n",
      "evaluation/env_infos/final/height Max                   0.190246\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0135313\n",
      "evaluation/env_infos/initial/height Std                 0.0538888\n",
      "evaluation/env_infos/initial/height Max                 0.0816488\n",
      "evaluation/env_infos/initial/height Min                -0.0892445\n",
      "evaluation/env_infos/height Mean                       -0.146772\n",
      "evaluation/env_infos/height Std                         0.223909\n",
      "evaluation/env_infos/height Max                         0.570877\n",
      "evaluation/env_infos/height Min                        -0.586563\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00243469\n",
      "evaluation/env_infos/final/reward_angular Std           1.72189\n",
      "evaluation/env_infos/final/reward_angular Max           2.45147\n",
      "evaluation/env_infos/final/reward_angular Min          -4.9144\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.241339\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11681\n",
      "evaluation/env_infos/initial/reward_angular Max         2.36766\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.81392\n",
      "evaluation/env_infos/reward_angular Mean                0.0464375\n",
      "evaluation/env_infos/reward_angular Std                 2.04084\n",
      "evaluation/env_infos/reward_angular Max                10.543\n",
      "evaluation/env_infos/reward_angular Min                -7.07399\n",
      "time/data storing (s)                                   0.0173687\n",
      "time/evaluation sampling (s)                           21.7571\n",
      "time/exploration sampling (s)                           1.24909\n",
      "time/logging (s)                                        0.257077\n",
      "time/saving (s)                                         0.030054\n",
      "time/training (s)                                       4.35386\n",
      "time/epoch (s)                                         27.6645\n",
      "time/total (s)                                       3995.04\n",
      "Epoch                                                 133\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:19:11.264575 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 134 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 136000\n",
      "trainer/QF1 Loss                                        1.55583\n",
      "trainer/QF2 Loss                                        1.33684\n",
      "trainer/Policy Loss                                    -4.39135\n",
      "trainer/Q1 Predictions Mean                            10.1287\n",
      "trainer/Q1 Predictions Std                             21.6095\n",
      "trainer/Q1 Predictions Max                             83.8691\n",
      "trainer/Q1 Predictions Min                            -14.3974\n",
      "trainer/Q2 Predictions Mean                            10.1103\n",
      "trainer/Q2 Predictions Std                             21.7188\n",
      "trainer/Q2 Predictions Max                             85.1187\n",
      "trainer/Q2 Predictions Min                            -14.2846\n",
      "trainer/Q Targets Mean                                 10.3053\n",
      "trainer/Q Targets Std                                  22.0847\n",
      "trainer/Q Targets Max                                  84.7486\n",
      "trainer/Q Targets Min                                 -15.2493\n",
      "trainer/Log Pis Mean                                    5.99786\n",
      "trainer/Log Pis Std                                     6.07423\n",
      "trainer/Log Pis Max                                    29.918\n",
      "trainer/Log Pis Min                                    -5.64663\n",
      "trainer/Policy mu Mean                                 -0.063027\n",
      "trainer/Policy mu Std                                   1.63175\n",
      "trainer/Policy mu Max                                   4.23963\n",
      "trainer/Policy mu Min                                  -6.66582\n",
      "trainer/Policy log std Mean                            -0.713829\n",
      "trainer/Policy log std Std                              0.282911\n",
      "trainer/Policy log std Max                              0.289895\n",
      "trainer/Policy log std Min                             -1.88849\n",
      "trainer/Alpha                                           0.0169214\n",
      "trainer/Alpha Loss                                     -0.00873235\n",
      "exploration/num steps total                        136000\n",
      "exploration/num paths total                           136\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.13161\n",
      "exploration/Rewards Std                                 1.58727\n",
      "exploration/Rewards Max                                 6.34089\n",
      "exploration/Rewards Min                                -2.25631\n",
      "exploration/Returns Mean                             2131.61\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2131.61\n",
      "exploration/Returns Min                              2131.61\n",
      "exploration/Actions Mean                                0.0514526\n",
      "exploration/Actions Std                                 0.879972\n",
      "exploration/Actions Max                                 0.99999\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2131.61\n",
      "exploration/env_infos/final/reward_run Mean            -4.06718\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -4.06718\n",
      "exploration/env_infos/final/reward_run Min             -4.06718\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0987867\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0987867\n",
      "exploration/env_infos/initial/reward_run Min           -0.0987867\n",
      "exploration/env_infos/reward_run Mean                  -3.8751\n",
      "exploration/env_infos/reward_run Std                    1.27144\n",
      "exploration/env_infos/reward_run Max                    0.959781\n",
      "exploration/env_infos/reward_run Min                   -6.7307\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.455566\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.455566\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.455566\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.249898\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.249898\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.249898\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.466199\n",
      "exploration/env_infos/reward_ctrl Std                   0.0712931\n",
      "exploration/env_infos/reward_ctrl Max                  -0.182692\n",
      "exploration/env_infos/reward_ctrl Min                  -0.597174\n",
      "exploration/env_infos/final/height Mean                 0.00243636\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.00243636\n",
      "exploration/env_infos/final/height Min                  0.00243636\n",
      "exploration/env_infos/initial/height Mean               0.076829\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.076829\n",
      "exploration/env_infos/initial/height Min                0.076829\n",
      "exploration/env_infos/height Mean                       0.00451085\n",
      "exploration/env_infos/height Std                        0.131039\n",
      "exploration/env_infos/height Max                        0.530685\n",
      "exploration/env_infos/height Min                       -0.320927\n",
      "exploration/env_infos/final/reward_angular Mean         2.40188\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.40188\n",
      "exploration/env_infos/final/reward_angular Min          2.40188\n",
      "exploration/env_infos/initial/reward_angular Mean       0.929268\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.929268\n",
      "exploration/env_infos/initial/reward_angular Min        0.929268\n",
      "exploration/env_infos/reward_angular Mean               0.0776842\n",
      "exploration/env_infos/reward_angular Std                3.17664\n",
      "exploration/env_infos/reward_angular Max                9.02258\n",
      "exploration/env_infos/reward_angular Min               -6.20118\n",
      "evaluation/num steps total                              3.375e+06\n",
      "evaluation/num paths total                           3375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.274433\n",
      "evaluation/Rewards Std                                  1.1855\n",
      "evaluation/Rewards Max                                  9.27565\n",
      "evaluation/Rewards Min                                 -7.80785\n",
      "evaluation/Returns Mean                               274.433\n",
      "evaluation/Returns Std                                720.795\n",
      "evaluation/Returns Max                               2048.92\n",
      "evaluation/Returns Min                               -510.502\n",
      "evaluation/Actions Mean                                 0.0458755\n",
      "evaluation/Actions Std                                  0.711994\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            274.433\n",
      "evaluation/env_infos/final/reward_run Mean             -0.065101\n",
      "evaluation/env_infos/final/reward_run Std               1.71809\n",
      "evaluation/env_infos/final/reward_run Max               2.79587\n",
      "evaluation/env_infos/final/reward_run Min              -5.18557\n",
      "evaluation/env_infos/initial/reward_run Mean           -0.158794\n",
      "evaluation/env_infos/initial/reward_run Std             0.513747\n",
      "evaluation/env_infos/initial/reward_run Max             0.987695\n",
      "evaluation/env_infos/initial/reward_run Min            -1.07919\n",
      "evaluation/env_infos/reward_run Mean                   -0.21322\n",
      "evaluation/env_infos/reward_run Std                     1.93883\n",
      "evaluation/env_infos/reward_run Max                     5.0567\n",
      "evaluation/env_infos/reward_run Min                    -7.17236\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.280921\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10518\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.12257\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.461739\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.263632\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0914218\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0848778\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.47658\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.305424\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113479\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0205967\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599733\n",
      "evaluation/env_infos/final/height Mean                 -0.325641\n",
      "evaluation/env_infos/final/height Std                   0.250155\n",
      "evaluation/env_infos/final/height Max                   0.0471154\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00340425\n",
      "evaluation/env_infos/initial/height Std                 0.0535563\n",
      "evaluation/env_infos/initial/height Max                 0.0890264\n",
      "evaluation/env_infos/initial/height Min                -0.118934\n",
      "evaluation/env_infos/height Mean                       -0.275005\n",
      "evaluation/env_infos/height Std                         0.265134\n",
      "evaluation/env_infos/height Max                         0.522578\n",
      "evaluation/env_infos/height Min                        -0.599287\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.238851\n",
      "evaluation/env_infos/final/reward_angular Std           1.2978\n",
      "evaluation/env_infos/final/reward_angular Max           3.91544\n",
      "evaluation/env_infos/final/reward_angular Min          -2.69816\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0282944\n",
      "evaluation/env_infos/initial/reward_angular Std         1.08216\n",
      "evaluation/env_infos/initial/reward_angular Max         3.03384\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.6154\n",
      "evaluation/env_infos/reward_angular Mean                0.00581406\n",
      "evaluation/env_infos/reward_angular Std                 1.75616\n",
      "evaluation/env_infos/reward_angular Max                11.2716\n",
      "evaluation/env_infos/reward_angular Min                -7.0745\n",
      "time/data storing (s)                                   0.0173194\n",
      "time/evaluation sampling (s)                           23.9727\n",
      "time/exploration sampling (s)                           1.3168\n",
      "time/logging (s)                                        0.277634\n",
      "time/saving (s)                                         0.0315487\n",
      "time/training (s)                                       6.75951\n",
      "time/epoch (s)                                         32.3755\n",
      "time/total (s)                                       4028.48\n",
      "Epoch                                                 134\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:19:41.289738 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 135 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 137000\n",
      "trainer/QF1 Loss                                        2.61381\n",
      "trainer/QF2 Loss                                        1.93082\n",
      "trainer/Policy Loss                                    -5.68787\n",
      "trainer/Q1 Predictions Mean                            11.3382\n",
      "trainer/Q1 Predictions Std                             22.6018\n",
      "trainer/Q1 Predictions Max                             89.9671\n",
      "trainer/Q1 Predictions Min                            -15.6159\n",
      "trainer/Q2 Predictions Mean                            11.297\n",
      "trainer/Q2 Predictions Std                             22.5976\n",
      "trainer/Q2 Predictions Max                             90.1033\n",
      "trainer/Q2 Predictions Min                            -16.0369\n",
      "trainer/Q Targets Mean                                 11.4656\n",
      "trainer/Q Targets Std                                  22.8641\n",
      "trainer/Q Targets Max                                  87.7135\n",
      "trainer/Q Targets Min                                 -15.22\n",
      "trainer/Log Pis Mean                                    5.96005\n",
      "trainer/Log Pis Std                                     5.66579\n",
      "trainer/Log Pis Max                                    27.9073\n",
      "trainer/Log Pis Min                                    -5.63002\n",
      "trainer/Policy mu Mean                                  0.154121\n",
      "trainer/Policy mu Std                                   1.58395\n",
      "trainer/Policy mu Max                                   5.32575\n",
      "trainer/Policy mu Min                                  -4.90219\n",
      "trainer/Policy log std Mean                            -0.751396\n",
      "trainer/Policy log std Std                              0.27295\n",
      "trainer/Policy log std Max                              0.0902745\n",
      "trainer/Policy log std Min                             -1.79451\n",
      "trainer/Alpha                                           0.0169139\n",
      "trainer/Alpha Loss                                     -0.162942\n",
      "exploration/num steps total                        137000\n",
      "exploration/num paths total                           137\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.19875\n",
      "exploration/Rewards Std                                 1.24372\n",
      "exploration/Rewards Max                                 5.07183\n",
      "exploration/Rewards Min                                -2.93602\n",
      "exploration/Returns Mean                             1198.75\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1198.75\n",
      "exploration/Returns Min                              1198.75\n",
      "exploration/Actions Mean                                0.171103\n",
      "exploration/Actions Std                                 0.827405\n",
      "exploration/Actions Max                                 0.999927\n",
      "exploration/Actions Min                                -0.999932\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1198.75\n",
      "exploration/env_infos/final/reward_run Mean             2.36122\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.36122\n",
      "exploration/env_infos/final/reward_run Min              2.36122\n",
      "exploration/env_infos/initial/reward_run Mean           1.0361\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.0361\n",
      "exploration/env_infos/initial/reward_run Min            1.0361\n",
      "exploration/env_infos/reward_run Mean                   2.29976\n",
      "exploration/env_infos/reward_run Std                    0.905546\n",
      "exploration/env_infos/reward_run Max                    4.41517\n",
      "exploration/env_infos/reward_run Min                   -0.674583\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.446877\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.446877\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.446877\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.329331\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.329331\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.329331\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.428325\n",
      "exploration/env_infos/reward_ctrl Std                   0.0842545\n",
      "exploration/env_infos/reward_ctrl Max                  -0.167514\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596559\n",
      "exploration/env_infos/final/height Mean                 0.0228048\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0228048\n",
      "exploration/env_infos/final/height Min                  0.0228048\n",
      "exploration/env_infos/initial/height Mean               0.0562696\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0562696\n",
      "exploration/env_infos/initial/height Min                0.0562696\n",
      "exploration/env_infos/height Mean                      -0.129401\n",
      "exploration/env_infos/height Std                        0.101149\n",
      "exploration/env_infos/height Max                        0.25319\n",
      "exploration/env_infos/height Min                       -0.358344\n",
      "exploration/env_infos/final/reward_angular Mean        -1.12971\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.12971\n",
      "exploration/env_infos/final/reward_angular Min         -1.12971\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.471229\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.471229\n",
      "exploration/env_infos/initial/reward_angular Min       -0.471229\n",
      "exploration/env_infos/reward_angular Mean              -0.0797438\n",
      "exploration/env_infos/reward_angular Std                1.90541\n",
      "exploration/env_infos/reward_angular Max                5.88368\n",
      "exploration/env_infos/reward_angular Min               -5.89562\n",
      "evaluation/num steps total                              3.4e+06\n",
      "evaluation/num paths total                           3400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.353113\n",
      "evaluation/Rewards Std                                  1.34011\n",
      "evaluation/Rewards Max                                  7.58625\n",
      "evaluation/Rewards Min                                 -9.38281\n",
      "evaluation/Returns Mean                               353.113\n",
      "evaluation/Returns Std                                520.592\n",
      "evaluation/Returns Max                               1765.79\n",
      "evaluation/Returns Min                               -368.287\n",
      "evaluation/Actions Mean                                 0.0723195\n",
      "evaluation/Actions Std                                  0.741508\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            353.113\n",
      "evaluation/env_infos/final/reward_run Mean             -0.119304\n",
      "evaluation/env_infos/final/reward_run Std               1.78942\n",
      "evaluation/env_infos/final/reward_run Max               2.62605\n",
      "evaluation/env_infos/final/reward_run Min              -4.96415\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0982348\n",
      "evaluation/env_infos/initial/reward_run Std             0.544926\n",
      "evaluation/env_infos/initial/reward_run Max             0.974831\n",
      "evaluation/env_infos/initial/reward_run Min            -0.755997\n",
      "evaluation/env_infos/reward_run Mean                   -0.335502\n",
      "evaluation/env_infos/reward_run Std                     2.31638\n",
      "evaluation/env_infos/reward_run Max                     5.39861\n",
      "evaluation/env_infos/reward_run Min                    -7.63243\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.2935\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.139607\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0597687\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.555584\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.303624\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.101851\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0648033\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.546544\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.333039\n",
      "evaluation/env_infos/reward_ctrl Std                    0.123279\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0148653\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597975\n",
      "evaluation/env_infos/final/height Mean                 -0.221204\n",
      "evaluation/env_infos/final/height Std                   0.265181\n",
      "evaluation/env_infos/final/height Max                   0.394799\n",
      "evaluation/env_infos/final/height Min                  -0.577285\n",
      "evaluation/env_infos/initial/height Mean               -0.0244577\n",
      "evaluation/env_infos/initial/height Std                 0.0490311\n",
      "evaluation/env_infos/initial/height Max                 0.064169\n",
      "evaluation/env_infos/initial/height Min                -0.115386\n",
      "evaluation/env_infos/height Mean                       -0.171832\n",
      "evaluation/env_infos/height Std                         0.227059\n",
      "evaluation/env_infos/height Max                         0.49977\n",
      "evaluation/env_infos/height Min                        -0.590573\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0719786\n",
      "evaluation/env_infos/final/reward_angular Std           0.915399\n",
      "evaluation/env_infos/final/reward_angular Max           1.46629\n",
      "evaluation/env_infos/final/reward_angular Min          -2.80114\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.140515\n",
      "evaluation/env_infos/initial/reward_angular Std         1.15447\n",
      "evaluation/env_infos/initial/reward_angular Max         2.99192\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.44952\n",
      "evaluation/env_infos/reward_angular Mean                0.0173051\n",
      "evaluation/env_infos/reward_angular Std                 2.01657\n",
      "evaluation/env_infos/reward_angular Max                 9.7388\n",
      "evaluation/env_infos/reward_angular Min                -9.41755\n",
      "time/data storing (s)                                   0.0158844\n",
      "time/evaluation sampling (s)                           23.2859\n",
      "time/exploration sampling (s)                           1.05954\n",
      "time/logging (s)                                        0.251925\n",
      "time/saving (s)                                         0.0316988\n",
      "time/training (s)                                       4.34358\n",
      "time/epoch (s)                                         28.9885\n",
      "time/total (s)                                       4058.48\n",
      "Epoch                                                 135\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:20:13.647592 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 136 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 138000\n",
      "trainer/QF1 Loss                                        1.53285\n",
      "trainer/QF2 Loss                                        1.28693\n",
      "trainer/Policy Loss                                    -3.68588\n",
      "trainer/Q1 Predictions Mean                             9.20887\n",
      "trainer/Q1 Predictions Std                             22.6304\n",
      "trainer/Q1 Predictions Max                             85.6922\n",
      "trainer/Q1 Predictions Min                            -14.5267\n",
      "trainer/Q2 Predictions Mean                             9.29025\n",
      "trainer/Q2 Predictions Std                             22.718\n",
      "trainer/Q2 Predictions Max                             84.4613\n",
      "trainer/Q2 Predictions Min                            -14.7826\n",
      "trainer/Q Targets Mean                                  9.16155\n",
      "trainer/Q Targets Std                                  22.4292\n",
      "trainer/Q Targets Max                                  86.7917\n",
      "trainer/Q Targets Min                                 -15.1763\n",
      "trainer/Log Pis Mean                                    5.81496\n",
      "trainer/Log Pis Std                                     5.27067\n",
      "trainer/Log Pis Max                                    22.1479\n",
      "trainer/Log Pis Min                                    -5.95554\n",
      "trainer/Policy mu Mean                                  0.0451387\n",
      "trainer/Policy mu Std                                   1.55287\n",
      "trainer/Policy mu Max                                   5.32892\n",
      "trainer/Policy mu Min                                  -4.42153\n",
      "trainer/Policy log std Mean                            -0.743527\n",
      "trainer/Policy log std Std                              0.273716\n",
      "trainer/Policy log std Max                              0.0711896\n",
      "trainer/Policy log std Min                             -1.97416\n",
      "trainer/Alpha                                           0.0174556\n",
      "trainer/Alpha Loss                                     -0.748761\n",
      "exploration/num steps total                        138000\n",
      "exploration/num paths total                           138\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.984897\n",
      "exploration/Rewards Std                                 0.566866\n",
      "exploration/Rewards Max                                 2.30453\n",
      "exploration/Rewards Min                                -1.00032\n",
      "exploration/Returns Mean                              984.897\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               984.897\n",
      "exploration/Returns Min                               984.897\n",
      "exploration/Actions Mean                                0.180108\n",
      "exploration/Actions Std                                 0.714484\n",
      "exploration/Actions Max                                 0.999389\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           984.897\n",
      "exploration/env_infos/final/reward_run Mean             1.13661\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.13661\n",
      "exploration/env_infos/final/reward_run Min              1.13661\n",
      "exploration/env_infos/initial/reward_run Mean           0.673979\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.673979\n",
      "exploration/env_infos/initial/reward_run Min            0.673979\n",
      "exploration/env_infos/reward_run Mean                   1.99557\n",
      "exploration/env_infos/reward_run Std                    0.859136\n",
      "exploration/env_infos/reward_run Max                    4.10782\n",
      "exploration/env_infos/reward_run Min                   -1.03755\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.444763\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.444763\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.444763\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21336\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21336\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21336\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.325755\n",
      "exploration/env_infos/reward_ctrl Std                   0.0991092\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0421125\n",
      "exploration/env_infos/reward_ctrl Min                  -0.578458\n",
      "exploration/env_infos/final/height Mean                -0.0574418\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0574418\n",
      "exploration/env_infos/final/height Min                 -0.0574418\n",
      "exploration/env_infos/initial/height Mean              -0.0438945\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0438945\n",
      "exploration/env_infos/initial/height Min               -0.0438945\n",
      "exploration/env_infos/height Mean                      -0.115573\n",
      "exploration/env_infos/height Std                        0.081316\n",
      "exploration/env_infos/height Max                        0.147609\n",
      "exploration/env_infos/height Min                       -0.332058\n",
      "exploration/env_infos/final/reward_angular Mean         0.0282404\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0282404\n",
      "exploration/env_infos/final/reward_angular Min          0.0282404\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.14728\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.14728\n",
      "exploration/env_infos/initial/reward_angular Min       -1.14728\n",
      "exploration/env_infos/reward_angular Mean              -0.0536323\n",
      "exploration/env_infos/reward_angular Std                1.71237\n",
      "exploration/env_infos/reward_angular Max                4.8518\n",
      "exploration/env_infos/reward_angular Min               -5.35097\n",
      "evaluation/num steps total                              3.425e+06\n",
      "evaluation/num paths total                           3425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.421214\n",
      "evaluation/Rewards Std                                  1.27334\n",
      "evaluation/Rewards Max                                  7.30334\n",
      "evaluation/Rewards Min                                 -7.23345\n",
      "evaluation/Returns Mean                               421.214\n",
      "evaluation/Returns Std                                687.744\n",
      "evaluation/Returns Max                               2285.99\n",
      "evaluation/Returns Min                               -495.426\n",
      "evaluation/Actions Mean                                 0.0221913\n",
      "evaluation/Actions Std                                  0.718979\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999995\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            421.214\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0708675\n",
      "evaluation/env_infos/final/reward_run Std               1.99887\n",
      "evaluation/env_infos/final/reward_run Max               2.90689\n",
      "evaluation/env_infos/final/reward_run Min              -5.10888\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0440958\n",
      "evaluation/env_infos/initial/reward_run Std             0.576343\n",
      "evaluation/env_infos/initial/reward_run Max             1.04444\n",
      "evaluation/env_infos/initial/reward_run Min            -1.24472\n",
      "evaluation/env_infos/reward_run Mean                   -0.525392\n",
      "evaluation/env_infos/reward_run Std                     2.3395\n",
      "evaluation/env_infos/reward_run Max                     5.18346\n",
      "evaluation/env_infos/reward_run Min                    -7.73192\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.271203\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.100766\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0738183\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.435148\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.265163\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0948449\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0493684\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.469909\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.310454\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112283\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0159452\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595798\n",
      "evaluation/env_infos/final/height Mean                 -0.199471\n",
      "evaluation/env_infos/final/height Std                   0.252849\n",
      "evaluation/env_infos/final/height Max                   0.107043\n",
      "evaluation/env_infos/final/height Min                  -0.577278\n",
      "evaluation/env_infos/initial/height Mean               -0.0240693\n",
      "evaluation/env_infos/initial/height Std                 0.0660306\n",
      "evaluation/env_infos/initial/height Max                 0.0801191\n",
      "evaluation/env_infos/initial/height Min                -0.109798\n",
      "evaluation/env_infos/height Mean                       -0.155423\n",
      "evaluation/env_infos/height Std                         0.22023\n",
      "evaluation/env_infos/height Max                         0.582067\n",
      "evaluation/env_infos/height Min                        -0.590217\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.177803\n",
      "evaluation/env_infos/final/reward_angular Std           1.8549\n",
      "evaluation/env_infos/final/reward_angular Max           6.79146\n",
      "evaluation/env_infos/final/reward_angular Min          -3.09547\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.229598\n",
      "evaluation/env_infos/initial/reward_angular Std         1.24787\n",
      "evaluation/env_infos/initial/reward_angular Max         2.71883\n",
      "evaluation/env_infos/initial/reward_angular Min        -3.15028\n",
      "evaluation/env_infos/reward_angular Mean                0.0175135\n",
      "evaluation/env_infos/reward_angular Std                 1.89925\n",
      "evaluation/env_infos/reward_angular Max                10.1626\n",
      "evaluation/env_infos/reward_angular Min                -8.48599\n",
      "time/data storing (s)                                   0.0155994\n",
      "time/evaluation sampling (s)                           25.8616\n",
      "time/exploration sampling (s)                           0.997585\n",
      "time/logging (s)                                        0.250075\n",
      "time/saving (s)                                         0.0301773\n",
      "time/training (s)                                       4.16509\n",
      "time/epoch (s)                                         31.3201\n",
      "time/total (s)                                       4090.83\n",
      "Epoch                                                 136\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:20:43.942494 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 137 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 139000\n",
      "trainer/QF1 Loss                                        1.55145\n",
      "trainer/QF2 Loss                                        1.58813\n",
      "trainer/Policy Loss                                    -5.6066\n",
      "trainer/Q1 Predictions Mean                            12.0995\n",
      "trainer/Q1 Predictions Std                             25.5017\n",
      "trainer/Q1 Predictions Max                             81.7059\n",
      "trainer/Q1 Predictions Min                            -14.4074\n",
      "trainer/Q2 Predictions Mean                            12.0242\n",
      "trainer/Q2 Predictions Std                             25.7522\n",
      "trainer/Q2 Predictions Max                             82.4004\n",
      "trainer/Q2 Predictions Min                            -15.679\n",
      "trainer/Q Targets Mean                                 11.9781\n",
      "trainer/Q Targets Std                                  25.5348\n",
      "trainer/Q Targets Max                                  85.2328\n",
      "trainer/Q Targets Min                                 -16.0249\n",
      "trainer/Log Pis Mean                                    6.62661\n",
      "trainer/Log Pis Std                                     6.25043\n",
      "trainer/Log Pis Max                                    25.3011\n",
      "trainer/Log Pis Min                                    -6.00228\n",
      "trainer/Policy mu Mean                                  0.168693\n",
      "trainer/Policy mu Std                                   1.69133\n",
      "trainer/Policy mu Max                                   6.08276\n",
      "trainer/Policy mu Min                                  -5.8353\n",
      "trainer/Policy log std Mean                            -0.72083\n",
      "trainer/Policy log std Std                              0.294021\n",
      "trainer/Policy log std Max                              0.264963\n",
      "trainer/Policy log std Min                             -1.97847\n",
      "trainer/Alpha                                           0.0181975\n",
      "trainer/Alpha Loss                                      2.51144\n",
      "exploration/num steps total                        139000\n",
      "exploration/num paths total                           139\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.58625\n",
      "exploration/Rewards Std                                 0.992635\n",
      "exploration/Rewards Max                                 4.01724\n",
      "exploration/Rewards Min                                -1.95521\n",
      "exploration/Returns Mean                             1586.25\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1586.25\n",
      "exploration/Returns Min                              1586.25\n",
      "exploration/Actions Mean                                0.161258\n",
      "exploration/Actions Std                                 0.847943\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1586.25\n",
      "exploration/env_infos/final/reward_run Mean            -0.0100953\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0100953\n",
      "exploration/env_infos/final/reward_run Min             -0.0100953\n",
      "exploration/env_infos/initial/reward_run Mean          -0.644562\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.644562\n",
      "exploration/env_infos/initial/reward_run Min           -0.644562\n",
      "exploration/env_infos/reward_run Mean                  -3.72541\n",
      "exploration/env_infos/reward_run Std                    1.68353\n",
      "exploration/env_infos/reward_run Max                    0.84766\n",
      "exploration/env_infos/reward_run Min                   -7.4599\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.298283\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.298283\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.298283\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.208101\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.208101\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.208101\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.447007\n",
      "exploration/env_infos/reward_ctrl Std                   0.109952\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0176295\n",
      "exploration/env_infos/reward_ctrl Min                  -0.598228\n",
      "exploration/env_infos/final/height Mean                -0.576806\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576806\n",
      "exploration/env_infos/final/height Min                 -0.576806\n",
      "exploration/env_infos/initial/height Mean              -0.0883357\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0883357\n",
      "exploration/env_infos/initial/height Min               -0.0883357\n",
      "exploration/env_infos/height Mean                      -0.0383024\n",
      "exploration/env_infos/height Std                        0.203474\n",
      "exploration/env_infos/height Max                        0.528096\n",
      "exploration/env_infos/height Min                       -0.577206\n",
      "exploration/env_infos/final/reward_angular Mean        -0.00341335\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.00341335\n",
      "exploration/env_infos/final/reward_angular Min         -0.00341335\n",
      "exploration/env_infos/initial/reward_angular Mean       0.607643\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.607643\n",
      "exploration/env_infos/initial/reward_angular Min        0.607643\n",
      "exploration/env_infos/reward_angular Mean               0.0236552\n",
      "exploration/env_infos/reward_angular Std                3.19135\n",
      "exploration/env_infos/reward_angular Max               10.2126\n",
      "exploration/env_infos/reward_angular Min               -6.33873\n",
      "evaluation/num steps total                              3.45e+06\n",
      "evaluation/num paths total                           3450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.509183\n",
      "evaluation/Rewards Std                                  1.45267\n",
      "evaluation/Rewards Max                                  7.89586\n",
      "evaluation/Rewards Min                                 -7.30336\n",
      "evaluation/Returns Mean                               509.183\n",
      "evaluation/Returns Std                                771.372\n",
      "evaluation/Returns Max                               2085.83\n",
      "evaluation/Returns Min                               -515.542\n",
      "evaluation/Actions Mean                                 0.096863\n",
      "evaluation/Actions Std                                  0.757744\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            509.183\n",
      "evaluation/env_infos/final/reward_run Mean             -0.284855\n",
      "evaluation/env_infos/final/reward_run Std               1.89958\n",
      "evaluation/env_infos/final/reward_run Max               3.55669\n",
      "evaluation/env_infos/final/reward_run Min              -4.66758\n",
      "evaluation/env_infos/initial/reward_run Mean            0.16185\n",
      "evaluation/env_infos/initial/reward_run Std             0.504795\n",
      "evaluation/env_infos/initial/reward_run Max             0.97964\n",
      "evaluation/env_infos/initial/reward_run Min            -0.731318\n",
      "evaluation/env_infos/reward_run Mean                   -0.741438\n",
      "evaluation/env_infos/reward_run Std                     2.2504\n",
      "evaluation/env_infos/reward_run Max                     5.19291\n",
      "evaluation/env_infos/reward_run Min                    -7.41807\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.33004\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11552\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.133192\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.570173\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.296905\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.088927\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0974424\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.48148\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.350135\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115378\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0302471\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598783\n",
      "evaluation/env_infos/final/height Mean                 -0.230795\n",
      "evaluation/env_infos/final/height Std                   0.30356\n",
      "evaluation/env_infos/final/height Max                   0.42424\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.000260628\n",
      "evaluation/env_infos/initial/height Std                 0.0531851\n",
      "evaluation/env_infos/initial/height Max                 0.0798659\n",
      "evaluation/env_infos/initial/height Min                -0.0906651\n",
      "evaluation/env_infos/height Mean                       -0.157545\n",
      "evaluation/env_infos/height Std                         0.244118\n",
      "evaluation/env_infos/height Max                         0.650599\n",
      "evaluation/env_infos/height Min                        -0.587577\n",
      "evaluation/env_infos/final/reward_angular Mean          0.183061\n",
      "evaluation/env_infos/final/reward_angular Std           1.78872\n",
      "evaluation/env_infos/final/reward_angular Max           4.9427\n",
      "evaluation/env_infos/final/reward_angular Min          -3.58893\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.440789\n",
      "evaluation/env_infos/initial/reward_angular Std         0.935744\n",
      "evaluation/env_infos/initial/reward_angular Max         2.24505\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.02806\n",
      "evaluation/env_infos/reward_angular Mean                0.0347238\n",
      "evaluation/env_infos/reward_angular Std                 2.19207\n",
      "evaluation/env_infos/reward_angular Max                10.5668\n",
      "evaluation/env_infos/reward_angular Min                -7.64725\n",
      "time/data storing (s)                                   0.0165662\n",
      "time/evaluation sampling (s)                           22.8831\n",
      "time/exploration sampling (s)                           1.0737\n",
      "time/logging (s)                                        0.261072\n",
      "time/saving (s)                                         0.0294687\n",
      "time/training (s)                                       4.74856\n",
      "time/epoch (s)                                         29.0124\n",
      "time/total (s)                                       4121.13\n",
      "Epoch                                                 137\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:21:16.337166 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 138 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 140000\n",
      "trainer/QF1 Loss                                        1.25981\n",
      "trainer/QF2 Loss                                        1.15134\n",
      "trainer/Policy Loss                                    -4.32853\n",
      "trainer/Q1 Predictions Mean                            10.1058\n",
      "trainer/Q1 Predictions Std                             21.5206\n",
      "trainer/Q1 Predictions Max                             85.0645\n",
      "trainer/Q1 Predictions Min                            -15.7885\n",
      "trainer/Q2 Predictions Mean                             9.97463\n",
      "trainer/Q2 Predictions Std                             21.549\n",
      "trainer/Q2 Predictions Max                             84.8002\n",
      "trainer/Q2 Predictions Min                            -15.781\n",
      "trainer/Q Targets Mean                                 10.0405\n",
      "trainer/Q Targets Std                                  21.8835\n",
      "trainer/Q Targets Max                                  85.1882\n",
      "trainer/Q Targets Min                                 -16.4514\n",
      "trainer/Log Pis Mean                                    5.97639\n",
      "trainer/Log Pis Std                                     6.24188\n",
      "trainer/Log Pis Max                                    33.5086\n",
      "trainer/Log Pis Min                                    -6.01831\n",
      "trainer/Policy mu Mean                                  0.0653675\n",
      "trainer/Policy mu Std                                   1.60464\n",
      "trainer/Policy mu Max                                   4.52221\n",
      "trainer/Policy mu Min                                  -5.29569\n",
      "trainer/Policy log std Mean                            -0.740223\n",
      "trainer/Policy log std Std                              0.281073\n",
      "trainer/Policy log std Max                              0.372799\n",
      "trainer/Policy log std Min                             -1.84662\n",
      "trainer/Alpha                                           0.0188589\n",
      "trainer/Alpha Loss                                     -0.0937674\n",
      "exploration/num steps total                        140000\n",
      "exploration/num paths total                           140\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.303783\n",
      "exploration/Rewards Std                                 0.520168\n",
      "exploration/Rewards Max                                 3.29163\n",
      "exploration/Rewards Min                                -1.08958\n",
      "exploration/Returns Mean                             -303.783\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -303.783\n",
      "exploration/Returns Min                              -303.783\n",
      "exploration/Actions Mean                                0.467684\n",
      "exploration/Actions Std                                 0.514194\n",
      "exploration/Actions Max                                 0.999941\n",
      "exploration/Actions Min                                -0.999981\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -303.783\n",
      "exploration/env_infos/final/reward_run Mean             0.0771547\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0771547\n",
      "exploration/env_infos/final/reward_run Min              0.0771547\n",
      "exploration/env_infos/initial/reward_run Mean           0.240838\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.240838\n",
      "exploration/env_infos/initial/reward_run Min            0.240838\n",
      "exploration/env_infos/reward_run Mean                  -0.221351\n",
      "exploration/env_infos/reward_run Std                    0.608202\n",
      "exploration/env_infos/reward_run Max                    0.672302\n",
      "exploration/env_infos/reward_run Min                   -4.48169\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.340189\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.340189\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.340189\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21055\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21055\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21055\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.289874\n",
      "exploration/env_infos/reward_ctrl Std                   0.0789854\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0381054\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596696\n",
      "exploration/env_infos/final/height Mean                -0.577059\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577059\n",
      "exploration/env_infos/final/height Min                 -0.577059\n",
      "exploration/env_infos/initial/height Mean              -0.0461195\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0461195\n",
      "exploration/env_infos/initial/height Min               -0.0461195\n",
      "exploration/env_infos/height Mean                      -0.535114\n",
      "exploration/env_infos/height Std                        0.149044\n",
      "exploration/env_infos/height Max                        0.331906\n",
      "exploration/env_infos/height Min                       -0.593892\n",
      "exploration/env_infos/final/reward_angular Mean         0.00981567\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.00981567\n",
      "exploration/env_infos/final/reward_angular Min          0.00981567\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.119504\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.119504\n",
      "exploration/env_infos/initial/reward_angular Min       -0.119504\n",
      "exploration/env_infos/reward_angular Mean              -0.0562214\n",
      "exploration/env_infos/reward_angular Std                0.832591\n",
      "exploration/env_infos/reward_angular Max                8.05761\n",
      "exploration/env_infos/reward_angular Min               -5.13751\n",
      "evaluation/num steps total                              3.475e+06\n",
      "evaluation/num paths total                           3475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.184482\n",
      "evaluation/Rewards Std                                  1.02283\n",
      "evaluation/Rewards Max                                  6.28412\n",
      "evaluation/Rewards Min                                 -6.7301\n",
      "evaluation/Returns Mean                               184.482\n",
      "evaluation/Returns Std                                524.496\n",
      "evaluation/Returns Max                               1881.7\n",
      "evaluation/Returns Min                               -407.28\n",
      "evaluation/Actions Mean                                 0.149281\n",
      "evaluation/Actions Std                                  0.685476\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            184.482\n",
      "evaluation/env_infos/final/reward_run Mean              0.102304\n",
      "evaluation/env_infos/final/reward_run Std               0.820311\n",
      "evaluation/env_infos/final/reward_run Max               2.86641\n",
      "evaluation/env_infos/final/reward_run Min              -1.42556\n",
      "evaluation/env_infos/initial/reward_run Mean            0.140079\n",
      "evaluation/env_infos/initial/reward_run Std             0.550407\n",
      "evaluation/env_infos/initial/reward_run Max             1.13419\n",
      "evaluation/env_infos/initial/reward_run Min            -1.19714\n",
      "evaluation/env_infos/reward_run Mean                   -0.280564\n",
      "evaluation/env_infos/reward_run Std                     1.8649\n",
      "evaluation/env_infos/reward_run Max                     4.81622\n",
      "evaluation/env_infos/reward_run Min                    -7.24865\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.243296\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0936773\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.104315\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.396004\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.284502\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.088188\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0963598\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.496473\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.295298\n",
      "evaluation/env_infos/reward_ctrl Std                    0.12408\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0487694\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596459\n",
      "evaluation/env_infos/final/height Mean                 -0.356573\n",
      "evaluation/env_infos/final/height Std                   0.225434\n",
      "evaluation/env_infos/final/height Max                  -0.0182501\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0234503\n",
      "evaluation/env_infos/initial/height Std                 0.0539775\n",
      "evaluation/env_infos/initial/height Max                 0.0675597\n",
      "evaluation/env_infos/initial/height Min                -0.117873\n",
      "evaluation/env_infos/height Mean                       -0.255742\n",
      "evaluation/env_infos/height Std                         0.236427\n",
      "evaluation/env_infos/height Max                         0.470104\n",
      "evaluation/env_infos/height Min                        -0.589009\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0409451\n",
      "evaluation/env_infos/final/reward_angular Std           0.974291\n",
      "evaluation/env_infos/final/reward_angular Max           2.38226\n",
      "evaluation/env_infos/final/reward_angular Min          -3.78029\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.476276\n",
      "evaluation/env_infos/initial/reward_angular Std         1.03145\n",
      "evaluation/env_infos/initial/reward_angular Max         1.60609\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.26023\n",
      "evaluation/env_infos/reward_angular Mean                0.0102011\n",
      "evaluation/env_infos/reward_angular Std                 1.70523\n",
      "evaluation/env_infos/reward_angular Max                10.3163\n",
      "evaluation/env_infos/reward_angular Min                -6.8645\n",
      "time/data storing (s)                                   0.0167133\n",
      "time/evaluation sampling (s)                           25.32\n",
      "time/exploration sampling (s)                           1.18053\n",
      "time/logging (s)                                        0.239583\n",
      "time/saving (s)                                         0.0267532\n",
      "time/training (s)                                       4.42576\n",
      "time/epoch (s)                                         31.2094\n",
      "time/total (s)                                       4153.5\n",
      "Epoch                                                 138\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:21:52.463928 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 139 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 141000\n",
      "trainer/QF1 Loss                                        1.33527\n",
      "trainer/QF2 Loss                                        1.27409\n",
      "trainer/Policy Loss                                    -3.47139\n",
      "trainer/Q1 Predictions Mean                             8.40478\n",
      "trainer/Q1 Predictions Std                             21.1989\n",
      "trainer/Q1 Predictions Max                             80.0893\n",
      "trainer/Q1 Predictions Min                            -15.2838\n",
      "trainer/Q2 Predictions Mean                             8.40637\n",
      "trainer/Q2 Predictions Std                             21.1219\n",
      "trainer/Q2 Predictions Max                             78.5469\n",
      "trainer/Q2 Predictions Min                            -16.102\n",
      "trainer/Q Targets Mean                                  8.54249\n",
      "trainer/Q Targets Std                                  21.0668\n",
      "trainer/Q Targets Max                                  81.6841\n",
      "trainer/Q Targets Min                                 -15.0462\n",
      "trainer/Log Pis Mean                                    5.12115\n",
      "trainer/Log Pis Std                                     5.51245\n",
      "trainer/Log Pis Max                                    20.592\n",
      "trainer/Log Pis Min                                    -6.95436\n",
      "trainer/Policy mu Mean                                  0.00487056\n",
      "trainer/Policy mu Std                                   1.49713\n",
      "trainer/Policy mu Max                                   4.09835\n",
      "trainer/Policy mu Min                                  -4.59053\n",
      "trainer/Policy log std Mean                            -0.693508\n",
      "trainer/Policy log std Std                              0.275846\n",
      "trainer/Policy log std Max                              0.44131\n",
      "trainer/Policy log std Min                             -1.71577\n",
      "trainer/Alpha                                           0.0184762\n",
      "trainer/Alpha Loss                                     -3.5064\n",
      "exploration/num steps total                        141000\n",
      "exploration/num paths total                           141\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.07516\n",
      "exploration/Rewards Std                                 2.27395\n",
      "exploration/Rewards Max                                 8.71318\n",
      "exploration/Rewards Min                                -4.23228\n",
      "exploration/Returns Mean                             1075.16\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1075.16\n",
      "exploration/Returns Min                              1075.16\n",
      "exploration/Actions Mean                               -0.113783\n",
      "exploration/Actions Std                                 0.791524\n",
      "exploration/Actions Max                                 0.999995\n",
      "exploration/Actions Min                                -0.99996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1075.16\n",
      "exploration/env_infos/final/reward_run Mean            -0.119591\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.119591\n",
      "exploration/env_infos/final/reward_run Min             -0.119591\n",
      "exploration/env_infos/initial/reward_run Mean           0.796188\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.796188\n",
      "exploration/env_infos/initial/reward_run Min            0.796188\n",
      "exploration/env_infos/reward_run Mean                  -3.29537\n",
      "exploration/env_infos/reward_run Std                    1.86364\n",
      "exploration/env_infos/reward_run Max                    1.52952\n",
      "exploration/env_infos/reward_run Min                   -6.87329\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.263447\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.263447\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.263447\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.447557\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.447557\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.447557\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.383674\n",
      "exploration/env_infos/reward_ctrl Std                   0.0878024\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0782799\n",
      "exploration/env_infos/reward_ctrl Min                  -0.559478\n",
      "exploration/env_infos/final/height Mean                -0.577365\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577365\n",
      "exploration/env_infos/final/height Min                 -0.577365\n",
      "exploration/env_infos/initial/height Mean              -0.00200615\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00200615\n",
      "exploration/env_infos/initial/height Min               -0.00200615\n",
      "exploration/env_infos/height Mean                      -0.107404\n",
      "exploration/env_infos/height Std                        0.213104\n",
      "exploration/env_infos/height Max                        0.288293\n",
      "exploration/env_infos/height Min                       -0.58026\n",
      "exploration/env_infos/final/reward_angular Mean         0.071972\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.071972\n",
      "exploration/env_infos/final/reward_angular Min          0.071972\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.904668\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.904668\n",
      "exploration/env_infos/initial/reward_angular Min       -0.904668\n",
      "exploration/env_infos/reward_angular Mean              -0.00136175\n",
      "exploration/env_infos/reward_angular Std                2.50815\n",
      "exploration/env_infos/reward_angular Max                8.74605\n",
      "exploration/env_infos/reward_angular Min               -5.96746\n",
      "evaluation/num steps total                              3.5e+06\n",
      "evaluation/num paths total                           3500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.568619\n",
      "evaluation/Rewards Std                                  1.468\n",
      "evaluation/Rewards Max                                  9.54087\n",
      "evaluation/Rewards Min                                 -8.06922\n",
      "evaluation/Returns Mean                               568.619\n",
      "evaluation/Returns Std                                721.415\n",
      "evaluation/Returns Max                               2159.35\n",
      "evaluation/Returns Min                               -412.368\n",
      "evaluation/Actions Mean                                 0.0420763\n",
      "evaluation/Actions Std                                  0.763626\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            568.619\n",
      "evaluation/env_infos/final/reward_run Mean             -1.20929\n",
      "evaluation/env_infos/final/reward_run Std               2.43025\n",
      "evaluation/env_infos/final/reward_run Max               2.54099\n",
      "evaluation/env_infos/final/reward_run Min              -5.66835\n",
      "evaluation/env_infos/initial/reward_run Mean            0.126195\n",
      "evaluation/env_infos/initial/reward_run Std             0.494463\n",
      "evaluation/env_infos/initial/reward_run Max             0.898303\n",
      "evaluation/env_infos/initial/reward_run Min            -0.677416\n",
      "evaluation/env_infos/reward_run Mean                   -1.24649\n",
      "evaluation/env_infos/reward_run Std                     2.47683\n",
      "evaluation/env_infos/reward_run Max                     4.40875\n",
      "evaluation/env_infos/reward_run Min                    -7.66346\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.354575\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0931428\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.207316\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.528763\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.252871\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0818399\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.106177\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.504328\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.350937\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113025\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0106612\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593651\n",
      "evaluation/env_infos/final/height Mean                 -0.153807\n",
      "evaluation/env_infos/final/height Std                   0.240913\n",
      "evaluation/env_infos/final/height Max                   0.28945\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean                0.0102052\n",
      "evaluation/env_infos/initial/height Std                 0.0447215\n",
      "evaluation/env_infos/initial/height Max                 0.0755039\n",
      "evaluation/env_infos/initial/height Min                -0.0737459\n",
      "evaluation/env_infos/height Mean                       -0.132761\n",
      "evaluation/env_infos/height Std                         0.213728\n",
      "evaluation/env_infos/height Max                         0.540893\n",
      "evaluation/env_infos/height Min                        -0.587566\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0213417\n",
      "evaluation/env_infos/final/reward_angular Std           1.72763\n",
      "evaluation/env_infos/final/reward_angular Max           5.34736\n",
      "evaluation/env_infos/final/reward_angular Min          -2.15848\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.550281\n",
      "evaluation/env_infos/initial/reward_angular Std         0.544879\n",
      "evaluation/env_infos/initial/reward_angular Max         0.669713\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.57154\n",
      "evaluation/env_infos/reward_angular Mean                0.0501449\n",
      "evaluation/env_infos/reward_angular Std                 2.26494\n",
      "evaluation/env_infos/reward_angular Max                10.7741\n",
      "evaluation/env_infos/reward_angular Min                -6.11847\n",
      "time/data storing (s)                                   0.0207487\n",
      "time/evaluation sampling (s)                           26.4336\n",
      "time/exploration sampling (s)                           1.58079\n",
      "time/logging (s)                                        0.289179\n",
      "time/saving (s)                                         0.0349595\n",
      "time/training (s)                                       6.79439\n",
      "time/epoch (s)                                         35.1537\n",
      "time/total (s)                                       4189.67\n",
      "Epoch                                                 139\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:22:26.829102 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 140 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 142000\n",
      "trainer/QF1 Loss                                        1.30624\n",
      "trainer/QF2 Loss                                        1.42062\n",
      "trainer/Policy Loss                                    -2.50479\n",
      "trainer/Q1 Predictions Mean                             8.47048\n",
      "trainer/Q1 Predictions Std                             22.064\n",
      "trainer/Q1 Predictions Max                             86.4316\n",
      "trainer/Q1 Predictions Min                            -16.1323\n",
      "trainer/Q2 Predictions Mean                             8.25315\n",
      "trainer/Q2 Predictions Std                             22.2351\n",
      "trainer/Q2 Predictions Max                             87.9343\n",
      "trainer/Q2 Predictions Min                            -16.4088\n",
      "trainer/Q Targets Mean                                  8.38876\n",
      "trainer/Q Targets Std                                  22.1676\n",
      "trainer/Q Targets Max                                  87.7332\n",
      "trainer/Q Targets Min                                 -15.8768\n",
      "trainer/Log Pis Mean                                    6.11494\n",
      "trainer/Log Pis Std                                     6.15882\n",
      "trainer/Log Pis Max                                    27.8771\n",
      "trainer/Log Pis Min                                    -5.39397\n",
      "trainer/Policy mu Mean                                  0.00816468\n",
      "trainer/Policy mu Std                                   1.6667\n",
      "trainer/Policy mu Max                                   4.57962\n",
      "trainer/Policy mu Min                                  -5.65127\n",
      "trainer/Policy log std Mean                            -0.704573\n",
      "trainer/Policy log std Std                              0.295742\n",
      "trainer/Policy log std Max                              0.319251\n",
      "trainer/Policy log std Min                             -1.73117\n",
      "trainer/Alpha                                           0.0171771\n",
      "trainer/Alpha Loss                                      0.467199\n",
      "exploration/num steps total                        142000\n",
      "exploration/num paths total                           142\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0229993\n",
      "exploration/Rewards Std                                 0.772767\n",
      "exploration/Rewards Max                                 3.2248\n",
      "exploration/Rewards Min                                -2.87915\n",
      "exploration/Returns Mean                               22.9993\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                22.9993\n",
      "exploration/Returns Min                                22.9993\n",
      "exploration/Actions Mean                               -0.0638662\n",
      "exploration/Actions Std                                 0.681487\n",
      "exploration/Actions Max                                 0.999956\n",
      "exploration/Actions Min                                -0.999907\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            22.9993\n",
      "exploration/env_infos/final/reward_run Mean            -0.530681\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.530681\n",
      "exploration/env_infos/final/reward_run Min             -0.530681\n",
      "exploration/env_infos/initial/reward_run Mean          -0.415637\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.415637\n",
      "exploration/env_infos/initial/reward_run Min           -0.415637\n",
      "exploration/env_infos/reward_run Mean                   0.355557\n",
      "exploration/env_infos/reward_run Std                    0.814937\n",
      "exploration/env_infos/reward_run Max                    3.30858\n",
      "exploration/env_infos/reward_run Min                   -1.04527\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.168634\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.168634\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.168634\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.112816\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.112816\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.112816\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.281102\n",
      "exploration/env_infos/reward_ctrl Std                   0.07983\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0448967\n",
      "exploration/env_infos/reward_ctrl Min                  -0.567339\n",
      "exploration/env_infos/final/height Mean                -0.126309\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.126309\n",
      "exploration/env_infos/final/height Min                 -0.126309\n",
      "exploration/env_infos/initial/height Mean              -0.0181853\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0181853\n",
      "exploration/env_infos/initial/height Min               -0.0181853\n",
      "exploration/env_infos/height Mean                      -0.140071\n",
      "exploration/env_infos/height Std                        0.0995174\n",
      "exploration/env_infos/height Max                        0.263187\n",
      "exploration/env_infos/height Min                       -0.583357\n",
      "exploration/env_infos/final/reward_angular Mean        -0.238766\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.238766\n",
      "exploration/env_infos/final/reward_angular Min         -0.238766\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.170601\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.170601\n",
      "exploration/env_infos/initial/reward_angular Min       -0.170601\n",
      "exploration/env_infos/reward_angular Mean               0.115872\n",
      "exploration/env_infos/reward_angular Std                1.37429\n",
      "exploration/env_infos/reward_angular Max                5.6443\n",
      "exploration/env_infos/reward_angular Min               -5.09245\n",
      "evaluation/num steps total                              3.525e+06\n",
      "evaluation/num paths total                           3525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.289377\n",
      "evaluation/Rewards Std                                  1.20922\n",
      "evaluation/Rewards Max                                  8.53351\n",
      "evaluation/Rewards Min                                 -7.52931\n",
      "evaluation/Returns Mean                               289.377\n",
      "evaluation/Returns Std                                753.239\n",
      "evaluation/Returns Max                               2813.3\n",
      "evaluation/Returns Min                               -499.684\n",
      "evaluation/Actions Mean                                 0.0695497\n",
      "evaluation/Actions Std                                  0.731949\n",
      "evaluation/Actions Max                                  0.999996\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            289.377\n",
      "evaluation/env_infos/final/reward_run Mean              0.244205\n",
      "evaluation/env_infos/final/reward_run Std               1.47191\n",
      "evaluation/env_infos/final/reward_run Max               4.00806\n",
      "evaluation/env_infos/final/reward_run Min              -3.92228\n",
      "evaluation/env_infos/initial/reward_run Mean            0.061439\n",
      "evaluation/env_infos/initial/reward_run Std             0.593745\n",
      "evaluation/env_infos/initial/reward_run Max             1.14142\n",
      "evaluation/env_infos/initial/reward_run Min            -0.947831\n",
      "evaluation/env_infos/reward_run Mean                   -0.147904\n",
      "evaluation/env_infos/reward_run Std                     1.85715\n",
      "evaluation/env_infos/reward_run Max                     5.17133\n",
      "evaluation/env_infos/reward_run Min                    -7.17091\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.319951\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0918887\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.197279\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.497032\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.278646\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0819318\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.143785\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.43103\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.324352\n",
      "evaluation/env_infos/reward_ctrl Std                    0.108224\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00939094\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59821\n",
      "evaluation/env_infos/final/height Mean                 -0.2928\n",
      "evaluation/env_infos/final/height Std                   0.279079\n",
      "evaluation/env_infos/final/height Max                   0.304225\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0186541\n",
      "evaluation/env_infos/initial/height Std                 0.0609539\n",
      "evaluation/env_infos/initial/height Max                 0.0639101\n",
      "evaluation/env_infos/initial/height Min                -0.115193\n",
      "evaluation/env_infos/height Mean                       -0.249208\n",
      "evaluation/env_infos/height Std                         0.260352\n",
      "evaluation/env_infos/height Max                         0.565382\n",
      "evaluation/env_infos/height Min                        -0.592459\n",
      "evaluation/env_infos/final/reward_angular Mean          0.118357\n",
      "evaluation/env_infos/final/reward_angular Std           1.51468\n",
      "evaluation/env_infos/final/reward_angular Max           3.87966\n",
      "evaluation/env_infos/final/reward_angular Min          -4.64491\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.608316\n",
      "evaluation/env_infos/initial/reward_angular Std         1.06921\n",
      "evaluation/env_infos/initial/reward_angular Max         3.47536\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.95868\n",
      "evaluation/env_infos/reward_angular Mean                0.00579337\n",
      "evaluation/env_infos/reward_angular Std                 1.69617\n",
      "evaluation/env_infos/reward_angular Max                11.8247\n",
      "evaluation/env_infos/reward_angular Min                -8.20726\n",
      "time/data storing (s)                                   0.0166613\n",
      "time/evaluation sampling (s)                           26.6922\n",
      "time/exploration sampling (s)                           1.14682\n",
      "time/logging (s)                                        0.253067\n",
      "time/saving (s)                                         0.0288626\n",
      "time/training (s)                                       4.49267\n",
      "time/epoch (s)                                         32.6302\n",
      "time/total (s)                                       4224\n",
      "Epoch                                                 140\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:22:56.788558 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 141 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 143000\n",
      "trainer/QF1 Loss                                        1.61298\n",
      "trainer/QF2 Loss                                        1.7203\n",
      "trainer/Policy Loss                                    -5.95978\n",
      "trainer/Q1 Predictions Mean                            11.777\n",
      "trainer/Q1 Predictions Std                             23.0993\n",
      "trainer/Q1 Predictions Max                             91.9187\n",
      "trainer/Q1 Predictions Min                            -15.0936\n",
      "trainer/Q2 Predictions Mean                            11.7888\n",
      "trainer/Q2 Predictions Std                             23.1303\n",
      "trainer/Q2 Predictions Max                             92.3662\n",
      "trainer/Q2 Predictions Min                            -15.2983\n",
      "trainer/Q Targets Mean                                 11.9234\n",
      "trainer/Q Targets Std                                  23.3724\n",
      "trainer/Q Targets Max                                  92.4544\n",
      "trainer/Q Targets Min                                 -14.9426\n",
      "trainer/Log Pis Mean                                    6.0234\n",
      "trainer/Log Pis Std                                     5.26951\n",
      "trainer/Log Pis Max                                    24.4645\n",
      "trainer/Log Pis Min                                    -4.6791\n",
      "trainer/Policy mu Mean                                  0.157337\n",
      "trainer/Policy mu Std                                   1.56388\n",
      "trainer/Policy mu Max                                   4.47331\n",
      "trainer/Policy mu Min                                  -4.68408\n",
      "trainer/Policy log std Mean                            -0.747913\n",
      "trainer/Policy log std Std                              0.284106\n",
      "trainer/Policy log std Max                              0.38457\n",
      "trainer/Policy log std Min                             -1.78022\n",
      "trainer/Alpha                                           0.017695\n",
      "trainer/Alpha Loss                                      0.0944207\n",
      "exploration/num steps total                        143000\n",
      "exploration/num paths total                           143\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.553571\n",
      "exploration/Rewards Std                                 1.70862\n",
      "exploration/Rewards Max                                 6.90043\n",
      "exploration/Rewards Min                                -3.13368\n",
      "exploration/Returns Mean                              553.571\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               553.571\n",
      "exploration/Returns Min                               553.571\n",
      "exploration/Actions Mean                               -0.0904191\n",
      "exploration/Actions Std                                 0.746485\n",
      "exploration/Actions Max                                 0.999951\n",
      "exploration/Actions Min                                -0.999995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           553.571\n",
      "exploration/env_infos/final/reward_run Mean            -0.00142726\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.00142726\n",
      "exploration/env_infos/final/reward_run Min             -0.00142726\n",
      "exploration/env_infos/initial/reward_run Mean          -0.321934\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.321934\n",
      "exploration/env_infos/initial/reward_run Min           -0.321934\n",
      "exploration/env_infos/reward_run Mean                  -1.34786\n",
      "exploration/env_infos/reward_run Std                    2.01187\n",
      "exploration/env_infos/reward_run Max                    1.98531\n",
      "exploration/env_infos/reward_run Min                   -7.51112\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.349161\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.349161\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.349161\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.286614\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.286614\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.286614\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.339249\n",
      "exploration/env_infos/reward_ctrl Std                   0.126396\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0343593\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595944\n",
      "exploration/env_infos/final/height Mean                -0.510616\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.510616\n",
      "exploration/env_infos/final/height Min                 -0.510616\n",
      "exploration/env_infos/initial/height Mean               0.00733802\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00733802\n",
      "exploration/env_infos/initial/height Min                0.00733802\n",
      "exploration/env_infos/height Mean                      -0.311922\n",
      "exploration/env_infos/height Std                        0.236573\n",
      "exploration/env_infos/height Max                        0.342341\n",
      "exploration/env_infos/height Min                       -0.586728\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0449579\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0449579\n",
      "exploration/env_infos/final/reward_angular Min         -0.0449579\n",
      "exploration/env_infos/initial/reward_angular Mean       0.994087\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.994087\n",
      "exploration/env_infos/initial/reward_angular Min        0.994087\n",
      "exploration/env_infos/reward_angular Mean              -0.06413\n",
      "exploration/env_infos/reward_angular Std                2.5802\n",
      "exploration/env_infos/reward_angular Max                8.8387\n",
      "exploration/env_infos/reward_angular Min               -7.97608\n",
      "evaluation/num steps total                              3.55e+06\n",
      "evaluation/num paths total                           3550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.177227\n",
      "evaluation/Rewards Std                                  1.15636\n",
      "evaluation/Rewards Max                                  8.36939\n",
      "evaluation/Rewards Min                                 -6.94514\n",
      "evaluation/Returns Mean                               177.227\n",
      "evaluation/Returns Std                                480.509\n",
      "evaluation/Returns Max                               1541.5\n",
      "evaluation/Returns Min                               -460.232\n",
      "evaluation/Actions Mean                                 0.0884356\n",
      "evaluation/Actions Std                                  0.707869\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            177.227\n",
      "evaluation/env_infos/final/reward_run Mean              0.407377\n",
      "evaluation/env_infos/final/reward_run Std               0.868088\n",
      "evaluation/env_infos/final/reward_run Max               2.36908\n",
      "evaluation/env_infos/final/reward_run Min              -0.734882\n",
      "evaluation/env_infos/initial/reward_run Mean            0.153786\n",
      "evaluation/env_infos/initial/reward_run Std             0.595311\n",
      "evaluation/env_infos/initial/reward_run Max             1.10915\n",
      "evaluation/env_infos/initial/reward_run Min            -0.711269\n",
      "evaluation/env_infos/reward_run Mean                    0.0411825\n",
      "evaluation/env_infos/reward_run Std                     1.69373\n",
      "evaluation/env_infos/reward_run Max                     4.95717\n",
      "evaluation/env_infos/reward_run Min                    -7.21948\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.283882\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.105615\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.11917\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.554392\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.300665\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0886476\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.131199\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.501824\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.305339\n",
      "evaluation/env_infos/reward_ctrl Std                    0.117377\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0498308\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596729\n",
      "evaluation/env_infos/final/height Mean                 -0.386553\n",
      "evaluation/env_infos/final/height Std                   0.199595\n",
      "evaluation/env_infos/final/height Max                  -0.0702402\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0318282\n",
      "evaluation/env_infos/initial/height Std                 0.0411807\n",
      "evaluation/env_infos/initial/height Max                 0.0680506\n",
      "evaluation/env_infos/initial/height Min                -0.0961089\n",
      "evaluation/env_infos/height Mean                       -0.309611\n",
      "evaluation/env_infos/height Std                         0.221898\n",
      "evaluation/env_infos/height Max                         0.404142\n",
      "evaluation/env_infos/height Min                        -0.591705\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.208322\n",
      "evaluation/env_infos/final/reward_angular Std           0.854717\n",
      "evaluation/env_infos/final/reward_angular Max           2.83993\n",
      "evaluation/env_infos/final/reward_angular Min          -1.97612\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.591754\n",
      "evaluation/env_infos/initial/reward_angular Std         0.781304\n",
      "evaluation/env_infos/initial/reward_angular Max         0.675757\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.69693\n",
      "evaluation/env_infos/reward_angular Mean               -0.0131724\n",
      "evaluation/env_infos/reward_angular Std                 1.62495\n",
      "evaluation/env_infos/reward_angular Max                 9.719\n",
      "evaluation/env_infos/reward_angular Min                -7.49271\n",
      "time/data storing (s)                                   0.0166342\n",
      "time/evaluation sampling (s)                           22.8155\n",
      "time/exploration sampling (s)                           1.21374\n",
      "time/logging (s)                                        0.282454\n",
      "time/saving (s)                                         0.0266543\n",
      "time/training (s)                                       4.55165\n",
      "time/epoch (s)                                         28.9066\n",
      "time/total (s)                                       4253.98\n",
      "Epoch                                                 141\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:23:26.207329 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 142 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 144000\n",
      "trainer/QF1 Loss                                        2.06094\n",
      "trainer/QF2 Loss                                        1.66698\n",
      "trainer/Policy Loss                                    -6.24704\n",
      "trainer/Q1 Predictions Mean                            12.3409\n",
      "trainer/Q1 Predictions Std                             25.4096\n",
      "trainer/Q1 Predictions Max                             87.4614\n",
      "trainer/Q1 Predictions Min                            -14.9934\n",
      "trainer/Q2 Predictions Mean                            12.3929\n",
      "trainer/Q2 Predictions Std                             25.365\n",
      "trainer/Q2 Predictions Max                             86.6888\n",
      "trainer/Q2 Predictions Min                            -15.2426\n",
      "trainer/Q Targets Mean                                 12.5545\n",
      "trainer/Q Targets Std                                  25.6443\n",
      "trainer/Q Targets Max                                  90.0067\n",
      "trainer/Q Targets Min                                 -16.1304\n",
      "trainer/Log Pis Mean                                    6.38099\n",
      "trainer/Log Pis Std                                     5.37516\n",
      "trainer/Log Pis Max                                    25.0404\n",
      "trainer/Log Pis Min                                    -4.29101\n",
      "trainer/Policy mu Mean                                 -0.147078\n",
      "trainer/Policy mu Std                                   1.61548\n",
      "trainer/Policy mu Max                                   3.94707\n",
      "trainer/Policy mu Min                                  -5.5137\n",
      "trainer/Policy log std Mean                            -0.711523\n",
      "trainer/Policy log std Std                              0.273136\n",
      "trainer/Policy log std Max                              0.254258\n",
      "trainer/Policy log std Min                             -1.82293\n",
      "trainer/Alpha                                           0.0186485\n",
      "trainer/Alpha Loss                                      1.51726\n",
      "exploration/num steps total                        144000\n",
      "exploration/num paths total                           144\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.215248\n",
      "exploration/Rewards Std                                 1.54048\n",
      "exploration/Rewards Max                                 8.07797\n",
      "exploration/Rewards Min                                -6.23077\n",
      "exploration/Returns Mean                              215.248\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               215.248\n",
      "exploration/Returns Min                               215.248\n",
      "exploration/Actions Mean                                0.0695263\n",
      "exploration/Actions Std                                 0.63526\n",
      "exploration/Actions Max                                 0.999767\n",
      "exploration/Actions Min                                -0.999956\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           215.248\n",
      "exploration/env_infos/final/reward_run Mean             0.242954\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.242954\n",
      "exploration/env_infos/final/reward_run Min              0.242954\n",
      "exploration/env_infos/initial/reward_run Mean          -0.162915\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.162915\n",
      "exploration/env_infos/initial/reward_run Min           -0.162915\n",
      "exploration/env_infos/reward_run Mean                  -0.589176\n",
      "exploration/env_infos/reward_run Std                    1.31521\n",
      "exploration/env_infos/reward_run Max                    1.26475\n",
      "exploration/env_infos/reward_run Min                   -6.29624\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.257336\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.257336\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.257336\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.280483\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.280483\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.280483\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.245034\n",
      "exploration/env_infos/reward_ctrl Std                   0.11068\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0344979\n",
      "exploration/env_infos/reward_ctrl Min                  -0.586455\n",
      "exploration/env_infos/final/height Mean                -0.543584\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.543584\n",
      "exploration/env_infos/final/height Min                 -0.543584\n",
      "exploration/env_infos/initial/height Mean               0.0522317\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0522317\n",
      "exploration/env_infos/initial/height Min                0.0522317\n",
      "exploration/env_infos/height Mean                      -0.466003\n",
      "exploration/env_infos/height Std                        0.174674\n",
      "exploration/env_infos/height Max                        0.348932\n",
      "exploration/env_infos/height Min                       -0.58503\n",
      "exploration/env_infos/final/reward_angular Mean         0.672656\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.672656\n",
      "exploration/env_infos/final/reward_angular Min          0.672656\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.979706\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.979706\n",
      "exploration/env_infos/initial/reward_angular Min       -0.979706\n",
      "exploration/env_infos/reward_angular Mean              -0.0597416\n",
      "exploration/env_infos/reward_angular Std                1.60582\n",
      "exploration/env_infos/reward_angular Max                8.98772\n",
      "exploration/env_infos/reward_angular Min               -6.64057\n",
      "evaluation/num steps total                              3.575e+06\n",
      "evaluation/num paths total                           3575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.254101\n",
      "evaluation/Rewards Std                                  1.21586\n",
      "evaluation/Rewards Max                                  9.05013\n",
      "evaluation/Rewards Min                                 -8.85766\n",
      "evaluation/Returns Mean                               254.101\n",
      "evaluation/Returns Std                                680.027\n",
      "evaluation/Returns Max                               2932.97\n",
      "evaluation/Returns Min                               -508.252\n",
      "evaluation/Actions Mean                                -0.0605467\n",
      "evaluation/Actions Std                                  0.740129\n",
      "evaluation/Actions Max                                  0.999992\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            254.101\n",
      "evaluation/env_infos/final/reward_run Mean              0.36015\n",
      "evaluation/env_infos/final/reward_run Std               1.05912\n",
      "evaluation/env_infos/final/reward_run Max               2.92879\n",
      "evaluation/env_infos/final/reward_run Min              -2.9555\n",
      "evaluation/env_infos/initial/reward_run Mean            0.181327\n",
      "evaluation/env_infos/initial/reward_run Std             0.532686\n",
      "evaluation/env_infos/initial/reward_run Max             1.02151\n",
      "evaluation/env_infos/initial/reward_run Min            -0.755748\n",
      "evaluation/env_infos/reward_run Mean                   -0.172716\n",
      "evaluation/env_infos/reward_run Std                     1.89631\n",
      "evaluation/env_infos/reward_run Max                     5.6672\n",
      "evaluation/env_infos/reward_run Min                    -7.8518\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.304125\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.113975\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.119381\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.535059\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.278695\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0852396\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0696902\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.402363\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.330874\n",
      "evaluation/env_infos/reward_ctrl Std                    0.11469\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0358157\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598448\n",
      "evaluation/env_infos/final/height Mean                 -0.301867\n",
      "evaluation/env_infos/final/height Std                   0.251563\n",
      "evaluation/env_infos/final/height Max                   0.08288\n",
      "evaluation/env_infos/final/height Min                  -0.577276\n",
      "evaluation/env_infos/initial/height Mean               -0.0198372\n",
      "evaluation/env_infos/initial/height Std                 0.0504924\n",
      "evaluation/env_infos/initial/height Max                 0.0734184\n",
      "evaluation/env_infos/initial/height Min                -0.105106\n",
      "evaluation/env_infos/height Mean                       -0.240294\n",
      "evaluation/env_infos/height Std                         0.249425\n",
      "evaluation/env_infos/height Max                         0.588174\n",
      "evaluation/env_infos/height Min                        -0.592908\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.325584\n",
      "evaluation/env_infos/final/reward_angular Std           0.972945\n",
      "evaluation/env_infos/final/reward_angular Max           2.26459\n",
      "evaluation/env_infos/final/reward_angular Min          -2.58743\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.366608\n",
      "evaluation/env_infos/initial/reward_angular Std         1.13215\n",
      "evaluation/env_infos/initial/reward_angular Max         3.06187\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.16595\n",
      "evaluation/env_infos/reward_angular Mean                0.000240556\n",
      "evaluation/env_infos/reward_angular Std                 1.76025\n",
      "evaluation/env_infos/reward_angular Max                10.1931\n",
      "evaluation/env_infos/reward_angular Min                -6.88162\n",
      "time/data storing (s)                                   0.0136921\n",
      "time/evaluation sampling (s)                           22.4436\n",
      "time/exploration sampling (s)                           0.991858\n",
      "time/logging (s)                                        0.241721\n",
      "time/saving (s)                                         0.0258171\n",
      "time/training (s)                                       4.59889\n",
      "time/epoch (s)                                         28.3156\n",
      "time/total (s)                                       4283.35\n",
      "Epoch                                                 142\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:23:55.576405 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 143 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 145000\n",
      "trainer/QF1 Loss                                        1.23399\n",
      "trainer/QF2 Loss                                        1.30703\n",
      "trainer/Policy Loss                                    -5.47546\n",
      "trainer/Q1 Predictions Mean                            10.9391\n",
      "trainer/Q1 Predictions Std                             21.4323\n",
      "trainer/Q1 Predictions Max                             82.9721\n",
      "trainer/Q1 Predictions Min                            -17.4364\n",
      "trainer/Q2 Predictions Mean                            10.8238\n",
      "trainer/Q2 Predictions Std                             21.2507\n",
      "trainer/Q2 Predictions Max                             81.5099\n",
      "trainer/Q2 Predictions Min                            -16.9866\n",
      "trainer/Q Targets Mean                                 11.0098\n",
      "trainer/Q Targets Std                                  21.3337\n",
      "trainer/Q Targets Max                                  83.8288\n",
      "trainer/Q Targets Min                                 -17.1537\n",
      "trainer/Log Pis Mean                                    5.59358\n",
      "trainer/Log Pis Std                                     5.25959\n",
      "trainer/Log Pis Max                                    21.8976\n",
      "trainer/Log Pis Min                                    -4.61478\n",
      "trainer/Policy mu Mean                                 -0.0301568\n",
      "trainer/Policy mu Std                                   1.52166\n",
      "trainer/Policy mu Max                                   4.56584\n",
      "trainer/Policy mu Min                                  -4.9066\n",
      "trainer/Policy log std Mean                            -0.767296\n",
      "trainer/Policy log std Std                              0.2823\n",
      "trainer/Policy log std Max                              0.181322\n",
      "trainer/Policy log std Min                             -2.15782\n",
      "trainer/Alpha                                           0.0182986\n",
      "trainer/Alpha Loss                                     -1.62565\n",
      "exploration/num steps total                        145000\n",
      "exploration/num paths total                           145\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.38745\n",
      "exploration/Rewards Std                                 0.69243\n",
      "exploration/Rewards Max                                 3.35796\n",
      "exploration/Rewards Min                                -1.13084\n",
      "exploration/Returns Mean                             1387.45\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1387.45\n",
      "exploration/Returns Min                              1387.45\n",
      "exploration/Actions Mean                                0.176895\n",
      "exploration/Actions Std                                 0.786297\n",
      "exploration/Actions Max                                 0.999565\n",
      "exploration/Actions Min                                -0.999964\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1387.45\n",
      "exploration/env_infos/final/reward_run Mean             1.51058\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.51058\n",
      "exploration/env_infos/final/reward_run Min              1.51058\n",
      "exploration/env_infos/initial/reward_run Mean           0.941802\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.941802\n",
      "exploration/env_infos/initial/reward_run Min            0.941802\n",
      "exploration/env_infos/reward_run Mean                   2.01992\n",
      "exploration/env_infos/reward_run Std                    0.626296\n",
      "exploration/env_infos/reward_run Max                    3.75411\n",
      "exploration/env_infos/reward_run Min                   -0.811848\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.468321\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.468321\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.468321\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.376445\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.376445\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.376445\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.389733\n",
      "exploration/env_infos/reward_ctrl Std                   0.0751396\n",
      "exploration/env_infos/reward_ctrl Max                  -0.114589\n",
      "exploration/env_infos/reward_ctrl Min                  -0.557744\n",
      "exploration/env_infos/final/height Mean                -0.0806056\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0806056\n",
      "exploration/env_infos/final/height Min                 -0.0806056\n",
      "exploration/env_infos/initial/height Mean               0.00128704\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00128704\n",
      "exploration/env_infos/initial/height Min                0.00128704\n",
      "exploration/env_infos/height Mean                      -0.15085\n",
      "exploration/env_infos/height Std                        0.0881713\n",
      "exploration/env_infos/height Max                        0.18409\n",
      "exploration/env_infos/height Min                       -0.321724\n",
      "exploration/env_infos/final/reward_angular Mean        -0.114581\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.114581\n",
      "exploration/env_infos/final/reward_angular Min         -0.114581\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.827838\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.827838\n",
      "exploration/env_infos/initial/reward_angular Min       -0.827838\n",
      "exploration/env_infos/reward_angular Mean              -0.067925\n",
      "exploration/env_infos/reward_angular Std                1.85279\n",
      "exploration/env_infos/reward_angular Max                4.83478\n",
      "exploration/env_infos/reward_angular Min               -5.48773\n",
      "evaluation/num steps total                              3.6e+06\n",
      "evaluation/num paths total                           3600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.568795\n",
      "evaluation/Rewards Std                                  1.40041\n",
      "evaluation/Rewards Max                                  8.70081\n",
      "evaluation/Rewards Min                                 -8.6726\n",
      "evaluation/Returns Mean                               568.795\n",
      "evaluation/Returns Std                                865.209\n",
      "evaluation/Returns Max                               2601.98\n",
      "evaluation/Returns Min                               -427.216\n",
      "evaluation/Actions Mean                                 0.0521262\n",
      "evaluation/Actions Std                                  0.769226\n",
      "evaluation/Actions Max                                  0.999988\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            568.795\n",
      "evaluation/env_infos/final/reward_run Mean             -0.783095\n",
      "evaluation/env_infos/final/reward_run Std               2.45579\n",
      "evaluation/env_infos/final/reward_run Max               2.56014\n",
      "evaluation/env_infos/final/reward_run Min              -5.20446\n",
      "evaluation/env_infos/initial/reward_run Mean            0.179359\n",
      "evaluation/env_infos/initial/reward_run Std             0.420925\n",
      "evaluation/env_infos/initial/reward_run Max             0.889875\n",
      "evaluation/env_infos/initial/reward_run Min            -0.489284\n",
      "evaluation/env_infos/reward_run Mean                   -0.876984\n",
      "evaluation/env_infos/reward_run Std                     2.48883\n",
      "evaluation/env_infos/reward_run Max                     4.54912\n",
      "evaluation/env_infos/reward_run Min                    -7.45809\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.353146\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.111054\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.104507\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.567708\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.267154\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0785081\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.125859\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.410868\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.356655\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113781\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0274842\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596047\n",
      "evaluation/env_infos/final/height Mean                 -0.15229\n",
      "evaluation/env_infos/final/height Std                   0.237342\n",
      "evaluation/env_infos/final/height Max                   0.315054\n",
      "evaluation/env_infos/final/height Min                  -0.577272\n",
      "evaluation/env_infos/initial/height Mean               -0.0090286\n",
      "evaluation/env_infos/initial/height Std                 0.048405\n",
      "evaluation/env_infos/initial/height Max                 0.066642\n",
      "evaluation/env_infos/initial/height Min                -0.107716\n",
      "evaluation/env_infos/height Mean                       -0.127095\n",
      "evaluation/env_infos/height Std                         0.207278\n",
      "evaluation/env_infos/height Max                         0.539319\n",
      "evaluation/env_infos/height Min                        -0.57728\n",
      "evaluation/env_infos/final/reward_angular Mean          0.21307\n",
      "evaluation/env_infos/final/reward_angular Std           2.12794\n",
      "evaluation/env_infos/final/reward_angular Max           6.2938\n",
      "evaluation/env_infos/final/reward_angular Min          -4.37147\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.30294\n",
      "evaluation/env_infos/initial/reward_angular Std         0.885372\n",
      "evaluation/env_infos/initial/reward_angular Max         1.34065\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.90816\n",
      "evaluation/env_infos/reward_angular Mean                0.039813\n",
      "evaluation/env_infos/reward_angular Std                 2.14755\n",
      "evaluation/env_infos/reward_angular Max                10.2746\n",
      "evaluation/env_infos/reward_angular Min                -6.77337\n",
      "time/data storing (s)                                   0.0220001\n",
      "time/evaluation sampling (s)                           22.18\n",
      "time/exploration sampling (s)                           1.36302\n",
      "time/logging (s)                                        0.259262\n",
      "time/saving (s)                                         0.0292788\n",
      "time/training (s)                                       4.41277\n",
      "time/epoch (s)                                         28.2663\n",
      "time/total (s)                                       4312.74\n",
      "Epoch                                                 143\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:24:26.803653 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 144 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 146000\n",
      "trainer/QF1 Loss                                        1.97033\n",
      "trainer/QF2 Loss                                        2.00894\n",
      "trainer/Policy Loss                                    -6.40489\n",
      "trainer/Q1 Predictions Mean                            12.5197\n",
      "trainer/Q1 Predictions Std                             24.0246\n",
      "trainer/Q1 Predictions Max                             92.4987\n",
      "trainer/Q1 Predictions Min                            -16.3056\n",
      "trainer/Q2 Predictions Mean                            12.7478\n",
      "trainer/Q2 Predictions Std                             24.1662\n",
      "trainer/Q2 Predictions Max                             93.8342\n",
      "trainer/Q2 Predictions Min                            -16.8367\n",
      "trainer/Q Targets Mean                                 12.8724\n",
      "trainer/Q Targets Std                                  24.4497\n",
      "trainer/Q Targets Max                                  97.6155\n",
      "trainer/Q Targets Min                                 -16.3522\n",
      "trainer/Log Pis Mean                                    6.43585\n",
      "trainer/Log Pis Std                                     5.98853\n",
      "trainer/Log Pis Max                                    31.7948\n",
      "trainer/Log Pis Min                                    -5.56353\n",
      "trainer/Policy mu Mean                                 -0.0761357\n",
      "trainer/Policy mu Std                                   1.65345\n",
      "trainer/Policy mu Max                                   4.51655\n",
      "trainer/Policy mu Min                                  -5.77783\n",
      "trainer/Policy log std Mean                            -0.707312\n",
      "trainer/Policy log std Std                              0.277917\n",
      "trainer/Policy log std Max                              0.409629\n",
      "trainer/Policy log std Min                             -2.22128\n",
      "trainer/Alpha                                           0.0183238\n",
      "trainer/Alpha Loss                                      1.7436\n",
      "exploration/num steps total                        146000\n",
      "exploration/num paths total                           146\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.05845\n",
      "exploration/Rewards Std                                 0.753955\n",
      "exploration/Rewards Max                                 3.21492\n",
      "exploration/Rewards Min                                -1.6252\n",
      "exploration/Returns Mean                             1058.45\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1058.45\n",
      "exploration/Returns Min                              1058.45\n",
      "exploration/Actions Mean                                0.182892\n",
      "exploration/Actions Std                                 0.776856\n",
      "exploration/Actions Max                                 0.999988\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1058.45\n",
      "exploration/env_infos/final/reward_run Mean             0.819145\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.819145\n",
      "exploration/env_infos/final/reward_run Min              0.819145\n",
      "exploration/env_infos/initial/reward_run Mean          -0.124468\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.124468\n",
      "exploration/env_infos/initial/reward_run Min           -0.124468\n",
      "exploration/env_infos/reward_run Mean                   1.72179\n",
      "exploration/env_infos/reward_run Std                    0.969359\n",
      "exploration/env_infos/reward_run Max                    3.8522\n",
      "exploration/env_infos/reward_run Min                   -1.24707\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.333473\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.333473\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.333473\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.211373\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.211373\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.211373\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.382173\n",
      "exploration/env_infos/reward_ctrl Std                   0.0933109\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0281098\n",
      "exploration/env_infos/reward_ctrl Min                  -0.577798\n",
      "exploration/env_infos/final/height Mean                -0.278785\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.278785\n",
      "exploration/env_infos/final/height Min                 -0.278785\n",
      "exploration/env_infos/initial/height Mean               0.00575036\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00575036\n",
      "exploration/env_infos/initial/height Min                0.00575036\n",
      "exploration/env_infos/height Mean                      -0.160516\n",
      "exploration/env_infos/height Std                        0.116235\n",
      "exploration/env_infos/height Max                        0.163705\n",
      "exploration/env_infos/height Min                       -0.415204\n",
      "exploration/env_infos/final/reward_angular Mean        -0.977759\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.977759\n",
      "exploration/env_infos/final/reward_angular Min         -0.977759\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.192556\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.192556\n",
      "exploration/env_infos/initial/reward_angular Min       -0.192556\n",
      "exploration/env_infos/reward_angular Mean              -0.0764454\n",
      "exploration/env_infos/reward_angular Std                2.20083\n",
      "exploration/env_infos/reward_angular Max                5.99402\n",
      "exploration/env_infos/reward_angular Min               -5.99355\n",
      "evaluation/num steps total                              3.625e+06\n",
      "evaluation/num paths total                           3625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.414972\n",
      "evaluation/Rewards Std                                  1.41724\n",
      "evaluation/Rewards Max                                  8.70753\n",
      "evaluation/Rewards Min                                 -7.65909\n",
      "evaluation/Returns Mean                               414.972\n",
      "evaluation/Returns Std                                671.062\n",
      "evaluation/Returns Max                               2354.81\n",
      "evaluation/Returns Min                               -569.666\n",
      "evaluation/Actions Mean                                -0.0533218\n",
      "evaluation/Actions Std                                  0.74966\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            414.972\n",
      "evaluation/env_infos/final/reward_run Mean              0.143221\n",
      "evaluation/env_infos/final/reward_run Std               1.19425\n",
      "evaluation/env_infos/final/reward_run Max               3.10915\n",
      "evaluation/env_infos/final/reward_run Min              -3.03271\n",
      "evaluation/env_infos/initial/reward_run Mean            0.264497\n",
      "evaluation/env_infos/initial/reward_run Std             0.374275\n",
      "evaluation/env_infos/initial/reward_run Max             1.0438\n",
      "evaluation/env_infos/initial/reward_run Min            -0.367457\n",
      "evaluation/env_infos/reward_run Mean                   -0.384124\n",
      "evaluation/env_infos/reward_run Std                     2.15987\n",
      "evaluation/env_infos/reward_run Max                     5.11972\n",
      "evaluation/env_infos/reward_run Min                    -6.99905\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.317613\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.121338\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0747418\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.532466\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.285795\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0917552\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.118881\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.42301\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.3389\n",
      "evaluation/env_infos/reward_ctrl Std                    0.117701\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0170191\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599331\n",
      "evaluation/env_infos/final/height Mean                 -0.284151\n",
      "evaluation/env_infos/final/height Std                   0.267305\n",
      "evaluation/env_infos/final/height Max                   0.11754\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0155329\n",
      "evaluation/env_infos/initial/height Std                 0.0523826\n",
      "evaluation/env_infos/initial/height Max                 0.0783946\n",
      "evaluation/env_infos/initial/height Min                -0.091113\n",
      "evaluation/env_infos/height Mean                       -0.173998\n",
      "evaluation/env_infos/height Std                         0.246149\n",
      "evaluation/env_infos/height Max                         0.532445\n",
      "evaluation/env_infos/height Min                        -0.587174\n",
      "evaluation/env_infos/final/reward_angular Mean          0.18249\n",
      "evaluation/env_infos/final/reward_angular Std           1.39901\n",
      "evaluation/env_infos/final/reward_angular Max           3.97584\n",
      "evaluation/env_infos/final/reward_angular Min          -4.10971\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.148674\n",
      "evaluation/env_infos/initial/reward_angular Std         1.05764\n",
      "evaluation/env_infos/initial/reward_angular Max         2.65779\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.91901\n",
      "evaluation/env_infos/reward_angular Mean                0.0201353\n",
      "evaluation/env_infos/reward_angular Std                 1.98414\n",
      "evaluation/env_infos/reward_angular Max                10.1016\n",
      "evaluation/env_infos/reward_angular Min                -8.07464\n",
      "time/data storing (s)                                   0.0156755\n",
      "time/evaluation sampling (s)                           24.4792\n",
      "time/exploration sampling (s)                           1.09519\n",
      "time/logging (s)                                        0.247303\n",
      "time/saving (s)                                         0.0279045\n",
      "time/training (s)                                       4.29541\n",
      "time/epoch (s)                                         30.1607\n",
      "time/total (s)                                       4343.95\n",
      "Epoch                                                 144\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:24:57.471540 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 145 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 147000\n",
      "trainer/QF1 Loss                                        2.01056\n",
      "trainer/QF2 Loss                                        2.22958\n",
      "trainer/Policy Loss                                    -7.71872\n",
      "trainer/Q1 Predictions Mean                            13.3632\n",
      "trainer/Q1 Predictions Std                             24.7127\n",
      "trainer/Q1 Predictions Max                             92.115\n",
      "trainer/Q1 Predictions Min                            -14.5921\n",
      "trainer/Q2 Predictions Mean                            13.2543\n",
      "trainer/Q2 Predictions Std                             24.6771\n",
      "trainer/Q2 Predictions Max                             93.6865\n",
      "trainer/Q2 Predictions Min                            -14.7443\n",
      "trainer/Q Targets Mean                                 13.3433\n",
      "trainer/Q Targets Std                                  24.7299\n",
      "trainer/Q Targets Max                                  93.1351\n",
      "trainer/Q Targets Min                                 -15.1579\n",
      "trainer/Log Pis Mean                                    5.69224\n",
      "trainer/Log Pis Std                                     5.46162\n",
      "trainer/Log Pis Max                                    21.01\n",
      "trainer/Log Pis Min                                    -6.5878\n",
      "trainer/Policy mu Mean                                  0.0736726\n",
      "trainer/Policy mu Std                                   1.56539\n",
      "trainer/Policy mu Max                                   4.42362\n",
      "trainer/Policy mu Min                                  -5.07548\n",
      "trainer/Policy log std Mean                            -0.71663\n",
      "trainer/Policy log std Std                              0.267776\n",
      "trainer/Policy log std Max                              0.210105\n",
      "trainer/Policy log std Min                             -1.91856\n",
      "trainer/Alpha                                           0.0186405\n",
      "trainer/Alpha Loss                                     -1.22574\n",
      "exploration/num steps total                        147000\n",
      "exploration/num paths total                           147\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.024634\n",
      "exploration/Rewards Std                                 0.88557\n",
      "exploration/Rewards Max                                 4.26546\n",
      "exploration/Rewards Min                                -3.06913\n",
      "exploration/Returns Mean                              -24.634\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -24.634\n",
      "exploration/Returns Min                               -24.634\n",
      "exploration/Actions Mean                                0.205312\n",
      "exploration/Actions Std                                 0.673973\n",
      "exploration/Actions Max                                 0.999978\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -24.634\n",
      "exploration/env_infos/final/reward_run Mean            -0.326188\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.326188\n",
      "exploration/env_infos/final/reward_run Min             -0.326188\n",
      "exploration/env_infos/initial/reward_run Mean          -0.35259\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.35259\n",
      "exploration/env_infos/initial/reward_run Min           -0.35259\n",
      "exploration/env_infos/reward_run Mean                   0.129505\n",
      "exploration/env_infos/reward_run Std                    0.515862\n",
      "exploration/env_infos/reward_run Max                    2.60423\n",
      "exploration/env_infos/reward_run Min                   -1.16456\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.181917\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.181917\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.181917\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.164981\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.164981\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.164981\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.297836\n",
      "exploration/env_infos/reward_ctrl Std                   0.0768925\n",
      "exploration/env_infos/reward_ctrl Max                  -0.064608\n",
      "exploration/env_infos/reward_ctrl Min                  -0.591199\n",
      "exploration/env_infos/final/height Mean                -0.573984\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.573984\n",
      "exploration/env_infos/final/height Min                 -0.573984\n",
      "exploration/env_infos/initial/height Mean              -0.0864447\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0864447\n",
      "exploration/env_infos/initial/height Min               -0.0864447\n",
      "exploration/env_infos/height Mean                      -0.494207\n",
      "exploration/env_infos/height Std                        0.184013\n",
      "exploration/env_infos/height Max                        0.184943\n",
      "exploration/env_infos/height Min                       -0.583787\n",
      "exploration/env_infos/final/reward_angular Mean        -0.216708\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.216708\n",
      "exploration/env_infos/final/reward_angular Min         -0.216708\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.31656\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.31656\n",
      "exploration/env_infos/initial/reward_angular Min       -1.31656\n",
      "exploration/env_infos/reward_angular Mean               0.189718\n",
      "exploration/env_infos/reward_angular Std                0.929771\n",
      "exploration/env_infos/reward_angular Max                5.60548\n",
      "exploration/env_infos/reward_angular Min               -3.63161\n",
      "evaluation/num steps total                              3.65e+06\n",
      "evaluation/num paths total                           3650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.565796\n",
      "evaluation/Rewards Std                                  1.44012\n",
      "evaluation/Rewards Max                                  8.19426\n",
      "evaluation/Rewards Min                                 -8.86141\n",
      "evaluation/Returns Mean                               565.796\n",
      "evaluation/Returns Std                                758.109\n",
      "evaluation/Returns Max                               2283.51\n",
      "evaluation/Returns Min                               -536.342\n",
      "evaluation/Actions Mean                                 0.0772762\n",
      "evaluation/Actions Std                                  0.759413\n",
      "evaluation/Actions Max                                  0.999983\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            565.796\n",
      "evaluation/env_infos/final/reward_run Mean             -0.318578\n",
      "evaluation/env_infos/final/reward_run Std               2.19168\n",
      "evaluation/env_infos/final/reward_run Max               2.90336\n",
      "evaluation/env_infos/final/reward_run Min              -5.76892\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0415155\n",
      "evaluation/env_infos/initial/reward_run Std             0.425681\n",
      "evaluation/env_infos/initial/reward_run Max             0.857992\n",
      "evaluation/env_infos/initial/reward_run Min            -0.672792\n",
      "evaluation/env_infos/reward_run Mean                   -0.555916\n",
      "evaluation/env_infos/reward_run Std                     2.39462\n",
      "evaluation/env_infos/reward_run Max                     5.49091\n",
      "evaluation/env_infos/reward_run Min                    -6.99612\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.326666\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.123734\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.141975\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.521368\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.326431\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0722512\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.211453\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.463027\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.349608\n",
      "evaluation/env_infos/reward_ctrl Std                    0.119035\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.037812\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596173\n",
      "evaluation/env_infos/final/height Mean                 -0.215639\n",
      "evaluation/env_infos/final/height Std                   0.214193\n",
      "evaluation/env_infos/final/height Max                   0.201354\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00756667\n",
      "evaluation/env_infos/initial/height Std                 0.0526305\n",
      "evaluation/env_infos/initial/height Max                 0.0839749\n",
      "evaluation/env_infos/initial/height Min                -0.0983267\n",
      "evaluation/env_infos/height Mean                       -0.140724\n",
      "evaluation/env_infos/height Std                         0.205332\n",
      "evaluation/env_infos/height Max                         0.588879\n",
      "evaluation/env_infos/height Min                        -0.579222\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0576212\n",
      "evaluation/env_infos/final/reward_angular Std           1.9409\n",
      "evaluation/env_infos/final/reward_angular Max           4.07341\n",
      "evaluation/env_infos/final/reward_angular Min          -4.63674\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.477977\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11112\n",
      "evaluation/env_infos/initial/reward_angular Max         2.66831\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.8506\n",
      "evaluation/env_infos/reward_angular Mean                0.0230991\n",
      "evaluation/env_infos/reward_angular Std                 2.26281\n",
      "evaluation/env_infos/reward_angular Max                10.6097\n",
      "evaluation/env_infos/reward_angular Min                -8.38202\n",
      "time/data storing (s)                                   0.01525\n",
      "time/evaluation sampling (s)                           23.3672\n",
      "time/exploration sampling (s)                           1.15703\n",
      "time/logging (s)                                        0.241378\n",
      "time/saving (s)                                         0.0281059\n",
      "time/training (s)                                       4.63507\n",
      "time/epoch (s)                                         29.4441\n",
      "time/total (s)                                       4374.61\n",
      "Epoch                                                 145\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:25:25.280820 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 146 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 148000\n",
      "trainer/QF1 Loss                                        1.39376\n",
      "trainer/QF2 Loss                                        1.56361\n",
      "trainer/Policy Loss                                    -5.82691\n",
      "trainer/Q1 Predictions Mean                            10.8253\n",
      "trainer/Q1 Predictions Std                             23.7338\n",
      "trainer/Q1 Predictions Max                             91.2643\n",
      "trainer/Q1 Predictions Min                            -17.5321\n",
      "trainer/Q2 Predictions Mean                            10.998\n",
      "trainer/Q2 Predictions Std                             23.8379\n",
      "trainer/Q2 Predictions Max                             89.606\n",
      "trainer/Q2 Predictions Min                            -16.9475\n",
      "trainer/Q Targets Mean                                 11.1224\n",
      "trainer/Q Targets Std                                  23.9881\n",
      "trainer/Q Targets Max                                  89.9852\n",
      "trainer/Q Targets Min                                 -17.6796\n",
      "trainer/Log Pis Mean                                    5.26024\n",
      "trainer/Log Pis Std                                     5.15009\n",
      "trainer/Log Pis Max                                    23.8101\n",
      "trainer/Log Pis Min                                    -4.46434\n",
      "trainer/Policy mu Mean                                  0.0382781\n",
      "trainer/Policy mu Std                                   1.47403\n",
      "trainer/Policy mu Max                                   4.30644\n",
      "trainer/Policy mu Min                                  -4.72093\n",
      "trainer/Policy log std Mean                            -0.742057\n",
      "trainer/Policy log std Std                              0.259648\n",
      "trainer/Policy log std Max                              0.0620843\n",
      "trainer/Policy log std Min                             -1.91543\n",
      "trainer/Alpha                                           0.0193858\n",
      "trainer/Alpha Loss                                     -2.91777\n",
      "exploration/num steps total                        148000\n",
      "exploration/num paths total                           148\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.110619\n",
      "exploration/Rewards Std                                 0.939241\n",
      "exploration/Rewards Max                                 6.29726\n",
      "exploration/Rewards Min                                -5.17991\n",
      "exploration/Returns Mean                             -110.619\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -110.619\n",
      "exploration/Returns Min                              -110.619\n",
      "exploration/Actions Mean                                0.24588\n",
      "exploration/Actions Std                                 0.679903\n",
      "exploration/Actions Max                                 0.999986\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -110.619\n",
      "exploration/env_infos/final/reward_run Mean            -0.49197\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.49197\n",
      "exploration/env_infos/final/reward_run Min             -0.49197\n",
      "exploration/env_infos/initial/reward_run Mean           0.862812\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.862812\n",
      "exploration/env_infos/initial/reward_run Min            0.862812\n",
      "exploration/env_infos/reward_run Mean                   0.019374\n",
      "exploration/env_infos/reward_run Std                    0.463171\n",
      "exploration/env_infos/reward_run Max                    1.59015\n",
      "exploration/env_infos/reward_run Min                   -2.49792\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.149633\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.149633\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.149633\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.408858\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.408858\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.408858\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.313635\n",
      "exploration/env_infos/reward_ctrl Std                   0.0801246\n",
      "exploration/env_infos/reward_ctrl Max                  -0.114695\n",
      "exploration/env_infos/reward_ctrl Min                  -0.559342\n",
      "exploration/env_infos/final/height Mean                -0.532865\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.532865\n",
      "exploration/env_infos/final/height Min                 -0.532865\n",
      "exploration/env_infos/initial/height Mean              -0.0435661\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0435661\n",
      "exploration/env_infos/initial/height Min               -0.0435661\n",
      "exploration/env_infos/height Mean                      -0.518356\n",
      "exploration/env_infos/height Std                        0.165516\n",
      "exploration/env_infos/height Max                        0.396182\n",
      "exploration/env_infos/height Min                       -0.587079\n",
      "exploration/env_infos/final/reward_angular Mean         0.295023\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.295023\n",
      "exploration/env_infos/final/reward_angular Min          0.295023\n",
      "exploration/env_infos/initial/reward_angular Mean       2.17641\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.17641\n",
      "exploration/env_infos/initial/reward_angular Min        2.17641\n",
      "exploration/env_infos/reward_angular Mean               0.193688\n",
      "exploration/env_infos/reward_angular Std                1.09138\n",
      "exploration/env_infos/reward_angular Max                7.69111\n",
      "exploration/env_infos/reward_angular Min               -5.97023\n",
      "evaluation/num steps total                              3.675e+06\n",
      "evaluation/num paths total                           3675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.353604\n",
      "evaluation/Rewards Std                                  1.08906\n",
      "evaluation/Rewards Max                                  7.66762\n",
      "evaluation/Rewards Min                                 -7.58117\n",
      "evaluation/Returns Mean                               353.604\n",
      "evaluation/Returns Std                                631.53\n",
      "evaluation/Returns Max                               2187.77\n",
      "evaluation/Returns Min                               -385.836\n",
      "evaluation/Actions Mean                                 0.0745658\n",
      "evaluation/Actions Std                                  0.720774\n",
      "evaluation/Actions Max                                  0.999993\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            353.604\n",
      "evaluation/env_infos/final/reward_run Mean              0.301625\n",
      "evaluation/env_infos/final/reward_run Std               2.07726\n",
      "evaluation/env_infos/final/reward_run Max               3.79344\n",
      "evaluation/env_infos/final/reward_run Min              -5.37757\n",
      "evaluation/env_infos/initial/reward_run Mean            0.312056\n",
      "evaluation/env_infos/initial/reward_run Std             0.4394\n",
      "evaluation/env_infos/initial/reward_run Max             1.09405\n",
      "evaluation/env_infos/initial/reward_run Min            -0.502074\n",
      "evaluation/env_infos/reward_run Mean                   -0.271045\n",
      "evaluation/env_infos/reward_run Std                     2.21617\n",
      "evaluation/env_infos/reward_run Max                     5.42643\n",
      "evaluation/env_infos/reward_run Min                    -7.70626\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.293156\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.115717\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0987992\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.544212\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.300541\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0944317\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.122982\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.466903\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.315045\n",
      "evaluation/env_infos/reward_ctrl Std                    0.118382\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0424403\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59382\n",
      "evaluation/env_infos/final/height Mean                 -0.324601\n",
      "evaluation/env_infos/final/height Std                   0.253266\n",
      "evaluation/env_infos/final/height Max                   0.0798392\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00863699\n",
      "evaluation/env_infos/initial/height Std                 0.0580876\n",
      "evaluation/env_infos/initial/height Max                 0.0909579\n",
      "evaluation/env_infos/initial/height Min                -0.0996638\n",
      "evaluation/env_infos/height Mean                       -0.239403\n",
      "evaluation/env_infos/height Std                         0.26023\n",
      "evaluation/env_infos/height Max                         0.472981\n",
      "evaluation/env_infos/height Min                        -0.592206\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.467492\n",
      "evaluation/env_infos/final/reward_angular Std           0.802987\n",
      "evaluation/env_infos/final/reward_angular Max           0.991903\n",
      "evaluation/env_infos/final/reward_angular Min          -2.18899\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.622988\n",
      "evaluation/env_infos/initial/reward_angular Std         0.700746\n",
      "evaluation/env_infos/initial/reward_angular Max         0.971583\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.21304\n",
      "evaluation/env_infos/reward_angular Mean                0.0107646\n",
      "evaluation/env_infos/reward_angular Std                 1.72999\n",
      "evaluation/env_infos/reward_angular Max                10.4761\n",
      "evaluation/env_infos/reward_angular Min                -8.2423\n",
      "time/data storing (s)                                   0.0155685\n",
      "time/evaluation sampling (s)                           21.0376\n",
      "time/exploration sampling (s)                           1.07646\n",
      "time/logging (s)                                        0.239198\n",
      "time/saving (s)                                         0.0268783\n",
      "time/training (s)                                       4.27596\n",
      "time/epoch (s)                                         26.6716\n",
      "time/total (s)                                       4402.41\n",
      "Epoch                                                 146\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:25:54.733770 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 147 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 149000\n",
      "trainer/QF1 Loss                                        1.69113\n",
      "trainer/QF2 Loss                                        1.6241\n",
      "trainer/Policy Loss                                    -5.88785\n",
      "trainer/Q1 Predictions Mean                            11.7493\n",
      "trainer/Q1 Predictions Std                             22.3367\n",
      "trainer/Q1 Predictions Max                             87.4181\n",
      "trainer/Q1 Predictions Min                            -14.3516\n",
      "trainer/Q2 Predictions Mean                            11.7201\n",
      "trainer/Q2 Predictions Std                             22.3423\n",
      "trainer/Q2 Predictions Max                             86.3189\n",
      "trainer/Q2 Predictions Min                            -14.154\n",
      "trainer/Q Targets Mean                                 11.7968\n",
      "trainer/Q Targets Std                                  22.5721\n",
      "trainer/Q Targets Max                                  88.5745\n",
      "trainer/Q Targets Min                                 -13.8817\n",
      "trainer/Log Pis Mean                                    6.05909\n",
      "trainer/Log Pis Std                                     5.80122\n",
      "trainer/Log Pis Max                                    24.0776\n",
      "trainer/Log Pis Min                                    -6.64782\n",
      "trainer/Policy mu Mean                                  0.0949014\n",
      "trainer/Policy mu Std                                   1.62104\n",
      "trainer/Policy mu Max                                   4.69382\n",
      "trainer/Policy mu Min                                  -5.00285\n",
      "trainer/Policy log std Mean                            -0.700073\n",
      "trainer/Policy log std Std                              0.297289\n",
      "trainer/Policy log std Max                              0.658006\n",
      "trainer/Policy log std Min                             -1.73772\n",
      "trainer/Alpha                                           0.0187616\n",
      "trainer/Alpha Loss                                      0.234983\n",
      "exploration/num steps total                        149000\n",
      "exploration/num paths total                           149\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.94061\n",
      "exploration/Rewards Std                                 1.66527\n",
      "exploration/Rewards Max                                 4.40101\n",
      "exploration/Rewards Min                                -4.67007\n",
      "exploration/Returns Mean                              940.61\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               940.61\n",
      "exploration/Returns Min                               940.61\n",
      "exploration/Actions Mean                                0.0247346\n",
      "exploration/Actions Std                                 0.860044\n",
      "exploration/Actions Max                                 0.999975\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           940.61\n",
      "exploration/env_infos/final/reward_run Mean            -1.03651\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -1.03651\n",
      "exploration/env_infos/final/reward_run Min             -1.03651\n",
      "exploration/env_infos/initial/reward_run Mean          -0.362361\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.362361\n",
      "exploration/env_infos/initial/reward_run Min           -0.362361\n",
      "exploration/env_infos/reward_run Mean                  -3.48205\n",
      "exploration/env_infos/reward_run Std                    1.5137\n",
      "exploration/env_infos/reward_run Max                    2.85359\n",
      "exploration/env_infos/reward_run Min                   -6.88466\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.407655\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.407655\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.407655\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.305622\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.305622\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.305622\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.444173\n",
      "exploration/env_infos/reward_ctrl Std                   0.0736861\n",
      "exploration/env_infos/reward_ctrl Max                  -0.113368\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594797\n",
      "exploration/env_infos/final/height Mean                 0.0299832\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0299832\n",
      "exploration/env_infos/final/height Min                  0.0299832\n",
      "exploration/env_infos/initial/height Mean              -0.0694171\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0694171\n",
      "exploration/env_infos/initial/height Min               -0.0694171\n",
      "exploration/env_infos/height Mean                       0.00836716\n",
      "exploration/env_infos/height Std                        0.151394\n",
      "exploration/env_infos/height Max                        0.465209\n",
      "exploration/env_infos/height Min                       -0.357313\n",
      "exploration/env_infos/final/reward_angular Mean        -1.05111\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.05111\n",
      "exploration/env_infos/final/reward_angular Min         -1.05111\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.750837\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.750837\n",
      "exploration/env_infos/initial/reward_angular Min       -0.750837\n",
      "exploration/env_infos/reward_angular Mean               0.112554\n",
      "exploration/env_infos/reward_angular Std                3.10786\n",
      "exploration/env_infos/reward_angular Max               10.9928\n",
      "exploration/env_infos/reward_angular Min               -5.64011\n",
      "evaluation/num steps total                              3.7e+06\n",
      "evaluation/num paths total                           3700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.416647\n",
      "evaluation/Rewards Std                                  1.54336\n",
      "evaluation/Rewards Max                                  8.96308\n",
      "evaluation/Rewards Min                                 -9.30336\n",
      "evaluation/Returns Mean                               416.647\n",
      "evaluation/Returns Std                                735.988\n",
      "evaluation/Returns Max                               2606.27\n",
      "evaluation/Returns Min                               -558.802\n",
      "evaluation/Actions Mean                                 0.0721749\n",
      "evaluation/Actions Std                                  0.741962\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            416.647\n",
      "evaluation/env_infos/final/reward_run Mean             -0.273865\n",
      "evaluation/env_infos/final/reward_run Std               1.85579\n",
      "evaluation/env_infos/final/reward_run Max               3.05955\n",
      "evaluation/env_infos/final/reward_run Min              -5.26138\n",
      "evaluation/env_infos/initial/reward_run Mean            0.315654\n",
      "evaluation/env_infos/initial/reward_run Std             0.458944\n",
      "evaluation/env_infos/initial/reward_run Max             0.982804\n",
      "evaluation/env_infos/initial/reward_run Min            -0.538889\n",
      "evaluation/env_infos/reward_run Mean                   -0.727719\n",
      "evaluation/env_infos/reward_run Std                     2.31571\n",
      "evaluation/env_infos/reward_run Max                     5.25128\n",
      "evaluation/env_infos/reward_run Min                    -7.19448\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.284365\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11713\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0849377\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.490565\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.31227\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.112696\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.141667\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.547454\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.33343\n",
      "evaluation/env_infos/reward_ctrl Std                    0.124291\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0162818\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598401\n",
      "evaluation/env_infos/final/height Mean                 -0.308708\n",
      "evaluation/env_infos/final/height Std                   0.229651\n",
      "evaluation/env_infos/final/height Max                   0.146953\n",
      "evaluation/env_infos/final/height Min                  -0.577292\n",
      "evaluation/env_infos/initial/height Mean               -0.0195103\n",
      "evaluation/env_infos/initial/height Std                 0.0515101\n",
      "evaluation/env_infos/initial/height Max                 0.0695369\n",
      "evaluation/env_infos/initial/height Min                -0.0830468\n",
      "evaluation/env_infos/height Mean                       -0.18346\n",
      "evaluation/env_infos/height Std                         0.237357\n",
      "evaluation/env_infos/height Max                         0.569704\n",
      "evaluation/env_infos/height Min                        -0.59052\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.51869\n",
      "evaluation/env_infos/final/reward_angular Std           1.71903\n",
      "evaluation/env_infos/final/reward_angular Max           3.509\n",
      "evaluation/env_infos/final/reward_angular Min          -4.7638\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.537045\n",
      "evaluation/env_infos/initial/reward_angular Std         1.1247\n",
      "evaluation/env_infos/initial/reward_angular Max         1.80272\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.93021\n",
      "evaluation/env_infos/reward_angular Mean                0.0276336\n",
      "evaluation/env_infos/reward_angular Std                 2.20422\n",
      "evaluation/env_infos/reward_angular Max                11.2707\n",
      "evaluation/env_infos/reward_angular Min                -7.34136\n",
      "time/data storing (s)                                   0.0163036\n",
      "time/evaluation sampling (s)                           22.2292\n",
      "time/exploration sampling (s)                           1.05722\n",
      "time/logging (s)                                        0.266019\n",
      "time/saving (s)                                         0.0289473\n",
      "time/training (s)                                       4.7483\n",
      "time/epoch (s)                                         28.346\n",
      "time/total (s)                                       4431.88\n",
      "Epoch                                                 147\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:26:24.346179 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 148 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 150000\n",
      "trainer/QF1 Loss                                        2.16625\n",
      "trainer/QF2 Loss                                        2.27727\n",
      "trainer/Policy Loss                                    -6.18751\n",
      "trainer/Q1 Predictions Mean                            11.8037\n",
      "trainer/Q1 Predictions Std                             25.7385\n",
      "trainer/Q1 Predictions Max                             99.7877\n",
      "trainer/Q1 Predictions Min                            -16.1169\n",
      "trainer/Q2 Predictions Mean                            11.9843\n",
      "trainer/Q2 Predictions Std                             26.038\n",
      "trainer/Q2 Predictions Max                            100.547\n",
      "trainer/Q2 Predictions Min                            -16.7142\n",
      "trainer/Q Targets Mean                                 11.8811\n",
      "trainer/Q Targets Std                                  25.6369\n",
      "trainer/Q Targets Max                                  98.6791\n",
      "trainer/Q Targets Min                                 -17.0975\n",
      "trainer/Log Pis Mean                                    5.86409\n",
      "trainer/Log Pis Std                                     5.88114\n",
      "trainer/Log Pis Max                                    24.7982\n",
      "trainer/Log Pis Min                                    -5.80008\n",
      "trainer/Policy mu Mean                                 -0.109597\n",
      "trainer/Policy mu Std                                   1.61113\n",
      "trainer/Policy mu Max                                   5.46698\n",
      "trainer/Policy mu Min                                  -6.38484\n",
      "trainer/Policy log std Mean                            -0.694559\n",
      "trainer/Policy log std Std                              0.290301\n",
      "trainer/Policy log std Max                              0.41454\n",
      "trainer/Policy log std Min                             -1.91923\n",
      "trainer/Alpha                                           0.0188534\n",
      "trainer/Alpha Loss                                     -0.539812\n",
      "exploration/num steps total                        150000\n",
      "exploration/num paths total                           150\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.084344\n",
      "exploration/Rewards Std                                 1.35849\n",
      "exploration/Rewards Max                                 6.51863\n",
      "exploration/Rewards Min                                -3.70521\n",
      "exploration/Returns Mean                               84.344\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                84.344\n",
      "exploration/Returns Min                                84.344\n",
      "exploration/Actions Mean                                0.121111\n",
      "exploration/Actions Std                                 0.807361\n",
      "exploration/Actions Max                                 0.999998\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            84.344\n",
      "exploration/env_infos/final/reward_run Mean            -0.98406\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.98406\n",
      "exploration/env_infos/final/reward_run Min             -0.98406\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0492788\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0492788\n",
      "exploration/env_infos/initial/reward_run Min           -0.0492788\n",
      "exploration/env_infos/reward_run Mean                  -0.00701816\n",
      "exploration/env_infos/reward_run Std                    0.731385\n",
      "exploration/env_infos/reward_run Max                    2.56796\n",
      "exploration/env_infos/reward_run Min                   -2.10158\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.398904\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.398904\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.398904\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.283942\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.283942\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.283942\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.3999\n",
      "exploration/env_infos/reward_ctrl Std                   0.0878082\n",
      "exploration/env_infos/reward_ctrl Max                  -0.113748\n",
      "exploration/env_infos/reward_ctrl Min                  -0.591374\n",
      "exploration/env_infos/final/height Mean                -0.486336\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.486336\n",
      "exploration/env_infos/final/height Min                 -0.486336\n",
      "exploration/env_infos/initial/height Mean              -0.0485044\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0485044\n",
      "exploration/env_infos/initial/height Min               -0.0485044\n",
      "exploration/env_infos/height Mean                      -0.480755\n",
      "exploration/env_infos/height Std                        0.172581\n",
      "exploration/env_infos/height Max                        0.185284\n",
      "exploration/env_infos/height Min                       -0.583939\n",
      "exploration/env_infos/final/reward_angular Mean        -1.32285\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.32285\n",
      "exploration/env_infos/final/reward_angular Min         -1.32285\n",
      "exploration/env_infos/initial/reward_angular Mean       1.28417\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.28417\n",
      "exploration/env_infos/initial/reward_angular Min        1.28417\n",
      "exploration/env_infos/reward_angular Mean               0.243801\n",
      "exploration/env_infos/reward_angular Std                1.39384\n",
      "exploration/env_infos/reward_angular Max                6.66072\n",
      "exploration/env_infos/reward_angular Min               -4.09239\n",
      "evaluation/num steps total                              3.725e+06\n",
      "evaluation/num paths total                           3725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.627646\n",
      "evaluation/Rewards Std                                  1.43304\n",
      "evaluation/Rewards Max                                  7.9734\n",
      "evaluation/Rewards Min                                 -7.74205\n",
      "evaluation/Returns Mean                               627.646\n",
      "evaluation/Returns Std                                827.034\n",
      "evaluation/Returns Max                               2413.01\n",
      "evaluation/Returns Min                               -478.903\n",
      "evaluation/Actions Mean                                 0.0548904\n",
      "evaluation/Actions Std                                  0.791182\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            627.646\n",
      "evaluation/env_infos/final/reward_run Mean             -0.822377\n",
      "evaluation/env_infos/final/reward_run Std               2.74797\n",
      "evaluation/env_infos/final/reward_run Max               3.89715\n",
      "evaluation/env_infos/final/reward_run Min              -6.50039\n",
      "evaluation/env_infos/initial/reward_run Mean            0.297097\n",
      "evaluation/env_infos/initial/reward_run Std             0.356869\n",
      "evaluation/env_infos/initial/reward_run Max             0.850452\n",
      "evaluation/env_infos/initial/reward_run Min            -0.355696\n",
      "evaluation/env_infos/reward_run Mean                   -0.955394\n",
      "evaluation/env_infos/reward_run Std                     2.56138\n",
      "evaluation/env_infos/reward_run Max                     4.69542\n",
      "evaluation/env_infos/reward_run Min                    -7.05721\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.364347\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.121242\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.17207\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.570582\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.258866\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0868214\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.126621\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.449127\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.377389\n",
      "evaluation/env_infos/reward_ctrl Std                    0.122431\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0300281\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597747\n",
      "evaluation/env_infos/final/height Mean                 -0.201418\n",
      "evaluation/env_infos/final/height Std                   0.20062\n",
      "evaluation/env_infos/final/height Max                   0.0259806\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0329409\n",
      "evaluation/env_infos/initial/height Std                 0.0466182\n",
      "evaluation/env_infos/initial/height Max                 0.0725088\n",
      "evaluation/env_infos/initial/height Min                -0.118794\n",
      "evaluation/env_infos/height Mean                       -0.147197\n",
      "evaluation/env_infos/height Std                         0.196056\n",
      "evaluation/env_infos/height Max                         0.411524\n",
      "evaluation/env_infos/height Min                        -0.595462\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.216289\n",
      "evaluation/env_infos/final/reward_angular Std           1.74892\n",
      "evaluation/env_infos/final/reward_angular Max           4.49726\n",
      "evaluation/env_infos/final/reward_angular Min          -3.71039\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.483061\n",
      "evaluation/env_infos/initial/reward_angular Std         1.03025\n",
      "evaluation/env_infos/initial/reward_angular Max         2.92884\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.77935\n",
      "evaluation/env_infos/reward_angular Mean                0.0480948\n",
      "evaluation/env_infos/reward_angular Std                 2.19574\n",
      "evaluation/env_infos/reward_angular Max                10.0763\n",
      "evaluation/env_infos/reward_angular Min                -6.93266\n",
      "time/data storing (s)                                   0.0159498\n",
      "time/evaluation sampling (s)                           22.7633\n",
      "time/exploration sampling (s)                           1.00574\n",
      "time/logging (s)                                        0.257265\n",
      "time/saving (s)                                         0.04193\n",
      "time/training (s)                                       4.24594\n",
      "time/epoch (s)                                         28.3301\n",
      "time/total (s)                                       4461.48\n",
      "Epoch                                                 148\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:26:56.364900 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 149 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 151000\n",
      "trainer/QF1 Loss                                        2.43099\n",
      "trainer/QF2 Loss                                        2.5698\n",
      "trainer/Policy Loss                                    -6.32576\n",
      "trainer/Q1 Predictions Mean                            12.7299\n",
      "trainer/Q1 Predictions Std                             25.4366\n",
      "trainer/Q1 Predictions Max                             93.4935\n",
      "trainer/Q1 Predictions Min                            -13.9954\n",
      "trainer/Q2 Predictions Mean                            12.7191\n",
      "trainer/Q2 Predictions Std                             25.2975\n",
      "trainer/Q2 Predictions Max                             93.5055\n",
      "trainer/Q2 Predictions Min                            -14.1154\n",
      "trainer/Q Targets Mean                                 12.6817\n",
      "trainer/Q Targets Std                                  25.8391\n",
      "trainer/Q Targets Max                                  96.6829\n",
      "trainer/Q Targets Min                                 -14.0075\n",
      "trainer/Log Pis Mean                                    6.58199\n",
      "trainer/Log Pis Std                                     5.98402\n",
      "trainer/Log Pis Max                                    25.4544\n",
      "trainer/Log Pis Min                                    -5.96457\n",
      "trainer/Policy mu Mean                                 -0.126482\n",
      "trainer/Policy mu Std                                   1.67152\n",
      "trainer/Policy mu Max                                   5.2667\n",
      "trainer/Policy mu Min                                  -4.86332\n",
      "trainer/Policy log std Mean                            -0.655432\n",
      "trainer/Policy log std Std                              0.293227\n",
      "trainer/Policy log std Max                              0.658821\n",
      "trainer/Policy log std Min                             -1.57493\n",
      "trainer/Alpha                                           0.0202851\n",
      "trainer/Alpha Loss                                      2.26929\n",
      "exploration/num steps total                        151000\n",
      "exploration/num paths total                           151\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.572422\n",
      "exploration/Rewards Std                                 0.613697\n",
      "exploration/Rewards Max                                 2.81621\n",
      "exploration/Rewards Min                                -1.70561\n",
      "exploration/Returns Mean                              572.422\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               572.422\n",
      "exploration/Returns Min                               572.422\n",
      "exploration/Actions Mean                                0.196308\n",
      "exploration/Actions Std                                 0.738401\n",
      "exploration/Actions Max                                 0.999718\n",
      "exploration/Actions Min                                -0.999918\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           572.422\n",
      "exploration/env_infos/final/reward_run Mean             1.45794\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.45794\n",
      "exploration/env_infos/final/reward_run Min              1.45794\n",
      "exploration/env_infos/initial/reward_run Mean           0.613556\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.613556\n",
      "exploration/env_infos/initial/reward_run Min            0.613556\n",
      "exploration/env_infos/reward_run Mean                   1.32607\n",
      "exploration/env_infos/reward_run Std                    0.629054\n",
      "exploration/env_infos/reward_run Max                    3.69506\n",
      "exploration/env_infos/reward_run Min                   -0.964068\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.335879\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.335879\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.335879\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.269756\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.269756\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.269756\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.350264\n",
      "exploration/env_infos/reward_ctrl Std                   0.0820303\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0847644\n",
      "exploration/env_infos/reward_ctrl Min                  -0.556912\n",
      "exploration/env_infos/final/height Mean                -0.122214\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.122214\n",
      "exploration/env_infos/final/height Min                 -0.122214\n",
      "exploration/env_infos/initial/height Mean              -0.0741001\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0741001\n",
      "exploration/env_infos/initial/height Min               -0.0741001\n",
      "exploration/env_infos/height Mean                      -0.205657\n",
      "exploration/env_infos/height Std                        0.0848425\n",
      "exploration/env_infos/height Max                        0.140976\n",
      "exploration/env_infos/height Min                       -0.397777\n",
      "exploration/env_infos/final/reward_angular Mean        -0.207992\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.207992\n",
      "exploration/env_infos/final/reward_angular Min         -0.207992\n",
      "exploration/env_infos/initial/reward_angular Mean       1.33347\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.33347\n",
      "exploration/env_infos/initial/reward_angular Min        1.33347\n",
      "exploration/env_infos/reward_angular Mean              -0.0579763\n",
      "exploration/env_infos/reward_angular Std                2.0601\n",
      "exploration/env_infos/reward_angular Max                5.38424\n",
      "exploration/env_infos/reward_angular Min               -5.20571\n",
      "evaluation/num steps total                              3.75e+06\n",
      "evaluation/num paths total                           3750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.187869\n",
      "evaluation/Rewards Std                                  1.19002\n",
      "evaluation/Rewards Max                                  9.93823\n",
      "evaluation/Rewards Min                                 -9.42788\n",
      "evaluation/Returns Mean                               187.869\n",
      "evaluation/Returns Std                                437.334\n",
      "evaluation/Returns Max                               1225.03\n",
      "evaluation/Returns Min                               -486.749\n",
      "evaluation/Actions Mean                                -0.0275435\n",
      "evaluation/Actions Std                                  0.759146\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            187.869\n",
      "evaluation/env_infos/final/reward_run Mean              0.230886\n",
      "evaluation/env_infos/final/reward_run Std               1.61442\n",
      "evaluation/env_infos/final/reward_run Max               2.85647\n",
      "evaluation/env_infos/final/reward_run Min              -4.9013\n",
      "evaluation/env_infos/initial/reward_run Mean            0.234037\n",
      "evaluation/env_infos/initial/reward_run Std             0.46072\n",
      "evaluation/env_infos/initial/reward_run Max             1.06425\n",
      "evaluation/env_infos/initial/reward_run Min            -0.586629\n",
      "evaluation/env_infos/reward_run Mean                   -0.403154\n",
      "evaluation/env_infos/reward_run Std                     2.20852\n",
      "evaluation/env_infos/reward_run Max                     4.60524\n",
      "evaluation/env_infos/reward_run Min                    -8.13241\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.327203\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11284\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0651047\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.568911\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.266782\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0892154\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0950869\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.470371\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.346237\n",
      "evaluation/env_infos/reward_ctrl Std                    0.110244\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.040303\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596832\n",
      "evaluation/env_infos/final/height Mean                 -0.329729\n",
      "evaluation/env_infos/final/height Std                   0.205144\n",
      "evaluation/env_infos/final/height Max                   0.0240596\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.0158669\n",
      "evaluation/env_infos/initial/height Std                 0.0469609\n",
      "evaluation/env_infos/initial/height Max                 0.0844777\n",
      "evaluation/env_infos/initial/height Min                -0.0708008\n",
      "evaluation/env_infos/height Mean                       -0.252737\n",
      "evaluation/env_infos/height Std                         0.217264\n",
      "evaluation/env_infos/height Max                         0.552509\n",
      "evaluation/env_infos/height Min                        -0.592713\n",
      "evaluation/env_infos/final/reward_angular Mean          0.183882\n",
      "evaluation/env_infos/final/reward_angular Std           1.54338\n",
      "evaluation/env_infos/final/reward_angular Max           3.97086\n",
      "evaluation/env_infos/final/reward_angular Min          -4.51874\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.493585\n",
      "evaluation/env_infos/initial/reward_angular Std         1.00294\n",
      "evaluation/env_infos/initial/reward_angular Max         2.01315\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.39827\n",
      "evaluation/env_infos/reward_angular Mean                0.0136591\n",
      "evaluation/env_infos/reward_angular Std                 1.7499\n",
      "evaluation/env_infos/reward_angular Max                11.9316\n",
      "evaluation/env_infos/reward_angular Min                -6.81\n",
      "time/data storing (s)                                   0.0160074\n",
      "time/evaluation sampling (s)                           23.7399\n",
      "time/exploration sampling (s)                           1.47314\n",
      "time/logging (s)                                        0.247474\n",
      "time/saving (s)                                         0.0288653\n",
      "time/training (s)                                       5.38386\n",
      "time/epoch (s)                                         30.8893\n",
      "time/total (s)                                       4493.49\n",
      "Epoch                                                 149\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:27:28.238717 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 150 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 152000\n",
      "trainer/QF1 Loss                                        2.30291\n",
      "trainer/QF2 Loss                                        1.68737\n",
      "trainer/Policy Loss                                    -6.08452\n",
      "trainer/Q1 Predictions Mean                            12.1757\n",
      "trainer/Q1 Predictions Std                             24.0269\n",
      "trainer/Q1 Predictions Max                             95.2598\n",
      "trainer/Q1 Predictions Min                            -17.0926\n",
      "trainer/Q2 Predictions Mean                            11.9009\n",
      "trainer/Q2 Predictions Std                             23.7846\n",
      "trainer/Q2 Predictions Max                             95.4722\n",
      "trainer/Q2 Predictions Min                            -17.2892\n",
      "trainer/Q Targets Mean                                 11.8942\n",
      "trainer/Q Targets Std                                  23.7118\n",
      "trainer/Q Targets Max                                  95.8235\n",
      "trainer/Q Targets Min                                 -16.6694\n",
      "trainer/Log Pis Mean                                    6.20186\n",
      "trainer/Log Pis Std                                     5.81814\n",
      "trainer/Log Pis Max                                    31.8826\n",
      "trainer/Log Pis Min                                    -7.66969\n",
      "trainer/Policy mu Mean                                 -0.140897\n",
      "trainer/Policy mu Std                                   1.64743\n",
      "trainer/Policy mu Max                                   6.23771\n",
      "trainer/Policy mu Min                                  -6.64534\n",
      "trainer/Policy log std Mean                            -0.668897\n",
      "trainer/Policy log std Std                              0.292507\n",
      "trainer/Policy log std Max                              0.50099\n",
      "trainer/Policy log std Min                             -1.74132\n",
      "trainer/Alpha                                           0.0197705\n",
      "trainer/Alpha Loss                                      0.792209\n",
      "exploration/num steps total                        152000\n",
      "exploration/num paths total                           152\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.317528\n",
      "exploration/Rewards Std                                 1.713\n",
      "exploration/Rewards Max                                 3.37956\n",
      "exploration/Rewards Min                                -5.36635\n",
      "exploration/Returns Mean                             -317.528\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -317.528\n",
      "exploration/Returns Min                              -317.528\n",
      "exploration/Actions Mean                               -0.00312024\n",
      "exploration/Actions Std                                 0.82828\n",
      "exploration/Actions Max                                 0.999711\n",
      "exploration/Actions Min                                -0.999832\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -317.528\n",
      "exploration/env_infos/final/reward_run Mean            -3.98462\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.98462\n",
      "exploration/env_infos/final/reward_run Min             -3.98462\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0848323\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0848323\n",
      "exploration/env_infos/initial/reward_run Min           -0.0848323\n",
      "exploration/env_infos/reward_run Mean                  -4.21745\n",
      "exploration/env_infos/reward_run Std                    1.14654\n",
      "exploration/env_infos/reward_run Max                   -0.0848323\n",
      "exploration/env_infos/reward_run Min                   -7.41321\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.503643\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.503643\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.503643\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.406838\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.406838\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.406838\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.411634\n",
      "exploration/env_infos/reward_ctrl Std                   0.0856896\n",
      "exploration/env_infos/reward_ctrl Max                  -0.136393\n",
      "exploration/env_infos/reward_ctrl Min                  -0.582046\n",
      "exploration/env_infos/final/height Mean                -0.17525\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.17525\n",
      "exploration/env_infos/final/height Min                 -0.17525\n",
      "exploration/env_infos/initial/height Mean               0.0717603\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0717603\n",
      "exploration/env_infos/initial/height Min                0.0717603\n",
      "exploration/env_infos/height Mean                      -0.0179295\n",
      "exploration/env_infos/height Std                        0.115482\n",
      "exploration/env_infos/height Max                        0.433379\n",
      "exploration/env_infos/height Min                       -0.297116\n",
      "exploration/env_infos/final/reward_angular Mean        -3.22899\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.22899\n",
      "exploration/env_infos/final/reward_angular Min         -3.22899\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.62896\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.62896\n",
      "exploration/env_infos/initial/reward_angular Min       -0.62896\n",
      "exploration/env_infos/reward_angular Mean               0.107891\n",
      "exploration/env_infos/reward_angular Std                3.02236\n",
      "exploration/env_infos/reward_angular Max                9.09021\n",
      "exploration/env_infos/reward_angular Min               -6.5364\n",
      "evaluation/num steps total                              3.775e+06\n",
      "evaluation/num paths total                           3775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.524157\n",
      "evaluation/Rewards Std                                  1.52642\n",
      "evaluation/Rewards Max                                  8.35205\n",
      "evaluation/Rewards Min                                 -9.18865\n",
      "evaluation/Returns Mean                               524.157\n",
      "evaluation/Returns Std                                801.364\n",
      "evaluation/Returns Max                               2521.09\n",
      "evaluation/Returns Min                               -440.816\n",
      "evaluation/Actions Mean                                 0.0284825\n",
      "evaluation/Actions Std                                  0.791348\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            524.157\n",
      "evaluation/env_infos/final/reward_run Mean             -0.55291\n",
      "evaluation/env_infos/final/reward_run Std               2.2939\n",
      "evaluation/env_infos/final/reward_run Max               4.15016\n",
      "evaluation/env_infos/final/reward_run Min              -5.53429\n",
      "evaluation/env_infos/initial/reward_run Mean            0.238418\n",
      "evaluation/env_infos/initial/reward_run Std             0.403806\n",
      "evaluation/env_infos/initial/reward_run Max             0.942757\n",
      "evaluation/env_infos/initial/reward_run Min            -0.534422\n",
      "evaluation/env_infos/reward_run Mean                   -0.946325\n",
      "evaluation/env_infos/reward_run Std                     2.63836\n",
      "evaluation/env_infos/reward_run Max                     5.16723\n",
      "evaluation/env_infos/reward_run Min                    -7.45537\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.341687\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109877\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.126802\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.5218\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.291683\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.104533\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.1155\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.453863\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.376226\n",
      "evaluation/env_infos/reward_ctrl Std                    0.104814\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0377971\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595019\n",
      "evaluation/env_infos/final/height Mean                 -0.136048\n",
      "evaluation/env_infos/final/height Std                   0.210492\n",
      "evaluation/env_infos/final/height Max                   0.223765\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00270649\n",
      "evaluation/env_infos/initial/height Std                 0.0436284\n",
      "evaluation/env_infos/initial/height Max                 0.090958\n",
      "evaluation/env_infos/initial/height Min                -0.0875866\n",
      "evaluation/env_infos/height Mean                       -0.104764\n",
      "evaluation/env_infos/height Std                         0.189777\n",
      "evaluation/env_infos/height Max                         0.55257\n",
      "evaluation/env_infos/height Min                        -0.58245\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.065915\n",
      "evaluation/env_infos/final/reward_angular Std           1.30952\n",
      "evaluation/env_infos/final/reward_angular Max           3.77366\n",
      "evaluation/env_infos/final/reward_angular Min          -2.78117\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.431696\n",
      "evaluation/env_infos/initial/reward_angular Std         0.942962\n",
      "evaluation/env_infos/initial/reward_angular Max         1.78482\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.60648\n",
      "evaluation/env_infos/reward_angular Mean                0.04692\n",
      "evaluation/env_infos/reward_angular Std                 2.28352\n",
      "evaluation/env_infos/reward_angular Max                10.8369\n",
      "evaluation/env_infos/reward_angular Min                -7.01983\n",
      "time/data storing (s)                                   0.0171897\n",
      "time/evaluation sampling (s)                           22.5595\n",
      "time/exploration sampling (s)                           1.34889\n",
      "time/logging (s)                                        0.253274\n",
      "time/saving (s)                                         0.0577226\n",
      "time/training (s)                                       6.43731\n",
      "time/epoch (s)                                         30.6739\n",
      "time/total (s)                                       4525.36\n",
      "Epoch                                                 150\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:28:04.611489 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 151 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 153000\n",
      "trainer/QF1 Loss                                        1.36958\n",
      "trainer/QF2 Loss                                        1.53033\n",
      "trainer/Policy Loss                                    -8.7776\n",
      "trainer/Q1 Predictions Mean                            14.5657\n",
      "trainer/Q1 Predictions Std                             25.2927\n",
      "trainer/Q1 Predictions Max                            101.057\n",
      "trainer/Q1 Predictions Min                            -15.5544\n",
      "trainer/Q2 Predictions Mean                            14.466\n",
      "trainer/Q2 Predictions Std                             25.3718\n",
      "trainer/Q2 Predictions Max                            100.651\n",
      "trainer/Q2 Predictions Min                            -16.5334\n",
      "trainer/Q Targets Mean                                 14.3323\n",
      "trainer/Q Targets Std                                  25.1022\n",
      "trainer/Q Targets Max                                  99.1828\n",
      "trainer/Q Targets Min                                 -16.9205\n",
      "trainer/Log Pis Mean                                    5.92546\n",
      "trainer/Log Pis Std                                     5.53263\n",
      "trainer/Log Pis Max                                    25.9966\n",
      "trainer/Log Pis Min                                    -5.00464\n",
      "trainer/Policy mu Mean                                  0.0785187\n",
      "trainer/Policy mu Std                                   1.62098\n",
      "trainer/Policy mu Max                                   3.87999\n",
      "trainer/Policy mu Min                                  -5.1515\n",
      "trainer/Policy log std Mean                            -0.679966\n",
      "trainer/Policy log std Std                              0.254545\n",
      "trainer/Policy log std Max                              0.281373\n",
      "trainer/Policy log std Min                             -1.9775\n",
      "trainer/Alpha                                           0.020071\n",
      "trainer/Alpha Loss                                     -0.291289\n",
      "exploration/num steps total                        153000\n",
      "exploration/num paths total                           153\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0652733\n",
      "exploration/Rewards Std                                 0.866001\n",
      "exploration/Rewards Max                                 6.56161\n",
      "exploration/Rewards Min                                -5.41102\n",
      "exploration/Returns Mean                               65.2733\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                65.2733\n",
      "exploration/Returns Min                                65.2733\n",
      "exploration/Actions Mean                               -0.0146503\n",
      "exploration/Actions Std                                 0.778995\n",
      "exploration/Actions Max                                 0.999997\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            65.2733\n",
      "exploration/env_infos/final/reward_run Mean            -0.124451\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.124451\n",
      "exploration/env_infos/final/reward_run Min             -0.124451\n",
      "exploration/env_infos/initial/reward_run Mean           0.443059\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.443059\n",
      "exploration/env_infos/initial/reward_run Min            0.443059\n",
      "exploration/env_infos/reward_run Mean                   0.12543\n",
      "exploration/env_infos/reward_run Std                    0.3325\n",
      "exploration/env_infos/reward_run Max                    1.74179\n",
      "exploration/env_infos/reward_run Min                   -1.74591\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.302591\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.302591\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.302591\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.456296\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.456296\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.456296\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.364229\n",
      "exploration/env_infos/reward_ctrl Std                   0.0672101\n",
      "exploration/env_infos/reward_ctrl Max                  -0.134223\n",
      "exploration/env_infos/reward_ctrl Min                  -0.582134\n",
      "exploration/env_infos/final/height Mean                -0.18117\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.18117\n",
      "exploration/env_infos/final/height Min                 -0.18117\n",
      "exploration/env_infos/initial/height Mean              -0.0687089\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0687089\n",
      "exploration/env_infos/initial/height Min               -0.0687089\n",
      "exploration/env_infos/height Mean                      -0.150726\n",
      "exploration/env_infos/height Std                        0.0892527\n",
      "exploration/env_infos/height Max                        0.386752\n",
      "exploration/env_infos/height Min                       -0.570984\n",
      "exploration/env_infos/final/reward_angular Mean        -0.429037\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.429037\n",
      "exploration/env_infos/final/reward_angular Min         -0.429037\n",
      "exploration/env_infos/initial/reward_angular Mean       2.63837\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.63837\n",
      "exploration/env_infos/initial/reward_angular Min        2.63837\n",
      "exploration/env_infos/reward_angular Mean               0.149991\n",
      "exploration/env_infos/reward_angular Std                0.928877\n",
      "exploration/env_infos/reward_angular Max                6.77478\n",
      "exploration/env_infos/reward_angular Min               -5.85504\n",
      "evaluation/num steps total                              3.8e+06\n",
      "evaluation/num paths total                           3800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.527988\n",
      "evaluation/Rewards Std                                  1.58485\n",
      "evaluation/Rewards Max                                  7.83038\n",
      "evaluation/Rewards Min                                 -8.1871\n",
      "evaluation/Returns Mean                               527.988\n",
      "evaluation/Returns Std                                818.067\n",
      "evaluation/Returns Max                               2321.04\n",
      "evaluation/Returns Min                               -789.703\n",
      "evaluation/Actions Mean                                 0.0595724\n",
      "evaluation/Actions Std                                  0.825705\n",
      "evaluation/Actions Max                                  0.999995\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            527.988\n",
      "evaluation/env_infos/final/reward_run Mean             -1.63339\n",
      "evaluation/env_infos/final/reward_run Std               2.69883\n",
      "evaluation/env_infos/final/reward_run Max               3.76383\n",
      "evaluation/env_infos/final/reward_run Min              -5.95366\n",
      "evaluation/env_infos/initial/reward_run Mean            0.164362\n",
      "evaluation/env_infos/initial/reward_run Std             0.42215\n",
      "evaluation/env_infos/initial/reward_run Max             1.04934\n",
      "evaluation/env_infos/initial/reward_run Min            -0.425861\n",
      "evaluation/env_infos/reward_run Mean                   -1.51873\n",
      "evaluation/env_infos/reward_run Std                     2.46392\n",
      "evaluation/env_infos/reward_run Max                     4.81498\n",
      "evaluation/env_infos/reward_run Min                    -6.99909\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.413188\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104627\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.133825\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.543634\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.307584\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.103023\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.134948\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.519582\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.411203\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0902985\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0167767\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597922\n",
      "evaluation/env_infos/final/height Mean                 -0.134518\n",
      "evaluation/env_infos/final/height Std                   0.134229\n",
      "evaluation/env_infos/final/height Max                   0.0664486\n",
      "evaluation/env_infos/final/height Min                  -0.57682\n",
      "evaluation/env_infos/initial/height Mean               -0.0159591\n",
      "evaluation/env_infos/initial/height Std                 0.053707\n",
      "evaluation/env_infos/initial/height Max                 0.079825\n",
      "evaluation/env_infos/initial/height Min                -0.0857901\n",
      "evaluation/env_infos/height Mean                       -0.0692027\n",
      "evaluation/env_infos/height Std                         0.144953\n",
      "evaluation/env_infos/height Max                         0.487311\n",
      "evaluation/env_infos/height Min                        -0.579883\n",
      "evaluation/env_infos/final/reward_angular Mean         -1.27167\n",
      "evaluation/env_infos/final/reward_angular Std           2.26946\n",
      "evaluation/env_infos/final/reward_angular Max           3.0429\n",
      "evaluation/env_infos/final/reward_angular Min          -4.81605\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.738073\n",
      "evaluation/env_infos/initial/reward_angular Std         0.668489\n",
      "evaluation/env_infos/initial/reward_angular Max         1.40407\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.71104\n",
      "evaluation/env_infos/reward_angular Mean                0.0837808\n",
      "evaluation/env_infos/reward_angular Std                 2.30727\n",
      "evaluation/env_infos/reward_angular Max                10.5633\n",
      "evaluation/env_infos/reward_angular Min                -7.05232\n",
      "time/data storing (s)                                   0.0172588\n",
      "time/evaluation sampling (s)                           28.938\n",
      "time/exploration sampling (s)                           1.33995\n",
      "time/logging (s)                                        0.2544\n",
      "time/saving (s)                                         0.0295985\n",
      "time/training (s)                                       4.45073\n",
      "time/epoch (s)                                         35.0299\n",
      "time/total (s)                                       4561.73\n",
      "Epoch                                                 151\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:28:39.926726 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 152 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 154000\n",
      "trainer/QF1 Loss                                        1.75575\n",
      "trainer/QF2 Loss                                        1.92693\n",
      "trainer/Policy Loss                                    -5.27128\n",
      "trainer/Q1 Predictions Mean                            11.2144\n",
      "trainer/Q1 Predictions Std                             24.4428\n",
      "trainer/Q1 Predictions Max                            102.832\n",
      "trainer/Q1 Predictions Min                            -15.4517\n",
      "trainer/Q2 Predictions Mean                            11.2325\n",
      "trainer/Q2 Predictions Std                             24.4826\n",
      "trainer/Q2 Predictions Max                            102.535\n",
      "trainer/Q2 Predictions Min                            -15.9929\n",
      "trainer/Q Targets Mean                                 11.3915\n",
      "trainer/Q Targets Std                                  24.4041\n",
      "trainer/Q Targets Max                                 106.87\n",
      "trainer/Q Targets Min                                 -16.3369\n",
      "trainer/Log Pis Mean                                    6.12657\n",
      "trainer/Log Pis Std                                     5.55415\n",
      "trainer/Log Pis Max                                    29.2334\n",
      "trainer/Log Pis Min                                    -4.30769\n",
      "trainer/Policy mu Mean                                  0.0564454\n",
      "trainer/Policy mu Std                                   1.63925\n",
      "trainer/Policy mu Max                                   4.61226\n",
      "trainer/Policy mu Min                                  -5.6175\n",
      "trainer/Policy log std Mean                            -0.681263\n",
      "trainer/Policy log std Std                              0.269299\n",
      "trainer/Policy log std Max                              0.145138\n",
      "trainer/Policy log std Min                             -2.03455\n",
      "trainer/Alpha                                           0.0193842\n",
      "trainer/Alpha Loss                                      0.499147\n",
      "exploration/num steps total                        154000\n",
      "exploration/num paths total                           154\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.361791\n",
      "exploration/Rewards Std                                 0.42234\n",
      "exploration/Rewards Max                                 1.51429\n",
      "exploration/Rewards Min                                -1.30387\n",
      "exploration/Returns Mean                              361.791\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               361.791\n",
      "exploration/Returns Min                               361.791\n",
      "exploration/Actions Mean                                0.139356\n",
      "exploration/Actions Std                                 0.693274\n",
      "exploration/Actions Max                                 0.999972\n",
      "exploration/Actions Min                                -0.999989\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           361.791\n",
      "exploration/env_infos/final/reward_run Mean             1.85943\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.85943\n",
      "exploration/env_infos/final/reward_run Min              1.85943\n",
      "exploration/env_infos/initial/reward_run Mean           0.621896\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.621896\n",
      "exploration/env_infos/initial/reward_run Min            0.621896\n",
      "exploration/env_infos/reward_run Mean                   1.87796\n",
      "exploration/env_infos/reward_run Std                    0.953693\n",
      "exploration/env_infos/reward_run Max                    4.38902\n",
      "exploration/env_infos/reward_run Min                   -1.02472\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.314944\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.314944\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.314944\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.145979\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.145979\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.145979\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.300029\n",
      "exploration/env_infos/reward_ctrl Std                   0.0869132\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0520512\n",
      "exploration/env_infos/reward_ctrl Min                  -0.536222\n",
      "exploration/env_infos/final/height Mean                -0.0308999\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0308999\n",
      "exploration/env_infos/final/height Min                 -0.0308999\n",
      "exploration/env_infos/initial/height Mean               0.0680069\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0680069\n",
      "exploration/env_infos/initial/height Min                0.0680069\n",
      "exploration/env_infos/height Mean                      -0.0607823\n",
      "exploration/env_infos/height Std                        0.0928237\n",
      "exploration/env_infos/height Max                        0.253743\n",
      "exploration/env_infos/height Min                       -0.351124\n",
      "exploration/env_infos/final/reward_angular Mean         3.1479\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.1479\n",
      "exploration/env_infos/final/reward_angular Min          3.1479\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.443838\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.443838\n",
      "exploration/env_infos/initial/reward_angular Min       -0.443838\n",
      "exploration/env_infos/reward_angular Mean              -0.0520146\n",
      "exploration/env_infos/reward_angular Std                2.07636\n",
      "exploration/env_infos/reward_angular Max                5.9008\n",
      "exploration/env_infos/reward_angular Min               -6.67025\n",
      "evaluation/num steps total                              3.825e+06\n",
      "evaluation/num paths total                           3825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.223005\n",
      "evaluation/Rewards Std                                  1.26206\n",
      "evaluation/Rewards Max                                  7.77145\n",
      "evaluation/Rewards Min                                 -8.60745\n",
      "evaluation/Returns Mean                               223.005\n",
      "evaluation/Returns Std                                584.582\n",
      "evaluation/Returns Max                               1832.05\n",
      "evaluation/Returns Min                               -512.589\n",
      "evaluation/Actions Mean                                -0.0150639\n",
      "evaluation/Actions Std                                  0.754091\n",
      "evaluation/Actions Max                                  0.999994\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            223.005\n",
      "evaluation/env_infos/final/reward_run Mean              0.475567\n",
      "evaluation/env_infos/final/reward_run Std               1.2114\n",
      "evaluation/env_infos/final/reward_run Max               4.35957\n",
      "evaluation/env_infos/final/reward_run Min              -0.989049\n",
      "evaluation/env_infos/initial/reward_run Mean            0.20565\n",
      "evaluation/env_infos/initial/reward_run Std             0.498211\n",
      "evaluation/env_infos/initial/reward_run Max             1.09667\n",
      "evaluation/env_infos/initial/reward_run Min            -0.607407\n",
      "evaluation/env_infos/reward_run Mean                    0.0523003\n",
      "evaluation/env_infos/reward_run Std                     1.79817\n",
      "evaluation/env_infos/reward_run Max                     5.02651\n",
      "evaluation/env_infos/reward_run Min                    -6.96666\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.306071\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.115643\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0815233\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.487365\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.296158\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107131\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.118807\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.491277\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.341329\n",
      "evaluation/env_infos/reward_ctrl Std                    0.110756\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0353742\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59837\n",
      "evaluation/env_infos/final/height Mean                 -0.278388\n",
      "evaluation/env_infos/final/height Std                   0.24769\n",
      "evaluation/env_infos/final/height Max                   0.165131\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0213041\n",
      "evaluation/env_infos/initial/height Std                 0.0493168\n",
      "evaluation/env_infos/initial/height Max                 0.0749663\n",
      "evaluation/env_infos/initial/height Min                -0.0993152\n",
      "evaluation/env_infos/height Mean                       -0.219518\n",
      "evaluation/env_infos/height Std                         0.233302\n",
      "evaluation/env_infos/height Max                         0.541739\n",
      "evaluation/env_infos/height Min                        -0.590614\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.188809\n",
      "evaluation/env_infos/final/reward_angular Std           1.1212\n",
      "evaluation/env_infos/final/reward_angular Max           1.81431\n",
      "evaluation/env_infos/final/reward_angular Min          -3.58728\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.639114\n",
      "evaluation/env_infos/initial/reward_angular Std         0.835139\n",
      "evaluation/env_infos/initial/reward_angular Max         1.18031\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.90623\n",
      "evaluation/env_infos/reward_angular Mean               -0.00624483\n",
      "evaluation/env_infos/reward_angular Std                 1.83125\n",
      "evaluation/env_infos/reward_angular Max                 9.58267\n",
      "evaluation/env_infos/reward_angular Min                -8.11784\n",
      "time/data storing (s)                                   0.0178535\n",
      "time/evaluation sampling (s)                           24.8952\n",
      "time/exploration sampling (s)                           1.28359\n",
      "time/logging (s)                                        0.283716\n",
      "time/saving (s)                                         0.0342065\n",
      "time/training (s)                                       7.47219\n",
      "time/epoch (s)                                         33.9867\n",
      "time/total (s)                                       4597.07\n",
      "Epoch                                                 152\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:29:11.095617 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 153 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 155000\n",
      "trainer/QF1 Loss                                        1.50227\n",
      "trainer/QF2 Loss                                        1.36065\n",
      "trainer/Policy Loss                                    -6.16136\n",
      "trainer/Q1 Predictions Mean                            12.0772\n",
      "trainer/Q1 Predictions Std                             25.0309\n",
      "trainer/Q1 Predictions Max                            101.157\n",
      "trainer/Q1 Predictions Min                            -16.4416\n",
      "trainer/Q2 Predictions Mean                            12.052\n",
      "trainer/Q2 Predictions Std                             24.8148\n",
      "trainer/Q2 Predictions Max                            102.058\n",
      "trainer/Q2 Predictions Min                            -16.2666\n",
      "trainer/Q Targets Mean                                 11.8095\n",
      "trainer/Q Targets Std                                  24.9555\n",
      "trainer/Q Targets Max                                 100.81\n",
      "trainer/Q Targets Min                                 -16.9773\n",
      "trainer/Log Pis Mean                                    6.13631\n",
      "trainer/Log Pis Std                                     6.12842\n",
      "trainer/Log Pis Max                                    34.6156\n",
      "trainer/Log Pis Min                                    -4.79404\n",
      "trainer/Policy mu Mean                                 -0.0209329\n",
      "trainer/Policy mu Std                                   1.60811\n",
      "trainer/Policy mu Max                                   7.37976\n",
      "trainer/Policy mu Min                                  -5.91616\n",
      "trainer/Policy log std Mean                            -0.710127\n",
      "trainer/Policy log std Std                              0.28261\n",
      "trainer/Policy log std Max                              0.560544\n",
      "trainer/Policy log std Min                             -2.15508\n",
      "trainer/Alpha                                           0.019517\n",
      "trainer/Alpha Loss                                      0.536801\n",
      "exploration/num steps total                        155000\n",
      "exploration/num paths total                           155\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.52047\n",
      "exploration/Rewards Std                                 1.04351\n",
      "exploration/Rewards Max                                 3.95578\n",
      "exploration/Rewards Min                                -2.24848\n",
      "exploration/Returns Mean                             1520.47\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1520.47\n",
      "exploration/Returns Min                              1520.47\n",
      "exploration/Actions Mean                                0.21784\n",
      "exploration/Actions Std                                 0.822319\n",
      "exploration/Actions Max                                 0.999969\n",
      "exploration/Actions Min                                -0.999927\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1520.47\n",
      "exploration/env_infos/final/reward_run Mean             2.02105\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.02105\n",
      "exploration/env_infos/final/reward_run Min              2.02105\n",
      "exploration/env_infos/initial/reward_run Mean           0.897685\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.897685\n",
      "exploration/env_infos/initial/reward_run Min            0.897685\n",
      "exploration/env_infos/reward_run Mean                   1.94601\n",
      "exploration/env_infos/reward_run Std                    0.756457\n",
      "exploration/env_infos/reward_run Max                    3.93834\n",
      "exploration/env_infos/reward_run Min                   -1.3492\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.484236\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.484236\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.484236\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.422434\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.422434\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.422434\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.434197\n",
      "exploration/env_infos/reward_ctrl Std                   0.0813267\n",
      "exploration/env_infos/reward_ctrl Max                  -0.187502\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593316\n",
      "exploration/env_infos/final/height Mean                -0.263205\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.263205\n",
      "exploration/env_infos/final/height Min                 -0.263205\n",
      "exploration/env_infos/initial/height Mean               0.00873854\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00873854\n",
      "exploration/env_infos/initial/height Min                0.00873854\n",
      "exploration/env_infos/height Mean                      -0.170947\n",
      "exploration/env_infos/height Std                        0.100461\n",
      "exploration/env_infos/height Max                        0.186076\n",
      "exploration/env_infos/height Min                       -0.389188\n",
      "exploration/env_infos/final/reward_angular Mean        -2.65637\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.65637\n",
      "exploration/env_infos/final/reward_angular Min         -2.65637\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.01694\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.01694\n",
      "exploration/env_infos/initial/reward_angular Min       -1.01694\n",
      "exploration/env_infos/reward_angular Mean              -0.0770407\n",
      "exploration/env_infos/reward_angular Std                1.89715\n",
      "exploration/env_infos/reward_angular Max                5.05611\n",
      "exploration/env_infos/reward_angular Min               -4.78995\n",
      "evaluation/num steps total                              3.85e+06\n",
      "evaluation/num paths total                           3850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.376099\n",
      "evaluation/Rewards Std                                  1.2765\n",
      "evaluation/Rewards Max                                  8.91901\n",
      "evaluation/Rewards Min                                -10.1375\n",
      "evaluation/Returns Mean                               376.099\n",
      "evaluation/Returns Std                                632.077\n",
      "evaluation/Returns Max                               1778.78\n",
      "evaluation/Returns Min                               -482.262\n",
      "evaluation/Actions Mean                                -0.0488555\n",
      "evaluation/Actions Std                                  0.752093\n",
      "evaluation/Actions Max                                  0.999993\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            376.099\n",
      "evaluation/env_infos/final/reward_run Mean              0.311246\n",
      "evaluation/env_infos/final/reward_run Std               1.6459\n",
      "evaluation/env_infos/final/reward_run Max               3.17875\n",
      "evaluation/env_infos/final/reward_run Min              -3.84284\n",
      "evaluation/env_infos/initial/reward_run Mean            0.160839\n",
      "evaluation/env_infos/initial/reward_run Std             0.450479\n",
      "evaluation/env_infos/initial/reward_run Max             1.10712\n",
      "evaluation/env_infos/initial/reward_run Min            -0.651251\n",
      "evaluation/env_infos/reward_run Mean                   -0.356338\n",
      "evaluation/env_infos/reward_run Std                     2.34172\n",
      "evaluation/env_infos/reward_run Max                     5.35821\n",
      "evaluation/env_infos/reward_run Min                    -7.44455\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.287103\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.106615\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0981515\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.533294\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.257856\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0845504\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.111379\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.423955\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.340818\n",
      "evaluation/env_infos/reward_ctrl Std                    0.128506\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.033919\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599049\n",
      "evaluation/env_infos/final/height Mean                 -0.31878\n",
      "evaluation/env_infos/final/height Std                   0.262147\n",
      "evaluation/env_infos/final/height Max                   0.094144\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.027887\n",
      "evaluation/env_infos/initial/height Std                 0.0520942\n",
      "evaluation/env_infos/initial/height Max                 0.0823202\n",
      "evaluation/env_infos/initial/height Min                -0.114936\n",
      "evaluation/env_infos/height Mean                       -0.197265\n",
      "evaluation/env_infos/height Std                         0.266016\n",
      "evaluation/env_infos/height Max                         0.576958\n",
      "evaluation/env_infos/height Min                        -0.587464\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.248062\n",
      "evaluation/env_infos/final/reward_angular Std           1.26216\n",
      "evaluation/env_infos/final/reward_angular Max           2.8031\n",
      "evaluation/env_infos/final/reward_angular Min          -3.35843\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.218644\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07767\n",
      "evaluation/env_infos/initial/reward_angular Max         2.41843\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.4152\n",
      "evaluation/env_infos/reward_angular Mean               -0.00787792\n",
      "evaluation/env_infos/reward_angular Std                 1.93791\n",
      "evaluation/env_infos/reward_angular Max                10.9602\n",
      "evaluation/env_infos/reward_angular Min                -7.40608\n",
      "time/data storing (s)                                   0.016575\n",
      "time/evaluation sampling (s)                           23.8918\n",
      "time/exploration sampling (s)                           1.12264\n",
      "time/logging (s)                                        0.250174\n",
      "time/saving (s)                                         0.0282013\n",
      "time/training (s)                                       4.57054\n",
      "time/epoch (s)                                         29.8799\n",
      "time/total (s)                                       4628.2\n",
      "Epoch                                                 153\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:29:42.625377 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 154 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 156000\n",
      "trainer/QF1 Loss                                        1.79863\n",
      "trainer/QF2 Loss                                        1.58694\n",
      "trainer/Policy Loss                                    -6.91606\n",
      "trainer/Q1 Predictions Mean                            11.99\n",
      "trainer/Q1 Predictions Std                             23.8955\n",
      "trainer/Q1 Predictions Max                            108.66\n",
      "trainer/Q1 Predictions Min                            -17.3987\n",
      "trainer/Q2 Predictions Mean                            11.9588\n",
      "trainer/Q2 Predictions Std                             23.8743\n",
      "trainer/Q2 Predictions Max                            108.418\n",
      "trainer/Q2 Predictions Min                            -17.2537\n",
      "trainer/Q Targets Mean                                 11.8598\n",
      "trainer/Q Targets Std                                  23.8766\n",
      "trainer/Q Targets Max                                 106.803\n",
      "trainer/Q Targets Min                                 -16.63\n",
      "trainer/Log Pis Mean                                    5.16029\n",
      "trainer/Log Pis Std                                     5.3488\n",
      "trainer/Log Pis Max                                    24.0549\n",
      "trainer/Log Pis Min                                    -5.20556\n",
      "trainer/Policy mu Mean                                  0.168293\n",
      "trainer/Policy mu Std                                   1.46126\n",
      "trainer/Policy mu Max                                   4.03325\n",
      "trainer/Policy mu Min                                  -5.75983\n",
      "trainer/Policy log std Mean                            -0.725176\n",
      "trainer/Policy log std Std                              0.280464\n",
      "trainer/Policy log std Max                              0.216492\n",
      "trainer/Policy log std Min                             -2.08036\n",
      "trainer/Alpha                                           0.0191124\n",
      "trainer/Alpha Loss                                     -3.32131\n",
      "exploration/num steps total                        156000\n",
      "exploration/num paths total                           156\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.117691\n",
      "exploration/Rewards Std                                 0.432531\n",
      "exploration/Rewards Max                                 1.70075\n",
      "exploration/Rewards Min                                -1.06753\n",
      "exploration/Returns Mean                             -117.691\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -117.691\n",
      "exploration/Returns Min                              -117.691\n",
      "exploration/Actions Mean                               -0.101938\n",
      "exploration/Actions Std                                 0.711404\n",
      "exploration/Actions Max                                 0.999719\n",
      "exploration/Actions Min                                -0.999891\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -117.691\n",
      "exploration/env_infos/final/reward_run Mean            -0.0782686\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0782686\n",
      "exploration/env_infos/final/reward_run Min             -0.0782686\n",
      "exploration/env_infos/initial/reward_run Mean           0.411704\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.411704\n",
      "exploration/env_infos/initial/reward_run Min            0.411704\n",
      "exploration/env_infos/reward_run Mean                  -1.38857\n",
      "exploration/env_infos/reward_run Std                    2.00475\n",
      "exploration/env_infos/reward_run Max                    1.0496\n",
      "exploration/env_infos/reward_run Min                   -7.08174\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.361672\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.361672\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.361672\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.192648\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.192648\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.192648\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.309892\n",
      "exploration/env_infos/reward_ctrl Std                   0.117509\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0528379\n",
      "exploration/env_infos/reward_ctrl Min                  -0.57572\n",
      "exploration/env_infos/final/height Mean                -0.577101\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577101\n",
      "exploration/env_infos/final/height Min                 -0.577101\n",
      "exploration/env_infos/initial/height Mean              -0.0774806\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0774806\n",
      "exploration/env_infos/initial/height Min               -0.0774806\n",
      "exploration/env_infos/height Mean                      -0.343742\n",
      "exploration/env_infos/height Std                        0.304415\n",
      "exploration/env_infos/height Max                        0.530901\n",
      "exploration/env_infos/height Min                       -0.578729\n",
      "exploration/env_infos/final/reward_angular Mean         0.00848694\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.00848694\n",
      "exploration/env_infos/final/reward_angular Min          0.00848694\n",
      "exploration/env_infos/initial/reward_angular Mean       1.81747\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.81747\n",
      "exploration/env_infos/initial/reward_angular Min        1.81747\n",
      "exploration/env_infos/reward_angular Mean              -0.0249637\n",
      "exploration/env_infos/reward_angular Std                1.86643\n",
      "exploration/env_infos/reward_angular Max                9.82263\n",
      "exploration/env_infos/reward_angular Min               -5.92134\n",
      "evaluation/num steps total                              3.875e+06\n",
      "evaluation/num paths total                           3875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.588438\n",
      "evaluation/Rewards Std                                  1.55961\n",
      "evaluation/Rewards Max                                  9.02906\n",
      "evaluation/Rewards Min                                 -7.47726\n",
      "evaluation/Returns Mean                               588.438\n",
      "evaluation/Returns Std                                669.708\n",
      "evaluation/Returns Max                               2051.59\n",
      "evaluation/Returns Min                               -441.321\n",
      "evaluation/Actions Mean                                 0.127605\n",
      "evaluation/Actions Std                                  0.799832\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            588.438\n",
      "evaluation/env_infos/final/reward_run Mean             -0.725166\n",
      "evaluation/env_infos/final/reward_run Std               2.39353\n",
      "evaluation/env_infos/final/reward_run Max               3.02486\n",
      "evaluation/env_infos/final/reward_run Min              -4.60545\n",
      "evaluation/env_infos/initial/reward_run Mean            0.171271\n",
      "evaluation/env_infos/initial/reward_run Std             0.460058\n",
      "evaluation/env_infos/initial/reward_run Max             0.953953\n",
      "evaluation/env_infos/initial/reward_run Min            -0.888257\n",
      "evaluation/env_infos/reward_run Mean                   -1.1761\n",
      "evaluation/env_infos/reward_run Std                     2.69964\n",
      "evaluation/env_infos/reward_run Max                     5.317\n",
      "evaluation/env_infos/reward_run Min                    -7.46151\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.377347\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.105209\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.131238\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.548495\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.307608\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0990849\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0804815\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.468879\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.393609\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112543\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0172388\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596483\n",
      "evaluation/env_infos/final/height Mean                 -0.205475\n",
      "evaluation/env_infos/final/height Std                   0.227945\n",
      "evaluation/env_infos/final/height Max                   0.0958305\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0112867\n",
      "evaluation/env_infos/initial/height Std                 0.052036\n",
      "evaluation/env_infos/initial/height Max                 0.0832679\n",
      "evaluation/env_infos/initial/height Min                -0.078916\n",
      "evaluation/env_infos/height Mean                       -0.129387\n",
      "evaluation/env_infos/height Std                         0.222047\n",
      "evaluation/env_infos/height Max                         0.524045\n",
      "evaluation/env_infos/height Min                        -0.59129\n",
      "evaluation/env_infos/final/reward_angular Mean          0.277959\n",
      "evaluation/env_infos/final/reward_angular Std           2.51898\n",
      "evaluation/env_infos/final/reward_angular Max           8.44093\n",
      "evaluation/env_infos/final/reward_angular Min          -4.49549\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.06828\n",
      "evaluation/env_infos/initial/reward_angular Std         1.14092\n",
      "evaluation/env_infos/initial/reward_angular Max         2.47707\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.64942\n",
      "evaluation/env_infos/reward_angular Mean                0.0474972\n",
      "evaluation/env_infos/reward_angular Std                 2.39414\n",
      "evaluation/env_infos/reward_angular Max                11.1026\n",
      "evaluation/env_infos/reward_angular Min                -7.36672\n",
      "time/data storing (s)                                   0.016286\n",
      "time/evaluation sampling (s)                           24.9312\n",
      "time/exploration sampling (s)                           1.00049\n",
      "time/logging (s)                                        0.235933\n",
      "time/saving (s)                                         0.027258\n",
      "time/training (s)                                       4.03856\n",
      "time/epoch (s)                                         30.2497\n",
      "time/total (s)                                       4659.71\n",
      "Epoch                                                 154\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:30:22.203514 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 155 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 157000\n",
      "trainer/QF1 Loss                                        1.67188\n",
      "trainer/QF2 Loss                                        2.0982\n",
      "trainer/Policy Loss                                    -7.45803\n",
      "trainer/Q1 Predictions Mean                            13.6274\n",
      "trainer/Q1 Predictions Std                             25.4713\n",
      "trainer/Q1 Predictions Max                            107.163\n",
      "trainer/Q1 Predictions Min                            -16.5466\n",
      "trainer/Q2 Predictions Mean                            13.3651\n",
      "trainer/Q2 Predictions Std                             25.1701\n",
      "trainer/Q2 Predictions Max                            108.506\n",
      "trainer/Q2 Predictions Min                            -16.939\n",
      "trainer/Q Targets Mean                                 13.4812\n",
      "trainer/Q Targets Std                                  25.4433\n",
      "trainer/Q Targets Max                                 108.325\n",
      "trainer/Q Targets Min                                 -16.2294\n",
      "trainer/Log Pis Mean                                    6.17601\n",
      "trainer/Log Pis Std                                     6.33779\n",
      "trainer/Log Pis Max                                    35.0361\n",
      "trainer/Log Pis Min                                    -4.3651\n",
      "trainer/Policy mu Mean                                  0.021827\n",
      "trainer/Policy mu Std                                   1.66199\n",
      "trainer/Policy mu Max                                   4.14426\n",
      "trainer/Policy mu Min                                  -9.66052\n",
      "trainer/Policy log std Mean                            -0.704438\n",
      "trainer/Policy log std Std                              0.281019\n",
      "trainer/Policy log std Max                              0.441698\n",
      "trainer/Policy log std Min                             -1.95407\n",
      "trainer/Alpha                                           0.0187926\n",
      "trainer/Alpha Loss                                      0.699762\n",
      "exploration/num steps total                        157000\n",
      "exploration/num paths total                           157\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.16934\n",
      "exploration/Rewards Std                                 1.23643\n",
      "exploration/Rewards Max                                 4.16036\n",
      "exploration/Rewards Min                                -2.25259\n",
      "exploration/Returns Mean                             1169.34\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1169.34\n",
      "exploration/Returns Min                              1169.34\n",
      "exploration/Actions Mean                                0.24168\n",
      "exploration/Actions Std                                 0.760063\n",
      "exploration/Actions Max                                 0.999849\n",
      "exploration/Actions Min                                -0.999809\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1169.34\n",
      "exploration/env_infos/final/reward_run Mean             0.339855\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.339855\n",
      "exploration/env_infos/final/reward_run Min              0.339855\n",
      "exploration/env_infos/initial/reward_run Mean           1.17132\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.17132\n",
      "exploration/env_infos/initial/reward_run Min            1.17132\n",
      "exploration/env_infos/reward_run Mean                   1.52591\n",
      "exploration/env_infos/reward_run Std                    0.59425\n",
      "exploration/env_infos/reward_run Max                    3.47763\n",
      "exploration/env_infos/reward_run Min                   -1.19368\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.387073\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.387073\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.387073\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.442662\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.442662\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.442662\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.381663\n",
      "exploration/env_infos/reward_ctrl Std                   0.092802\n",
      "exploration/env_infos/reward_ctrl Max                  -0.113165\n",
      "exploration/env_infos/reward_ctrl Min                  -0.560319\n",
      "exploration/env_infos/final/height Mean                -0.284646\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.284646\n",
      "exploration/env_infos/final/height Min                 -0.284646\n",
      "exploration/env_infos/initial/height Mean              -0.0563376\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0563376\n",
      "exploration/env_infos/initial/height Min               -0.0563376\n",
      "exploration/env_infos/height Mean                      -0.241115\n",
      "exploration/env_infos/height Std                        0.0751255\n",
      "exploration/env_infos/height Max                        0.0449946\n",
      "exploration/env_infos/height Min                       -0.402758\n",
      "exploration/env_infos/final/reward_angular Mean         1.80881\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.80881\n",
      "exploration/env_infos/final/reward_angular Min          1.80881\n",
      "exploration/env_infos/initial/reward_angular Mean       1.73206\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.73206\n",
      "exploration/env_infos/initial/reward_angular Min        1.73206\n",
      "exploration/env_infos/reward_angular Mean              -0.0958047\n",
      "exploration/env_infos/reward_angular Std                1.87449\n",
      "exploration/env_infos/reward_angular Max                5.37028\n",
      "exploration/env_infos/reward_angular Min               -6.51759\n",
      "evaluation/num steps total                              3.9e+06\n",
      "evaluation/num paths total                           3900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.290641\n",
      "evaluation/Rewards Std                                  1.42741\n",
      "evaluation/Rewards Max                                  7.9746\n",
      "evaluation/Rewards Min                                 -8.15008\n",
      "evaluation/Returns Mean                               290.641\n",
      "evaluation/Returns Std                                653.829\n",
      "evaluation/Returns Max                               1974.4\n",
      "evaluation/Returns Min                               -570.243\n",
      "evaluation/Actions Mean                                 0.0867555\n",
      "evaluation/Actions Std                                  0.779073\n",
      "evaluation/Actions Max                                  0.999985\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            290.641\n",
      "evaluation/env_infos/final/reward_run Mean             -0.247486\n",
      "evaluation/env_infos/final/reward_run Std               1.68878\n",
      "evaluation/env_infos/final/reward_run Max               2.76025\n",
      "evaluation/env_infos/final/reward_run Min              -4.76273\n",
      "evaluation/env_infos/initial/reward_run Mean            0.191341\n",
      "evaluation/env_infos/initial/reward_run Std             0.462724\n",
      "evaluation/env_infos/initial/reward_run Max             1.40408\n",
      "evaluation/env_infos/initial/reward_run Min            -0.405967\n",
      "evaluation/env_infos/reward_run Mean                   -0.611224\n",
      "evaluation/env_infos/reward_run Std                     2.4445\n",
      "evaluation/env_infos/reward_run Max                     5.3765\n",
      "evaluation/env_infos/reward_run Min                    -7.71696\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.358876\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10354\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.125707\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.579541\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.304721\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.106706\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0326759\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.530901\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.368689\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106292\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0109431\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595186\n",
      "evaluation/env_infos/final/height Mean                 -0.330772\n",
      "evaluation/env_infos/final/height Std                   0.238861\n",
      "evaluation/env_infos/final/height Max                   0.0230364\n",
      "evaluation/env_infos/final/height Min                  -0.577318\n",
      "evaluation/env_infos/initial/height Mean               -0.0103747\n",
      "evaluation/env_infos/initial/height Std                 0.0463505\n",
      "evaluation/env_infos/initial/height Max                 0.0703734\n",
      "evaluation/env_infos/initial/height Min                -0.0878982\n",
      "evaluation/env_infos/height Mean                       -0.232327\n",
      "evaluation/env_infos/height Std                         0.254498\n",
      "evaluation/env_infos/height Max                         0.601261\n",
      "evaluation/env_infos/height Min                        -0.586535\n",
      "evaluation/env_infos/final/reward_angular Mean          0.328309\n",
      "evaluation/env_infos/final/reward_angular Std           1.51107\n",
      "evaluation/env_infos/final/reward_angular Max           5.13689\n",
      "evaluation/env_infos/final/reward_angular Min          -2.29882\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0598291\n",
      "evaluation/env_infos/initial/reward_angular Std         1.17913\n",
      "evaluation/env_infos/initial/reward_angular Max         2.25337\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.53929\n",
      "evaluation/env_infos/reward_angular Mean                0.0273233\n",
      "evaluation/env_infos/reward_angular Std                 1.98079\n",
      "evaluation/env_infos/reward_angular Max                10.9818\n",
      "evaluation/env_infos/reward_angular Min                -8.40897\n",
      "time/data storing (s)                                   0.0160364\n",
      "time/evaluation sampling (s)                           32.1923\n",
      "time/exploration sampling (s)                           1.09404\n",
      "time/logging (s)                                        0.262559\n",
      "time/saving (s)                                         0.0285365\n",
      "time/training (s)                                       4.77771\n",
      "time/epoch (s)                                         38.3712\n",
      "time/total (s)                                       4699.32\n",
      "Epoch                                                 155\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:30:58.523625 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 156 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 158000\n",
      "trainer/QF1 Loss                                        2.10605\n",
      "trainer/QF2 Loss                                        2.0355\n",
      "trainer/Policy Loss                                    -7.62347\n",
      "trainer/Q1 Predictions Mean                            13.768\n",
      "trainer/Q1 Predictions Std                             25.1164\n",
      "trainer/Q1 Predictions Max                            105.26\n",
      "trainer/Q1 Predictions Min                            -17.3825\n",
      "trainer/Q2 Predictions Mean                            13.7438\n",
      "trainer/Q2 Predictions Std                             25.0764\n",
      "trainer/Q2 Predictions Max                            104.737\n",
      "trainer/Q2 Predictions Min                            -16.6139\n",
      "trainer/Q Targets Mean                                 13.6532\n",
      "trainer/Q Targets Std                                  25.4079\n",
      "trainer/Q Targets Max                                 106.413\n",
      "trainer/Q Targets Min                                 -17.4068\n",
      "trainer/Log Pis Mean                                    6.34829\n",
      "trainer/Log Pis Std                                     6.07587\n",
      "trainer/Log Pis Max                                    25.1432\n",
      "trainer/Log Pis Min                                    -6.86858\n",
      "trainer/Policy mu Mean                                 -0.081448\n",
      "trainer/Policy mu Std                                   1.64424\n",
      "trainer/Policy mu Max                                   5.94392\n",
      "trainer/Policy mu Min                                  -5.26909\n",
      "trainer/Policy log std Mean                            -0.735478\n",
      "trainer/Policy log std Std                              0.296441\n",
      "trainer/Policy log std Max                              0.347236\n",
      "trainer/Policy log std Min                             -1.95204\n",
      "trainer/Alpha                                           0.0190467\n",
      "trainer/Alpha Loss                                      1.37971\n",
      "exploration/num steps total                        158000\n",
      "exploration/num paths total                           158\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.146747\n",
      "exploration/Rewards Std                                 1.02216\n",
      "exploration/Rewards Max                                 4.60421\n",
      "exploration/Rewards Min                                -2.88336\n",
      "exploration/Returns Mean                             -146.747\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -146.747\n",
      "exploration/Returns Min                              -146.747\n",
      "exploration/Actions Mean                               -0.0139688\n",
      "exploration/Actions Std                                 0.677969\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999989\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -146.747\n",
      "exploration/env_infos/final/reward_run Mean            -1.81635\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -1.81635\n",
      "exploration/env_infos/final/reward_run Min             -1.81635\n",
      "exploration/env_infos/initial/reward_run Mean           0.154163\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.154163\n",
      "exploration/env_infos/initial/reward_run Min            0.154163\n",
      "exploration/env_infos/reward_run Mean                  -0.0361272\n",
      "exploration/env_infos/reward_run Std                    0.660232\n",
      "exploration/env_infos/reward_run Max                    2.44848\n",
      "exploration/env_infos/reward_run Min                   -2.02423\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.35396\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.35396\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.35396\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.275532\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.275532\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.275532\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.275902\n",
      "exploration/env_infos/reward_ctrl Std                   0.0900261\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0498057\n",
      "exploration/env_infos/reward_ctrl Min                  -0.597061\n",
      "exploration/env_infos/final/height Mean                -0.363362\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.363362\n",
      "exploration/env_infos/final/height Min                 -0.363362\n",
      "exploration/env_infos/initial/height Mean              -0.0258838\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0258838\n",
      "exploration/env_infos/initial/height Min               -0.0258838\n",
      "exploration/env_infos/height Mean                      -0.397294\n",
      "exploration/env_infos/height Std                        0.244678\n",
      "exploration/env_infos/height Max                        0.244672\n",
      "exploration/env_infos/height Min                       -0.588836\n",
      "exploration/env_infos/final/reward_angular Mean        -2.16525\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.16525\n",
      "exploration/env_infos/final/reward_angular Min         -2.16525\n",
      "exploration/env_infos/initial/reward_angular Mean       2.60697\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.60697\n",
      "exploration/env_infos/initial/reward_angular Min        2.60697\n",
      "exploration/env_infos/reward_angular Mean               0.261654\n",
      "exploration/env_infos/reward_angular Std                1.37074\n",
      "exploration/env_infos/reward_angular Max                6.86622\n",
      "exploration/env_infos/reward_angular Min               -3.98327\n",
      "evaluation/num steps total                              3.925e+06\n",
      "evaluation/num paths total                           3925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.654973\n",
      "evaluation/Rewards Std                                  1.57625\n",
      "evaluation/Rewards Max                                  8.48159\n",
      "evaluation/Rewards Min                                 -7.89176\n",
      "evaluation/Returns Mean                               654.973\n",
      "evaluation/Returns Std                                800.052\n",
      "evaluation/Returns Max                               2410.74\n",
      "evaluation/Returns Min                               -683.833\n",
      "evaluation/Actions Mean                                 0.0315952\n",
      "evaluation/Actions Std                                  0.791946\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            654.973\n",
      "evaluation/env_infos/final/reward_run Mean             -0.492416\n",
      "evaluation/env_infos/final/reward_run Std               2.33898\n",
      "evaluation/env_infos/final/reward_run Max               4.6632\n",
      "evaluation/env_infos/final/reward_run Min              -3.85358\n",
      "evaluation/env_infos/initial/reward_run Mean            0.254302\n",
      "evaluation/env_infos/initial/reward_run Std             0.439929\n",
      "evaluation/env_infos/initial/reward_run Max             1.10807\n",
      "evaluation/env_infos/initial/reward_run Min            -0.356776\n",
      "evaluation/env_infos/reward_run Mean                   -1.16178\n",
      "evaluation/env_infos/reward_run Std                     2.79179\n",
      "evaluation/env_infos/reward_run Max                     5.79518\n",
      "evaluation/env_infos/reward_run Min                    -8.0719\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.35427\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.145178\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.133897\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.588609\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.300353\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107434\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.108986\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.511916\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.376906\n",
      "evaluation/env_infos/reward_ctrl Std                    0.126191\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0109564\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599101\n",
      "evaluation/env_infos/final/height Mean                 -0.171487\n",
      "evaluation/env_infos/final/height Std                   0.222842\n",
      "evaluation/env_infos/final/height Max                   0.171537\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00377491\n",
      "evaluation/env_infos/initial/height Std                 0.0492693\n",
      "evaluation/env_infos/initial/height Max                 0.0814356\n",
      "evaluation/env_infos/initial/height Min                -0.0832057\n",
      "evaluation/env_infos/height Mean                       -0.0927204\n",
      "evaluation/env_infos/height Std                         0.206729\n",
      "evaluation/env_infos/height Max                         0.551677\n",
      "evaluation/env_infos/height Min                        -0.591419\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.260407\n",
      "evaluation/env_infos/final/reward_angular Std           1.57223\n",
      "evaluation/env_infos/final/reward_angular Max           4.16787\n",
      "evaluation/env_infos/final/reward_angular Min          -3.01075\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.224521\n",
      "evaluation/env_infos/initial/reward_angular Std         0.937011\n",
      "evaluation/env_infos/initial/reward_angular Max         1.53143\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.31313\n",
      "evaluation/env_infos/reward_angular Mean                0.038922\n",
      "evaluation/env_infos/reward_angular Std                 2.22444\n",
      "evaluation/env_infos/reward_angular Max                10.509\n",
      "evaluation/env_infos/reward_angular Min                -7.42328\n",
      "time/data storing (s)                                   0.0205685\n",
      "time/evaluation sampling (s)                           26.901\n",
      "time/exploration sampling (s)                           1.56402\n",
      "time/logging (s)                                        0.266147\n",
      "time/saving (s)                                         0.0305308\n",
      "time/training (s)                                       6.27772\n",
      "time/epoch (s)                                         35.06\n",
      "time/total (s)                                       4735.63\n",
      "Epoch                                                 156\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:31:27.774873 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 157 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 159000\n",
      "trainer/QF1 Loss                                        1.7023\n",
      "trainer/QF2 Loss                                        1.86664\n",
      "trainer/Policy Loss                                    -4.07483\n",
      "trainer/Q1 Predictions Mean                             9.98759\n",
      "trainer/Q1 Predictions Std                             23.4637\n",
      "trainer/Q1 Predictions Max                            109.871\n",
      "trainer/Q1 Predictions Min                            -16.3317\n",
      "trainer/Q2 Predictions Mean                            10.1359\n",
      "trainer/Q2 Predictions Std                             23.4859\n",
      "trainer/Q2 Predictions Max                            109.615\n",
      "trainer/Q2 Predictions Min                            -15.8242\n",
      "trainer/Q Targets Mean                                  9.86288\n",
      "trainer/Q Targets Std                                  23.8639\n",
      "trainer/Q Targets Max                                 112.522\n",
      "trainer/Q Targets Min                                 -16.4416\n",
      "trainer/Log Pis Mean                                    6.20845\n",
      "trainer/Log Pis Std                                     5.64266\n",
      "trainer/Log Pis Max                                    23.6608\n",
      "trainer/Log Pis Min                                    -5.40356\n",
      "trainer/Policy mu Mean                                 -0.00811546\n",
      "trainer/Policy mu Std                                   1.57633\n",
      "trainer/Policy mu Max                                   4.16928\n",
      "trainer/Policy mu Min                                  -5.0246\n",
      "trainer/Policy log std Mean                            -0.726922\n",
      "trainer/Policy log std Std                              0.3114\n",
      "trainer/Policy log std Max                              0.125584\n",
      "trainer/Policy log std Min                             -2.17189\n",
      "trainer/Alpha                                           0.0195133\n",
      "trainer/Alpha Loss                                      0.820893\n",
      "exploration/num steps total                        159000\n",
      "exploration/num paths total                           159\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                3.12772\n",
      "exploration/Rewards Std                                 1.462\n",
      "exploration/Rewards Max                                 7.11923\n",
      "exploration/Rewards Min                                -1.12319\n",
      "exploration/Returns Mean                             3127.72\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              3127.72\n",
      "exploration/Returns Min                              3127.72\n",
      "exploration/Actions Mean                                0.178305\n",
      "exploration/Actions Std                                 0.898864\n",
      "exploration/Actions Max                                 0.999993\n",
      "exploration/Actions Min                                -0.999997\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          3127.72\n",
      "exploration/env_infos/final/reward_run Mean            -3.07253\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.07253\n",
      "exploration/env_infos/final/reward_run Min             -3.07253\n",
      "exploration/env_infos/initial/reward_run Mean           0.329205\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.329205\n",
      "exploration/env_infos/initial/reward_run Min            0.329205\n",
      "exploration/env_infos/reward_run Mean                  -3.42335\n",
      "exploration/env_infos/reward_run Std                    1.10296\n",
      "exploration/env_infos/reward_run Max                    0.75056\n",
      "exploration/env_infos/reward_run Min                   -6.174\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.455246\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.455246\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.455246\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.346628\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.346628\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.346628\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.503849\n",
      "exploration/env_infos/reward_ctrl Std                   0.0662218\n",
      "exploration/env_infos/reward_ctrl Max                  -0.19448\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594795\n",
      "exploration/env_infos/final/height Mean                -0.0278319\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0278319\n",
      "exploration/env_infos/final/height Min                 -0.0278319\n",
      "exploration/env_infos/initial/height Mean               0.0324312\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0324312\n",
      "exploration/env_infos/initial/height Min                0.0324312\n",
      "exploration/env_infos/height Mean                      -0.0183372\n",
      "exploration/env_infos/height Std                        0.129158\n",
      "exploration/env_infos/height Max                        0.395922\n",
      "exploration/env_infos/height Min                       -0.366883\n",
      "exploration/env_infos/final/reward_angular Mean        -2.68196\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.68196\n",
      "exploration/env_infos/final/reward_angular Min         -2.68196\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.10402\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.10402\n",
      "exploration/env_infos/initial/reward_angular Min       -1.10402\n",
      "exploration/env_infos/reward_angular Mean               0.125389\n",
      "exploration/env_infos/reward_angular Std                2.95292\n",
      "exploration/env_infos/reward_angular Max                9.49749\n",
      "exploration/env_infos/reward_angular Min               -6.47721\n",
      "evaluation/num steps total                              3.95e+06\n",
      "evaluation/num paths total                           3950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.604755\n",
      "evaluation/Rewards Std                                  1.55784\n",
      "evaluation/Rewards Max                                  8.36923\n",
      "evaluation/Rewards Min                                 -8.08309\n",
      "evaluation/Returns Mean                               604.755\n",
      "evaluation/Returns Std                                761.746\n",
      "evaluation/Returns Max                               2413.67\n",
      "evaluation/Returns Min                               -573.495\n",
      "evaluation/Actions Mean                                 0.152313\n",
      "evaluation/Actions Std                                  0.788761\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            604.755\n",
      "evaluation/env_infos/final/reward_run Mean             -0.66017\n",
      "evaluation/env_infos/final/reward_run Std               2.64074\n",
      "evaluation/env_infos/final/reward_run Max               5.25877\n",
      "evaluation/env_infos/final/reward_run Min              -5.53516\n",
      "evaluation/env_infos/initial/reward_run Mean            0.192169\n",
      "evaluation/env_infos/initial/reward_run Std             0.496399\n",
      "evaluation/env_infos/initial/reward_run Max             1.14111\n",
      "evaluation/env_infos/initial/reward_run Min            -0.63221\n",
      "evaluation/env_infos/reward_run Mean                   -1.06332\n",
      "evaluation/env_infos/reward_run Std                     2.58154\n",
      "evaluation/env_infos/reward_run Max                     5.25877\n",
      "evaluation/env_infos/reward_run Min                    -7.60516\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.371831\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.114719\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.16086\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.565606\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.311573\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.109458\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.092652\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.490093\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.387206\n",
      "evaluation/env_infos/reward_ctrl Std                    0.121393\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0345002\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594165\n",
      "evaluation/env_infos/final/height Mean                 -0.225834\n",
      "evaluation/env_infos/final/height Std                   0.258073\n",
      "evaluation/env_infos/final/height Max                   0.168484\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0166806\n",
      "evaluation/env_infos/initial/height Std                 0.0519873\n",
      "evaluation/env_infos/initial/height Max                 0.0751966\n",
      "evaluation/env_infos/initial/height Min                -0.105451\n",
      "evaluation/env_infos/height Mean                       -0.168371\n",
      "evaluation/env_infos/height Std                         0.249703\n",
      "evaluation/env_infos/height Max                         0.544244\n",
      "evaluation/env_infos/height Min                        -0.58846\n",
      "evaluation/env_infos/final/reward_angular Mean          0.213692\n",
      "evaluation/env_infos/final/reward_angular Std           2.10466\n",
      "evaluation/env_infos/final/reward_angular Max           4.18135\n",
      "evaluation/env_infos/final/reward_angular Min          -4.01014\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.488209\n",
      "evaluation/env_infos/initial/reward_angular Std         0.902108\n",
      "evaluation/env_infos/initial/reward_angular Max         1.33279\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.41901\n",
      "evaluation/env_infos/reward_angular Mean                0.0681156\n",
      "evaluation/env_infos/reward_angular Std                 2.23987\n",
      "evaluation/env_infos/reward_angular Max                11.164\n",
      "evaluation/env_infos/reward_angular Min                -6.38114\n",
      "time/data storing (s)                                   0.0165278\n",
      "time/evaluation sampling (s)                           22.6159\n",
      "time/exploration sampling (s)                           1.02885\n",
      "time/logging (s)                                        0.262253\n",
      "time/saving (s)                                         0.0282049\n",
      "time/training (s)                                       4.05304\n",
      "time/epoch (s)                                         28.0048\n",
      "time/total (s)                                       4764.88\n",
      "Epoch                                                 157\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:32:04.564861 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 158 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 160000\n",
      "trainer/QF1 Loss                                        1.85164\n",
      "trainer/QF2 Loss                                        1.78618\n",
      "trainer/Policy Loss                                    -8.99377\n",
      "trainer/Q1 Predictions Mean                            14.4063\n",
      "trainer/Q1 Predictions Std                             26.7686\n",
      "trainer/Q1 Predictions Max                             99.7246\n",
      "trainer/Q1 Predictions Min                            -15.9501\n",
      "trainer/Q2 Predictions Mean                            14.6873\n",
      "trainer/Q2 Predictions Std                             26.9107\n",
      "trainer/Q2 Predictions Max                            100.872\n",
      "trainer/Q2 Predictions Min                            -15.778\n",
      "trainer/Q Targets Mean                                 14.8622\n",
      "trainer/Q Targets Std                                  27.1185\n",
      "trainer/Q Targets Max                                 103.116\n",
      "trainer/Q Targets Min                                 -16.1518\n",
      "trainer/Log Pis Mean                                    5.76488\n",
      "trainer/Log Pis Std                                     5.39275\n",
      "trainer/Log Pis Max                                    23.6132\n",
      "trainer/Log Pis Min                                    -5.00098\n",
      "trainer/Policy mu Mean                                  0.0124439\n",
      "trainer/Policy mu Std                                   1.58133\n",
      "trainer/Policy mu Max                                   4.36689\n",
      "trainer/Policy mu Min                                  -5.61795\n",
      "trainer/Policy log std Mean                            -0.696724\n",
      "trainer/Policy log std Std                              0.279933\n",
      "trainer/Policy log std Max                              0.56153\n",
      "trainer/Policy log std Min                             -1.84676\n",
      "trainer/Alpha                                           0.018331\n",
      "trainer/Alpha Loss                                     -0.940149\n",
      "exploration/num steps total                        160000\n",
      "exploration/num paths total                           160\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.56882\n",
      "exploration/Rewards Std                                 0.807248\n",
      "exploration/Rewards Max                                 3.97775\n",
      "exploration/Rewards Min                                -1.75473\n",
      "exploration/Returns Mean                             1568.82\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1568.82\n",
      "exploration/Returns Min                              1568.82\n",
      "exploration/Actions Mean                                0.151461\n",
      "exploration/Actions Std                                 0.771065\n",
      "exploration/Actions Max                                 0.999976\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1568.82\n",
      "exploration/env_infos/final/reward_run Mean             1.83205\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.83205\n",
      "exploration/env_infos/final/reward_run Min              1.83205\n",
      "exploration/env_infos/initial/reward_run Mean           0.0843356\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.0843356\n",
      "exploration/env_infos/initial/reward_run Min            0.0843356\n",
      "exploration/env_infos/reward_run Mean                   1.76504\n",
      "exploration/env_infos/reward_run Std                    0.884749\n",
      "exploration/env_infos/reward_run Max                    4.40224\n",
      "exploration/env_infos/reward_run Min                   -1.00403\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.486753\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.486753\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.486753\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.33483\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.33483\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.33483\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.370489\n",
      "exploration/env_infos/reward_ctrl Std                   0.110165\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0605765\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590102\n",
      "exploration/env_infos/final/height Mean                -0.0587403\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0587403\n",
      "exploration/env_infos/final/height Min                 -0.0587403\n",
      "exploration/env_infos/initial/height Mean              -0.0413036\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0413036\n",
      "exploration/env_infos/initial/height Min               -0.0413036\n",
      "exploration/env_infos/height Mean                      -0.135458\n",
      "exploration/env_infos/height Std                        0.11715\n",
      "exploration/env_infos/height Max                        0.224439\n",
      "exploration/env_infos/height Min                       -0.405811\n",
      "exploration/env_infos/final/reward_angular Mean        -1.15478\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.15478\n",
      "exploration/env_infos/final/reward_angular Min         -1.15478\n",
      "exploration/env_infos/initial/reward_angular Mean       0.517523\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.517523\n",
      "exploration/env_infos/initial/reward_angular Min        0.517523\n",
      "exploration/env_infos/reward_angular Mean              -0.0509005\n",
      "exploration/env_infos/reward_angular Std                2.23602\n",
      "exploration/env_infos/reward_angular Max                6.71743\n",
      "exploration/env_infos/reward_angular Min               -7.32115\n",
      "evaluation/num steps total                              3.975e+06\n",
      "evaluation/num paths total                           3975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.388299\n",
      "evaluation/Rewards Std                                  1.49258\n",
      "evaluation/Rewards Max                                  8.28203\n",
      "evaluation/Rewards Min                                 -7.76963\n",
      "evaluation/Returns Mean                               388.299\n",
      "evaluation/Returns Std                                580.234\n",
      "evaluation/Returns Max                               1887.19\n",
      "evaluation/Returns Min                               -758.91\n",
      "evaluation/Actions Mean                                 0.0604107\n",
      "evaluation/Actions Std                                  0.780517\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            388.299\n",
      "evaluation/env_infos/final/reward_run Mean             -0.28053\n",
      "evaluation/env_infos/final/reward_run Std               2.17368\n",
      "evaluation/env_infos/final/reward_run Max               2.61785\n",
      "evaluation/env_infos/final/reward_run Min              -5.18719\n",
      "evaluation/env_infos/initial/reward_run Mean            0.293931\n",
      "evaluation/env_infos/initial/reward_run Std             0.449866\n",
      "evaluation/env_infos/initial/reward_run Max             0.994067\n",
      "evaluation/env_infos/initial/reward_run Min            -0.504198\n",
      "evaluation/env_infos/reward_run Mean                   -0.997649\n",
      "evaluation/env_infos/reward_run Std                     2.63535\n",
      "evaluation/env_infos/reward_run Max                     5.22232\n",
      "evaluation/env_infos/reward_run Min                    -7.66866\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.359644\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.100945\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.145039\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.576341\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.263081\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0934979\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.094349\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.472585\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.367714\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109013\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0196175\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596033\n",
      "evaluation/env_infos/final/height Mean                 -0.297955\n",
      "evaluation/env_infos/final/height Std                   0.271441\n",
      "evaluation/env_infos/final/height Max                   0.268783\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0108954\n",
      "evaluation/env_infos/initial/height Std                 0.0510231\n",
      "evaluation/env_infos/initial/height Max                 0.0744563\n",
      "evaluation/env_infos/initial/height Min                -0.0899138\n",
      "evaluation/env_infos/height Mean                       -0.191173\n",
      "evaluation/env_infos/height Std                         0.246638\n",
      "evaluation/env_infos/height Max                         0.596254\n",
      "evaluation/env_infos/height Min                        -0.589624\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.177737\n",
      "evaluation/env_infos/final/reward_angular Std           1.43103\n",
      "evaluation/env_infos/final/reward_angular Max           2.3708\n",
      "evaluation/env_infos/final/reward_angular Min          -4.18672\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.677142\n",
      "evaluation/env_infos/initial/reward_angular Std         0.793436\n",
      "evaluation/env_infos/initial/reward_angular Max         1.51935\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.13244\n",
      "evaluation/env_infos/reward_angular Mean                0.0184295\n",
      "evaluation/env_infos/reward_angular Std                 2.22055\n",
      "evaluation/env_infos/reward_angular Max                 9.98324\n",
      "evaluation/env_infos/reward_angular Min                -7.70737\n",
      "time/data storing (s)                                   0.0168763\n",
      "time/evaluation sampling (s)                           29.3471\n",
      "time/exploration sampling (s)                           1.27734\n",
      "time/logging (s)                                        0.248622\n",
      "time/saving (s)                                         0.0273927\n",
      "time/training (s)                                       4.6425\n",
      "time/epoch (s)                                         35.5598\n",
      "time/total (s)                                       4801.65\n",
      "Epoch                                                 158\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:32:37.927697 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 159 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 161000\n",
      "trainer/QF1 Loss                                        1.89367\n",
      "trainer/QF2 Loss                                        2.02352\n",
      "trainer/Policy Loss                                    -8.85575\n",
      "trainer/Q1 Predictions Mean                            14.7971\n",
      "trainer/Q1 Predictions Std                             27.7153\n",
      "trainer/Q1 Predictions Max                            108.963\n",
      "trainer/Q1 Predictions Min                            -15.8906\n",
      "trainer/Q2 Predictions Mean                            14.4666\n",
      "trainer/Q2 Predictions Std                             27.5452\n",
      "trainer/Q2 Predictions Max                            108.334\n",
      "trainer/Q2 Predictions Min                            -15.6355\n",
      "trainer/Q Targets Mean                                 14.8203\n",
      "trainer/Q Targets Std                                  27.5033\n",
      "trainer/Q Targets Max                                 109.11\n",
      "trainer/Q Targets Min                                 -15.5314\n",
      "trainer/Log Pis Mean                                    5.9596\n",
      "trainer/Log Pis Std                                     6.64185\n",
      "trainer/Log Pis Max                                    35.1449\n",
      "trainer/Log Pis Min                                    -6.55427\n",
      "trainer/Policy mu Mean                                  0.0577663\n",
      "trainer/Policy mu Std                                   1.62418\n",
      "trainer/Policy mu Max                                   6.10787\n",
      "trainer/Policy mu Min                                  -6.11081\n",
      "trainer/Policy log std Mean                            -0.709034\n",
      "trainer/Policy log std Std                              0.311924\n",
      "trainer/Policy log std Max                              0.834799\n",
      "trainer/Policy log std Min                             -2.36681\n",
      "trainer/Alpha                                           0.0204418\n",
      "trainer/Alpha Loss                                     -0.157219\n",
      "exploration/num steps total                        161000\n",
      "exploration/num paths total                           161\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.539475\n",
      "exploration/Rewards Std                                 0.144781\n",
      "exploration/Rewards Max                                 0.0949742\n",
      "exploration/Rewards Min                                -0.83278\n",
      "exploration/Returns Mean                             -539.475\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -539.475\n",
      "exploration/Returns Min                              -539.475\n",
      "exploration/Actions Mean                               -0.0481959\n",
      "exploration/Actions Std                                 0.70003\n",
      "exploration/Actions Max                                 0.999983\n",
      "exploration/Actions Min                                -0.999908\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -539.475\n",
      "exploration/env_infos/final/reward_run Mean            -0.0837785\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0837785\n",
      "exploration/env_infos/final/reward_run Min             -0.0837785\n",
      "exploration/env_infos/initial/reward_run Mean          -0.286783\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.286783\n",
      "exploration/env_infos/initial/reward_run Min           -0.286783\n",
      "exploration/env_infos/reward_run Mean                  -0.229804\n",
      "exploration/env_infos/reward_run Std                    0.915057\n",
      "exploration/env_infos/reward_run Max                    1.60905\n",
      "exploration/env_infos/reward_run Min                   -5.9297\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.26598\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.26598\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.26598\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.20708\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.20708\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.20708\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.295419\n",
      "exploration/env_infos/reward_ctrl Std                   0.0916507\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0304811\n",
      "exploration/env_infos/reward_ctrl Min                  -0.564072\n",
      "exploration/env_infos/final/height Mean                -0.542259\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.542259\n",
      "exploration/env_infos/final/height Min                 -0.542259\n",
      "exploration/env_infos/initial/height Mean              -0.0698238\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0698238\n",
      "exploration/env_infos/initial/height Min               -0.0698238\n",
      "exploration/env_infos/height Mean                      -0.493842\n",
      "exploration/env_infos/height Std                        0.156192\n",
      "exploration/env_infos/height Max                        0.185172\n",
      "exploration/env_infos/height Min                       -0.586504\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0117452\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0117452\n",
      "exploration/env_infos/final/reward_angular Min         -0.0117452\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.66656\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.66656\n",
      "exploration/env_infos/initial/reward_angular Min       -1.66656\n",
      "exploration/env_infos/reward_angular Mean              -0.0948213\n",
      "exploration/env_infos/reward_angular Std                1.35634\n",
      "exploration/env_infos/reward_angular Max                7.24308\n",
      "exploration/env_infos/reward_angular Min               -5.51749\n",
      "evaluation/num steps total                              4e+06\n",
      "evaluation/num paths total                           4000\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.299839\n",
      "evaluation/Rewards Std                                  1.51597\n",
      "evaluation/Rewards Max                                  8.58046\n",
      "evaluation/Rewards Min                                 -8.15078\n",
      "evaluation/Returns Mean                               299.839\n",
      "evaluation/Returns Std                                869.068\n",
      "evaluation/Returns Max                               2897.51\n",
      "evaluation/Returns Min                               -773.637\n",
      "evaluation/Actions Mean                                -0.0215043\n",
      "evaluation/Actions Std                                  0.781281\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            299.839\n",
      "evaluation/env_infos/final/reward_run Mean             -0.465901\n",
      "evaluation/env_infos/final/reward_run Std               2.17894\n",
      "evaluation/env_infos/final/reward_run Max               3.34015\n",
      "evaluation/env_infos/final/reward_run Min              -5.64841\n",
      "evaluation/env_infos/initial/reward_run Mean            0.277455\n",
      "evaluation/env_infos/initial/reward_run Std             0.431429\n",
      "evaluation/env_infos/initial/reward_run Max             1.14779\n",
      "evaluation/env_infos/initial/reward_run Min            -0.542389\n",
      "evaluation/env_infos/reward_run Mean                   -0.514529\n",
      "evaluation/env_infos/reward_run Std                     2.41954\n",
      "evaluation/env_infos/reward_run Max                     5.24044\n",
      "evaluation/env_infos/reward_run Min                    -7.74023\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.377873\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0958283\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.197716\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.543222\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.295876\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0979074\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0639369\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.442018\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.366518\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0994039\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0311285\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593917\n",
      "evaluation/env_infos/final/height Mean                 -0.257837\n",
      "evaluation/env_infos/final/height Std                   0.258832\n",
      "evaluation/env_infos/final/height Max                   0.0920579\n",
      "evaluation/env_infos/final/height Min                  -0.577613\n",
      "evaluation/env_infos/initial/height Mean               -0.00840821\n",
      "evaluation/env_infos/initial/height Std                 0.0525882\n",
      "evaluation/env_infos/initial/height Max                 0.0878546\n",
      "evaluation/env_infos/initial/height Min                -0.084647\n",
      "evaluation/env_infos/height Mean                       -0.226197\n",
      "evaluation/env_infos/height Std                         0.257922\n",
      "evaluation/env_infos/height Max                         0.410717\n",
      "evaluation/env_infos/height Min                        -0.599862\n",
      "evaluation/env_infos/final/reward_angular Mean          0.560476\n",
      "evaluation/env_infos/final/reward_angular Std           1.75379\n",
      "evaluation/env_infos/final/reward_angular Max           4.34355\n",
      "evaluation/env_infos/final/reward_angular Min          -3.54337\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.389868\n",
      "evaluation/env_infos/initial/reward_angular Std         0.971801\n",
      "evaluation/env_infos/initial/reward_angular Max         1.69599\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.81568\n",
      "evaluation/env_infos/reward_angular Mean                0.0182306\n",
      "evaluation/env_infos/reward_angular Std                 1.91215\n",
      "evaluation/env_infos/reward_angular Max                 9.93192\n",
      "evaluation/env_infos/reward_angular Min                -7.04788\n",
      "time/data storing (s)                                   0.0161497\n",
      "time/evaluation sampling (s)                           21.8533\n",
      "time/exploration sampling (s)                           1.25939\n",
      "time/logging (s)                                        0.479468\n",
      "time/saving (s)                                         0.0380597\n",
      "time/training (s)                                       8.72927\n",
      "time/epoch (s)                                         32.3756\n",
      "time/total (s)                                       4835.24\n",
      "Epoch                                                 159\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:33:26.496786 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 160 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 162000\n",
      "trainer/QF1 Loss                                        1.56027\n",
      "trainer/QF2 Loss                                        1.41635\n",
      "trainer/Policy Loss                                    -8.34047\n",
      "trainer/Q1 Predictions Mean                            13.9882\n",
      "trainer/Q1 Predictions Std                             29.1549\n",
      "trainer/Q1 Predictions Max                            114.454\n",
      "trainer/Q1 Predictions Min                            -16.7077\n",
      "trainer/Q2 Predictions Mean                            13.759\n",
      "trainer/Q2 Predictions Std                             29.1051\n",
      "trainer/Q2 Predictions Max                            115.036\n",
      "trainer/Q2 Predictions Min                            -16.0752\n",
      "trainer/Q Targets Mean                                 13.871\n",
      "trainer/Q Targets Std                                  29.1144\n",
      "trainer/Q Targets Max                                 115.422\n",
      "trainer/Q Targets Min                                 -16.8344\n",
      "trainer/Log Pis Mean                                    5.70601\n",
      "trainer/Log Pis Std                                     5.67117\n",
      "trainer/Log Pis Max                                    23.4394\n",
      "trainer/Log Pis Min                                    -7.44174\n",
      "trainer/Policy mu Mean                                 -0.0974975\n",
      "trainer/Policy mu Std                                   1.54036\n",
      "trainer/Policy mu Max                                   4.06211\n",
      "trainer/Policy mu Min                                  -4.54845\n",
      "trainer/Policy log std Mean                            -0.706887\n",
      "trainer/Policy log std Std                              0.287367\n",
      "trainer/Policy log std Max                              0.43894\n",
      "trainer/Policy log std Min                             -1.7898\n",
      "trainer/Alpha                                           0.0201881\n",
      "trainer/Alpha Loss                                     -1.14639\n",
      "exploration/num steps total                        162000\n",
      "exploration/num paths total                           162\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.194015\n",
      "exploration/Rewards Std                                 1.36077\n",
      "exploration/Rewards Max                                 6.15502\n",
      "exploration/Rewards Min                                -6.17954\n",
      "exploration/Returns Mean                              194.015\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               194.015\n",
      "exploration/Returns Min                               194.015\n",
      "exploration/Actions Mean                                0.109925\n",
      "exploration/Actions Std                                 0.614997\n",
      "exploration/Actions Max                                 0.999555\n",
      "exploration/Actions Min                                -0.999887\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           194.015\n",
      "exploration/env_infos/final/reward_run Mean            -0.102925\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.102925\n",
      "exploration/env_infos/final/reward_run Min             -0.102925\n",
      "exploration/env_infos/initial/reward_run Mean          -0.488126\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.488126\n",
      "exploration/env_infos/initial/reward_run Min           -0.488126\n",
      "exploration/env_infos/reward_run Mean                  -0.789142\n",
      "exploration/env_infos/reward_run Std                    1.51174\n",
      "exploration/env_infos/reward_run Max                    0.869559\n",
      "exploration/env_infos/reward_run Min                   -5.82026\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.220968\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.220968\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.220968\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.135242\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.135242\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.135242\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.234183\n",
      "exploration/env_infos/reward_ctrl Std                   0.134454\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0323724\n",
      "exploration/env_infos/reward_ctrl Min                  -0.5861\n",
      "exploration/env_infos/final/height Mean                -0.558416\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.558416\n",
      "exploration/env_infos/final/height Min                 -0.558416\n",
      "exploration/env_infos/initial/height Mean              -0.0623162\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0623162\n",
      "exploration/env_infos/initial/height Min               -0.0623162\n",
      "exploration/env_infos/height Mean                      -0.42668\n",
      "exploration/env_infos/height Std                        0.256545\n",
      "exploration/env_infos/height Max                        0.370127\n",
      "exploration/env_infos/height Min                       -0.582025\n",
      "exploration/env_infos/final/reward_angular Mean        -0.549848\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.549848\n",
      "exploration/env_infos/final/reward_angular Min         -0.549848\n",
      "exploration/env_infos/initial/reward_angular Mean       0.781545\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.781545\n",
      "exploration/env_infos/initial/reward_angular Min        0.781545\n",
      "exploration/env_infos/reward_angular Mean              -0.0411878\n",
      "exploration/env_infos/reward_angular Std                1.65577\n",
      "exploration/env_infos/reward_angular Max                9.50703\n",
      "exploration/env_infos/reward_angular Min               -5.71005\n",
      "evaluation/num steps total                              4.025e+06\n",
      "evaluation/num paths total                           4025\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.495241\n",
      "evaluation/Rewards Std                                  1.6738\n",
      "evaluation/Rewards Max                                  8.52637\n",
      "evaluation/Rewards Min                                 -9.29183\n",
      "evaluation/Returns Mean                               495.241\n",
      "evaluation/Returns Std                                772.474\n",
      "evaluation/Returns Max                               2701.57\n",
      "evaluation/Returns Min                               -722.003\n",
      "evaluation/Actions Mean                                 0.0149787\n",
      "evaluation/Actions Std                                  0.765505\n",
      "evaluation/Actions Max                                  0.999992\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            495.241\n",
      "evaluation/env_infos/final/reward_run Mean             -0.590633\n",
      "evaluation/env_infos/final/reward_run Std               2.2159\n",
      "evaluation/env_infos/final/reward_run Max               2.52078\n",
      "evaluation/env_infos/final/reward_run Min              -4.92843\n",
      "evaluation/env_infos/initial/reward_run Mean            0.0961071\n",
      "evaluation/env_infos/initial/reward_run Std             0.49292\n",
      "evaluation/env_infos/initial/reward_run Max             1.0184\n",
      "evaluation/env_infos/initial/reward_run Min            -0.828531\n",
      "evaluation/env_infos/reward_run Mean                   -0.942265\n",
      "evaluation/env_infos/reward_run Std                     2.58525\n",
      "evaluation/env_infos/reward_run Max                     4.87174\n",
      "evaluation/env_infos/reward_run Min                    -7.11077\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.325389\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.113904\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.161061\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.540471\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.28423\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0921868\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.115664\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.546164\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.351733\n",
      "evaluation/env_infos/reward_ctrl Std                    0.118512\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0364686\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59587\n",
      "evaluation/env_infos/final/height Mean                 -0.177707\n",
      "evaluation/env_infos/final/height Std                   0.261223\n",
      "evaluation/env_infos/final/height Max                   0.182844\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00620363\n",
      "evaluation/env_infos/initial/height Std                 0.0495845\n",
      "evaluation/env_infos/initial/height Max                 0.0766068\n",
      "evaluation/env_infos/initial/height Min                -0.0909328\n",
      "evaluation/env_infos/height Mean                       -0.120512\n",
      "evaluation/env_infos/height Std                         0.233908\n",
      "evaluation/env_infos/height Max                         0.626185\n",
      "evaluation/env_infos/height Min                        -0.58765\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.183304\n",
      "evaluation/env_infos/final/reward_angular Std           2.11282\n",
      "evaluation/env_infos/final/reward_angular Max           4.95901\n",
      "evaluation/env_infos/final/reward_angular Min          -4.37533\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.192087\n",
      "evaluation/env_infos/initial/reward_angular Std         1.06163\n",
      "evaluation/env_infos/initial/reward_angular Max         1.59753\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.67616\n",
      "evaluation/env_infos/reward_angular Mean                0.0411202\n",
      "evaluation/env_infos/reward_angular Std                 2.26713\n",
      "evaluation/env_infos/reward_angular Max                 9.61313\n",
      "evaluation/env_infos/reward_angular Min                -7.54531\n",
      "time/data storing (s)                                   0.0158216\n",
      "time/evaluation sampling (s)                           40.0735\n",
      "time/exploration sampling (s)                           1.2321\n",
      "time/logging (s)                                        0.279972\n",
      "time/saving (s)                                         0.108966\n",
      "time/training (s)                                       4.72401\n",
      "time/epoch (s)                                         46.4344\n",
      "time/total (s)                                       4883.6\n",
      "Epoch                                                 160\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:34:02.001723 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 161 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 163000\n",
      "trainer/QF1 Loss                                        2.72128\n",
      "trainer/QF2 Loss                                        1.58033\n",
      "trainer/Policy Loss                                   -10.4187\n",
      "trainer/Q1 Predictions Mean                            16.6203\n",
      "trainer/Q1 Predictions Std                             27.9784\n",
      "trainer/Q1 Predictions Max                            109.524\n",
      "trainer/Q1 Predictions Min                            -15.7184\n",
      "trainer/Q2 Predictions Mean                            16.8883\n",
      "trainer/Q2 Predictions Std                             28.3111\n",
      "trainer/Q2 Predictions Max                            111.886\n",
      "trainer/Q2 Predictions Min                            -15.754\n",
      "trainer/Q Targets Mean                                 17.0956\n",
      "trainer/Q Targets Std                                  28.5447\n",
      "trainer/Q Targets Max                                 112.634\n",
      "trainer/Q Targets Min                                 -16.1514\n",
      "trainer/Log Pis Mean                                    6.51934\n",
      "trainer/Log Pis Std                                     6.16902\n",
      "trainer/Log Pis Max                                    25.1683\n",
      "trainer/Log Pis Min                                    -6.90317\n",
      "trainer/Policy mu Mean                                 -0.0393521\n",
      "trainer/Policy mu Std                                   1.65633\n",
      "trainer/Policy mu Max                                   5.44946\n",
      "trainer/Policy mu Min                                  -6.26073\n",
      "trainer/Policy log std Mean                            -0.719768\n",
      "trainer/Policy log std Std                              0.30705\n",
      "trainer/Policy log std Max                              0.282539\n",
      "trainer/Policy log std Min                             -2.67687\n",
      "trainer/Alpha                                           0.0201144\n",
      "trainer/Alpha Loss                                      2.02874\n",
      "exploration/num steps total                        163000\n",
      "exploration/num paths total                           163\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.49297\n",
      "exploration/Rewards Std                                 0.980315\n",
      "exploration/Rewards Max                                 4.41438\n",
      "exploration/Rewards Min                                -1.34831\n",
      "exploration/Returns Mean                             1492.97\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1492.97\n",
      "exploration/Returns Min                              1492.97\n",
      "exploration/Actions Mean                                0.110584\n",
      "exploration/Actions Std                                 0.831125\n",
      "exploration/Actions Max                                 0.999967\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1492.97\n",
      "exploration/env_infos/final/reward_run Mean             3.53371\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              3.53371\n",
      "exploration/env_infos/final/reward_run Min              3.53371\n",
      "exploration/env_infos/initial/reward_run Mean           0.907529\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.907529\n",
      "exploration/env_infos/initial/reward_run Min            0.907529\n",
      "exploration/env_infos/reward_run Mean                   2.23859\n",
      "exploration/env_infos/reward_run Std                    0.871595\n",
      "exploration/env_infos/reward_run Max                    4.7503\n",
      "exploration/env_infos/reward_run Min                   -1.31387\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.49279\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.49279\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.49279\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.388711\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.388711\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.388711\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.421798\n",
      "exploration/env_infos/reward_ctrl Std                   0.0843533\n",
      "exploration/env_infos/reward_ctrl Max                  -0.168557\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595843\n",
      "exploration/env_infos/final/height Mean                -0.100853\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.100853\n",
      "exploration/env_infos/final/height Min                 -0.100853\n",
      "exploration/env_infos/initial/height Mean               0.0632172\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0632172\n",
      "exploration/env_infos/initial/height Min                0.0632172\n",
      "exploration/env_infos/height Mean                      -0.11858\n",
      "exploration/env_infos/height Std                        0.113013\n",
      "exploration/env_infos/height Max                        0.267556\n",
      "exploration/env_infos/height Min                       -0.345837\n",
      "exploration/env_infos/final/reward_angular Mean        -1.16314\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.16314\n",
      "exploration/env_infos/final/reward_angular Min         -1.16314\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.841178\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.841178\n",
      "exploration/env_infos/initial/reward_angular Min       -0.841178\n",
      "exploration/env_infos/reward_angular Mean              -0.0740206\n",
      "exploration/env_infos/reward_angular Std                1.95562\n",
      "exploration/env_infos/reward_angular Max                5.64029\n",
      "exploration/env_infos/reward_angular Min               -5.34994\n",
      "evaluation/num steps total                              4.05e+06\n",
      "evaluation/num paths total                           4050\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.678521\n",
      "evaluation/Rewards Std                                  1.68808\n",
      "evaluation/Rewards Max                                  9.09793\n",
      "evaluation/Rewards Min                                 -6.94797\n",
      "evaluation/Returns Mean                               678.521\n",
      "evaluation/Returns Std                                914.571\n",
      "evaluation/Returns Max                               2545.45\n",
      "evaluation/Returns Min                               -469.684\n",
      "evaluation/Actions Mean                                 0.111792\n",
      "evaluation/Actions Std                                  0.789979\n",
      "evaluation/Actions Max                                  0.999993\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            678.521\n",
      "evaluation/env_infos/final/reward_run Mean             -1.78104\n",
      "evaluation/env_infos/final/reward_run Std               2.84958\n",
      "evaluation/env_infos/final/reward_run Max               3.04079\n",
      "evaluation/env_infos/final/reward_run Min              -5.75022\n",
      "evaluation/env_infos/initial/reward_run Mean            0.265777\n",
      "evaluation/env_infos/initial/reward_run Std             0.375193\n",
      "evaluation/env_infos/initial/reward_run Max             0.981576\n",
      "evaluation/env_infos/initial/reward_run Min            -0.202278\n",
      "evaluation/env_infos/reward_run Mean                   -1.63036\n",
      "evaluation/env_infos/reward_run Std                     2.75643\n",
      "evaluation/env_infos/reward_run Max                     5.03347\n",
      "evaluation/env_infos/reward_run Min                    -7.85787\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.371226\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0997438\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.166054\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.559773\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277662\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.114341\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.111534\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.499088\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.381938\n",
      "evaluation/env_infos/reward_ctrl Std                    0.108141\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0308587\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596801\n",
      "evaluation/env_infos/final/height Mean                 -0.148237\n",
      "evaluation/env_infos/final/height Std                   0.213287\n",
      "evaluation/env_infos/final/height Max                   0.232057\n",
      "evaluation/env_infos/final/height Min                  -0.576808\n",
      "evaluation/env_infos/initial/height Mean               -0.0173041\n",
      "evaluation/env_infos/initial/height Std                 0.0607458\n",
      "evaluation/env_infos/initial/height Max                 0.0918785\n",
      "evaluation/env_infos/initial/height Min                -0.110967\n",
      "evaluation/env_infos/height Mean                       -0.129438\n",
      "evaluation/env_infos/height Std                         0.216488\n",
      "evaluation/env_infos/height Max                         0.557622\n",
      "evaluation/env_infos/height Min                        -0.597364\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.813641\n",
      "evaluation/env_infos/final/reward_angular Std           1.87921\n",
      "evaluation/env_infos/final/reward_angular Max           3.34844\n",
      "evaluation/env_infos/final/reward_angular Min          -4.58033\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.4129\n",
      "evaluation/env_infos/initial/reward_angular Std         1.13075\n",
      "evaluation/env_infos/initial/reward_angular Max         2.24859\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.50966\n",
      "evaluation/env_infos/reward_angular Mean                0.0756841\n",
      "evaluation/env_infos/reward_angular Std                 2.30396\n",
      "evaluation/env_infos/reward_angular Max                 9.74538\n",
      "evaluation/env_infos/reward_angular Min                -8.1961\n",
      "time/data storing (s)                                   0.0194331\n",
      "time/evaluation sampling (s)                           25.1177\n",
      "time/exploration sampling (s)                           4.07646\n",
      "time/logging (s)                                        0.25535\n",
      "time/saving (s)                                         0.0270999\n",
      "time/training (s)                                       4.6545\n",
      "time/epoch (s)                                         34.1506\n",
      "time/total (s)                                       4919.07\n",
      "Epoch                                                 161\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:34:35.612549 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 162 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 164000\n",
      "trainer/QF1 Loss                                        1.16763\n",
      "trainer/QF2 Loss                                        1.27331\n",
      "trainer/Policy Loss                                    -6.29683\n",
      "trainer/Q1 Predictions Mean                            11.3662\n",
      "trainer/Q1 Predictions Std                             25.713\n",
      "trainer/Q1 Predictions Max                            110.209\n",
      "trainer/Q1 Predictions Min                            -18.3922\n",
      "trainer/Q2 Predictions Mean                            11.1907\n",
      "trainer/Q2 Predictions Std                             25.4682\n",
      "trainer/Q2 Predictions Max                            108.293\n",
      "trainer/Q2 Predictions Min                            -17.9158\n",
      "trainer/Q Targets Mean                                 11.3107\n",
      "trainer/Q Targets Std                                  25.6039\n",
      "trainer/Q Targets Max                                 110.191\n",
      "trainer/Q Targets Min                                 -18.2767\n",
      "trainer/Log Pis Mean                                    5.17154\n",
      "trainer/Log Pis Std                                     4.95627\n",
      "trainer/Log Pis Max                                    22.215\n",
      "trainer/Log Pis Min                                    -8.03446\n",
      "trainer/Policy mu Mean                                  0.00489805\n",
      "trainer/Policy mu Std                                   1.46629\n",
      "trainer/Policy mu Max                                   4.12351\n",
      "trainer/Policy mu Min                                  -4.00453\n",
      "trainer/Policy log std Mean                            -0.732758\n",
      "trainer/Policy log std Std                              0.296108\n",
      "trainer/Policy log std Max                              0.266851\n",
      "trainer/Policy log std Min                             -1.68283\n",
      "trainer/Alpha                                           0.0201253\n",
      "trainer/Alpha Loss                                     -3.23546\n",
      "exploration/num steps total                        164000\n",
      "exploration/num paths total                           164\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.453901\n",
      "exploration/Rewards Std                                 1.46176\n",
      "exploration/Rewards Max                                 5.17417\n",
      "exploration/Rewards Min                                -4.56932\n",
      "exploration/Returns Mean                              453.901\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               453.901\n",
      "exploration/Returns Min                               453.901\n",
      "exploration/Actions Mean                                0.0872135\n",
      "exploration/Actions Std                                 0.769503\n",
      "exploration/Actions Max                                 0.999972\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           453.901\n",
      "exploration/env_infos/final/reward_run Mean             4.30032\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              4.30032\n",
      "exploration/env_infos/final/reward_run Min              4.30032\n",
      "exploration/env_infos/initial/reward_run Mean           0.823097\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.823097\n",
      "exploration/env_infos/initial/reward_run Min            0.823097\n",
      "exploration/env_infos/reward_run Mean                   2.5475\n",
      "exploration/env_infos/reward_run Std                    1.07825\n",
      "exploration/env_infos/reward_run Max                    5.02875\n",
      "exploration/env_infos/reward_run Min                   -0.733719\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.441388\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.441388\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.441388\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.388585\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.388585\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.388585\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.359845\n",
      "exploration/env_infos/reward_ctrl Std                   0.0829005\n",
      "exploration/env_infos/reward_ctrl Max                  -0.10529\n",
      "exploration/env_infos/reward_ctrl Min                  -0.566581\n",
      "exploration/env_infos/final/height Mean                -0.0521422\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0521422\n",
      "exploration/env_infos/final/height Min                 -0.0521422\n",
      "exploration/env_infos/initial/height Mean              -0.0863427\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0863427\n",
      "exploration/env_infos/initial/height Min               -0.0863427\n",
      "exploration/env_infos/height Mean                      -0.0466409\n",
      "exploration/env_infos/height Std                        0.074187\n",
      "exploration/env_infos/height Max                        0.177037\n",
      "exploration/env_infos/height Min                       -0.253759\n",
      "exploration/env_infos/final/reward_angular Mean         0.171821\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.171821\n",
      "exploration/env_infos/final/reward_angular Min          0.171821\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.58522\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.58522\n",
      "exploration/env_infos/initial/reward_angular Min       -1.58522\n",
      "exploration/env_infos/reward_angular Mean              -0.0682512\n",
      "exploration/env_infos/reward_angular Std                1.65381\n",
      "exploration/env_infos/reward_angular Max                5.72491\n",
      "exploration/env_infos/reward_angular Min               -5.57254\n",
      "evaluation/num steps total                              4.075e+06\n",
      "evaluation/num paths total                           4075\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.738997\n",
      "evaluation/Rewards Std                                  1.54471\n",
      "evaluation/Rewards Max                                  7.66067\n",
      "evaluation/Rewards Min                                 -6.8931\n",
      "evaluation/Returns Mean                               738.997\n",
      "evaluation/Returns Std                                874.026\n",
      "evaluation/Returns Max                               2671.71\n",
      "evaluation/Returns Min                               -456.778\n",
      "evaluation/Actions Mean                                 0.138296\n",
      "evaluation/Actions Std                                  0.765773\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            738.997\n",
      "evaluation/env_infos/final/reward_run Mean             -1.49374\n",
      "evaluation/env_infos/final/reward_run Std               3.0114\n",
      "evaluation/env_infos/final/reward_run Max               4.34249\n",
      "evaluation/env_infos/final/reward_run Min              -5.92846\n",
      "evaluation/env_infos/initial/reward_run Mean            0.268713\n",
      "evaluation/env_infos/initial/reward_run Std             0.459642\n",
      "evaluation/env_infos/initial/reward_run Max             1.19519\n",
      "evaluation/env_infos/initial/reward_run Min            -0.561795\n",
      "evaluation/env_infos/reward_run Mean                   -1.30535\n",
      "evaluation/env_infos/reward_run Std                     2.78098\n",
      "evaluation/env_infos/reward_run Max                     5.53359\n",
      "evaluation/env_infos/reward_run Min                    -7.04463\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.36515\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11545\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.15574\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.553475\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.279425\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.117638\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0666501\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.512179\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.363321\n",
      "evaluation/env_infos/reward_ctrl Std                    0.11634\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0286371\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595363\n",
      "evaluation/env_infos/final/height Mean                 -0.169808\n",
      "evaluation/env_infos/final/height Std                   0.211972\n",
      "evaluation/env_infos/final/height Max                   0.111804\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00305791\n",
      "evaluation/env_infos/initial/height Std                 0.0550325\n",
      "evaluation/env_infos/initial/height Max                 0.0837674\n",
      "evaluation/env_infos/initial/height Min                -0.114599\n",
      "evaluation/env_infos/height Mean                       -0.144899\n",
      "evaluation/env_infos/height Std                         0.218485\n",
      "evaluation/env_infos/height Max                         0.5956\n",
      "evaluation/env_infos/height Min                        -0.581474\n",
      "evaluation/env_infos/final/reward_angular Mean          0.394849\n",
      "evaluation/env_infos/final/reward_angular Std           2.50296\n",
      "evaluation/env_infos/final/reward_angular Max           6.4111\n",
      "evaluation/env_infos/final/reward_angular Min          -3.68443\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.748484\n",
      "evaluation/env_infos/initial/reward_angular Std         0.933039\n",
      "evaluation/env_infos/initial/reward_angular Max         1.07409\n",
      "evaluation/env_infos/initial/reward_angular Min        -3.19941\n",
      "evaluation/env_infos/reward_angular Mean                0.0902787\n",
      "evaluation/env_infos/reward_angular Std                 2.16326\n",
      "evaluation/env_infos/reward_angular Max                 9.91704\n",
      "evaluation/env_infos/reward_angular Min                -7.40079\n",
      "time/data storing (s)                                   0.0174488\n",
      "time/evaluation sampling (s)                           25.5062\n",
      "time/exploration sampling (s)                           1.27626\n",
      "time/logging (s)                                        0.263271\n",
      "time/saving (s)                                         0.0286661\n",
      "time/training (s)                                       5.23\n",
      "time/epoch (s)                                         32.3219\n",
      "time/total (s)                                       4952.68\n",
      "Epoch                                                 162\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:35:15.485533 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 163 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 165000\n",
      "trainer/QF1 Loss                                        1.79534\n",
      "trainer/QF2 Loss                                        1.54244\n",
      "trainer/Policy Loss                                    -8.28526\n",
      "trainer/Q1 Predictions Mean                            14.7777\n",
      "trainer/Q1 Predictions Std                             28.7088\n",
      "trainer/Q1 Predictions Max                            110.378\n",
      "trainer/Q1 Predictions Min                            -15.9903\n",
      "trainer/Q2 Predictions Mean                            14.7574\n",
      "trainer/Q2 Predictions Std                             28.7638\n",
      "trainer/Q2 Predictions Max                            109.861\n",
      "trainer/Q2 Predictions Min                            -15.8163\n",
      "trainer/Q Targets Mean                                 14.6625\n",
      "trainer/Q Targets Std                                  28.6775\n",
      "trainer/Q Targets Max                                 109.937\n",
      "trainer/Q Targets Min                                 -15.6791\n",
      "trainer/Log Pis Mean                                    6.81356\n",
      "trainer/Log Pis Std                                     6.22685\n",
      "trainer/Log Pis Max                                    26.4139\n",
      "trainer/Log Pis Min                                    -9.19958\n",
      "trainer/Policy mu Mean                                  0.023481\n",
      "trainer/Policy mu Std                                   1.6842\n",
      "trainer/Policy mu Max                                   4.25247\n",
      "trainer/Policy mu Min                                  -5.33292\n",
      "trainer/Policy log std Mean                            -0.739284\n",
      "trainer/Policy log std Std                              0.279299\n",
      "trainer/Policy log std Max                              0.279951\n",
      "trainer/Policy log std Min                             -1.7942\n",
      "trainer/Alpha                                           0.019645\n",
      "trainer/Alpha Loss                                      3.20003\n",
      "exploration/num steps total                        165000\n",
      "exploration/num paths total                           165\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.613287\n",
      "exploration/Rewards Std                                 0.511574\n",
      "exploration/Rewards Max                                 0.597514\n",
      "exploration/Rewards Min                                -1.86232\n",
      "exploration/Returns Mean                             -613.287\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -613.287\n",
      "exploration/Returns Min                              -613.287\n",
      "exploration/Actions Mean                                0.0822931\n",
      "exploration/Actions Std                                 0.793316\n",
      "exploration/Actions Max                                 0.999996\n",
      "exploration/Actions Min                                -0.999984\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -613.287\n",
      "exploration/env_infos/final/reward_run Mean            -1.0325\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -1.0325\n",
      "exploration/env_infos/final/reward_run Min             -1.0325\n",
      "exploration/env_infos/initial/reward_run Mean           0.263499\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.263499\n",
      "exploration/env_infos/initial/reward_run Min            0.263499\n",
      "exploration/env_infos/reward_run Mean                  -3.84293\n",
      "exploration/env_infos/reward_run Std                    1.67922\n",
      "exploration/env_infos/reward_run Max                    1.29433\n",
      "exploration/env_infos/reward_run Min                   -7.10882\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270689\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270689\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270689\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.145415\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.145415\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.145415\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.381673\n",
      "exploration/env_infos/reward_ctrl Std                   0.096452\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0495938\n",
      "exploration/env_infos/reward_ctrl Min                  -0.584781\n",
      "exploration/env_infos/final/height Mean                -0.101199\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.101199\n",
      "exploration/env_infos/final/height Min                 -0.101199\n",
      "exploration/env_infos/initial/height Mean              -0.0943761\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0943761\n",
      "exploration/env_infos/initial/height Min               -0.0943761\n",
      "exploration/env_infos/height Mean                      -0.0412232\n",
      "exploration/env_infos/height Std                        0.099384\n",
      "exploration/env_infos/height Max                        0.333933\n",
      "exploration/env_infos/height Min                       -0.309749\n",
      "exploration/env_infos/final/reward_angular Mean         2.39293\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.39293\n",
      "exploration/env_infos/final/reward_angular Min          2.39293\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.797329\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.797329\n",
      "exploration/env_infos/initial/reward_angular Min       -0.797329\n",
      "exploration/env_infos/reward_angular Mean               0.113305\n",
      "exploration/env_infos/reward_angular Std                2.81989\n",
      "exploration/env_infos/reward_angular Max                8.69729\n",
      "exploration/env_infos/reward_angular Min               -5.85826\n",
      "evaluation/num steps total                              4.1e+06\n",
      "evaluation/num paths total                           4100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.599754\n",
      "evaluation/Rewards Std                                  1.69843\n",
      "evaluation/Rewards Max                                  8.93151\n",
      "evaluation/Rewards Min                                 -9.01346\n",
      "evaluation/Returns Mean                               599.754\n",
      "evaluation/Returns Std                                843.008\n",
      "evaluation/Returns Max                               2910.38\n",
      "evaluation/Returns Min                               -488.184\n",
      "evaluation/Actions Mean                                 0.133201\n",
      "evaluation/Actions Std                                  0.805717\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            599.754\n",
      "evaluation/env_infos/final/reward_run Mean             -1.29418\n",
      "evaluation/env_infos/final/reward_run Std               2.97006\n",
      "evaluation/env_infos/final/reward_run Max               3.44999\n",
      "evaluation/env_infos/final/reward_run Min              -6.42675\n",
      "evaluation/env_infos/initial/reward_run Mean            0.27536\n",
      "evaluation/env_infos/initial/reward_run Std             0.44747\n",
      "evaluation/env_infos/initial/reward_run Max             1.00674\n",
      "evaluation/env_infos/initial/reward_run Min            -0.51353\n",
      "evaluation/env_infos/reward_run Mean                   -1.36457\n",
      "evaluation/env_infos/reward_run Std                     2.9857\n",
      "evaluation/env_infos/reward_run Max                     5.41082\n",
      "evaluation/env_infos/reward_run Min                    -8.67752\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.372105\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0953584\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.190604\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.55498\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.280629\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.112475\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.10856\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.481781\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.400154\n",
      "evaluation/env_infos/reward_ctrl Std                    0.105548\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0571385\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594946\n",
      "evaluation/env_infos/final/height Mean                 -0.184785\n",
      "evaluation/env_infos/final/height Std                   0.283023\n",
      "evaluation/env_infos/final/height Max                   0.195898\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00355356\n",
      "evaluation/env_infos/initial/height Std                 0.0523628\n",
      "evaluation/env_infos/initial/height Max                 0.0742883\n",
      "evaluation/env_infos/initial/height Min                -0.107185\n",
      "evaluation/env_infos/height Mean                       -0.156842\n",
      "evaluation/env_infos/height Std                         0.231343\n",
      "evaluation/env_infos/height Max                         0.43957\n",
      "evaluation/env_infos/height Min                        -0.588697\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0421475\n",
      "evaluation/env_infos/final/reward_angular Std           2.03328\n",
      "evaluation/env_infos/final/reward_angular Max           4.26118\n",
      "evaluation/env_infos/final/reward_angular Min          -4.27252\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.833968\n",
      "evaluation/env_infos/initial/reward_angular Std         1.03207\n",
      "evaluation/env_infos/initial/reward_angular Max         2.10453\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.72356\n",
      "evaluation/env_infos/reward_angular Mean                0.0685943\n",
      "evaluation/env_infos/reward_angular Std                 2.25698\n",
      "evaluation/env_infos/reward_angular Max                 9.82944\n",
      "evaluation/env_infos/reward_angular Min                -6.52517\n",
      "time/data storing (s)                                   0.0196795\n",
      "time/evaluation sampling (s)                           30.3786\n",
      "time/exploration sampling (s)                           2.11681\n",
      "time/logging (s)                                        0.282588\n",
      "time/saving (s)                                         0.0291761\n",
      "time/training (s)                                       5.65133\n",
      "time/epoch (s)                                         38.4782\n",
      "time/total (s)                                       4992.57\n",
      "Epoch                                                 163\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:35:49.200241 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 164 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 166000\n",
      "trainer/QF1 Loss                                        2.02592\n",
      "trainer/QF2 Loss                                        2.28945\n",
      "trainer/Policy Loss                                    -7.40777\n",
      "trainer/Q1 Predictions Mean                            14.0497\n",
      "trainer/Q1 Predictions Std                             26.7946\n",
      "trainer/Q1 Predictions Max                            107.107\n",
      "trainer/Q1 Predictions Min                            -16.3952\n",
      "trainer/Q2 Predictions Mean                            14.0016\n",
      "trainer/Q2 Predictions Std                             26.9069\n",
      "trainer/Q2 Predictions Max                            108.091\n",
      "trainer/Q2 Predictions Min                            -16.9172\n",
      "trainer/Q Targets Mean                                 14.4332\n",
      "trainer/Q Targets Std                                  27.1015\n",
      "trainer/Q Targets Max                                 110.077\n",
      "trainer/Q Targets Min                                 -17.2461\n",
      "trainer/Log Pis Mean                                    6.87527\n",
      "trainer/Log Pis Std                                     5.49229\n",
      "trainer/Log Pis Max                                    22.2837\n",
      "trainer/Log Pis Min                                    -6.16973\n",
      "trainer/Policy mu Mean                                  0.0357412\n",
      "trainer/Policy mu Std                                   1.65941\n",
      "trainer/Policy mu Max                                   4.40816\n",
      "trainer/Policy mu Min                                  -5.67613\n",
      "trainer/Policy log std Mean                            -0.718066\n",
      "trainer/Policy log std Std                              0.284337\n",
      "trainer/Policy log std Max                              0.25661\n",
      "trainer/Policy log std Min                             -1.85449\n",
      "trainer/Alpha                                           0.0196114\n",
      "trainer/Alpha Loss                                      3.44169\n",
      "exploration/num steps total                        166000\n",
      "exploration/num paths total                           166\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.16429\n",
      "exploration/Rewards Std                                 1.19035\n",
      "exploration/Rewards Max                                 4.90145\n",
      "exploration/Rewards Min                                -2.40819\n",
      "exploration/Returns Mean                             1164.29\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1164.29\n",
      "exploration/Returns Min                              1164.29\n",
      "exploration/Actions Mean                                0.185419\n",
      "exploration/Actions Std                                 0.802522\n",
      "exploration/Actions Max                                 0.99994\n",
      "exploration/Actions Min                                -0.999856\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1164.29\n",
      "exploration/env_infos/final/reward_run Mean             2.17834\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.17834\n",
      "exploration/env_infos/final/reward_run Min              2.17834\n",
      "exploration/env_infos/initial/reward_run Mean           0.96354\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.96354\n",
      "exploration/env_infos/initial/reward_run Min            0.96354\n",
      "exploration/env_infos/reward_run Mean                   2.32069\n",
      "exploration/env_infos/reward_run Std                    0.943447\n",
      "exploration/env_infos/reward_run Max                    5.00526\n",
      "exploration/env_infos/reward_run Min                   -0.865934\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.479608\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.479608\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.479608\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.507718\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.507718\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.507718\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.407053\n",
      "exploration/env_infos/reward_ctrl Std                   0.0738106\n",
      "exploration/env_infos/reward_ctrl Max                  -0.151705\n",
      "exploration/env_infos/reward_ctrl Min                  -0.578168\n",
      "exploration/env_infos/final/height Mean                -0.12686\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.12686\n",
      "exploration/env_infos/final/height Min                 -0.12686\n",
      "exploration/env_infos/initial/height Mean              -0.00627636\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00627636\n",
      "exploration/env_infos/initial/height Min               -0.00627636\n",
      "exploration/env_infos/height Mean                      -0.123077\n",
      "exploration/env_infos/height Std                        0.101779\n",
      "exploration/env_infos/height Max                        0.235951\n",
      "exploration/env_infos/height Min                       -0.364555\n",
      "exploration/env_infos/final/reward_angular Mean        -1.92056\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.92056\n",
      "exploration/env_infos/final/reward_angular Min         -1.92056\n",
      "exploration/env_infos/initial/reward_angular Mean       0.935356\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.935356\n",
      "exploration/env_infos/initial/reward_angular Min        0.935356\n",
      "exploration/env_infos/reward_angular Mean              -0.0594973\n",
      "exploration/env_infos/reward_angular Std                1.76218\n",
      "exploration/env_infos/reward_angular Max                4.45025\n",
      "exploration/env_infos/reward_angular Min               -5.38474\n",
      "evaluation/num steps total                              4.125e+06\n",
      "evaluation/num paths total                           4125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.737554\n",
      "evaluation/Rewards Std                                  1.50282\n",
      "evaluation/Rewards Max                                  7.73508\n",
      "evaluation/Rewards Min                                 -7.21422\n",
      "evaluation/Returns Mean                               737.554\n",
      "evaluation/Returns Std                                839.746\n",
      "evaluation/Returns Max                               2573.32\n",
      "evaluation/Returns Min                               -428.421\n",
      "evaluation/Actions Mean                                 0.125075\n",
      "evaluation/Actions Std                                  0.783888\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            737.554\n",
      "evaluation/env_infos/final/reward_run Mean             -1.37224\n",
      "evaluation/env_infos/final/reward_run Std               2.87063\n",
      "evaluation/env_infos/final/reward_run Max               3.41394\n",
      "evaluation/env_infos/final/reward_run Min              -5.86673\n",
      "evaluation/env_infos/initial/reward_run Mean            0.329413\n",
      "evaluation/env_infos/initial/reward_run Std             0.486945\n",
      "evaluation/env_infos/initial/reward_run Max             1.18258\n",
      "evaluation/env_infos/initial/reward_run Min            -0.552223\n",
      "evaluation/env_infos/reward_run Mean                   -1.23923\n",
      "evaluation/env_infos/reward_run Std                     2.79561\n",
      "evaluation/env_infos/reward_run Max                     5.67324\n",
      "evaluation/env_infos/reward_run Min                    -7.03237\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.377333\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.128839\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.131988\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.541025\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.299405\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0956763\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.131835\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.48953\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.378074\n",
      "evaluation/env_infos/reward_ctrl Std                    0.114338\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0387299\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594452\n",
      "evaluation/env_infos/final/height Mean                 -0.126229\n",
      "evaluation/env_infos/final/height Std                   0.20223\n",
      "evaluation/env_infos/final/height Max                   0.218875\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0131398\n",
      "evaluation/env_infos/initial/height Std                 0.0494315\n",
      "evaluation/env_infos/initial/height Max                 0.0887466\n",
      "evaluation/env_infos/initial/height Min                -0.11181\n",
      "evaluation/env_infos/height Mean                       -0.0927982\n",
      "evaluation/env_infos/height Std                         0.188087\n",
      "evaluation/env_infos/height Max                         0.557707\n",
      "evaluation/env_infos/height Min                        -0.591316\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00520011\n",
      "evaluation/env_infos/final/reward_angular Std           2.0337\n",
      "evaluation/env_infos/final/reward_angular Max           5.78138\n",
      "evaluation/env_infos/final/reward_angular Min          -3.81895\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.150833\n",
      "evaluation/env_infos/initial/reward_angular Std         1.02287\n",
      "evaluation/env_infos/initial/reward_angular Max         2.27143\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.52646\n",
      "evaluation/env_infos/reward_angular Mean                0.0707907\n",
      "evaluation/env_infos/reward_angular Std                 2.09198\n",
      "evaluation/env_infos/reward_angular Max                 9.64526\n",
      "evaluation/env_infos/reward_angular Min                -6.29393\n",
      "time/data storing (s)                                   0.0159\n",
      "time/evaluation sampling (s)                           26.522\n",
      "time/exploration sampling (s)                           1.02629\n",
      "time/logging (s)                                        0.277977\n",
      "time/saving (s)                                         0.0330681\n",
      "time/training (s)                                       4.43287\n",
      "time/epoch (s)                                         32.3081\n",
      "time/total (s)                                       5026.28\n",
      "Epoch                                                 164\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:36:21.073581 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 165 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 167000\n",
      "trainer/QF1 Loss                                        1.87592\n",
      "trainer/QF2 Loss                                        2.88877\n",
      "trainer/Policy Loss                                   -10.5349\n",
      "trainer/Q1 Predictions Mean                            16.4912\n",
      "trainer/Q1 Predictions Std                             28.7946\n",
      "trainer/Q1 Predictions Max                            113.158\n",
      "trainer/Q1 Predictions Min                            -17.5069\n",
      "trainer/Q2 Predictions Mean                            16.5988\n",
      "trainer/Q2 Predictions Std                             28.9053\n",
      "trainer/Q2 Predictions Max                            112.038\n",
      "trainer/Q2 Predictions Min                            -17.6567\n",
      "trainer/Q Targets Mean                                 16.6103\n",
      "trainer/Q Targets Std                                  29.0368\n",
      "trainer/Q Targets Max                                 110.317\n",
      "trainer/Q Targets Min                                 -17.5886\n",
      "trainer/Log Pis Mean                                    6.29146\n",
      "trainer/Log Pis Std                                     5.89314\n",
      "trainer/Log Pis Max                                    22.0202\n",
      "trainer/Log Pis Min                                    -4.7334\n",
      "trainer/Policy mu Mean                                 -0.0434108\n",
      "trainer/Policy mu Std                                   1.6087\n",
      "trainer/Policy mu Max                                   4.7905\n",
      "trainer/Policy mu Min                                  -5.06765\n",
      "trainer/Policy log std Mean                            -0.715605\n",
      "trainer/Policy log std Std                              0.309698\n",
      "trainer/Policy log std Max                              0.35536\n",
      "trainer/Policy log std Min                             -2.1639\n",
      "trainer/Alpha                                           0.0208872\n",
      "trainer/Alpha Loss                                      1.128\n",
      "exploration/num steps total                        167000\n",
      "exploration/num paths total                           167\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.7065\n",
      "exploration/Rewards Std                                 0.965614\n",
      "exploration/Rewards Max                                 3.96065\n",
      "exploration/Rewards Min                                -1.43357\n",
      "exploration/Returns Mean                             1706.5\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1706.5\n",
      "exploration/Returns Min                              1706.5\n",
      "exploration/Actions Mean                                0.227692\n",
      "exploration/Actions Std                                 0.807332\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1706.5\n",
      "exploration/env_infos/final/reward_run Mean             1.02062\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.02062\n",
      "exploration/env_infos/final/reward_run Min              1.02062\n",
      "exploration/env_infos/initial/reward_run Mean           0.93829\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.93829\n",
      "exploration/env_infos/initial/reward_run Min            0.93829\n",
      "exploration/env_infos/reward_run Mean                   2.44583\n",
      "exploration/env_infos/reward_run Std                    1.09837\n",
      "exploration/env_infos/reward_run Max                    5.40463\n",
      "exploration/env_infos/reward_run Min                   -1.3611\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.481445\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.481445\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.481445\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.457041\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.457041\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.457041\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.422177\n",
      "exploration/env_infos/reward_ctrl Std                   0.0806058\n",
      "exploration/env_infos/reward_ctrl Max                  -0.125812\n",
      "exploration/env_infos/reward_ctrl Min                  -0.587228\n",
      "exploration/env_infos/final/height Mean                 0.146439\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.146439\n",
      "exploration/env_infos/final/height Min                  0.146439\n",
      "exploration/env_infos/initial/height Mean              -0.0434215\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0434215\n",
      "exploration/env_infos/initial/height Min               -0.0434215\n",
      "exploration/env_infos/height Mean                      -0.0671946\n",
      "exploration/env_infos/height Std                        0.108356\n",
      "exploration/env_infos/height Max                        0.342119\n",
      "exploration/env_infos/height Min                       -0.329317\n",
      "exploration/env_infos/final/reward_angular Mean         3.76325\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.76325\n",
      "exploration/env_infos/final/reward_angular Min          3.76325\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.776538\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.776538\n",
      "exploration/env_infos/initial/reward_angular Min       -0.776538\n",
      "exploration/env_infos/reward_angular Mean              -0.0682148\n",
      "exploration/env_infos/reward_angular Std                2.10248\n",
      "exploration/env_infos/reward_angular Max                5.54579\n",
      "exploration/env_infos/reward_angular Min               -6.94635\n",
      "evaluation/num steps total                              4.15e+06\n",
      "evaluation/num paths total                           4150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.589473\n",
      "evaluation/Rewards Std                                  1.64073\n",
      "evaluation/Rewards Max                                  8.74436\n",
      "evaluation/Rewards Min                                 -9.66418\n",
      "evaluation/Returns Mean                               589.473\n",
      "evaluation/Returns Std                                946.74\n",
      "evaluation/Returns Max                               3014.84\n",
      "evaluation/Returns Min                               -867.966\n",
      "evaluation/Actions Mean                                 0.0536868\n",
      "evaluation/Actions Std                                  0.800495\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            589.473\n",
      "evaluation/env_infos/final/reward_run Mean             -0.414667\n",
      "evaluation/env_infos/final/reward_run Std               2.33904\n",
      "evaluation/env_infos/final/reward_run Max               3.10241\n",
      "evaluation/env_infos/final/reward_run Min              -6.18222\n",
      "evaluation/env_infos/initial/reward_run Mean            0.35495\n",
      "evaluation/env_infos/initial/reward_run Std             0.468004\n",
      "evaluation/env_infos/initial/reward_run Max             1.04503\n",
      "evaluation/env_infos/initial/reward_run Min            -0.66537\n",
      "evaluation/env_infos/reward_run Mean                   -0.839235\n",
      "evaluation/env_infos/reward_run Std                     2.8074\n",
      "evaluation/env_infos/reward_run Max                     5.36527\n",
      "evaluation/env_infos/reward_run Min                    -7.87984\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.351339\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.10717\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.180194\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.571084\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.287277\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.102761\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.130556\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.464672\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.386205\n",
      "evaluation/env_infos/reward_ctrl Std                    0.118083\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0220604\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595192\n",
      "evaluation/env_infos/final/height Mean                 -0.295046\n",
      "evaluation/env_infos/final/height Std                   0.261262\n",
      "evaluation/env_infos/final/height Max                   0.153661\n",
      "evaluation/env_infos/final/height Min                  -0.577277\n",
      "evaluation/env_infos/initial/height Mean               -0.015172\n",
      "evaluation/env_infos/initial/height Std                 0.0501064\n",
      "evaluation/env_infos/initial/height Max                 0.0891206\n",
      "evaluation/env_infos/initial/height Min                -0.0735015\n",
      "evaluation/env_infos/height Mean                       -0.187026\n",
      "evaluation/env_infos/height Std                         0.243679\n",
      "evaluation/env_infos/height Max                         0.526534\n",
      "evaluation/env_infos/height Min                        -0.594406\n",
      "evaluation/env_infos/final/reward_angular Mean          0.487494\n",
      "evaluation/env_infos/final/reward_angular Std           1.8636\n",
      "evaluation/env_infos/final/reward_angular Max           4.36979\n",
      "evaluation/env_infos/final/reward_angular Min          -3.19778\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.148268\n",
      "evaluation/env_infos/initial/reward_angular Std         1.15086\n",
      "evaluation/env_infos/initial/reward_angular Max         2.25235\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.369\n",
      "evaluation/env_infos/reward_angular Mean                0.0362946\n",
      "evaluation/env_infos/reward_angular Std                 2.11955\n",
      "evaluation/env_infos/reward_angular Max                10.0446\n",
      "evaluation/env_infos/reward_angular Min                -6.59964\n",
      "time/data storing (s)                                   0.0157903\n",
      "time/evaluation sampling (s)                           24.0901\n",
      "time/exploration sampling (s)                           1.03428\n",
      "time/logging (s)                                        0.251357\n",
      "time/saving (s)                                         0.027577\n",
      "time/training (s)                                       4.95047\n",
      "time/epoch (s)                                         30.3695\n",
      "time/total (s)                                       5058.12\n",
      "Epoch                                                 165\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:36:59.703038 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 166 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 168000\n",
      "trainer/QF1 Loss                                        2.73303\n",
      "trainer/QF2 Loss                                        2.94205\n",
      "trainer/Policy Loss                                   -10.2404\n",
      "trainer/Q1 Predictions Mean                            16.6673\n",
      "trainer/Q1 Predictions Std                             30.3155\n",
      "trainer/Q1 Predictions Max                            124.93\n",
      "trainer/Q1 Predictions Min                            -17.4982\n",
      "trainer/Q2 Predictions Mean                            16.4144\n",
      "trainer/Q2 Predictions Std                             30.1426\n",
      "trainer/Q2 Predictions Max                            121.612\n",
      "trainer/Q2 Predictions Min                            -17.9912\n",
      "trainer/Q Targets Mean                                 16.8292\n",
      "trainer/Q Targets Std                                  30.667\n",
      "trainer/Q Targets Max                                 124.064\n",
      "trainer/Q Targets Min                                 -18.1204\n",
      "trainer/Log Pis Mean                                    6.42075\n",
      "trainer/Log Pis Std                                     5.4088\n",
      "trainer/Log Pis Max                                    21.0804\n",
      "trainer/Log Pis Min                                    -4.56721\n",
      "trainer/Policy mu Mean                                  0.0507796\n",
      "trainer/Policy mu Std                                   1.65415\n",
      "trainer/Policy mu Max                                   3.77217\n",
      "trainer/Policy mu Min                                  -4.19305\n",
      "trainer/Policy log std Mean                            -0.685834\n",
      "trainer/Policy log std Std                              0.318028\n",
      "trainer/Policy log std Max                              0.158949\n",
      "trainer/Policy log std Min                             -2.42825\n",
      "trainer/Alpha                                           0.0217538\n",
      "trainer/Alpha Loss                                      1.61132\n",
      "exploration/num steps total                        168000\n",
      "exploration/num paths total                           168\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.791173\n",
      "exploration/Rewards Std                                 0.617117\n",
      "exploration/Rewards Max                                 2.62561\n",
      "exploration/Rewards Min                                -1.13046\n",
      "exploration/Returns Mean                              791.173\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               791.173\n",
      "exploration/Returns Min                               791.173\n",
      "exploration/Actions Mean                                0.227159\n",
      "exploration/Actions Std                                 0.770422\n",
      "exploration/Actions Max                                 0.999816\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           791.173\n",
      "exploration/env_infos/final/reward_run Mean             4.10279\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              4.10279\n",
      "exploration/env_infos/final/reward_run Min              4.10279\n",
      "exploration/env_infos/initial/reward_run Mean           1.02476\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.02476\n",
      "exploration/env_infos/initial/reward_run Min            1.02476\n",
      "exploration/env_infos/reward_run Mean                   2.02214\n",
      "exploration/env_infos/reward_run Std                    1.1776\n",
      "exploration/env_infos/reward_run Max                    5.64581\n",
      "exploration/env_infos/reward_run Min                   -1.40591\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.497662\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.497662\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.497662\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.197803\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.197803\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.197803\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.38709\n",
      "exploration/env_infos/reward_ctrl Std                   0.0872069\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0440277\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590425\n",
      "exploration/env_infos/final/height Mean                 0.0577291\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0577291\n",
      "exploration/env_infos/final/height Min                  0.0577291\n",
      "exploration/env_infos/initial/height Mean               0.0429706\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0429706\n",
      "exploration/env_infos/initial/height Min                0.0429706\n",
      "exploration/env_infos/height Mean                      -0.0327091\n",
      "exploration/env_infos/height Std                        0.141932\n",
      "exploration/env_infos/height Max                        0.401954\n",
      "exploration/env_infos/height Min                       -0.401096\n",
      "exploration/env_infos/final/reward_angular Mean        -0.188529\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.188529\n",
      "exploration/env_infos/final/reward_angular Min         -0.188529\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.00202\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.00202\n",
      "exploration/env_infos/initial/reward_angular Min       -1.00202\n",
      "exploration/env_infos/reward_angular Mean              -0.0784511\n",
      "exploration/env_infos/reward_angular Std                2.2093\n",
      "exploration/env_infos/reward_angular Max                7.21443\n",
      "exploration/env_infos/reward_angular Min               -6.29115\n",
      "evaluation/num steps total                              4.175e+06\n",
      "evaluation/num paths total                           4175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.230988\n",
      "evaluation/Rewards Std                                  1.38514\n",
      "evaluation/Rewards Max                                  8.70979\n",
      "evaluation/Rewards Min                                 -9.80799\n",
      "evaluation/Returns Mean                               230.988\n",
      "evaluation/Returns Std                                706.114\n",
      "evaluation/Returns Max                               2784.15\n",
      "evaluation/Returns Min                               -491.389\n",
      "evaluation/Actions Mean                                 0.0263888\n",
      "evaluation/Actions Std                                  0.763453\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            230.988\n",
      "evaluation/env_infos/final/reward_run Mean              0.200522\n",
      "evaluation/env_infos/final/reward_run Std               1.6276\n",
      "evaluation/env_infos/final/reward_run Max               4.49207\n",
      "evaluation/env_infos/final/reward_run Min              -4.68523\n",
      "evaluation/env_infos/initial/reward_run Mean            0.363459\n",
      "evaluation/env_infos/initial/reward_run Std             0.441663\n",
      "evaluation/env_infos/initial/reward_run Max             1.0439\n",
      "evaluation/env_infos/initial/reward_run Min            -0.471491\n",
      "evaluation/env_infos/reward_run Mean                   -0.26298\n",
      "evaluation/env_infos/reward_run Std                     2.15969\n",
      "evaluation/env_infos/reward_run Max                     6.26013\n",
      "evaluation/env_infos/reward_run Min                    -7.78004\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.319169\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.100253\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.15079\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.507943\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.32469\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.120734\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.076814\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.533368\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.350134\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112713\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0158227\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597818\n",
      "evaluation/env_infos/final/height Mean                 -0.37211\n",
      "evaluation/env_infos/final/height Std                   0.242545\n",
      "evaluation/env_infos/final/height Max                   0.0741131\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.00773075\n",
      "evaluation/env_infos/initial/height Std                 0.0445168\n",
      "evaluation/env_infos/initial/height Max                 0.0884602\n",
      "evaluation/env_infos/initial/height Min                -0.0640956\n",
      "evaluation/env_infos/height Mean                       -0.278002\n",
      "evaluation/env_infos/height Std                         0.261774\n",
      "evaluation/env_infos/height Max                         0.45787\n",
      "evaluation/env_infos/height Min                        -0.598069\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.133177\n",
      "evaluation/env_infos/final/reward_angular Std           1.80682\n",
      "evaluation/env_infos/final/reward_angular Max           6.27772\n",
      "evaluation/env_infos/final/reward_angular Min          -4.97059\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.47312\n",
      "evaluation/env_infos/initial/reward_angular Std         0.812136\n",
      "evaluation/env_infos/initial/reward_angular Max         1.42209\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.75715\n",
      "evaluation/env_infos/reward_angular Mean                0.055351\n",
      "evaluation/env_infos/reward_angular Std                 1.811\n",
      "evaluation/env_infos/reward_angular Max                10.2179\n",
      "evaluation/env_infos/reward_angular Min                -7.29975\n",
      "time/data storing (s)                                   0.0161208\n",
      "time/evaluation sampling (s)                           31.2712\n",
      "time/exploration sampling (s)                           1.1153\n",
      "time/logging (s)                                        0.251018\n",
      "time/saving (s)                                         0.030296\n",
      "time/training (s)                                       4.55351\n",
      "time/epoch (s)                                         37.2375\n",
      "time/total (s)                                       5096.74\n",
      "Epoch                                                 166\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:37:27.215193 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 167 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 169000\n",
      "trainer/QF1 Loss                                        1.70843\n",
      "trainer/QF2 Loss                                        2.06306\n",
      "trainer/Policy Loss                                    -7.64596\n",
      "trainer/Q1 Predictions Mean                            13.9963\n",
      "trainer/Q1 Predictions Std                             26.2733\n",
      "trainer/Q1 Predictions Max                            108.617\n",
      "trainer/Q1 Predictions Min                            -17.1632\n",
      "trainer/Q2 Predictions Mean                            14.0913\n",
      "trainer/Q2 Predictions Std                             26.1955\n",
      "trainer/Q2 Predictions Max                            109.988\n",
      "trainer/Q2 Predictions Min                            -16.4807\n",
      "trainer/Q Targets Mean                                 14.0257\n",
      "trainer/Q Targets Std                                  26.0422\n",
      "trainer/Q Targets Max                                 109.699\n",
      "trainer/Q Targets Min                                 -17.3077\n",
      "trainer/Log Pis Mean                                    6.55842\n",
      "trainer/Log Pis Std                                     5.7788\n",
      "trainer/Log Pis Max                                    23.3561\n",
      "trainer/Log Pis Min                                    -6.17835\n",
      "trainer/Policy mu Mean                                 -0.00629902\n",
      "trainer/Policy mu Std                                   1.66254\n",
      "trainer/Policy mu Max                                   5.04777\n",
      "trainer/Policy mu Min                                  -4.48101\n",
      "trainer/Policy log std Mean                            -0.679276\n",
      "trainer/Policy log std Std                              0.310926\n",
      "trainer/Policy log std Max                              0.542407\n",
      "trainer/Policy log std Min                             -2.06091\n",
      "trainer/Alpha                                           0.0202312\n",
      "trainer/Alpha Loss                                      2.17855\n",
      "exploration/num steps total                        169000\n",
      "exploration/num paths total                           169\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.229882\n",
      "exploration/Rewards Std                                 0.740787\n",
      "exploration/Rewards Max                                 2.35321\n",
      "exploration/Rewards Min                                -1.71633\n",
      "exploration/Returns Mean                              229.882\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               229.882\n",
      "exploration/Returns Min                               229.882\n",
      "exploration/Actions Mean                               -0.0265923\n",
      "exploration/Actions Std                                 0.808459\n",
      "exploration/Actions Max                                 0.999988\n",
      "exploration/Actions Min                                -0.999962\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           229.882\n",
      "exploration/env_infos/final/reward_run Mean            -0.113054\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.113054\n",
      "exploration/env_infos/final/reward_run Min             -0.113054\n",
      "exploration/env_infos/initial/reward_run Mean           0.892244\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.892244\n",
      "exploration/env_infos/initial/reward_run Min            0.892244\n",
      "exploration/env_infos/reward_run Mean                   0.834505\n",
      "exploration/env_infos/reward_run Std                    1.07774\n",
      "exploration/env_infos/reward_run Max                    3.94942\n",
      "exploration/env_infos/reward_run Min                   -1.80791\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.352529\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.352529\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.352529\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.353463\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.353463\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.353463\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.392588\n",
      "exploration/env_infos/reward_ctrl Std                   0.106518\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0413635\n",
      "exploration/env_infos/reward_ctrl Min                  -0.584461\n",
      "exploration/env_infos/final/height Mean                -0.0201427\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0201427\n",
      "exploration/env_infos/final/height Min                 -0.0201427\n",
      "exploration/env_infos/initial/height Mean               0.0294752\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0294752\n",
      "exploration/env_infos/initial/height Min                0.0294752\n",
      "exploration/env_infos/height Mean                      -0.268667\n",
      "exploration/env_infos/height Std                        0.205575\n",
      "exploration/env_infos/height Max                        0.180916\n",
      "exploration/env_infos/height Min                       -0.590162\n",
      "exploration/env_infos/final/reward_angular Mean         0.658168\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.658168\n",
      "exploration/env_infos/final/reward_angular Min          0.658168\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.5561\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.5561\n",
      "exploration/env_infos/initial/reward_angular Min       -1.5561\n",
      "exploration/env_infos/reward_angular Mean               0.129725\n",
      "exploration/env_infos/reward_angular Std                2.15076\n",
      "exploration/env_infos/reward_angular Max                8.05816\n",
      "exploration/env_infos/reward_angular Min               -6.70475\n",
      "evaluation/num steps total                              4.2e+06\n",
      "evaluation/num paths total                           4200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.737909\n",
      "evaluation/Rewards Std                                  1.76874\n",
      "evaluation/Rewards Max                                  9.39997\n",
      "evaluation/Rewards Min                                 -8.48171\n",
      "evaluation/Returns Mean                               737.909\n",
      "evaluation/Returns Std                                941.39\n",
      "evaluation/Returns Max                               2725.49\n",
      "evaluation/Returns Min                               -817.63\n",
      "evaluation/Actions Mean                                 0.0828392\n",
      "evaluation/Actions Std                                  0.807292\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999992\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            737.909\n",
      "evaluation/env_infos/final/reward_run Mean             -1.72308\n",
      "evaluation/env_infos/final/reward_run Std               2.91244\n",
      "evaluation/env_infos/final/reward_run Max               3.09252\n",
      "evaluation/env_infos/final/reward_run Min              -5.70474\n",
      "evaluation/env_infos/initial/reward_run Mean            0.469288\n",
      "evaluation/env_infos/initial/reward_run Std             0.460029\n",
      "evaluation/env_infos/initial/reward_run Max             1.02463\n",
      "evaluation/env_infos/initial/reward_run Min            -0.308427\n",
      "evaluation/env_infos/reward_run Mean                   -1.61988\n",
      "evaluation/env_infos/reward_run Std                     2.95626\n",
      "evaluation/env_infos/reward_run Max                     5.37173\n",
      "evaluation/env_infos/reward_run Min                    -7.5632\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.400489\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0927945\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.205699\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.52823\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.336423\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.091039\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.192385\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.49071\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.395149\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0981359\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0357426\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59535\n",
      "evaluation/env_infos/final/height Mean                 -0.166402\n",
      "evaluation/env_infos/final/height Std                   0.213187\n",
      "evaluation/env_infos/final/height Max                   0.115856\n",
      "evaluation/env_infos/final/height Min                  -0.576253\n",
      "evaluation/env_infos/initial/height Mean               -0.018048\n",
      "evaluation/env_infos/initial/height Std                 0.0580811\n",
      "evaluation/env_infos/initial/height Max                 0.090895\n",
      "evaluation/env_infos/initial/height Min                -0.116521\n",
      "evaluation/env_infos/height Mean                       -0.145751\n",
      "evaluation/env_infos/height Std                         0.21738\n",
      "evaluation/env_infos/height Max                         0.462141\n",
      "evaluation/env_infos/height Min                        -0.604939\n",
      "evaluation/env_infos/final/reward_angular Mean          0.502111\n",
      "evaluation/env_infos/final/reward_angular Std           2.44977\n",
      "evaluation/env_infos/final/reward_angular Max           6.09225\n",
      "evaluation/env_infos/final/reward_angular Min          -3.37479\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.710313\n",
      "evaluation/env_infos/initial/reward_angular Std         0.952604\n",
      "evaluation/env_infos/initial/reward_angular Max         1.87207\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.9451\n",
      "evaluation/env_infos/reward_angular Mean                0.103663\n",
      "evaluation/env_infos/reward_angular Std                 2.46111\n",
      "evaluation/env_infos/reward_angular Max                10.2068\n",
      "evaluation/env_infos/reward_angular Min                -6.80124\n",
      "time/data storing (s)                                   0.0149572\n",
      "time/evaluation sampling (s)                           20.6481\n",
      "time/exploration sampling (s)                           0.982051\n",
      "time/logging (s)                                        0.239768\n",
      "time/saving (s)                                         0.0268611\n",
      "time/training (s)                                       4.17302\n",
      "time/epoch (s)                                         26.0847\n",
      "time/total (s)                                       5124.24\n",
      "Epoch                                                 167\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:37:55.322712 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 168 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 170000\n",
      "trainer/QF1 Loss                                        2.4726\n",
      "trainer/QF2 Loss                                        2.65738\n",
      "trainer/Policy Loss                                    -9.72207\n",
      "trainer/Q1 Predictions Mean                            15.5639\n",
      "trainer/Q1 Predictions Std                             30.8464\n",
      "trainer/Q1 Predictions Max                            115.7\n",
      "trainer/Q1 Predictions Min                            -16.2678\n",
      "trainer/Q2 Predictions Mean                            15.6561\n",
      "trainer/Q2 Predictions Std                             30.7658\n",
      "trainer/Q2 Predictions Max                            118.135\n",
      "trainer/Q2 Predictions Min                            -16.2028\n",
      "trainer/Q Targets Mean                                 15.4044\n",
      "trainer/Q Targets Std                                  30.6503\n",
      "trainer/Q Targets Max                                 117.088\n",
      "trainer/Q Targets Min                                 -16.5301\n",
      "trainer/Log Pis Mean                                    6.08701\n",
      "trainer/Log Pis Std                                     5.6475\n",
      "trainer/Log Pis Max                                    26.7173\n",
      "trainer/Log Pis Min                                    -8.0581\n",
      "trainer/Policy mu Mean                                 -0.0785799\n",
      "trainer/Policy mu Std                                   1.62465\n",
      "trainer/Policy mu Max                                   3.6716\n",
      "trainer/Policy mu Min                                  -5.40697\n",
      "trainer/Policy log std Mean                            -0.683019\n",
      "trainer/Policy log std Std                              0.308083\n",
      "trainer/Policy log std Max                              0.395079\n",
      "trainer/Policy log std Min                             -2.16089\n",
      "trainer/Alpha                                           0.0211278\n",
      "trainer/Alpha Loss                                      0.33573\n",
      "exploration/num steps total                        170000\n",
      "exploration/num paths total                           170\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.190662\n",
      "exploration/Rewards Std                                 1.8027\n",
      "exploration/Rewards Max                                 5.38247\n",
      "exploration/Rewards Min                                -5.7529\n",
      "exploration/Returns Mean                             -190.662\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -190.662\n",
      "exploration/Returns Min                              -190.662\n",
      "exploration/Actions Mean                               -0.104858\n",
      "exploration/Actions Std                                 0.787066\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -190.662\n",
      "exploration/env_infos/final/reward_run Mean             1.22101\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.22101\n",
      "exploration/env_infos/final/reward_run Min              1.22101\n",
      "exploration/env_infos/initial/reward_run Mean           0.702919\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.702919\n",
      "exploration/env_infos/initial/reward_run Min            0.702919\n",
      "exploration/env_infos/reward_run Mean                  -0.175735\n",
      "exploration/env_infos/reward_run Std                    0.890005\n",
      "exploration/env_infos/reward_run Max                    1.97103\n",
      "exploration/env_infos/reward_run Min                   -2.49677\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.490258\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.490258\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.490258\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21176\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21176\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21176\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.378281\n",
      "exploration/env_infos/reward_ctrl Std                   0.0906103\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0413889\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596669\n",
      "exploration/env_infos/final/height Mean                -0.342343\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.342343\n",
      "exploration/env_infos/final/height Min                 -0.342343\n",
      "exploration/env_infos/initial/height Mean              -0.0117179\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0117179\n",
      "exploration/env_infos/initial/height Min               -0.0117179\n",
      "exploration/env_infos/height Mean                      -0.416952\n",
      "exploration/env_infos/height Std                        0.139546\n",
      "exploration/env_infos/height Max                        0.360885\n",
      "exploration/env_infos/height Min                       -0.588253\n",
      "exploration/env_infos/final/reward_angular Mean         0.846832\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.846832\n",
      "exploration/env_infos/final/reward_angular Min          0.846832\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.380776\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.380776\n",
      "exploration/env_infos/initial/reward_angular Min       -0.380776\n",
      "exploration/env_infos/reward_angular Mean               0.199049\n",
      "exploration/env_infos/reward_angular Std                2.18831\n",
      "exploration/env_infos/reward_angular Max                7.92114\n",
      "exploration/env_infos/reward_angular Min               -6.08445\n",
      "evaluation/num steps total                              4.225e+06\n",
      "evaluation/num paths total                           4225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.737442\n",
      "evaluation/Rewards Std                                  1.71529\n",
      "evaluation/Rewards Max                                  8.68423\n",
      "evaluation/Rewards Min                                 -9.61791\n",
      "evaluation/Returns Mean                               737.442\n",
      "evaluation/Returns Std                                880.927\n",
      "evaluation/Returns Max                               2908.39\n",
      "evaluation/Returns Min                               -428.287\n",
      "evaluation/Actions Mean                                 0.119285\n",
      "evaluation/Actions Std                                  0.806448\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999988\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            737.442\n",
      "evaluation/env_infos/final/reward_run Mean             -0.800827\n",
      "evaluation/env_infos/final/reward_run Std               2.54279\n",
      "evaluation/env_infos/final/reward_run Max               3.40164\n",
      "evaluation/env_infos/final/reward_run Min              -5.49556\n",
      "evaluation/env_infos/initial/reward_run Mean            0.317102\n",
      "evaluation/env_infos/initial/reward_run Std             0.544481\n",
      "evaluation/env_infos/initial/reward_run Max             1.17146\n",
      "evaluation/env_infos/initial/reward_run Min            -0.522826\n",
      "evaluation/env_infos/reward_run Mean                   -1.10917\n",
      "evaluation/env_infos/reward_run Std                     2.7069\n",
      "evaluation/env_infos/reward_run Max                     4.59821\n",
      "evaluation/env_infos/reward_run Min                    -7.30039\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.382668\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.119798\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.097381\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.592395\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.341157\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0903501\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.19768\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.495027\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.398753\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115012\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0101201\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597605\n",
      "evaluation/env_infos/final/height Mean                 -0.178797\n",
      "evaluation/env_infos/final/height Std                   0.236941\n",
      "evaluation/env_infos/final/height Max                   0.144848\n",
      "evaluation/env_infos/final/height Min                  -0.576178\n",
      "evaluation/env_infos/initial/height Mean               -0.00970453\n",
      "evaluation/env_infos/initial/height Std                 0.0452718\n",
      "evaluation/env_infos/initial/height Max                 0.0662582\n",
      "evaluation/env_infos/initial/height Min                -0.0866558\n",
      "evaluation/env_infos/height Mean                       -0.10736\n",
      "evaluation/env_infos/height Std                         0.218803\n",
      "evaluation/env_infos/height Max                         0.448383\n",
      "evaluation/env_infos/height Min                        -0.584795\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.449936\n",
      "evaluation/env_infos/final/reward_angular Std           2.21032\n",
      "evaluation/env_infos/final/reward_angular Max           4.40526\n",
      "evaluation/env_infos/final/reward_angular Min          -4.88401\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.37997\n",
      "evaluation/env_infos/initial/reward_angular Std         1.10445\n",
      "evaluation/env_infos/initial/reward_angular Max         2.80432\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.23183\n",
      "evaluation/env_infos/reward_angular Mean                0.0626231\n",
      "evaluation/env_infos/reward_angular Std                 2.4542\n",
      "evaluation/env_infos/reward_angular Max                10.0268\n",
      "evaluation/env_infos/reward_angular Min                -8.22124\n",
      "time/data storing (s)                                   0.0151116\n",
      "time/evaluation sampling (s)                           20.4363\n",
      "time/exploration sampling (s)                           1.01287\n",
      "time/logging (s)                                        0.258437\n",
      "time/saving (s)                                         0.0277436\n",
      "time/training (s)                                       5.1807\n",
      "time/epoch (s)                                         26.9311\n",
      "time/total (s)                                       5152.36\n",
      "Epoch                                                 168\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:38:23.534824 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 169 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 171000\n",
      "trainer/QF1 Loss                                        2.11613\n",
      "trainer/QF2 Loss                                        2.06174\n",
      "trainer/Policy Loss                                   -12.7316\n",
      "trainer/Q1 Predictions Mean                            19.2378\n",
      "trainer/Q1 Predictions Std                             30.9903\n",
      "trainer/Q1 Predictions Max                            118.22\n",
      "trainer/Q1 Predictions Min                            -17.005\n",
      "trainer/Q2 Predictions Mean                            19.1319\n",
      "trainer/Q2 Predictions Std                             30.933\n",
      "trainer/Q2 Predictions Max                            117.995\n",
      "trainer/Q2 Predictions Min                            -18.0256\n",
      "trainer/Q Targets Mean                                 19.12\n",
      "trainer/Q Targets Std                                  30.8899\n",
      "trainer/Q Targets Max                                 117.269\n",
      "trainer/Q Targets Min                                 -17.7596\n",
      "trainer/Log Pis Mean                                    6.74849\n",
      "trainer/Log Pis Std                                     5.40044\n",
      "trainer/Log Pis Max                                    27.1312\n",
      "trainer/Log Pis Min                                    -4.36445\n",
      "trainer/Policy mu Mean                                  0.139656\n",
      "trainer/Policy mu Std                                   1.64042\n",
      "trainer/Policy mu Max                                   5.12264\n",
      "trainer/Policy mu Min                                  -4.36937\n",
      "trainer/Policy log std Mean                            -0.705769\n",
      "trainer/Policy log std Std                              0.294042\n",
      "trainer/Policy log std Max                              0.700458\n",
      "trainer/Policy log std Min                             -2.28415\n",
      "trainer/Alpha                                           0.0215447\n",
      "trainer/Alpha Loss                                      2.87282\n",
      "exploration/num steps total                        171000\n",
      "exploration/num paths total                           171\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.69326\n",
      "exploration/Rewards Std                                 1.23337\n",
      "exploration/Rewards Max                                 5.44432\n",
      "exploration/Rewards Min                                -3.9196\n",
      "exploration/Returns Mean                             1693.26\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1693.26\n",
      "exploration/Returns Min                              1693.26\n",
      "exploration/Actions Mean                                0.226968\n",
      "exploration/Actions Std                                 0.813375\n",
      "exploration/Actions Max                                 0.999995\n",
      "exploration/Actions Min                                -0.999973\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1693.26\n",
      "exploration/env_infos/final/reward_run Mean             2.76844\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.76844\n",
      "exploration/env_infos/final/reward_run Min              2.76844\n",
      "exploration/env_infos/initial/reward_run Mean           0.904609\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.904609\n",
      "exploration/env_infos/initial/reward_run Min            0.904609\n",
      "exploration/env_infos/reward_run Mean                   2.47558\n",
      "exploration/env_infos/reward_run Std                    0.830436\n",
      "exploration/env_infos/reward_run Max                    4.40806\n",
      "exploration/env_infos/reward_run Min                   -1.08793\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.45413\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.45413\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.45413\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.403567\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.403567\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.403567\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.427856\n",
      "exploration/env_infos/reward_ctrl Std                   0.0833343\n",
      "exploration/env_infos/reward_ctrl Max                  -0.115872\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590457\n",
      "exploration/env_infos/final/height Mean                -0.0531077\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0531077\n",
      "exploration/env_infos/final/height Min                 -0.0531077\n",
      "exploration/env_infos/initial/height Mean               0.08538\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.08538\n",
      "exploration/env_infos/initial/height Min                0.08538\n",
      "exploration/env_infos/height Mean                      -0.119879\n",
      "exploration/env_infos/height Std                        0.100326\n",
      "exploration/env_infos/height Max                        0.321498\n",
      "exploration/env_infos/height Min                       -0.329207\n",
      "exploration/env_infos/final/reward_angular Mean        -0.75449\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.75449\n",
      "exploration/env_infos/final/reward_angular Min         -0.75449\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.823399\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.823399\n",
      "exploration/env_infos/initial/reward_angular Min       -0.823399\n",
      "exploration/env_infos/reward_angular Mean              -0.0833943\n",
      "exploration/env_infos/reward_angular Std                1.7122\n",
      "exploration/env_infos/reward_angular Max                4.8441\n",
      "exploration/env_infos/reward_angular Min               -5.58646\n",
      "evaluation/num steps total                              4.25e+06\n",
      "evaluation/num paths total                           4250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.699276\n",
      "evaluation/Rewards Std                                  1.66328\n",
      "evaluation/Rewards Max                                  9.49314\n",
      "evaluation/Rewards Min                                 -8.81472\n",
      "evaluation/Returns Mean                               699.276\n",
      "evaluation/Returns Std                                956.809\n",
      "evaluation/Returns Max                               2903.34\n",
      "evaluation/Returns Min                               -414.823\n",
      "evaluation/Actions Mean                                 0.191756\n",
      "evaluation/Actions Std                                  0.786141\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            699.276\n",
      "evaluation/env_infos/final/reward_run Mean             -0.857664\n",
      "evaluation/env_infos/final/reward_run Std               2.92689\n",
      "evaluation/env_infos/final/reward_run Max               3.6947\n",
      "evaluation/env_infos/final/reward_run Min              -6.64689\n",
      "evaluation/env_infos/initial/reward_run Mean            0.388323\n",
      "evaluation/env_infos/initial/reward_run Std             0.423235\n",
      "evaluation/env_infos/initial/reward_run Max             1.09457\n",
      "evaluation/env_infos/initial/reward_run Min            -0.257167\n",
      "evaluation/env_infos/reward_run Mean                   -1.15857\n",
      "evaluation/env_infos/reward_run Std                     2.90505\n",
      "evaluation/env_infos/reward_run Max                     4.91264\n",
      "evaluation/env_infos/reward_run Min                    -7.64607\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.3936\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.103897\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.189933\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.535036\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.341405\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.131167\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0545628\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.559245\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.392872\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106194\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0102033\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59689\n",
      "evaluation/env_infos/final/height Mean                 -0.192612\n",
      "evaluation/env_infos/final/height Std                   0.239782\n",
      "evaluation/env_infos/final/height Max                   0.181886\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00403425\n",
      "evaluation/env_infos/initial/height Std                 0.0565442\n",
      "evaluation/env_infos/initial/height Max                 0.0765385\n",
      "evaluation/env_infos/initial/height Min                -0.118038\n",
      "evaluation/env_infos/height Mean                       -0.160168\n",
      "evaluation/env_infos/height Std                         0.241215\n",
      "evaluation/env_infos/height Max                         0.476664\n",
      "evaluation/env_infos/height Min                        -0.595342\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.761555\n",
      "evaluation/env_infos/final/reward_angular Std           1.48101\n",
      "evaluation/env_infos/final/reward_angular Max           1.51726\n",
      "evaluation/env_infos/final/reward_angular Min          -5.39927\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.943015\n",
      "evaluation/env_infos/initial/reward_angular Std         0.799317\n",
      "evaluation/env_infos/initial/reward_angular Max         1.12024\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.3616\n",
      "evaluation/env_infos/reward_angular Mean                0.0590584\n",
      "evaluation/env_infos/reward_angular Std                 2.30751\n",
      "evaluation/env_infos/reward_angular Max                10.6155\n",
      "evaluation/env_infos/reward_angular Min                -6.54886\n",
      "time/data storing (s)                                   0.0158211\n",
      "time/evaluation sampling (s)                           20.8815\n",
      "time/exploration sampling (s)                           1.04364\n",
      "time/logging (s)                                        0.23883\n",
      "time/saving (s)                                         0.0294748\n",
      "time/training (s)                                       4.04543\n",
      "time/epoch (s)                                         26.2547\n",
      "time/total (s)                                       5180.55\n",
      "Epoch                                                 169\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:38:51.932905 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 170 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 172000\n",
      "trainer/QF1 Loss                                        1.65826\n",
      "trainer/QF2 Loss                                        1.24969\n",
      "trainer/Policy Loss                                   -10.7546\n",
      "trainer/Q1 Predictions Mean                            16.099\n",
      "trainer/Q1 Predictions Std                             30.7927\n",
      "trainer/Q1 Predictions Max                            120.942\n",
      "trainer/Q1 Predictions Min                            -17.1865\n",
      "trainer/Q2 Predictions Mean                            16.2267\n",
      "trainer/Q2 Predictions Std                             30.6254\n",
      "trainer/Q2 Predictions Max                            120.639\n",
      "trainer/Q2 Predictions Min                            -17.471\n",
      "trainer/Q Targets Mean                                 16.1328\n",
      "trainer/Q Targets Std                                  30.9008\n",
      "trainer/Q Targets Max                                 122.209\n",
      "trainer/Q Targets Min                                 -17.4478\n",
      "trainer/Log Pis Mean                                    5.51014\n",
      "trainer/Log Pis Std                                     5.53289\n",
      "trainer/Log Pis Max                                    22.4071\n",
      "trainer/Log Pis Min                                    -5.32195\n",
      "trainer/Policy mu Mean                                  0.0590629\n",
      "trainer/Policy mu Std                                   1.55806\n",
      "trainer/Policy mu Max                                   3.77605\n",
      "trainer/Policy mu Min                                  -4.80112\n",
      "trainer/Policy log std Mean                            -0.693083\n",
      "trainer/Policy log std Std                              0.296751\n",
      "trainer/Policy log std Max                              0.189937\n",
      "trainer/Policy log std Min                             -2.2682\n",
      "trainer/Alpha                                           0.0217466\n",
      "trainer/Alpha Loss                                     -1.87579\n",
      "exploration/num steps total                        172000\n",
      "exploration/num paths total                           172\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.23382\n",
      "exploration/Rewards Std                                 0.886442\n",
      "exploration/Rewards Max                                 4.46511\n",
      "exploration/Rewards Min                                -0.67927\n",
      "exploration/Returns Mean                             2233.82\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2233.82\n",
      "exploration/Returns Min                              2233.82\n",
      "exploration/Actions Mean                                0.0724273\n",
      "exploration/Actions Std                                 0.873832\n",
      "exploration/Actions Max                                 0.999629\n",
      "exploration/Actions Min                                -0.999751\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2233.82\n",
      "exploration/env_infos/final/reward_run Mean            -3.37747\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.37747\n",
      "exploration/env_infos/final/reward_run Min             -3.37747\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0321741\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0321741\n",
      "exploration/env_infos/initial/reward_run Min           -0.0321741\n",
      "exploration/env_infos/reward_run Mean                  -3.85102\n",
      "exploration/env_infos/reward_run Std                    1.07152\n",
      "exploration/env_infos/reward_run Max                    0.111645\n",
      "exploration/env_infos/reward_run Min                   -6.44016\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.413536\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.413536\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.413536\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.186333\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.186333\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.186333\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.461297\n",
      "exploration/env_infos/reward_ctrl Std                   0.0658925\n",
      "exploration/env_infos/reward_ctrl Max                  -0.186333\n",
      "exploration/env_infos/reward_ctrl Min                  -0.581615\n",
      "exploration/env_infos/final/height Mean                -0.104572\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.104572\n",
      "exploration/env_infos/final/height Min                 -0.104572\n",
      "exploration/env_infos/initial/height Mean               0.0661388\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0661388\n",
      "exploration/env_infos/initial/height Min                0.0661388\n",
      "exploration/env_infos/height Mean                      -0.0495342\n",
      "exploration/env_infos/height Std                        0.111009\n",
      "exploration/env_infos/height Max                        0.272449\n",
      "exploration/env_infos/height Min                       -0.320442\n",
      "exploration/env_infos/final/reward_angular Mean         2.56324\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.56324\n",
      "exploration/env_infos/final/reward_angular Min          2.56324\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.702413\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.702413\n",
      "exploration/env_infos/initial/reward_angular Min       -0.702413\n",
      "exploration/env_infos/reward_angular Mean               0.11094\n",
      "exploration/env_infos/reward_angular Std                2.98243\n",
      "exploration/env_infos/reward_angular Max                8.81031\n",
      "exploration/env_infos/reward_angular Min               -5.78288\n",
      "evaluation/num steps total                              4.275e+06\n",
      "evaluation/num paths total                           4275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.793896\n",
      "evaluation/Rewards Std                                  1.70624\n",
      "evaluation/Rewards Max                                  8.24075\n",
      "evaluation/Rewards Min                                 -8.39724\n",
      "evaluation/Returns Mean                               793.896\n",
      "evaluation/Returns Std                                877.487\n",
      "evaluation/Returns Max                               2658.54\n",
      "evaluation/Returns Min                               -632.33\n",
      "evaluation/Actions Mean                                 0.111618\n",
      "evaluation/Actions Std                                  0.805462\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            793.896\n",
      "evaluation/env_infos/final/reward_run Mean             -1.65937\n",
      "evaluation/env_infos/final/reward_run Std               2.96125\n",
      "evaluation/env_infos/final/reward_run Max               3.256\n",
      "evaluation/env_infos/final/reward_run Min              -5.8236\n",
      "evaluation/env_infos/initial/reward_run Mean            0.455628\n",
      "evaluation/env_infos/initial/reward_run Std             0.462417\n",
      "evaluation/env_infos/initial/reward_run Max             1.13675\n",
      "evaluation/env_infos/initial/reward_run Min            -0.298178\n",
      "evaluation/env_infos/reward_run Mean                   -1.55185\n",
      "evaluation/env_infos/reward_run Std                     3.065\n",
      "evaluation/env_infos/reward_run Max                     5.98213\n",
      "evaluation/env_infos/reward_run Min                    -7.35036\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.411976\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.106745\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.170884\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.526031\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.305559\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.100186\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0894995\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.467311\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.396737\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0988399\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0351959\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594596\n",
      "evaluation/env_infos/final/height Mean                 -0.12672\n",
      "evaluation/env_infos/final/height Std                   0.204941\n",
      "evaluation/env_infos/final/height Max                   0.242788\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0214685\n",
      "evaluation/env_infos/initial/height Std                 0.0413238\n",
      "evaluation/env_infos/initial/height Max                 0.0844917\n",
      "evaluation/env_infos/initial/height Min                -0.0902463\n",
      "evaluation/env_infos/height Mean                       -0.100259\n",
      "evaluation/env_infos/height Std                         0.183723\n",
      "evaluation/env_infos/height Max                         0.458582\n",
      "evaluation/env_infos/height Min                        -0.577291\n",
      "evaluation/env_infos/final/reward_angular Mean          1.63373\n",
      "evaluation/env_infos/final/reward_angular Std           1.99039\n",
      "evaluation/env_infos/final/reward_angular Max           6.00514\n",
      "evaluation/env_infos/final/reward_angular Min          -2.68235\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.059107\n",
      "evaluation/env_infos/initial/reward_angular Std         1.23921\n",
      "evaluation/env_infos/initial/reward_angular Max         2.38365\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.23172\n",
      "evaluation/env_infos/reward_angular Mean                0.074975\n",
      "evaluation/env_infos/reward_angular Std                 2.39905\n",
      "evaluation/env_infos/reward_angular Max                 9.82199\n",
      "evaluation/env_infos/reward_angular Min                -7.11726\n",
      "time/data storing (s)                                   0.0163695\n",
      "time/evaluation sampling (s)                           21.2824\n",
      "time/exploration sampling (s)                           1.02825\n",
      "time/logging (s)                                        0.259067\n",
      "time/saving (s)                                         0.029035\n",
      "time/training (s)                                       4.5643\n",
      "time/epoch (s)                                         27.1794\n",
      "time/total (s)                                       5208.96\n",
      "Epoch                                                 170\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:39:22.084377 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 171 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 173000\n",
      "trainer/QF1 Loss                                        2.62662\n",
      "trainer/QF2 Loss                                        3.5089\n",
      "trainer/Policy Loss                                   -10.2594\n",
      "trainer/Q1 Predictions Mean                            16.3699\n",
      "trainer/Q1 Predictions Std                             30.0397\n",
      "trainer/Q1 Predictions Max                            130.695\n",
      "trainer/Q1 Predictions Min                            -17.5882\n",
      "trainer/Q2 Predictions Mean                            16.4163\n",
      "trainer/Q2 Predictions Std                             30.2315\n",
      "trainer/Q2 Predictions Max                            131.149\n",
      "trainer/Q2 Predictions Min                            -18.1619\n",
      "trainer/Q Targets Mean                                 16.1036\n",
      "trainer/Q Targets Std                                  29.8122\n",
      "trainer/Q Targets Max                                 129.593\n",
      "trainer/Q Targets Min                                 -18.5121\n",
      "trainer/Log Pis Mean                                    6.36346\n",
      "trainer/Log Pis Std                                     5.80596\n",
      "trainer/Log Pis Max                                    31.6793\n",
      "trainer/Log Pis Min                                    -6.27272\n",
      "trainer/Policy mu Mean                                  0.0323886\n",
      "trainer/Policy mu Std                                   1.60707\n",
      "trainer/Policy mu Max                                   4.91242\n",
      "trainer/Policy mu Min                                  -5.48612\n",
      "trainer/Policy log std Mean                            -0.729481\n",
      "trainer/Policy log std Std                              0.27129\n",
      "trainer/Policy log std Max                              0.152829\n",
      "trainer/Policy log std Min                             -1.93731\n",
      "trainer/Alpha                                           0.0215531\n",
      "trainer/Alpha Loss                                      1.39459\n",
      "exploration/num steps total                        173000\n",
      "exploration/num paths total                           173\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.00145787\n",
      "exploration/Rewards Std                                 1.06099\n",
      "exploration/Rewards Max                                 4.10378\n",
      "exploration/Rewards Min                                -2.95685\n",
      "exploration/Returns Mean                               -1.45787\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                -1.45787\n",
      "exploration/Returns Min                                -1.45787\n",
      "exploration/Actions Mean                               -0.238669\n",
      "exploration/Actions Std                                 0.794544\n",
      "exploration/Actions Max                                 0.99999\n",
      "exploration/Actions Min                                -0.999996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            -1.45787\n",
      "exploration/env_infos/final/reward_run Mean            -2.80854\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.80854\n",
      "exploration/env_infos/final/reward_run Min             -2.80854\n",
      "exploration/env_infos/initial/reward_run Mean           0.534123\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.534123\n",
      "exploration/env_infos/initial/reward_run Min            0.534123\n",
      "exploration/env_infos/reward_run Mean                  -2.88727\n",
      "exploration/env_infos/reward_run Std                    1.30763\n",
      "exploration/env_infos/reward_run Max                    0.681023\n",
      "exploration/env_infos/reward_run Min                   -6.92497\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.420728\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.420728\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.420728\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.428699\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.428699\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.428699\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.412958\n",
      "exploration/env_infos/reward_ctrl Std                   0.0880824\n",
      "exploration/env_infos/reward_ctrl Max                  -0.154933\n",
      "exploration/env_infos/reward_ctrl Min                  -0.577566\n",
      "exploration/env_infos/final/height Mean                 0.140521\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.140521\n",
      "exploration/env_infos/final/height Min                  0.140521\n",
      "exploration/env_infos/initial/height Mean              -0.069139\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.069139\n",
      "exploration/env_infos/initial/height Min               -0.069139\n",
      "exploration/env_infos/height Mean                      -0.0148047\n",
      "exploration/env_infos/height Std                        0.0818909\n",
      "exploration/env_infos/height Max                        0.347014\n",
      "exploration/env_infos/height Min                       -0.234776\n",
      "exploration/env_infos/final/reward_angular Mean        -0.70077\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.70077\n",
      "exploration/env_infos/final/reward_angular Min         -0.70077\n",
      "exploration/env_infos/initial/reward_angular Mean       1.80852\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.80852\n",
      "exploration/env_infos/initial/reward_angular Min        1.80852\n",
      "exploration/env_infos/reward_angular Mean               0.0395775\n",
      "exploration/env_infos/reward_angular Std                2.02622\n",
      "exploration/env_infos/reward_angular Max                7.99501\n",
      "exploration/env_infos/reward_angular Min               -5.53114\n",
      "evaluation/num steps total                              4.3e+06\n",
      "evaluation/num paths total                           4300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.800714\n",
      "evaluation/Rewards Std                                  1.60919\n",
      "evaluation/Rewards Max                                  9.50867\n",
      "evaluation/Rewards Min                                 -6.8475\n",
      "evaluation/Returns Mean                               800.714\n",
      "evaluation/Returns Std                                942.667\n",
      "evaluation/Returns Max                               2882.36\n",
      "evaluation/Returns Min                               -367.229\n",
      "evaluation/Actions Mean                                 0.168875\n",
      "evaluation/Actions Std                                  0.767764\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            800.714\n",
      "evaluation/env_infos/final/reward_run Mean             -0.931725\n",
      "evaluation/env_infos/final/reward_run Std               3.33471\n",
      "evaluation/env_infos/final/reward_run Max               4.37412\n",
      "evaluation/env_infos/final/reward_run Min              -6.91976\n",
      "evaluation/env_infos/initial/reward_run Mean            0.246747\n",
      "evaluation/env_infos/initial/reward_run Std             0.549534\n",
      "evaluation/env_infos/initial/reward_run Max             1.19673\n",
      "evaluation/env_infos/initial/reward_run Min            -0.731697\n",
      "evaluation/env_infos/reward_run Mean                   -1.0489\n",
      "evaluation/env_infos/reward_run Std                     2.94583\n",
      "evaluation/env_infos/reward_run Max                     5.59378\n",
      "evaluation/env_infos/reward_run Min                    -7.81616\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.380199\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104341\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.196598\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.563763\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.287561\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0795954\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.111945\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.409006\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.370788\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111607\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0340176\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594692\n",
      "evaluation/env_infos/final/height Mean                 -0.170319\n",
      "evaluation/env_infos/final/height Std                   0.226484\n",
      "evaluation/env_infos/final/height Max                   0.163084\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean                0.0044488\n",
      "evaluation/env_infos/initial/height Std                 0.0547852\n",
      "evaluation/env_infos/initial/height Max                 0.0793911\n",
      "evaluation/env_infos/initial/height Min                -0.0862592\n",
      "evaluation/env_infos/height Mean                       -0.123783\n",
      "evaluation/env_infos/height Std                         0.227712\n",
      "evaluation/env_infos/height Max                         0.598799\n",
      "evaluation/env_infos/height Min                        -0.580588\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.314109\n",
      "evaluation/env_infos/final/reward_angular Std           2.4413\n",
      "evaluation/env_infos/final/reward_angular Max           6.75896\n",
      "evaluation/env_infos/final/reward_angular Min          -5.94404\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.511863\n",
      "evaluation/env_infos/initial/reward_angular Std         0.984006\n",
      "evaluation/env_infos/initial/reward_angular Max         2.43715\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.79675\n",
      "evaluation/env_infos/reward_angular Mean                0.0507263\n",
      "evaluation/env_infos/reward_angular Std                 2.26974\n",
      "evaluation/env_infos/reward_angular Max                 9.37362\n",
      "evaluation/env_infos/reward_angular Min                -7.69986\n",
      "time/data storing (s)                                   0.016589\n",
      "time/evaluation sampling (s)                           23.0729\n",
      "time/exploration sampling (s)                           1.09768\n",
      "time/logging (s)                                        0.238291\n",
      "time/saving (s)                                         0.026058\n",
      "time/training (s)                                       4.33896\n",
      "time/epoch (s)                                         28.7905\n",
      "time/total (s)                                       5239.09\n",
      "Epoch                                                 171\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:39:51.378362 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 172 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 174000\n",
      "trainer/QF1 Loss                                        1.89524\n",
      "trainer/QF2 Loss                                        2.26271\n",
      "trainer/Policy Loss                                   -12.5222\n",
      "trainer/Q1 Predictions Mean                            17.8906\n",
      "trainer/Q1 Predictions Std                             31.4498\n",
      "trainer/Q1 Predictions Max                            123.406\n",
      "trainer/Q1 Predictions Min                            -16.4635\n",
      "trainer/Q2 Predictions Mean                            17.9982\n",
      "trainer/Q2 Predictions Std                             31.6859\n",
      "trainer/Q2 Predictions Max                            124.951\n",
      "trainer/Q2 Predictions Min                            -16.1988\n",
      "trainer/Q Targets Mean                                 18.1293\n",
      "trainer/Q Targets Std                                  31.8207\n",
      "trainer/Q Targets Max                                 125.781\n",
      "trainer/Q Targets Min                                 -16.3253\n",
      "trainer/Log Pis Mean                                    5.59829\n",
      "trainer/Log Pis Std                                     5.10577\n",
      "trainer/Log Pis Max                                    22.6699\n",
      "trainer/Log Pis Min                                    -9.35946\n",
      "trainer/Policy mu Mean                                  0.0655509\n",
      "trainer/Policy mu Std                                   1.56446\n",
      "trainer/Policy mu Max                                   3.9051\n",
      "trainer/Policy mu Min                                  -4.2353\n",
      "trainer/Policy log std Mean                            -0.684339\n",
      "trainer/Policy log std Std                              0.269531\n",
      "trainer/Policy log std Max                              0.619563\n",
      "trainer/Policy log std Min                             -2.25386\n",
      "trainer/Alpha                                           0.02292\n",
      "trainer/Alpha Loss                                     -1.51628\n",
      "exploration/num steps total                        174000\n",
      "exploration/num paths total                           174\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.1648\n",
      "exploration/Rewards Std                                 1.01692\n",
      "exploration/Rewards Max                                 4.74381\n",
      "exploration/Rewards Min                                -1.54752\n",
      "exploration/Returns Mean                             2164.8\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2164.8\n",
      "exploration/Returns Min                              2164.8\n",
      "exploration/Actions Mean                                0.156899\n",
      "exploration/Actions Std                                 0.820796\n",
      "exploration/Actions Max                                 0.999996\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2164.8\n",
      "exploration/env_infos/final/reward_run Mean             2.65533\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.65533\n",
      "exploration/env_infos/final/reward_run Min              2.65533\n",
      "exploration/env_infos/initial/reward_run Mean           0.872339\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.872339\n",
      "exploration/env_infos/initial/reward_run Min            0.872339\n",
      "exploration/env_infos/reward_run Mean                   2.34698\n",
      "exploration/env_infos/reward_run Std                    0.980354\n",
      "exploration/env_infos/reward_run Max                    5.0803\n",
      "exploration/env_infos/reward_run Min                   -1.22946\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.313138\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.313138\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.313138\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.429515\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.429515\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.429515\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.418994\n",
      "exploration/env_infos/reward_ctrl Std                   0.0814095\n",
      "exploration/env_infos/reward_ctrl Max                  -0.14515\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594777\n",
      "exploration/env_infos/final/height Mean                -0.0898859\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0898859\n",
      "exploration/env_infos/final/height Min                 -0.0898859\n",
      "exploration/env_infos/initial/height Mean               0.0331551\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0331551\n",
      "exploration/env_infos/initial/height Min                0.0331551\n",
      "exploration/env_infos/height Mean                      -0.0584154\n",
      "exploration/env_infos/height Std                        0.121477\n",
      "exploration/env_infos/height Max                        0.359026\n",
      "exploration/env_infos/height Min                       -0.342756\n",
      "exploration/env_infos/final/reward_angular Mean        -0.212926\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.212926\n",
      "exploration/env_infos/final/reward_angular Min         -0.212926\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.428124\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.428124\n",
      "exploration/env_infos/initial/reward_angular Min       -0.428124\n",
      "exploration/env_infos/reward_angular Mean              -0.0872807\n",
      "exploration/env_infos/reward_angular Std                2.02795\n",
      "exploration/env_infos/reward_angular Max                4.85354\n",
      "exploration/env_infos/reward_angular Min               -5.80892\n",
      "evaluation/num steps total                              4.325e+06\n",
      "evaluation/num paths total                           4325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.893795\n",
      "evaluation/Rewards Std                                  1.69876\n",
      "evaluation/Rewards Max                                  8.62667\n",
      "evaluation/Rewards Min                                 -8.83287\n",
      "evaluation/Returns Mean                               893.795\n",
      "evaluation/Returns Std                                975.831\n",
      "evaluation/Returns Max                               2953.15\n",
      "evaluation/Returns Min                               -501.049\n",
      "evaluation/Actions Mean                                 0.144332\n",
      "evaluation/Actions Std                                  0.797474\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            893.795\n",
      "evaluation/env_infos/final/reward_run Mean             -1.03678\n",
      "evaluation/env_infos/final/reward_run Std               3.24693\n",
      "evaluation/env_infos/final/reward_run Max               4.33915\n",
      "evaluation/env_infos/final/reward_run Min              -6.49198\n",
      "evaluation/env_infos/initial/reward_run Mean            0.275127\n",
      "evaluation/env_infos/initial/reward_run Std             0.543993\n",
      "evaluation/env_infos/initial/reward_run Max             1.01215\n",
      "evaluation/env_infos/initial/reward_run Min            -0.704854\n",
      "evaluation/env_infos/reward_run Mean                   -1.22044\n",
      "evaluation/env_infos/reward_run Std                     3.10866\n",
      "evaluation/env_infos/reward_run Max                     5.52939\n",
      "evaluation/env_infos/reward_run Min                    -7.93303\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.396337\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.103014\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.178493\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.562853\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.322389\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0840356\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.132321\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.457301\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.394078\n",
      "evaluation/env_infos/reward_ctrl Std                    0.106167\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0514325\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594745\n",
      "evaluation/env_infos/final/height Mean                 -0.12029\n",
      "evaluation/env_infos/final/height Std                   0.214264\n",
      "evaluation/env_infos/final/height Max                   0.13679\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00312695\n",
      "evaluation/env_infos/initial/height Std                 0.0491472\n",
      "evaluation/env_infos/initial/height Max                 0.0715965\n",
      "evaluation/env_infos/initial/height Min                -0.0867651\n",
      "evaluation/env_infos/height Mean                       -0.0833391\n",
      "evaluation/env_infos/height Std                         0.210214\n",
      "evaluation/env_infos/height Max                         0.495829\n",
      "evaluation/env_infos/height Min                        -0.592818\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.12625\n",
      "evaluation/env_infos/final/reward_angular Std           2.29986\n",
      "evaluation/env_infos/final/reward_angular Max           5.77434\n",
      "evaluation/env_infos/final/reward_angular Min          -4.6343\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.698992\n",
      "evaluation/env_infos/initial/reward_angular Std         0.954169\n",
      "evaluation/env_infos/initial/reward_angular Max         2.37242\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.27031\n",
      "evaluation/env_infos/reward_angular Mean                0.04928\n",
      "evaluation/env_infos/reward_angular Std                 2.31932\n",
      "evaluation/env_infos/reward_angular Max                10.0936\n",
      "evaluation/env_infos/reward_angular Min                -7.48181\n",
      "time/data storing (s)                                   0.0149463\n",
      "time/evaluation sampling (s)                           22.0944\n",
      "time/exploration sampling (s)                           1.18582\n",
      "time/logging (s)                                        0.243201\n",
      "time/saving (s)                                         0.0252053\n",
      "time/training (s)                                       4.40353\n",
      "time/epoch (s)                                         27.9671\n",
      "time/total (s)                                       5268.38\n",
      "Epoch                                                 172\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:40:20.752523 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 173 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 175000\n",
      "trainer/QF1 Loss                                        2.4285\n",
      "trainer/QF2 Loss                                        2.17205\n",
      "trainer/Policy Loss                                   -12.2459\n",
      "trainer/Q1 Predictions Mean                            18\n",
      "trainer/Q1 Predictions Std                             30.4792\n",
      "trainer/Q1 Predictions Max                            120.127\n",
      "trainer/Q1 Predictions Min                            -19.161\n",
      "trainer/Q2 Predictions Mean                            17.9812\n",
      "trainer/Q2 Predictions Std                             30.4508\n",
      "trainer/Q2 Predictions Max                            119.934\n",
      "trainer/Q2 Predictions Min                            -18.3103\n",
      "trainer/Q Targets Mean                                 18.1285\n",
      "trainer/Q Targets Std                                  30.7237\n",
      "trainer/Q Targets Max                                 119.959\n",
      "trainer/Q Targets Min                                 -19.1681\n",
      "trainer/Log Pis Mean                                    5.89351\n",
      "trainer/Log Pis Std                                     5.29144\n",
      "trainer/Log Pis Max                                    21.2783\n",
      "trainer/Log Pis Min                                    -4.87443\n",
      "trainer/Policy mu Mean                                  0.174786\n",
      "trainer/Policy mu Std                                   1.56674\n",
      "trainer/Policy mu Max                                   4.77131\n",
      "trainer/Policy mu Min                                  -4.54446\n",
      "trainer/Policy log std Mean                            -0.68503\n",
      "trainer/Policy log std Std                              0.278715\n",
      "trainer/Policy log std Max                              0.610113\n",
      "trainer/Policy log std Min                             -2.27524\n",
      "trainer/Alpha                                           0.0220246\n",
      "trainer/Alpha Loss                                     -0.406197\n",
      "exploration/num steps total                        175000\n",
      "exploration/num paths total                           175\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.718468\n",
      "exploration/Rewards Std                                 0.898043\n",
      "exploration/Rewards Max                                 3.32232\n",
      "exploration/Rewards Min                                -2.24387\n",
      "exploration/Returns Mean                              718.468\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               718.468\n",
      "exploration/Returns Min                               718.468\n",
      "exploration/Actions Mean                                0.175038\n",
      "exploration/Actions Std                                 0.776261\n",
      "exploration/Actions Max                                 0.999952\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           718.468\n",
      "exploration/env_infos/final/reward_run Mean             2.66877\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.66877\n",
      "exploration/env_infos/final/reward_run Min              2.66877\n",
      "exploration/env_infos/initial/reward_run Mean           0.510538\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.510538\n",
      "exploration/env_infos/initial/reward_run Min            0.510538\n",
      "exploration/env_infos/reward_run Mean                   2.10139\n",
      "exploration/env_infos/reward_run Std                    1.04373\n",
      "exploration/env_infos/reward_run Max                    4.65789\n",
      "exploration/env_infos/reward_run Min                   -1.28658\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.357879\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.357879\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.357879\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.318023\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.318023\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.318023\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.379932\n",
      "exploration/env_infos/reward_ctrl Std                   0.0883965\n",
      "exploration/env_infos/reward_ctrl Max                  -0.077892\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593505\n",
      "exploration/env_infos/final/height Mean                -0.0278928\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0278928\n",
      "exploration/env_infos/final/height Min                 -0.0278928\n",
      "exploration/env_infos/initial/height Mean               0.0153557\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0153557\n",
      "exploration/env_infos/initial/height Min                0.0153557\n",
      "exploration/env_infos/height Mean                      -0.00882042\n",
      "exploration/env_infos/height Std                        0.11299\n",
      "exploration/env_infos/height Max                        0.397682\n",
      "exploration/env_infos/height Min                       -0.339129\n",
      "exploration/env_infos/final/reward_angular Mean         2.22385\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          2.22385\n",
      "exploration/env_infos/final/reward_angular Min          2.22385\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.22045\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.22045\n",
      "exploration/env_infos/initial/reward_angular Min       -1.22045\n",
      "exploration/env_infos/reward_angular Mean              -0.0668817\n",
      "exploration/env_infos/reward_angular Std                2.04897\n",
      "exploration/env_infos/reward_angular Max                6.13566\n",
      "exploration/env_infos/reward_angular Min               -6.014\n",
      "evaluation/num steps total                              4.35e+06\n",
      "evaluation/num paths total                           4350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.862045\n",
      "evaluation/Rewards Std                                  1.68758\n",
      "evaluation/Rewards Max                                  8.6495\n",
      "evaluation/Rewards Min                                 -8.40125\n",
      "evaluation/Returns Mean                               862.045\n",
      "evaluation/Returns Std                                898.576\n",
      "evaluation/Returns Max                               2786.94\n",
      "evaluation/Returns Min                               -445.086\n",
      "evaluation/Actions Mean                                 0.186751\n",
      "evaluation/Actions Std                                  0.801127\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            862.045\n",
      "evaluation/env_infos/final/reward_run Mean             -1.32937\n",
      "evaluation/env_infos/final/reward_run Std               3.10971\n",
      "evaluation/env_infos/final/reward_run Max               4.81177\n",
      "evaluation/env_infos/final/reward_run Min              -5.78877\n",
      "evaluation/env_infos/initial/reward_run Mean            0.417367\n",
      "evaluation/env_infos/initial/reward_run Std             0.481032\n",
      "evaluation/env_infos/initial/reward_run Max             1.13079\n",
      "evaluation/env_infos/initial/reward_run Min            -0.434967\n",
      "evaluation/env_infos/reward_run Mean                   -1.24303\n",
      "evaluation/env_infos/reward_run Std                     3.1283\n",
      "evaluation/env_infos/reward_run Max                     6.05657\n",
      "evaluation/env_infos/reward_run Min                    -7.61179\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.422382\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109882\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.2024\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.560049\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.330714\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.113339\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.118979\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.521816\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.406008\n",
      "evaluation/env_infos/reward_ctrl Std                    0.119317\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0539191\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597367\n",
      "evaluation/env_infos/final/height Mean                 -0.132736\n",
      "evaluation/env_infos/final/height Std                   0.21594\n",
      "evaluation/env_infos/final/height Max                   0.247891\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00849838\n",
      "evaluation/env_infos/initial/height Std                 0.0484011\n",
      "evaluation/env_infos/initial/height Max                 0.0661879\n",
      "evaluation/env_infos/initial/height Min                -0.0867914\n",
      "evaluation/env_infos/height Mean                       -0.101789\n",
      "evaluation/env_infos/height Std                         0.214899\n",
      "evaluation/env_infos/height Max                         0.525896\n",
      "evaluation/env_infos/height Min                        -0.588131\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.295313\n",
      "evaluation/env_infos/final/reward_angular Std           1.93909\n",
      "evaluation/env_infos/final/reward_angular Max           3.41218\n",
      "evaluation/env_infos/final/reward_angular Min          -4.10336\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.584685\n",
      "evaluation/env_infos/initial/reward_angular Std         0.957454\n",
      "evaluation/env_infos/initial/reward_angular Max         1.62229\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.46488\n",
      "evaluation/env_infos/reward_angular Mean                0.06788\n",
      "evaluation/env_infos/reward_angular Std                 2.4073\n",
      "evaluation/env_infos/reward_angular Max                10.1645\n",
      "evaluation/env_infos/reward_angular Min                -6.52318\n",
      "time/data storing (s)                                   0.0158985\n",
      "time/evaluation sampling (s)                           22.0202\n",
      "time/exploration sampling (s)                           1.02373\n",
      "time/logging (s)                                        0.254798\n",
      "time/saving (s)                                         0.0279367\n",
      "time/training (s)                                       4.72064\n",
      "time/epoch (s)                                         28.0633\n",
      "time/total (s)                                       5297.76\n",
      "Epoch                                                 173\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:40:50.033856 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 174 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 176000\n",
      "trainer/QF1 Loss                                        2.56982\n",
      "trainer/QF2 Loss                                        1.97667\n",
      "trainer/Policy Loss                                   -12.5767\n",
      "trainer/Q1 Predictions Mean                            19.0418\n",
      "trainer/Q1 Predictions Std                             30.4432\n",
      "trainer/Q1 Predictions Max                            109.922\n",
      "trainer/Q1 Predictions Min                            -18.0413\n",
      "trainer/Q2 Predictions Mean                            19.0469\n",
      "trainer/Q2 Predictions Std                             30.4713\n",
      "trainer/Q2 Predictions Max                            110.415\n",
      "trainer/Q2 Predictions Min                            -17.7724\n",
      "trainer/Q Targets Mean                                 19.1474\n",
      "trainer/Q Targets Std                                  30.587\n",
      "trainer/Q Targets Max                                 109.891\n",
      "trainer/Q Targets Min                                 -17.9119\n",
      "trainer/Log Pis Mean                                    6.65671\n",
      "trainer/Log Pis Std                                     5.76898\n",
      "trainer/Log Pis Max                                    33.1775\n",
      "trainer/Log Pis Min                                    -6.4371\n",
      "trainer/Policy mu Mean                                 -0.00238455\n",
      "trainer/Policy mu Std                                   1.67482\n",
      "trainer/Policy mu Max                                   4.73051\n",
      "trainer/Policy mu Min                                  -5.77415\n",
      "trainer/Policy log std Mean                            -0.68541\n",
      "trainer/Policy log std Std                              0.312065\n",
      "trainer/Policy log std Max                              0.431108\n",
      "trainer/Policy log std Min                             -2.30466\n",
      "trainer/Alpha                                           0.0214208\n",
      "trainer/Alpha Loss                                      2.52368\n",
      "exploration/num steps total                        176000\n",
      "exploration/num paths total                           176\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.019026\n",
      "exploration/Rewards Std                                 1.63453\n",
      "exploration/Rewards Max                                 7.5428\n",
      "exploration/Rewards Min                                -5.14728\n",
      "exploration/Returns Mean                               19.026\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                19.026\n",
      "exploration/Returns Min                                19.026\n",
      "exploration/Actions Mean                                0.0665163\n",
      "exploration/Actions Std                                 0.753681\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -0.999998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            19.026\n",
      "exploration/env_infos/final/reward_run Mean             0.527895\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.527895\n",
      "exploration/env_infos/final/reward_run Min              0.527895\n",
      "exploration/env_infos/initial/reward_run Mean           0.524884\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.524884\n",
      "exploration/env_infos/initial/reward_run Min            0.524884\n",
      "exploration/env_infos/reward_run Mean                  -0.173887\n",
      "exploration/env_infos/reward_run Std                    0.796059\n",
      "exploration/env_infos/reward_run Max                    2.49756\n",
      "exploration/env_infos/reward_run Min                   -2.21206\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.380493\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.380493\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.380493\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.417033\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.417033\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.417033\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.343475\n",
      "exploration/env_infos/reward_ctrl Std                   0.0978948\n",
      "exploration/env_infos/reward_ctrl Max                  -0.106958\n",
      "exploration/env_infos/reward_ctrl Min                  -0.599153\n",
      "exploration/env_infos/final/height Mean                -0.502571\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.502571\n",
      "exploration/env_infos/final/height Min                 -0.502571\n",
      "exploration/env_infos/initial/height Mean               0.011453\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.011453\n",
      "exploration/env_infos/initial/height Min                0.011453\n",
      "exploration/env_infos/height Mean                      -0.397175\n",
      "exploration/env_infos/height Std                        0.225786\n",
      "exploration/env_infos/height Max                        0.341135\n",
      "exploration/env_infos/height Min                       -0.588275\n",
      "exploration/env_infos/final/reward_angular Mean         1.75796\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.75796\n",
      "exploration/env_infos/final/reward_angular Min          1.75796\n",
      "exploration/env_infos/initial/reward_angular Mean       0.380408\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.380408\n",
      "exploration/env_infos/initial/reward_angular Min        0.380408\n",
      "exploration/env_infos/reward_angular Mean               0.259733\n",
      "exploration/env_infos/reward_angular Std                1.74522\n",
      "exploration/env_infos/reward_angular Max                8.26102\n",
      "exploration/env_infos/reward_angular Min               -5.15515\n",
      "evaluation/num steps total                              4.375e+06\n",
      "evaluation/num paths total                           4375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.750394\n",
      "evaluation/Rewards Std                                  1.56212\n",
      "evaluation/Rewards Max                                  7.51978\n",
      "evaluation/Rewards Min                                 -7.64079\n",
      "evaluation/Returns Mean                               750.394\n",
      "evaluation/Returns Std                                826.832\n",
      "evaluation/Returns Max                               2309.04\n",
      "evaluation/Returns Min                               -399.892\n",
      "evaluation/Actions Mean                                 0.110785\n",
      "evaluation/Actions Std                                  0.806321\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            750.394\n",
      "evaluation/env_infos/final/reward_run Mean             -1.02505\n",
      "evaluation/env_infos/final/reward_run Std               2.66964\n",
      "evaluation/env_infos/final/reward_run Max               4.2589\n",
      "evaluation/env_infos/final/reward_run Min              -5.42759\n",
      "evaluation/env_infos/initial/reward_run Mean            0.334337\n",
      "evaluation/env_infos/initial/reward_run Std             0.54387\n",
      "evaluation/env_infos/initial/reward_run Max             1.07376\n",
      "evaluation/env_infos/initial/reward_run Min            -0.653785\n",
      "evaluation/env_infos/reward_run Mean                   -0.987148\n",
      "evaluation/env_infos/reward_run Std                     2.79759\n",
      "evaluation/env_infos/reward_run Max                     5.53411\n",
      "evaluation/env_infos/reward_run Min                    -7.01735\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.390627\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.132102\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0975704\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.562916\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.321768\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0938417\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.100736\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.489864\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.397456\n",
      "evaluation/env_infos/reward_ctrl Std                    0.114904\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0219348\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598728\n",
      "evaluation/env_infos/final/height Mean                 -0.125394\n",
      "evaluation/env_infos/final/height Std                   0.209947\n",
      "evaluation/env_infos/final/height Max                   0.107169\n",
      "evaluation/env_infos/final/height Min                  -0.577825\n",
      "evaluation/env_infos/initial/height Mean               -0.0145596\n",
      "evaluation/env_infos/initial/height Std                 0.0488075\n",
      "evaluation/env_infos/initial/height Max                 0.0712818\n",
      "evaluation/env_infos/initial/height Min                -0.0779584\n",
      "evaluation/env_infos/height Mean                       -0.0938778\n",
      "evaluation/env_infos/height Std                         0.202283\n",
      "evaluation/env_infos/height Max                         0.523797\n",
      "evaluation/env_infos/height Min                        -0.596943\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0963964\n",
      "evaluation/env_infos/final/reward_angular Std           1.58437\n",
      "evaluation/env_infos/final/reward_angular Max           3.95361\n",
      "evaluation/env_infos/final/reward_angular Min          -2.27701\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0349978\n",
      "evaluation/env_infos/initial/reward_angular Std         0.876966\n",
      "evaluation/env_infos/initial/reward_angular Max         2.01325\n",
      "evaluation/env_infos/initial/reward_angular Min        -0.962829\n",
      "evaluation/env_infos/reward_angular Mean                0.0689819\n",
      "evaluation/env_infos/reward_angular Std                 2.25285\n",
      "evaluation/env_infos/reward_angular Max                10.3574\n",
      "evaluation/env_infos/reward_angular Min                -7.55317\n",
      "time/data storing (s)                                   0.0146306\n",
      "time/evaluation sampling (s)                           22.2049\n",
      "time/exploration sampling (s)                           0.984501\n",
      "time/logging (s)                                        0.29284\n",
      "time/saving (s)                                         0.0300643\n",
      "time/training (s)                                       4.46618\n",
      "time/epoch (s)                                         27.9931\n",
      "time/total (s)                                       5327.08\n",
      "Epoch                                                 174\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:41:20.948285 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 175 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 177000\n",
      "trainer/QF1 Loss                                        3.22542\n",
      "trainer/QF2 Loss                                        3.5004\n",
      "trainer/Policy Loss                                    -9.16552\n",
      "trainer/Q1 Predictions Mean                            15.2183\n",
      "trainer/Q1 Predictions Std                             26.8341\n",
      "trainer/Q1 Predictions Max                            115.951\n",
      "trainer/Q1 Predictions Min                            -18.795\n",
      "trainer/Q2 Predictions Mean                            15.0603\n",
      "trainer/Q2 Predictions Std                             26.9265\n",
      "trainer/Q2 Predictions Max                            116.073\n",
      "trainer/Q2 Predictions Min                            -18.3476\n",
      "trainer/Q Targets Mean                                 15.3397\n",
      "trainer/Q Targets Std                                  26.8261\n",
      "trainer/Q Targets Max                                 117.034\n",
      "trainer/Q Targets Min                                 -18.7446\n",
      "trainer/Log Pis Mean                                    6.06654\n",
      "trainer/Log Pis Std                                     5.30261\n",
      "trainer/Log Pis Max                                    26.4864\n",
      "trainer/Log Pis Min                                    -5.82723\n",
      "trainer/Policy mu Mean                                  0.130758\n",
      "trainer/Policy mu Std                                   1.56187\n",
      "trainer/Policy mu Max                                   4.64937\n",
      "trainer/Policy mu Min                                  -4.8389\n",
      "trainer/Policy log std Mean                            -0.74185\n",
      "trainer/Policy log std Std                              0.328735\n",
      "trainer/Policy log std Max                              0.164064\n",
      "trainer/Policy log std Min                             -2.37607\n",
      "trainer/Alpha                                           0.0220058\n",
      "trainer/Alpha Loss                                      0.253869\n",
      "exploration/num steps total                        177000\n",
      "exploration/num paths total                           177\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.54273\n",
      "exploration/Rewards Std                                 1.75894\n",
      "exploration/Rewards Max                                 6.18494\n",
      "exploration/Rewards Min                                -2.38254\n",
      "exploration/Returns Mean                             1542.73\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1542.73\n",
      "exploration/Returns Min                              1542.73\n",
      "exploration/Actions Mean                                0.179648\n",
      "exploration/Actions Std                                 0.834074\n",
      "exploration/Actions Max                                 0.999983\n",
      "exploration/Actions Min                                -0.999773\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1542.73\n",
      "exploration/env_infos/final/reward_run Mean            -4.78139\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -4.78139\n",
      "exploration/env_infos/final/reward_run Min             -4.78139\n",
      "exploration/env_infos/initial/reward_run Mean           0.442842\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.442842\n",
      "exploration/env_infos/initial/reward_run Min            0.442842\n",
      "exploration/env_infos/reward_run Mean                  -3.5565\n",
      "exploration/env_infos/reward_run Std                    1.13816\n",
      "exploration/env_infos/reward_run Max                    0.442842\n",
      "exploration/env_infos/reward_run Min                   -7.00241\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.498904\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.498904\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.498904\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.375069\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.375069\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.375069\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.436771\n",
      "exploration/env_infos/reward_ctrl Std                   0.0774279\n",
      "exploration/env_infos/reward_ctrl Max                  -0.185569\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593024\n",
      "exploration/env_infos/final/height Mean                 0.136672\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.136672\n",
      "exploration/env_infos/final/height Min                  0.136672\n",
      "exploration/env_infos/initial/height Mean               0.0418662\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0418662\n",
      "exploration/env_infos/initial/height Min                0.0418662\n",
      "exploration/env_infos/height Mean                       0.0170206\n",
      "exploration/env_infos/height Std                        0.111643\n",
      "exploration/env_infos/height Max                        0.443308\n",
      "exploration/env_infos/height Min                       -0.260759\n",
      "exploration/env_infos/final/reward_angular Mean         0.834892\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.834892\n",
      "exploration/env_infos/final/reward_angular Min          0.834892\n",
      "exploration/env_infos/initial/reward_angular Mean       0.798559\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.798559\n",
      "exploration/env_infos/initial/reward_angular Min        0.798559\n",
      "exploration/env_infos/reward_angular Mean               0.0933\n",
      "exploration/env_infos/reward_angular Std                3.03104\n",
      "exploration/env_infos/reward_angular Max                8.28607\n",
      "exploration/env_infos/reward_angular Min               -7.27673\n",
      "evaluation/num steps total                              4.4e+06\n",
      "evaluation/num paths total                           4400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.713878\n",
      "evaluation/Rewards Std                                  1.62208\n",
      "evaluation/Rewards Max                                  9.13955\n",
      "evaluation/Rewards Min                                 -7.84094\n",
      "evaluation/Returns Mean                               713.878\n",
      "evaluation/Returns Std                                879.538\n",
      "evaluation/Returns Max                               2491.75\n",
      "evaluation/Returns Min                               -451.253\n",
      "evaluation/Actions Mean                                 0.225389\n",
      "evaluation/Actions Std                                  0.784792\n",
      "evaluation/Actions Max                                  0.999987\n",
      "evaluation/Actions Min                                 -0.999997\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            713.878\n",
      "evaluation/env_infos/final/reward_run Mean             -0.9821\n",
      "evaluation/env_infos/final/reward_run Std               2.30602\n",
      "evaluation/env_infos/final/reward_run Max               3.96279\n",
      "evaluation/env_infos/final/reward_run Min              -5.66759\n",
      "evaluation/env_infos/initial/reward_run Mean            0.372465\n",
      "evaluation/env_infos/initial/reward_run Std             0.44724\n",
      "evaluation/env_infos/initial/reward_run Max             1.13748\n",
      "evaluation/env_infos/initial/reward_run Min            -0.325029\n",
      "evaluation/env_infos/reward_run Mean                   -1.13887\n",
      "evaluation/env_infos/reward_run Std                     2.68639\n",
      "evaluation/env_infos/reward_run Max                     5.39514\n",
      "evaluation/env_infos/reward_run Min                    -7.62575\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.402062\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0897471\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.175798\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.577384\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.319207\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.1089\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.123282\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.534521\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.40002\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0928515\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0153418\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595095\n",
      "evaluation/env_infos/final/height Mean                 -0.133991\n",
      "evaluation/env_infos/final/height Std                   0.265335\n",
      "evaluation/env_infos/final/height Max                   0.221387\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0178397\n",
      "evaluation/env_infos/initial/height Std                 0.0548991\n",
      "evaluation/env_infos/initial/height Max                 0.0992263\n",
      "evaluation/env_infos/initial/height Min                -0.111685\n",
      "evaluation/env_infos/height Mean                       -0.137049\n",
      "evaluation/env_infos/height Std                         0.235051\n",
      "evaluation/env_infos/height Max                         0.482719\n",
      "evaluation/env_infos/height Min                        -0.591555\n",
      "evaluation/env_infos/final/reward_angular Mean          0.327457\n",
      "evaluation/env_infos/final/reward_angular Std           1.81785\n",
      "evaluation/env_infos/final/reward_angular Max           4.05413\n",
      "evaluation/env_infos/final/reward_angular Min          -4.58901\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.731913\n",
      "evaluation/env_infos/initial/reward_angular Std         0.955436\n",
      "evaluation/env_infos/initial/reward_angular Max         1.64395\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.57191\n",
      "evaluation/env_infos/reward_angular Mean                0.0718358\n",
      "evaluation/env_infos/reward_angular Std                 2.25342\n",
      "evaluation/env_infos/reward_angular Max                10.4429\n",
      "evaluation/env_infos/reward_angular Min                -7.14388\n",
      "time/data storing (s)                                   0.0159765\n",
      "time/evaluation sampling (s)                           23.6731\n",
      "time/exploration sampling (s)                           1.01395\n",
      "time/logging (s)                                        0.24606\n",
      "time/saving (s)                                         0.0289517\n",
      "time/training (s)                                       4.43044\n",
      "time/epoch (s)                                         29.4085\n",
      "time/total (s)                                       5357.94\n",
      "Epoch                                                 175\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:42:00.370123 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 176 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 178000\n",
      "trainer/QF1 Loss                                        2.04166\n",
      "trainer/QF2 Loss                                        2.10294\n",
      "trainer/Policy Loss                                   -10.0148\n",
      "trainer/Q1 Predictions Mean                            15.6449\n",
      "trainer/Q1 Predictions Std                             29.4701\n",
      "trainer/Q1 Predictions Max                            135.253\n",
      "trainer/Q1 Predictions Min                            -20.1884\n",
      "trainer/Q2 Predictions Mean                            15.2233\n",
      "trainer/Q2 Predictions Std                             28.9247\n",
      "trainer/Q2 Predictions Max                            129.976\n",
      "trainer/Q2 Predictions Min                            -20.9053\n",
      "trainer/Q Targets Mean                                 15.5507\n",
      "trainer/Q Targets Std                                  29.1137\n",
      "trainer/Q Targets Max                                 130.78\n",
      "trainer/Q Targets Min                                 -19.6794\n",
      "trainer/Log Pis Mean                                    5.59475\n",
      "trainer/Log Pis Std                                     5.27584\n",
      "trainer/Log Pis Max                                    24.2117\n",
      "trainer/Log Pis Min                                    -4.92788\n",
      "trainer/Policy mu Mean                                  0.0376603\n",
      "trainer/Policy mu Std                                   1.53197\n",
      "trainer/Policy mu Max                                   4.04459\n",
      "trainer/Policy mu Min                                  -5.52382\n",
      "trainer/Policy log std Mean                            -0.73106\n",
      "trainer/Policy log std Std                              0.305183\n",
      "trainer/Policy log std Max                              0.195402\n",
      "trainer/Policy log std Min                             -2.37797\n",
      "trainer/Alpha                                           0.0219416\n",
      "trainer/Alpha Loss                                     -1.54748\n",
      "exploration/num steps total                        178000\n",
      "exploration/num paths total                           178\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.48484\n",
      "exploration/Rewards Std                                 1.38564\n",
      "exploration/Rewards Max                                 5.89459\n",
      "exploration/Rewards Min                                -3.06636\n",
      "exploration/Returns Mean                             2484.84\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2484.84\n",
      "exploration/Returns Min                              2484.84\n",
      "exploration/Actions Mean                                0.173472\n",
      "exploration/Actions Std                                 0.880872\n",
      "exploration/Actions Max                                 0.999978\n",
      "exploration/Actions Min                                -0.999953\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2484.84\n",
      "exploration/env_infos/final/reward_run Mean            -3.97336\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.97336\n",
      "exploration/env_infos/final/reward_run Min             -3.97336\n",
      "exploration/env_infos/initial/reward_run Mean          -0.15602\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.15602\n",
      "exploration/env_infos/initial/reward_run Min           -0.15602\n",
      "exploration/env_infos/reward_run Mean                  -3.92946\n",
      "exploration/env_infos/reward_run Std                    1.10199\n",
      "exploration/env_infos/reward_run Max                    0.754871\n",
      "exploration/env_infos/reward_run Min                   -6.9472\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.530925\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.530925\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.530925\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.356272\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.356272\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.356272\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.483617\n",
      "exploration/env_infos/reward_ctrl Std                   0.0719531\n",
      "exploration/env_infos/reward_ctrl Max                  -0.233936\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595856\n",
      "exploration/env_infos/final/height Mean                 0.025633\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.025633\n",
      "exploration/env_infos/final/height Min                  0.025633\n",
      "exploration/env_infos/initial/height Mean              -0.0663729\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0663729\n",
      "exploration/env_infos/initial/height Min               -0.0663729\n",
      "exploration/env_infos/height Mean                      -0.00906532\n",
      "exploration/env_infos/height Std                        0.122721\n",
      "exploration/env_infos/height Max                        0.373906\n",
      "exploration/env_infos/height Min                       -0.321586\n",
      "exploration/env_infos/final/reward_angular Mean        -0.70514\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.70514\n",
      "exploration/env_infos/final/reward_angular Min         -0.70514\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.404121\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.404121\n",
      "exploration/env_infos/initial/reward_angular Min       -0.404121\n",
      "exploration/env_infos/reward_angular Mean               0.11924\n",
      "exploration/env_infos/reward_angular Std                3.0013\n",
      "exploration/env_infos/reward_angular Max                8.52491\n",
      "exploration/env_infos/reward_angular Min               -5.70959\n",
      "evaluation/num steps total                              4.425e+06\n",
      "evaluation/num paths total                           4425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.72598\n",
      "evaluation/Rewards Std                                  1.58859\n",
      "evaluation/Rewards Max                                  8.19824\n",
      "evaluation/Rewards Min                                 -8.03365\n",
      "evaluation/Returns Mean                               725.98\n",
      "evaluation/Returns Std                                869.804\n",
      "evaluation/Returns Max                               2514.21\n",
      "evaluation/Returns Min                               -432.223\n",
      "evaluation/Actions Mean                                 0.197047\n",
      "evaluation/Actions Std                                  0.784023\n",
      "evaluation/Actions Max                                  0.999995\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            725.98\n",
      "evaluation/env_infos/final/reward_run Mean             -0.957421\n",
      "evaluation/env_infos/final/reward_run Std               2.30652\n",
      "evaluation/env_infos/final/reward_run Max               3.15893\n",
      "evaluation/env_infos/final/reward_run Min              -4.91436\n",
      "evaluation/env_infos/initial/reward_run Mean            0.319456\n",
      "evaluation/env_infos/initial/reward_run Std             0.512437\n",
      "evaluation/env_infos/initial/reward_run Max             0.96617\n",
      "evaluation/env_infos/initial/reward_run Min            -0.515561\n",
      "evaluation/env_infos/reward_run Mean                   -1.13781\n",
      "evaluation/env_infos/reward_run Std                     2.72903\n",
      "evaluation/env_infos/reward_run Max                     5.71795\n",
      "evaluation/env_infos/reward_run Min                    -7.49223\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.389825\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.118827\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.170841\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.568032\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.321001\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.118117\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.122859\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.490947\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.392112\n",
      "evaluation/env_infos/reward_ctrl Std                    0.113016\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0149268\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593539\n",
      "evaluation/env_infos/final/height Mean                 -0.151728\n",
      "evaluation/env_infos/final/height Std                   0.242151\n",
      "evaluation/env_infos/final/height Max                   0.285717\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0100861\n",
      "evaluation/env_infos/initial/height Std                 0.056694\n",
      "evaluation/env_infos/initial/height Max                 0.0897467\n",
      "evaluation/env_infos/initial/height Min                -0.119132\n",
      "evaluation/env_infos/height Mean                       -0.134002\n",
      "evaluation/env_infos/height Std                         0.231225\n",
      "evaluation/env_infos/height Max                         0.524158\n",
      "evaluation/env_infos/height Min                        -0.59434\n",
      "evaluation/env_infos/final/reward_angular Mean          0.253667\n",
      "evaluation/env_infos/final/reward_angular Std           2.89886\n",
      "evaluation/env_infos/final/reward_angular Max           6.65044\n",
      "evaluation/env_infos/final/reward_angular Min          -4.80347\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.273157\n",
      "evaluation/env_infos/initial/reward_angular Std         0.891963\n",
      "evaluation/env_infos/initial/reward_angular Max         1.86333\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.55628\n",
      "evaluation/env_infos/reward_angular Mean                0.0864858\n",
      "evaluation/env_infos/reward_angular Std                 2.24819\n",
      "evaluation/env_infos/reward_angular Max                 9.67846\n",
      "evaluation/env_infos/reward_angular Min                -6.62318\n",
      "time/data storing (s)                                   0.0155503\n",
      "time/evaluation sampling (s)                           25.4024\n",
      "time/exploration sampling (s)                           1.07636\n",
      "time/logging (s)                                        0.248605\n",
      "time/saving (s)                                         0.031013\n",
      "time/training (s)                                      11.2578\n",
      "time/epoch (s)                                         38.0318\n",
      "time/total (s)                                       5397.36\n",
      "Epoch                                                 176\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:42:37.278855 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 177 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 179000\n",
      "trainer/QF1 Loss                                        2.19633\n",
      "trainer/QF2 Loss                                        2.50036\n",
      "trainer/Policy Loss                                   -16.1181\n",
      "trainer/Q1 Predictions Mean                            22.4635\n",
      "trainer/Q1 Predictions Std                             29.891\n",
      "trainer/Q1 Predictions Max                            108.549\n",
      "trainer/Q1 Predictions Min                            -18.2639\n",
      "trainer/Q2 Predictions Mean                            22.399\n",
      "trainer/Q2 Predictions Std                             29.8139\n",
      "trainer/Q2 Predictions Max                            108.187\n",
      "trainer/Q2 Predictions Min                            -18.8205\n",
      "trainer/Q Targets Mean                                 22.3332\n",
      "trainer/Q Targets Std                                  29.9504\n",
      "trainer/Q Targets Max                                 110.926\n",
      "trainer/Q Targets Min                                 -18.1912\n",
      "trainer/Log Pis Mean                                    6.54658\n",
      "trainer/Log Pis Std                                     6.12449\n",
      "trainer/Log Pis Max                                    35.8688\n",
      "trainer/Log Pis Min                                    -4.56875\n",
      "trainer/Policy mu Mean                                  0.0965115\n",
      "trainer/Policy mu Std                                   1.6683\n",
      "trainer/Policy mu Max                                   6.55297\n",
      "trainer/Policy mu Min                                  -6.46853\n",
      "trainer/Policy log std Mean                            -0.701802\n",
      "trainer/Policy log std Std                              0.270801\n",
      "trainer/Policy log std Max                              0.273284\n",
      "trainer/Policy log std Min                             -2.00319\n",
      "trainer/Alpha                                           0.0228145\n",
      "trainer/Alpha Loss                                      2.06579\n",
      "exploration/num steps total                        179000\n",
      "exploration/num paths total                           179\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.163087\n",
      "exploration/Rewards Std                                 0.96452\n",
      "exploration/Rewards Max                                 2.40678\n",
      "exploration/Rewards Min                                -3.45618\n",
      "exploration/Returns Mean                             -163.087\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -163.087\n",
      "exploration/Returns Min                              -163.087\n",
      "exploration/Actions Mean                                0.305625\n",
      "exploration/Actions Std                                 0.530214\n",
      "exploration/Actions Max                                 0.999471\n",
      "exploration/Actions Min                                -0.995331\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -163.087\n",
      "exploration/env_infos/final/reward_run Mean             0.348395\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.348395\n",
      "exploration/env_infos/final/reward_run Min              0.348395\n",
      "exploration/env_infos/initial/reward_run Mean          -0.221143\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.221143\n",
      "exploration/env_infos/initial/reward_run Min           -0.221143\n",
      "exploration/env_infos/reward_run Mean                  -0.138351\n",
      "exploration/env_infos/reward_run Std                    0.549447\n",
      "exploration/env_infos/reward_run Max                    2.11108\n",
      "exploration/env_infos/reward_run Min                   -1.99648\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.235924\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.235924\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.235924\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.295868\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.295868\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.295868\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.22472\n",
      "exploration/env_infos/reward_ctrl Std                   0.0718145\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0384213\n",
      "exploration/env_infos/reward_ctrl Min                  -0.500954\n",
      "exploration/env_infos/final/height Mean                -0.114594\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.114594\n",
      "exploration/env_infos/final/height Min                 -0.114594\n",
      "exploration/env_infos/initial/height Mean              -0.0800187\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0800187\n",
      "exploration/env_infos/initial/height Min               -0.0800187\n",
      "exploration/env_infos/height Mean                      -0.0827294\n",
      "exploration/env_infos/height Std                        0.0548241\n",
      "exploration/env_infos/height Max                        0.119754\n",
      "exploration/env_infos/height Min                       -0.27042\n",
      "exploration/env_infos/final/reward_angular Mean         0.385783\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.385783\n",
      "exploration/env_infos/final/reward_angular Min          0.385783\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.43304\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.43304\n",
      "exploration/env_infos/initial/reward_angular Min       -1.43304\n",
      "exploration/env_infos/reward_angular Mean               0.00970064\n",
      "exploration/env_infos/reward_angular Std                1.65069\n",
      "exploration/env_infos/reward_angular Max                5.67338\n",
      "exploration/env_infos/reward_angular Min               -4.57605\n",
      "evaluation/num steps total                              4.45e+06\n",
      "evaluation/num paths total                           4450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.340609\n",
      "evaluation/Rewards Std                                  1.37136\n",
      "evaluation/Rewards Max                                  7.87655\n",
      "evaluation/Rewards Min                                 -8.53595\n",
      "evaluation/Returns Mean                               340.609\n",
      "evaluation/Returns Std                                637.467\n",
      "evaluation/Returns Max                               2145.77\n",
      "evaluation/Returns Min                               -374.368\n",
      "evaluation/Actions Mean                                 0.168576\n",
      "evaluation/Actions Std                                  0.777728\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            340.609\n",
      "evaluation/env_infos/final/reward_run Mean             -0.816618\n",
      "evaluation/env_infos/final/reward_run Std               2.60184\n",
      "evaluation/env_infos/final/reward_run Max               4.60053\n",
      "evaluation/env_infos/final/reward_run Min              -6.92371\n",
      "evaluation/env_infos/initial/reward_run Mean            0.453519\n",
      "evaluation/env_infos/initial/reward_run Std             0.456129\n",
      "evaluation/env_infos/initial/reward_run Max             1.10126\n",
      "evaluation/env_infos/initial/reward_run Min            -0.3706\n",
      "evaluation/env_infos/reward_run Mean                   -0.643658\n",
      "evaluation/env_infos/reward_run Std                     2.46583\n",
      "evaluation/env_infos/reward_run Max                     5.90585\n",
      "evaluation/env_infos/reward_run Min                    -7.8535\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.371506\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109275\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.151694\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.514956\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.308311\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.117547\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0574245\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.496776\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.379967\n",
      "evaluation/env_infos/reward_ctrl Std                    0.108368\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0522089\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.591747\n",
      "evaluation/env_infos/final/height Mean                 -0.345961\n",
      "evaluation/env_infos/final/height Std                   0.238222\n",
      "evaluation/env_infos/final/height Max                   0.0512689\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00173253\n",
      "evaluation/env_infos/initial/height Std                 0.0541866\n",
      "evaluation/env_infos/initial/height Max                 0.0870784\n",
      "evaluation/env_infos/initial/height Min                -0.0946201\n",
      "evaluation/env_infos/height Mean                       -0.233872\n",
      "evaluation/env_infos/height Std                         0.268639\n",
      "evaluation/env_infos/height Max                         0.492194\n",
      "evaluation/env_infos/height Min                        -0.596505\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.323078\n",
      "evaluation/env_infos/final/reward_angular Std           1.69082\n",
      "evaluation/env_infos/final/reward_angular Max           3.24192\n",
      "evaluation/env_infos/final/reward_angular Min          -3.31999\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.600739\n",
      "evaluation/env_infos/initial/reward_angular Std         0.904941\n",
      "evaluation/env_infos/initial/reward_angular Max         1.17108\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.92393\n",
      "evaluation/env_infos/reward_angular Mean                0.0752318\n",
      "evaluation/env_infos/reward_angular Std                 2.0328\n",
      "evaluation/env_infos/reward_angular Max                10.4681\n",
      "evaluation/env_infos/reward_angular Min                -8.64852\n",
      "time/data storing (s)                                   0.0189985\n",
      "time/evaluation sampling (s)                           27.8636\n",
      "time/exploration sampling (s)                           1.61075\n",
      "time/logging (s)                                        0.262087\n",
      "time/saving (s)                                         0.0303276\n",
      "time/training (s)                                       5.66648\n",
      "time/epoch (s)                                         35.4522\n",
      "time/total (s)                                       5434.27\n",
      "Epoch                                                 177\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:43:09.205229 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 178 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 180000\n",
      "trainer/QF1 Loss                                        2.48594\n",
      "trainer/QF2 Loss                                        2.07826\n",
      "trainer/Policy Loss                                   -11.0727\n",
      "trainer/Q1 Predictions Mean                            16.007\n",
      "trainer/Q1 Predictions Std                             30.2854\n",
      "trainer/Q1 Predictions Max                            125.952\n",
      "trainer/Q1 Predictions Min                            -18.5593\n",
      "trainer/Q2 Predictions Mean                            15.7589\n",
      "trainer/Q2 Predictions Std                             30.1435\n",
      "trainer/Q2 Predictions Max                            122.271\n",
      "trainer/Q2 Predictions Min                            -19.1737\n",
      "trainer/Q Targets Mean                                 15.6721\n",
      "trainer/Q Targets Std                                  30.0141\n",
      "trainer/Q Targets Max                                 124.547\n",
      "trainer/Q Targets Min                                 -18.7614\n",
      "trainer/Log Pis Mean                                    5.01828\n",
      "trainer/Log Pis Std                                     5.16008\n",
      "trainer/Log Pis Max                                    20.373\n",
      "trainer/Log Pis Min                                    -9.5159\n",
      "trainer/Policy mu Mean                                  0.0241218\n",
      "trainer/Policy mu Std                                   1.51972\n",
      "trainer/Policy mu Max                                   4.30576\n",
      "trainer/Policy mu Min                                  -6.06504\n",
      "trainer/Policy log std Mean                            -0.668136\n",
      "trainer/Policy log std Std                              0.300429\n",
      "trainer/Policy log std Max                              0.666119\n",
      "trainer/Policy log std Min                             -2.22509\n",
      "trainer/Alpha                                           0.0232762\n",
      "trainer/Alpha Loss                                     -3.68989\n",
      "exploration/num steps total                        180000\n",
      "exploration/num paths total                           180\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0285021\n",
      "exploration/Rewards Std                                 0.433267\n",
      "exploration/Rewards Max                                 4.1045\n",
      "exploration/Rewards Min                                -2.86274\n",
      "exploration/Returns Mean                               28.5021\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                28.5021\n",
      "exploration/Returns Min                                28.5021\n",
      "exploration/Actions Mean                                0.0524314\n",
      "exploration/Actions Std                                 0.822966\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999969\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            28.5021\n",
      "exploration/env_infos/final/reward_run Mean            -0.169418\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.169418\n",
      "exploration/env_infos/final/reward_run Min             -0.169418\n",
      "exploration/env_infos/initial/reward_run Mean           0.967543\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.967543\n",
      "exploration/env_infos/initial/reward_run Min            0.967543\n",
      "exploration/env_infos/reward_run Mean                  -0.060323\n",
      "exploration/env_infos/reward_run Std                    0.193207\n",
      "exploration/env_infos/reward_run Max                    0.967543\n",
      "exploration/env_infos/reward_run Min                   -0.704334\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.42604\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.42604\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.42604\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.477574\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.477574\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.477574\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.408013\n",
      "exploration/env_infos/reward_ctrl Std                   0.0552582\n",
      "exploration/env_infos/reward_ctrl Max                  -0.151914\n",
      "exploration/env_infos/reward_ctrl Min                  -0.576795\n",
      "exploration/env_infos/final/height Mean                 0.0684454\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0684454\n",
      "exploration/env_infos/final/height Min                  0.0684454\n",
      "exploration/env_infos/initial/height Mean              -0.00258718\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00258718\n",
      "exploration/env_infos/initial/height Min               -0.00258718\n",
      "exploration/env_infos/height Mean                       0.0693998\n",
      "exploration/env_infos/height Std                        0.0363441\n",
      "exploration/env_infos/height Max                        0.335153\n",
      "exploration/env_infos/height Min                       -0.12999\n",
      "exploration/env_infos/final/reward_angular Mean         0.184048\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.184048\n",
      "exploration/env_infos/final/reward_angular Min          0.184048\n",
      "exploration/env_infos/initial/reward_angular Mean       0.619872\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.619872\n",
      "exploration/env_infos/initial/reward_angular Min        0.619872\n",
      "exploration/env_infos/reward_angular Mean               0.0412172\n",
      "exploration/env_infos/reward_angular Std                0.523807\n",
      "exploration/env_infos/reward_angular Max                4.79487\n",
      "exploration/env_infos/reward_angular Min               -4.03709\n",
      "evaluation/num steps total                              4.475e+06\n",
      "evaluation/num paths total                           4475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.294282\n",
      "evaluation/Rewards Std                                  1.43333\n",
      "evaluation/Rewards Max                                  9.57976\n",
      "evaluation/Rewards Min                                 -8.14945\n",
      "evaluation/Returns Mean                               294.282\n",
      "evaluation/Returns Std                                805.89\n",
      "evaluation/Returns Max                               2928.6\n",
      "evaluation/Returns Min                               -456.579\n",
      "evaluation/Actions Mean                                 0.0633629\n",
      "evaluation/Actions Std                                  0.732927\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            294.282\n",
      "evaluation/env_infos/final/reward_run Mean              0.0471407\n",
      "evaluation/env_infos/final/reward_run Std               1.85692\n",
      "evaluation/env_infos/final/reward_run Max               3.01627\n",
      "evaluation/env_infos/final/reward_run Min              -6.00032\n",
      "evaluation/env_infos/initial/reward_run Mean            0.304698\n",
      "evaluation/env_infos/initial/reward_run Std             0.552646\n",
      "evaluation/env_infos/initial/reward_run Max             1.01592\n",
      "evaluation/env_infos/initial/reward_run Min            -0.820651\n",
      "evaluation/env_infos/reward_run Mean                   -0.0237126\n",
      "evaluation/env_infos/reward_run Std                     2.13791\n",
      "evaluation/env_infos/reward_run Max                     5.15546\n",
      "evaluation/env_infos/reward_run Min                    -8.12933\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.309604\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.115096\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.08318\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.522646\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.299721\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.118284\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.108906\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.49164\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.324718\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109959\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0395665\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597213\n",
      "evaluation/env_infos/final/height Mean                 -0.326643\n",
      "evaluation/env_infos/final/height Std                   0.304017\n",
      "evaluation/env_infos/final/height Max                   0.325806\n",
      "evaluation/env_infos/final/height Min                  -0.580272\n",
      "evaluation/env_infos/initial/height Mean                0.0039696\n",
      "evaluation/env_infos/initial/height Std                 0.0545622\n",
      "evaluation/env_infos/initial/height Max                 0.101045\n",
      "evaluation/env_infos/initial/height Min                -0.0962263\n",
      "evaluation/env_infos/height Mean                       -0.272635\n",
      "evaluation/env_infos/height Std                         0.273717\n",
      "evaluation/env_infos/height Max                         0.482766\n",
      "evaluation/env_infos/height Min                        -0.59001\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.118981\n",
      "evaluation/env_infos/final/reward_angular Std           1.28198\n",
      "evaluation/env_infos/final/reward_angular Max           3.40495\n",
      "evaluation/env_infos/final/reward_angular Min          -2.52232\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.267092\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07089\n",
      "evaluation/env_infos/initial/reward_angular Max         2.60922\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.28148\n",
      "evaluation/env_infos/reward_angular Mean                0.00896174\n",
      "evaluation/env_infos/reward_angular Std                 1.77771\n",
      "evaluation/env_infos/reward_angular Max                 9.39845\n",
      "evaluation/env_infos/reward_angular Min                -7.55457\n",
      "time/data storing (s)                                   0.0156203\n",
      "time/evaluation sampling (s)                           24.6208\n",
      "time/exploration sampling (s)                           1.10119\n",
      "time/logging (s)                                        0.261999\n",
      "time/saving (s)                                         0.0310238\n",
      "time/training (s)                                       4.50431\n",
      "time/epoch (s)                                         30.535\n",
      "time/total (s)                                       5466.2\n",
      "Epoch                                                 178\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:43:38.592634 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 179 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 181000\n",
      "trainer/QF1 Loss                                        2.13468\n",
      "trainer/QF2 Loss                                        2.11997\n",
      "trainer/Policy Loss                                   -11.5198\n",
      "trainer/Q1 Predictions Mean                            17.0289\n",
      "trainer/Q1 Predictions Std                             30.6858\n",
      "trainer/Q1 Predictions Max                            123.429\n",
      "trainer/Q1 Predictions Min                            -18.1244\n",
      "trainer/Q2 Predictions Mean                            17.1717\n",
      "trainer/Q2 Predictions Std                             30.7737\n",
      "trainer/Q2 Predictions Max                            125.128\n",
      "trainer/Q2 Predictions Min                            -17.9109\n",
      "trainer/Q Targets Mean                                 16.753\n",
      "trainer/Q Targets Std                                  30.4707\n",
      "trainer/Q Targets Max                                 123.486\n",
      "trainer/Q Targets Min                                 -18.4455\n",
      "trainer/Log Pis Mean                                    5.77358\n",
      "trainer/Log Pis Std                                     5.1935\n",
      "trainer/Log Pis Max                                    25.6122\n",
      "trainer/Log Pis Min                                    -6.05952\n",
      "trainer/Policy mu Mean                                 -0.0933327\n",
      "trainer/Policy mu Std                                   1.55993\n",
      "trainer/Policy mu Max                                   4.75565\n",
      "trainer/Policy mu Min                                  -5.03735\n",
      "trainer/Policy log std Mean                            -0.703565\n",
      "trainer/Policy log std Std                              0.303605\n",
      "trainer/Policy log std Max                              0.448313\n",
      "trainer/Policy log std Min                             -2.03102\n",
      "trainer/Alpha                                           0.0240584\n",
      "trainer/Alpha Loss                                     -0.843662\n",
      "exploration/num steps total                        181000\n",
      "exploration/num paths total                           181\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.472509\n",
      "exploration/Rewards Std                                 0.434291\n",
      "exploration/Rewards Max                                 1.33118\n",
      "exploration/Rewards Min                                -0.951103\n",
      "exploration/Returns Mean                             -472.509\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -472.509\n",
      "exploration/Returns Min                              -472.509\n",
      "exploration/Actions Mean                               -0.462126\n",
      "exploration/Actions Std                                 0.708433\n",
      "exploration/Actions Max                                 0.999699\n",
      "exploration/Actions Min                                -0.999859\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -472.509\n",
      "exploration/env_infos/final/reward_run Mean            -0.0560354\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0560354\n",
      "exploration/env_infos/final/reward_run Min             -0.0560354\n",
      "exploration/env_infos/initial/reward_run Mean          -0.553177\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.553177\n",
      "exploration/env_infos/initial/reward_run Min           -0.553177\n",
      "exploration/env_infos/reward_run Mean                  -0.616794\n",
      "exploration/env_infos/reward_run Std                    1.48289\n",
      "exploration/env_infos/reward_run Max                    1.04458\n",
      "exploration/env_infos/reward_run Min                   -6.46816\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.425785\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.425785\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.425785\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.283675\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.283675\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.283675\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.429263\n",
      "exploration/env_infos/reward_ctrl Std                   0.0696637\n",
      "exploration/env_infos/reward_ctrl Max                  -0.187532\n",
      "exploration/env_infos/reward_ctrl Min                  -0.580293\n",
      "exploration/env_infos/final/height Mean                -0.577233\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577233\n",
      "exploration/env_infos/final/height Min                 -0.577233\n",
      "exploration/env_infos/initial/height Mean               0.0942258\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0942258\n",
      "exploration/env_infos/initial/height Min                0.0942258\n",
      "exploration/env_infos/height Mean                      -0.471616\n",
      "exploration/env_infos/height Std                        0.226181\n",
      "exploration/env_infos/height Max                        0.319382\n",
      "exploration/env_infos/height Min                       -0.578937\n",
      "exploration/env_infos/final/reward_angular Mean        -0.00129901\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.00129901\n",
      "exploration/env_infos/final/reward_angular Min         -0.00129901\n",
      "exploration/env_infos/initial/reward_angular Mean       2.00063\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        2.00063\n",
      "exploration/env_infos/initial/reward_angular Min        2.00063\n",
      "exploration/env_infos/reward_angular Mean              -0.0477106\n",
      "exploration/env_infos/reward_angular Std                1.37229\n",
      "exploration/env_infos/reward_angular Max                9.52695\n",
      "exploration/env_infos/reward_angular Min               -6.01155\n",
      "evaluation/num steps total                              4.5e+06\n",
      "evaluation/num paths total                           4500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.66658\n",
      "evaluation/Rewards Std                                  1.58614\n",
      "evaluation/Rewards Max                                  8.63388\n",
      "evaluation/Rewards Min                                 -8.26123\n",
      "evaluation/Returns Mean                               666.58\n",
      "evaluation/Returns Std                                883.577\n",
      "evaluation/Returns Max                               2563.57\n",
      "evaluation/Returns Min                               -425.534\n",
      "evaluation/Actions Mean                                 0.0698041\n",
      "evaluation/Actions Std                                  0.800684\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            666.58\n",
      "evaluation/env_infos/final/reward_run Mean             -0.233456\n",
      "evaluation/env_infos/final/reward_run Std               2.21018\n",
      "evaluation/env_infos/final/reward_run Max               3.3983\n",
      "evaluation/env_infos/final/reward_run Min              -5.4857\n",
      "evaluation/env_infos/initial/reward_run Mean            0.428653\n",
      "evaluation/env_infos/initial/reward_run Std             0.570623\n",
      "evaluation/env_infos/initial/reward_run Max             1.20616\n",
      "evaluation/env_infos/initial/reward_run Min            -0.646597\n",
      "evaluation/env_infos/reward_run Mean                   -0.871786\n",
      "evaluation/env_infos/reward_run Std                     2.99123\n",
      "evaluation/env_infos/reward_run Max                     6.37306\n",
      "evaluation/env_infos/reward_run Min                    -8.18697\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.370254\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0955172\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.127657\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.505478\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.320081\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.115489\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.120295\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.548494\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.38758\n",
      "evaluation/env_infos/reward_ctrl Std                    0.108945\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0418243\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.591369\n",
      "evaluation/env_infos/final/height Mean                 -0.28398\n",
      "evaluation/env_infos/final/height Std                   0.245902\n",
      "evaluation/env_infos/final/height Max                   0.14396\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean                0.0114143\n",
      "evaluation/env_infos/initial/height Std                 0.0516896\n",
      "evaluation/env_infos/initial/height Max                 0.0912591\n",
      "evaluation/env_infos/initial/height Min                -0.0805954\n",
      "evaluation/env_infos/height Mean                       -0.161803\n",
      "evaluation/env_infos/height Std                         0.237649\n",
      "evaluation/env_infos/height Max                         0.462482\n",
      "evaluation/env_infos/height Min                        -0.597223\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.204459\n",
      "evaluation/env_infos/final/reward_angular Std           1.65485\n",
      "evaluation/env_infos/final/reward_angular Max           4.03184\n",
      "evaluation/env_infos/final/reward_angular Min          -2.97412\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.592563\n",
      "evaluation/env_infos/initial/reward_angular Std         0.749803\n",
      "evaluation/env_infos/initial/reward_angular Max         1.10207\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.81732\n",
      "evaluation/env_infos/reward_angular Mean                0.0302554\n",
      "evaluation/env_infos/reward_angular Std                 2.16674\n",
      "evaluation/env_infos/reward_angular Max                11.1921\n",
      "evaluation/env_infos/reward_angular Min                -6.55489\n",
      "time/data storing (s)                                   0.0159901\n",
      "time/evaluation sampling (s)                           22.0749\n",
      "time/exploration sampling (s)                           1.18446\n",
      "time/logging (s)                                        0.243471\n",
      "time/saving (s)                                         0.0273316\n",
      "time/training (s)                                       4.29964\n",
      "time/epoch (s)                                         27.8457\n",
      "time/total (s)                                       5495.56\n",
      "Epoch                                                 179\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:44:09.553079 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 180 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 182000\n",
      "trainer/QF1 Loss                                        2.3901\n",
      "trainer/QF2 Loss                                        2.81344\n",
      "trainer/Policy Loss                                   -13.6105\n",
      "trainer/Q1 Predictions Mean                            18.9187\n",
      "trainer/Q1 Predictions Std                             34.8825\n",
      "trainer/Q1 Predictions Max                            126.124\n",
      "trainer/Q1 Predictions Min                            -19.7747\n",
      "trainer/Q2 Predictions Mean                            18.9991\n",
      "trainer/Q2 Predictions Std                             34.8792\n",
      "trainer/Q2 Predictions Max                            127.617\n",
      "trainer/Q2 Predictions Min                            -19.9811\n",
      "trainer/Q Targets Mean                                 19.1422\n",
      "trainer/Q Targets Std                                  34.9504\n",
      "trainer/Q Targets Max                                 128.489\n",
      "trainer/Q Targets Min                                 -19.061\n",
      "trainer/Log Pis Mean                                    5.59062\n",
      "trainer/Log Pis Std                                     5.46543\n",
      "trainer/Log Pis Max                                    23.4012\n",
      "trainer/Log Pis Min                                    -7.49811\n",
      "trainer/Policy mu Mean                                 -0.0284362\n",
      "trainer/Policy mu Std                                   1.56408\n",
      "trainer/Policy mu Max                                   4.35486\n",
      "trainer/Policy mu Min                                  -4.77104\n",
      "trainer/Policy log std Mean                            -0.721086\n",
      "trainer/Policy log std Std                              0.266175\n",
      "trainer/Policy log std Max                              0.0552086\n",
      "trainer/Policy log std Min                             -2.10207\n",
      "trainer/Alpha                                           0.0226338\n",
      "trainer/Alpha Loss                                     -1.55027\n",
      "exploration/num steps total                        182000\n",
      "exploration/num paths total                           182\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.833232\n",
      "exploration/Rewards Std                                 0.869928\n",
      "exploration/Rewards Max                                 3.02627\n",
      "exploration/Rewards Min                                -2.69052\n",
      "exploration/Returns Mean                              833.232\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               833.232\n",
      "exploration/Returns Min                               833.232\n",
      "exploration/Actions Mean                                0.135128\n",
      "exploration/Actions Std                                 0.824632\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           833.232\n",
      "exploration/env_infos/final/reward_run Mean             0.339215\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.339215\n",
      "exploration/env_infos/final/reward_run Min              0.339215\n",
      "exploration/env_infos/initial/reward_run Mean           0.739402\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.739402\n",
      "exploration/env_infos/initial/reward_run Min            0.739402\n",
      "exploration/env_infos/reward_run Mean                   2.06625\n",
      "exploration/env_infos/reward_run Std                    1.27352\n",
      "exploration/env_infos/reward_run Max                    5.67921\n",
      "exploration/env_infos/reward_run Min                   -1.96982\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.468991\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.468991\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.468991\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.380694\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.380694\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.380694\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.418966\n",
      "exploration/env_infos/reward_ctrl Std                   0.0925919\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0776118\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594346\n",
      "exploration/env_infos/final/height Mean                 0.162147\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.162147\n",
      "exploration/env_infos/final/height Min                  0.162147\n",
      "exploration/env_infos/initial/height Mean              -0.0364198\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0364198\n",
      "exploration/env_infos/initial/height Min               -0.0364198\n",
      "exploration/env_infos/height Mean                      -0.00115596\n",
      "exploration/env_infos/height Std                        0.136569\n",
      "exploration/env_infos/height Max                        0.447255\n",
      "exploration/env_infos/height Min                       -0.322682\n",
      "exploration/env_infos/final/reward_angular Mean        -1.29046\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.29046\n",
      "exploration/env_infos/final/reward_angular Min         -1.29046\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.35334\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.35334\n",
      "exploration/env_infos/initial/reward_angular Min       -1.35334\n",
      "exploration/env_infos/reward_angular Mean              -0.108576\n",
      "exploration/env_infos/reward_angular Std                2.09659\n",
      "exploration/env_infos/reward_angular Max                5.72377\n",
      "exploration/env_infos/reward_angular Min               -7.05965\n",
      "evaluation/num steps total                              4.525e+06\n",
      "evaluation/num paths total                           4525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.516298\n",
      "evaluation/Rewards Std                                  1.40067\n",
      "evaluation/Rewards Max                                  7.58913\n",
      "evaluation/Rewards Min                                 -8.96005\n",
      "evaluation/Returns Mean                               516.298\n",
      "evaluation/Returns Std                                690.508\n",
      "evaluation/Returns Max                               2240.17\n",
      "evaluation/Returns Min                               -412.278\n",
      "evaluation/Actions Mean                                 0.0511207\n",
      "evaluation/Actions Std                                  0.772838\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            516.298\n",
      "evaluation/env_infos/final/reward_run Mean              0.0445224\n",
      "evaluation/env_infos/final/reward_run Std               2.03313\n",
      "evaluation/env_infos/final/reward_run Max               3.8898\n",
      "evaluation/env_infos/final/reward_run Min              -5.08229\n",
      "evaluation/env_infos/initial/reward_run Mean            0.230763\n",
      "evaluation/env_infos/initial/reward_run Std             0.554831\n",
      "evaluation/env_infos/initial/reward_run Max             0.895621\n",
      "evaluation/env_infos/initial/reward_run Min            -0.726513\n",
      "evaluation/env_infos/reward_run Mean                   -0.520879\n",
      "evaluation/env_infos/reward_run Std                     2.62762\n",
      "evaluation/env_infos/reward_run Max                     5.83436\n",
      "evaluation/env_infos/reward_run Min                    -7.55781\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.324049\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11228\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.15704\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.510054\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.30467\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0836501\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.109569\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.441422\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.359935\n",
      "evaluation/env_infos/reward_ctrl Std                    0.120027\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0498062\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596947\n",
      "evaluation/env_infos/final/height Mean                 -0.251004\n",
      "evaluation/env_infos/final/height Std                   0.230028\n",
      "evaluation/env_infos/final/height Max                   0.0898286\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0102051\n",
      "evaluation/env_infos/initial/height Std                 0.0524177\n",
      "evaluation/env_infos/initial/height Max                 0.0827991\n",
      "evaluation/env_infos/initial/height Min                -0.106109\n",
      "evaluation/env_infos/height Mean                       -0.172425\n",
      "evaluation/env_infos/height Std                         0.239729\n",
      "evaluation/env_infos/height Max                         0.47685\n",
      "evaluation/env_infos/height Min                        -0.587036\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0283106\n",
      "evaluation/env_infos/final/reward_angular Std           1.81702\n",
      "evaluation/env_infos/final/reward_angular Max           5.05112\n",
      "evaluation/env_infos/final/reward_angular Min          -4.75749\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.641712\n",
      "evaluation/env_infos/initial/reward_angular Std         0.766669\n",
      "evaluation/env_infos/initial/reward_angular Max         0.945328\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.89535\n",
      "evaluation/env_infos/reward_angular Mean                0.0364539\n",
      "evaluation/env_infos/reward_angular Std                 1.99783\n",
      "evaluation/env_infos/reward_angular Max                10.0962\n",
      "evaluation/env_infos/reward_angular Min                -7.51496\n",
      "time/data storing (s)                                   0.0170012\n",
      "time/evaluation sampling (s)                           23.6994\n",
      "time/exploration sampling (s)                           1.07864\n",
      "time/logging (s)                                        0.251691\n",
      "time/saving (s)                                         0.0306211\n",
      "time/training (s)                                       4.50608\n",
      "time/epoch (s)                                         29.5835\n",
      "time/total (s)                                       5526.52\n",
      "Epoch                                                 180\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:44:41.040582 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 181 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 183000\n",
      "trainer/QF1 Loss                                        2.31614\n",
      "trainer/QF2 Loss                                        1.92503\n",
      "trainer/Policy Loss                                    -9.17566\n",
      "trainer/Q1 Predictions Mean                            14.266\n",
      "trainer/Q1 Predictions Std                             28.6603\n",
      "trainer/Q1 Predictions Max                            134.896\n",
      "trainer/Q1 Predictions Min                            -17.9463\n",
      "trainer/Q2 Predictions Mean                            14.5311\n",
      "trainer/Q2 Predictions Std                             28.6574\n",
      "trainer/Q2 Predictions Max                            133.838\n",
      "trainer/Q2 Predictions Min                            -17.7316\n",
      "trainer/Q Targets Mean                                 14.4229\n",
      "trainer/Q Targets Std                                  28.9362\n",
      "trainer/Q Targets Max                                 139.567\n",
      "trainer/Q Targets Min                                 -18.0516\n",
      "trainer/Log Pis Mean                                    5.28299\n",
      "trainer/Log Pis Std                                     5.76226\n",
      "trainer/Log Pis Max                                    25.8366\n",
      "trainer/Log Pis Min                                    -5.99193\n",
      "trainer/Policy mu Mean                                  0.0854793\n",
      "trainer/Policy mu Std                                   1.52757\n",
      "trainer/Policy mu Max                                   5.47129\n",
      "trainer/Policy mu Min                                  -4.05906\n",
      "trainer/Policy log std Mean                            -0.696909\n",
      "trainer/Policy log std Std                              0.303173\n",
      "trainer/Policy log std Max                              0.401698\n",
      "trainer/Policy log std Min                             -2.10262\n",
      "trainer/Alpha                                           0.0219624\n",
      "trainer/Alpha Loss                                     -2.73667\n",
      "exploration/num steps total                        183000\n",
      "exploration/num paths total                           183\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.313535\n",
      "exploration/Rewards Std                                 1.51985\n",
      "exploration/Rewards Max                                 3.74338\n",
      "exploration/Rewards Min                                -4.7109\n",
      "exploration/Returns Mean                              313.535\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               313.535\n",
      "exploration/Returns Min                               313.535\n",
      "exploration/Actions Mean                                0.204165\n",
      "exploration/Actions Std                                 0.839931\n",
      "exploration/Actions Max                                 0.999985\n",
      "exploration/Actions Min                                -0.999843\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           313.535\n",
      "exploration/env_infos/final/reward_run Mean            -6.33398\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -6.33398\n",
      "exploration/env_infos/final/reward_run Min             -6.33398\n",
      "exploration/env_infos/initial/reward_run Mean          -0.657742\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.657742\n",
      "exploration/env_infos/initial/reward_run Min           -0.657742\n",
      "exploration/env_infos/reward_run Mean                  -3.96843\n",
      "exploration/env_infos/reward_run Std                    1.26349\n",
      "exploration/env_infos/reward_run Max                    0.431625\n",
      "exploration/env_infos/reward_run Min                   -7.92041\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.568602\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.568602\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.568602\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.443562\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.443562\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.443562\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.448301\n",
      "exploration/env_infos/reward_ctrl Std                   0.0751166\n",
      "exploration/env_infos/reward_ctrl Max                  -0.133977\n",
      "exploration/env_infos/reward_ctrl Min                  -0.591913\n",
      "exploration/env_infos/final/height Mean                -0.0386732\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0386732\n",
      "exploration/env_infos/final/height Min                 -0.0386732\n",
      "exploration/env_infos/initial/height Mean              -0.0100208\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0100208\n",
      "exploration/env_infos/initial/height Min               -0.0100208\n",
      "exploration/env_infos/height Mean                       0.0134244\n",
      "exploration/env_infos/height Std                        0.132966\n",
      "exploration/env_infos/height Max                        0.424537\n",
      "exploration/env_infos/height Min                       -0.291767\n",
      "exploration/env_infos/final/reward_angular Mean         4.90783\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          4.90783\n",
      "exploration/env_infos/final/reward_angular Min          4.90783\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.343146\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.343146\n",
      "exploration/env_infos/initial/reward_angular Min       -0.343146\n",
      "exploration/env_infos/reward_angular Mean               0.113805\n",
      "exploration/env_infos/reward_angular Std                3.11268\n",
      "exploration/env_infos/reward_angular Max               10.0032\n",
      "exploration/env_infos/reward_angular Min               -6.76936\n",
      "evaluation/num steps total                              4.55e+06\n",
      "evaluation/num paths total                           4550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.665052\n",
      "evaluation/Rewards Std                                  1.47737\n",
      "evaluation/Rewards Max                                  8.85424\n",
      "evaluation/Rewards Min                                 -7.59229\n",
      "evaluation/Returns Mean                               665.052\n",
      "evaluation/Returns Std                                866.093\n",
      "evaluation/Returns Max                               2785.27\n",
      "evaluation/Returns Min                               -438.369\n",
      "evaluation/Actions Mean                                 0.0960443\n",
      "evaluation/Actions Std                                  0.768291\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999995\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            665.052\n",
      "evaluation/env_infos/final/reward_run Mean             -0.185187\n",
      "evaluation/env_infos/final/reward_run Std               2.66192\n",
      "evaluation/env_infos/final/reward_run Max               4.85466\n",
      "evaluation/env_infos/final/reward_run Min              -6.89805\n",
      "evaluation/env_infos/initial/reward_run Mean            0.358545\n",
      "evaluation/env_infos/initial/reward_run Std             0.575653\n",
      "evaluation/env_infos/initial/reward_run Max             1.16326\n",
      "evaluation/env_infos/initial/reward_run Min            -0.791698\n",
      "evaluation/env_infos/reward_run Mean                   -0.461018\n",
      "evaluation/env_infos/reward_run Std                     2.7629\n",
      "evaluation/env_infos/reward_run Max                     6.15033\n",
      "evaluation/env_infos/reward_run Min                    -7.9687\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.371359\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.122564\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0988186\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.569574\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.335899\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0981796\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.102367\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.498515\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.359697\n",
      "evaluation/env_infos/reward_ctrl Std                    0.127404\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0306915\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594219\n",
      "evaluation/env_infos/final/height Mean                 -0.244175\n",
      "evaluation/env_infos/final/height Std                   0.23821\n",
      "evaluation/env_infos/final/height Max                   0.0956356\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0282602\n",
      "evaluation/env_infos/initial/height Std                 0.0419257\n",
      "evaluation/env_infos/initial/height Max                 0.0861964\n",
      "evaluation/env_infos/initial/height Min                -0.0949672\n",
      "evaluation/env_infos/height Mean                       -0.191075\n",
      "evaluation/env_infos/height Std                         0.25107\n",
      "evaluation/env_infos/height Max                         0.484205\n",
      "evaluation/env_infos/height Min                        -0.587628\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.164303\n",
      "evaluation/env_infos/final/reward_angular Std           1.38884\n",
      "evaluation/env_infos/final/reward_angular Max           4.3308\n",
      "evaluation/env_infos/final/reward_angular Min          -3.02752\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.556449\n",
      "evaluation/env_infos/initial/reward_angular Std         1.12847\n",
      "evaluation/env_infos/initial/reward_angular Max         2.12132\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.19216\n",
      "evaluation/env_infos/reward_angular Mean                0.0363586\n",
      "evaluation/env_infos/reward_angular Std                 2.03545\n",
      "evaluation/env_infos/reward_angular Max                10.2569\n",
      "evaluation/env_infos/reward_angular Min                -6.43557\n",
      "time/data storing (s)                                   0.0160845\n",
      "time/evaluation sampling (s)                           23.9405\n",
      "time/exploration sampling (s)                           0.967729\n",
      "time/logging (s)                                        0.251579\n",
      "time/saving (s)                                         0.0293436\n",
      "time/training (s)                                       4.85657\n",
      "time/epoch (s)                                         30.0618\n",
      "time/total (s)                                       5558\n",
      "Epoch                                                 181\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:45:13.344041 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 182 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 184000\n",
      "trainer/QF1 Loss                                        2.402\n",
      "trainer/QF2 Loss                                        2.17558\n",
      "trainer/Policy Loss                                   -16.0127\n",
      "trainer/Q1 Predictions Mean                            22.2748\n",
      "trainer/Q1 Predictions Std                             35.0296\n",
      "trainer/Q1 Predictions Max                            127.239\n",
      "trainer/Q1 Predictions Min                            -18.2838\n",
      "trainer/Q2 Predictions Mean                            22.1178\n",
      "trainer/Q2 Predictions Std                             34.908\n",
      "trainer/Q2 Predictions Max                            126.544\n",
      "trainer/Q2 Predictions Min                            -18.2601\n",
      "trainer/Q Targets Mean                                 22.0879\n",
      "trainer/Q Targets Std                                  34.9156\n",
      "trainer/Q Targets Max                                 129.354\n",
      "trainer/Q Targets Min                                 -17.8063\n",
      "trainer/Log Pis Mean                                    6.3606\n",
      "trainer/Log Pis Std                                     5.49755\n",
      "trainer/Log Pis Max                                    29.7224\n",
      "trainer/Log Pis Min                                    -3.14177\n",
      "trainer/Policy mu Mean                                 -0.0133562\n",
      "trainer/Policy mu Std                                   1.63711\n",
      "trainer/Policy mu Max                                   3.46101\n",
      "trainer/Policy mu Min                                  -5.44057\n",
      "trainer/Policy log std Mean                            -0.712533\n",
      "trainer/Policy log std Std                              0.260881\n",
      "trainer/Policy log std Max                              0.13943\n",
      "trainer/Policy log std Min                             -2.23582\n",
      "trainer/Alpha                                           0.0227078\n",
      "trainer/Alpha Loss                                      1.36525\n",
      "exploration/num steps total                        184000\n",
      "exploration/num paths total                           184\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.177139\n",
      "exploration/Rewards Std                                 0.97722\n",
      "exploration/Rewards Max                                 4.01459\n",
      "exploration/Rewards Min                                -4.82814\n",
      "exploration/Returns Mean                             -177.139\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -177.139\n",
      "exploration/Returns Min                              -177.139\n",
      "exploration/Actions Mean                               -0.0423085\n",
      "exploration/Actions Std                                 0.719309\n",
      "exploration/Actions Max                                 0.999976\n",
      "exploration/Actions Min                                -0.999957\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -177.139\n",
      "exploration/env_infos/final/reward_run Mean             0.0663651\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.0663651\n",
      "exploration/env_infos/final/reward_run Min              0.0663651\n",
      "exploration/env_infos/initial/reward_run Mean           0.409418\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.409418\n",
      "exploration/env_infos/initial/reward_run Min            0.409418\n",
      "exploration/env_infos/reward_run Mean                  -0.104354\n",
      "exploration/env_infos/reward_run Std                    0.682595\n",
      "exploration/env_infos/reward_run Max                    2.0247\n",
      "exploration/env_infos/reward_run Min                   -3.03665\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.332484\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.332484\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.332484\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.425013\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.425013\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.425013\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.311518\n",
      "exploration/env_infos/reward_ctrl Std                   0.0790303\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0808849\n",
      "exploration/env_infos/reward_ctrl Min                  -0.579821\n",
      "exploration/env_infos/final/height Mean                -0.576912\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576912\n",
      "exploration/env_infos/final/height Min                 -0.576912\n",
      "exploration/env_infos/initial/height Mean               0.0658474\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0658474\n",
      "exploration/env_infos/initial/height Min                0.0658474\n",
      "exploration/env_infos/height Mean                      -0.37307\n",
      "exploration/env_infos/height Std                        0.290594\n",
      "exploration/env_infos/height Max                        0.312248\n",
      "exploration/env_infos/height Min                       -0.582941\n",
      "exploration/env_infos/final/reward_angular Mean         0.0264032\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0264032\n",
      "exploration/env_infos/final/reward_angular Min          0.0264032\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0568392\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0568392\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0568392\n",
      "exploration/env_infos/reward_angular Mean               0.209284\n",
      "exploration/env_infos/reward_angular Std                1.30945\n",
      "exploration/env_infos/reward_angular Max                5.82692\n",
      "exploration/env_infos/reward_angular Min               -6.26754\n",
      "evaluation/num steps total                              4.575e+06\n",
      "evaluation/num paths total                           4575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.489757\n",
      "evaluation/Rewards Std                                  1.37788\n",
      "evaluation/Rewards Max                                  9.02061\n",
      "evaluation/Rewards Min                                 -9.0572\n",
      "evaluation/Returns Mean                               489.757\n",
      "evaluation/Returns Std                                700.832\n",
      "evaluation/Returns Max                               2456.15\n",
      "evaluation/Returns Min                               -423.645\n",
      "evaluation/Actions Mean                                 0.0374397\n",
      "evaluation/Actions Std                                  0.761245\n",
      "evaluation/Actions Max                                  0.999995\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            489.757\n",
      "evaluation/env_infos/final/reward_run Mean              0.0437783\n",
      "evaluation/env_infos/final/reward_run Std               2.24003\n",
      "evaluation/env_infos/final/reward_run Max               3.44629\n",
      "evaluation/env_infos/final/reward_run Min              -6.13465\n",
      "evaluation/env_infos/initial/reward_run Mean            0.194325\n",
      "evaluation/env_infos/initial/reward_run Std             0.625253\n",
      "evaluation/env_infos/initial/reward_run Max             1.06024\n",
      "evaluation/env_infos/initial/reward_run Min            -0.878851\n",
      "evaluation/env_infos/reward_run Mean                   -0.404893\n",
      "evaluation/env_infos/reward_run Std                     2.55978\n",
      "evaluation/env_infos/reward_run Max                     5.70429\n",
      "evaluation/env_infos/reward_run Min                    -8.32857\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.325418\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.124496\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0742607\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.570139\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.330113\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.116321\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.125213\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.493124\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.348537\n",
      "evaluation/env_infos/reward_ctrl Std                    0.121773\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0365996\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594114\n",
      "evaluation/env_infos/final/height Mean                 -0.322152\n",
      "evaluation/env_infos/final/height Std                   0.290651\n",
      "evaluation/env_infos/final/height Max                   0.218052\n",
      "evaluation/env_infos/final/height Min                  -0.577394\n",
      "evaluation/env_infos/initial/height Mean               -0.00761367\n",
      "evaluation/env_infos/initial/height Std                 0.0486169\n",
      "evaluation/env_infos/initial/height Max                 0.082123\n",
      "evaluation/env_infos/initial/height Min                -0.0943902\n",
      "evaluation/env_infos/height Mean                       -0.245049\n",
      "evaluation/env_infos/height Std                         0.282039\n",
      "evaluation/env_infos/height Max                         0.606729\n",
      "evaluation/env_infos/height Min                        -0.590942\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0557931\n",
      "evaluation/env_infos/final/reward_angular Std           1.57268\n",
      "evaluation/env_infos/final/reward_angular Max           4.65222\n",
      "evaluation/env_infos/final/reward_angular Min          -3.56897\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.468879\n",
      "evaluation/env_infos/initial/reward_angular Std         1.02997\n",
      "evaluation/env_infos/initial/reward_angular Max         3.35633\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.17334\n",
      "evaluation/env_infos/reward_angular Mean                0.0412252\n",
      "evaluation/env_infos/reward_angular Std                 2.04405\n",
      "evaluation/env_infos/reward_angular Max                10.8632\n",
      "evaluation/env_infos/reward_angular Min                -7.83233\n",
      "time/data storing (s)                                   0.0149865\n",
      "time/evaluation sampling (s)                           23.9586\n",
      "time/exploration sampling (s)                           1.59143\n",
      "time/logging (s)                                        0.256341\n",
      "time/saving (s)                                         0.0287094\n",
      "time/training (s)                                       5.07717\n",
      "time/epoch (s)                                         30.9273\n",
      "time/total (s)                                       5590.3\n",
      "Epoch                                                 182\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:45:45.706597 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 183 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 185000\n",
      "trainer/QF1 Loss                                        2.23913\n",
      "trainer/QF2 Loss                                        2.11879\n",
      "trainer/Policy Loss                                   -11.9866\n",
      "trainer/Q1 Predictions Mean                            17.6518\n",
      "trainer/Q1 Predictions Std                             29.301\n",
      "trainer/Q1 Predictions Max                            130.846\n",
      "trainer/Q1 Predictions Min                            -19.2004\n",
      "trainer/Q2 Predictions Mean                            17.4121\n",
      "trainer/Q2 Predictions Std                             29.0721\n",
      "trainer/Q2 Predictions Max                            128.162\n",
      "trainer/Q2 Predictions Min                            -19.4567\n",
      "trainer/Q Targets Mean                                 17.3553\n",
      "trainer/Q Targets Std                                  29.4901\n",
      "trainer/Q Targets Max                                 133.04\n",
      "trainer/Q Targets Min                                 -19.5096\n",
      "trainer/Log Pis Mean                                    5.85409\n",
      "trainer/Log Pis Std                                     5.54008\n",
      "trainer/Log Pis Max                                    22.2291\n",
      "trainer/Log Pis Min                                    -5.77152\n",
      "trainer/Policy mu Mean                                 -0.00680036\n",
      "trainer/Policy mu Std                                   1.57369\n",
      "trainer/Policy mu Max                                   4.4246\n",
      "trainer/Policy mu Min                                  -5.53765\n",
      "trainer/Policy log std Mean                            -0.726926\n",
      "trainer/Policy log std Std                              0.307675\n",
      "trainer/Policy log std Max                              0.126552\n",
      "trainer/Policy log std Min                             -2.52137\n",
      "trainer/Alpha                                           0.0212149\n",
      "trainer/Alpha Loss                                     -0.562092\n",
      "exploration/num steps total                        185000\n",
      "exploration/num paths total                           185\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.2893\n",
      "exploration/Rewards Std                                 2.52025\n",
      "exploration/Rewards Max                                 6.50913\n",
      "exploration/Rewards Min                                -6.74955\n",
      "exploration/Returns Mean                             1289.3\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1289.3\n",
      "exploration/Returns Min                              1289.3\n",
      "exploration/Actions Mean                                0.119131\n",
      "exploration/Actions Std                                 0.851305\n",
      "exploration/Actions Max                                 0.999618\n",
      "exploration/Actions Min                                -0.999775\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1289.3\n",
      "exploration/env_infos/final/reward_run Mean            -3.50728\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.50728\n",
      "exploration/env_infos/final/reward_run Min             -3.50728\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0361867\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0361867\n",
      "exploration/env_infos/initial/reward_run Min           -0.0361867\n",
      "exploration/env_infos/reward_run Mean                  -4.12005\n",
      "exploration/env_infos/reward_run Std                    1.12816\n",
      "exploration/env_infos/reward_run Max                    0.223078\n",
      "exploration/env_infos/reward_run Min                   -7.04877\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.438925\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.438925\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.438925\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.18853\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.18853\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.18853\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.443348\n",
      "exploration/env_infos/reward_ctrl Std                   0.0654181\n",
      "exploration/env_infos/reward_ctrl Max                  -0.18853\n",
      "exploration/env_infos/reward_ctrl Min                  -0.565479\n",
      "exploration/env_infos/final/height Mean                 0.0242151\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0242151\n",
      "exploration/env_infos/final/height Min                  0.0242151\n",
      "exploration/env_infos/initial/height Mean              -0.0356916\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0356916\n",
      "exploration/env_infos/initial/height Min               -0.0356916\n",
      "exploration/env_infos/height Mean                      -0.015992\n",
      "exploration/env_infos/height Std                        0.128293\n",
      "exploration/env_infos/height Max                        0.393821\n",
      "exploration/env_infos/height Min                       -0.311595\n",
      "exploration/env_infos/final/reward_angular Mean        -0.305852\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.305852\n",
      "exploration/env_infos/final/reward_angular Min         -0.305852\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.168955\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.168955\n",
      "exploration/env_infos/initial/reward_angular Min       -0.168955\n",
      "exploration/env_infos/reward_angular Mean               0.140636\n",
      "exploration/env_infos/reward_angular Std                2.87868\n",
      "exploration/env_infos/reward_angular Max                8.95782\n",
      "exploration/env_infos/reward_angular Min               -5.09249\n",
      "evaluation/num steps total                              4.6e+06\n",
      "evaluation/num paths total                           4600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.799423\n",
      "evaluation/Rewards Std                                  1.70152\n",
      "evaluation/Rewards Max                                  8.47239\n",
      "evaluation/Rewards Min                                 -8.57401\n",
      "evaluation/Returns Mean                               799.423\n",
      "evaluation/Returns Std                                932.493\n",
      "evaluation/Returns Max                               2927.34\n",
      "evaluation/Returns Min                               -354.802\n",
      "evaluation/Actions Mean                                 0.0850883\n",
      "evaluation/Actions Std                                  0.798583\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            799.423\n",
      "evaluation/env_infos/final/reward_run Mean             -1.62376\n",
      "evaluation/env_infos/final/reward_run Std               2.81804\n",
      "evaluation/env_infos/final/reward_run Max               2.76474\n",
      "evaluation/env_infos/final/reward_run Min              -6.82136\n",
      "evaluation/env_infos/initial/reward_run Mean            0.362738\n",
      "evaluation/env_infos/initial/reward_run Std             0.541522\n",
      "evaluation/env_infos/initial/reward_run Max             1.32258\n",
      "evaluation/env_infos/initial/reward_run Min            -0.782882\n",
      "evaluation/env_infos/reward_run Mean                   -1.37366\n",
      "evaluation/env_infos/reward_run Std                     2.91453\n",
      "evaluation/env_infos/reward_run Max                     5.63748\n",
      "evaluation/env_infos/reward_run Min                    -7.37622\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.386322\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104749\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0823945\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.536677\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.317838\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.110598\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.116551\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.468163\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.386984\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111259\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.049016\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595095\n",
      "evaluation/env_infos/final/height Mean                 -0.146698\n",
      "evaluation/env_infos/final/height Std                   0.219996\n",
      "evaluation/env_infos/final/height Max                   0.143103\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0157531\n",
      "evaluation/env_infos/initial/height Std                 0.0518928\n",
      "evaluation/env_infos/initial/height Max                 0.0779139\n",
      "evaluation/env_infos/initial/height Min                -0.0962543\n",
      "evaluation/env_infos/height Mean                       -0.0985006\n",
      "evaluation/env_infos/height Std                         0.201537\n",
      "evaluation/env_infos/height Max                         0.483766\n",
      "evaluation/env_infos/height Min                        -0.589114\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0523022\n",
      "evaluation/env_infos/final/reward_angular Std           2.41771\n",
      "evaluation/env_infos/final/reward_angular Max           6.79872\n",
      "evaluation/env_infos/final/reward_angular Min          -5.44482\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0374013\n",
      "evaluation/env_infos/initial/reward_angular Std         0.815104\n",
      "evaluation/env_infos/initial/reward_angular Max         1.82677\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.17101\n",
      "evaluation/env_infos/reward_angular Mean                0.0745662\n",
      "evaluation/env_infos/reward_angular Std                 2.30365\n",
      "evaluation/env_infos/reward_angular Max                 9.79605\n",
      "evaluation/env_infos/reward_angular Min                -6.91068\n",
      "time/data storing (s)                                   0.016084\n",
      "time/evaluation sampling (s)                           24.1173\n",
      "time/exploration sampling (s)                           1.12811\n",
      "time/logging (s)                                        0.258256\n",
      "time/saving (s)                                         0.0286576\n",
      "time/training (s)                                       5.2692\n",
      "time/epoch (s)                                         30.8177\n",
      "time/total (s)                                       5622.66\n",
      "Epoch                                                 183\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:46:19.636385 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 184 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 186000\n",
      "trainer/QF1 Loss                                        1.84411\n",
      "trainer/QF2 Loss                                        1.79754\n",
      "trainer/Policy Loss                                   -14.5066\n",
      "trainer/Q1 Predictions Mean                            20.0421\n",
      "trainer/Q1 Predictions Std                             34.0828\n",
      "trainer/Q1 Predictions Max                            132.143\n",
      "trainer/Q1 Predictions Min                            -17.8761\n",
      "trainer/Q2 Predictions Mean                            19.8956\n",
      "trainer/Q2 Predictions Std                             34.0645\n",
      "trainer/Q2 Predictions Max                            132.915\n",
      "trainer/Q2 Predictions Min                            -18.1453\n",
      "trainer/Q Targets Mean                                 19.847\n",
      "trainer/Q Targets Std                                  33.9798\n",
      "trainer/Q Targets Max                                 131.263\n",
      "trainer/Q Targets Min                                 -18.5132\n",
      "trainer/Log Pis Mean                                    5.6395\n",
      "trainer/Log Pis Std                                     5.52471\n",
      "trainer/Log Pis Max                                    24.733\n",
      "trainer/Log Pis Min                                    -5.09247\n",
      "trainer/Policy mu Mean                                  0.0263057\n",
      "trainer/Policy mu Std                                   1.54297\n",
      "trainer/Policy mu Max                                   4.70862\n",
      "trainer/Policy mu Min                                  -5.59764\n",
      "trainer/Policy log std Mean                            -0.720998\n",
      "trainer/Policy log std Std                              0.282291\n",
      "trainer/Policy log std Max                              0.247709\n",
      "trainer/Policy log std Min                             -2.14585\n",
      "trainer/Alpha                                           0.0225978\n",
      "trainer/Alpha Loss                                     -1.36577\n",
      "exploration/num steps total                        186000\n",
      "exploration/num paths total                           186\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.67586\n",
      "exploration/Rewards Std                                 1.2027\n",
      "exploration/Rewards Max                                 3.87953\n",
      "exploration/Rewards Min                                -1.05403\n",
      "exploration/Returns Mean                              675.86\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               675.86\n",
      "exploration/Returns Min                               675.86\n",
      "exploration/Actions Mean                                0.0866376\n",
      "exploration/Actions Std                                 0.766881\n",
      "exploration/Actions Max                                 0.999983\n",
      "exploration/Actions Min                                -0.99996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           675.86\n",
      "exploration/env_infos/final/reward_run Mean            -0.44777\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.44777\n",
      "exploration/env_infos/final/reward_run Min             -0.44777\n",
      "exploration/env_infos/initial/reward_run Mean          -0.30544\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.30544\n",
      "exploration/env_infos/initial/reward_run Min           -0.30544\n",
      "exploration/env_infos/reward_run Mean                  -1.75567\n",
      "exploration/env_infos/reward_run Std                    2.29808\n",
      "exploration/env_infos/reward_run Max                    1.58326\n",
      "exploration/env_infos/reward_run Min                   -7.71669\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.256583\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.256583\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.256583\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.103912\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.103912\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.103912\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.357368\n",
      "exploration/env_infos/reward_ctrl Std                   0.120524\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0929598\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594751\n",
      "exploration/env_infos/final/height Mean                -0.554391\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.554391\n",
      "exploration/env_infos/final/height Min                 -0.554391\n",
      "exploration/env_infos/initial/height Mean               0.00458512\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.00458512\n",
      "exploration/env_infos/initial/height Min                0.00458512\n",
      "exploration/env_infos/height Mean                      -0.372863\n",
      "exploration/env_infos/height Std                        0.254965\n",
      "exploration/env_infos/height Max                        0.288321\n",
      "exploration/env_infos/height Min                       -0.588734\n",
      "exploration/env_infos/final/reward_angular Mean        -1.14141\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.14141\n",
      "exploration/env_infos/final/reward_angular Min         -1.14141\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0861905\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0861905\n",
      "exploration/env_infos/initial/reward_angular Min        0.0861905\n",
      "exploration/env_infos/reward_angular Mean              -0.0326209\n",
      "exploration/env_infos/reward_angular Std                1.82569\n",
      "exploration/env_infos/reward_angular Max                9.21406\n",
      "exploration/env_infos/reward_angular Min               -4.96572\n",
      "evaluation/num steps total                              4.625e+06\n",
      "evaluation/num paths total                           4625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.783134\n",
      "evaluation/Rewards Std                                  1.65494\n",
      "evaluation/Rewards Max                                  8.84608\n",
      "evaluation/Rewards Min                                 -8.69035\n",
      "evaluation/Returns Mean                               783.134\n",
      "evaluation/Returns Std                               1062.56\n",
      "evaluation/Returns Max                               3095.14\n",
      "evaluation/Returns Min                               -439.788\n",
      "evaluation/Actions Mean                                 0.117583\n",
      "evaluation/Actions Std                                  0.780393\n",
      "evaluation/Actions Max                                  0.999992\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            783.134\n",
      "evaluation/env_infos/final/reward_run Mean             -0.752394\n",
      "evaluation/env_infos/final/reward_run Std               2.49641\n",
      "evaluation/env_infos/final/reward_run Max               4.00268\n",
      "evaluation/env_infos/final/reward_run Min              -5.4661\n",
      "evaluation/env_infos/initial/reward_run Mean            0.275953\n",
      "evaluation/env_infos/initial/reward_run Std             0.561538\n",
      "evaluation/env_infos/initial/reward_run Max             1.14024\n",
      "evaluation/env_infos/initial/reward_run Min            -0.684702\n",
      "evaluation/env_infos/reward_run Mean                   -1.18202\n",
      "evaluation/env_infos/reward_run Std                     2.92698\n",
      "evaluation/env_infos/reward_run Max                     5.10255\n",
      "evaluation/env_infos/reward_run Min                    -8.33694\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.337242\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.146638\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.056571\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.58576\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.294171\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0884923\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0973617\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.454245\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.373703\n",
      "evaluation/env_infos/reward_ctrl Std                    0.130011\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0374002\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593274\n",
      "evaluation/env_infos/final/height Mean                 -0.285544\n",
      "evaluation/env_infos/final/height Std                   0.251053\n",
      "evaluation/env_infos/final/height Max                   0.175314\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.012279\n",
      "evaluation/env_infos/initial/height Std                 0.0502839\n",
      "evaluation/env_infos/initial/height Max                 0.0808657\n",
      "evaluation/env_infos/initial/height Min                -0.116195\n",
      "evaluation/env_infos/height Mean                       -0.191455\n",
      "evaluation/env_infos/height Std                         0.253099\n",
      "evaluation/env_infos/height Max                         0.480854\n",
      "evaluation/env_infos/height Min                        -0.589802\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.0731624\n",
      "evaluation/env_infos/final/reward_angular Std           2.32268\n",
      "evaluation/env_infos/final/reward_angular Max           6.77655\n",
      "evaluation/env_infos/final/reward_angular Min          -4.03998\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.610289\n",
      "evaluation/env_infos/initial/reward_angular Std         0.633882\n",
      "evaluation/env_infos/initial/reward_angular Max         0.975972\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.75191\n",
      "evaluation/env_infos/reward_angular Mean                0.0546991\n",
      "evaluation/env_infos/reward_angular Std                 2.11046\n",
      "evaluation/env_infos/reward_angular Max                10.492\n",
      "evaluation/env_infos/reward_angular Min                -6.63105\n",
      "time/data storing (s)                                   0.0168871\n",
      "time/evaluation sampling (s)                           25.6328\n",
      "time/exploration sampling (s)                           1.12807\n",
      "time/logging (s)                                        0.278215\n",
      "time/saving (s)                                         0.027694\n",
      "time/training (s)                                       5.35605\n",
      "time/epoch (s)                                         32.4397\n",
      "time/total (s)                                       5656.61\n",
      "Epoch                                                 184\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:46:50.015562 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 185 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 187000\n",
      "trainer/QF1 Loss                                        2.03396\n",
      "trainer/QF2 Loss                                        1.62995\n",
      "trainer/Policy Loss                                   -10.3609\n",
      "trainer/Q1 Predictions Mean                            15.7965\n",
      "trainer/Q1 Predictions Std                             30.9722\n",
      "trainer/Q1 Predictions Max                            129.001\n",
      "trainer/Q1 Predictions Min                            -18.4487\n",
      "trainer/Q2 Predictions Mean                            15.5578\n",
      "trainer/Q2 Predictions Std                             31.1446\n",
      "trainer/Q2 Predictions Max                            131.402\n",
      "trainer/Q2 Predictions Min                            -19.3166\n",
      "trainer/Q Targets Mean                                 15.4668\n",
      "trainer/Q Targets Std                                  31.1024\n",
      "trainer/Q Targets Max                                 130.234\n",
      "trainer/Q Targets Min                                 -19.207\n",
      "trainer/Log Pis Mean                                    5.55933\n",
      "trainer/Log Pis Std                                     5.39869\n",
      "trainer/Log Pis Max                                    29.6209\n",
      "trainer/Log Pis Min                                    -4.71622\n",
      "trainer/Policy mu Mean                                 -0.0916027\n",
      "trainer/Policy mu Std                                   1.5344\n",
      "trainer/Policy mu Max                                   5.0694\n",
      "trainer/Policy mu Min                                  -5.16354\n",
      "trainer/Policy log std Mean                            -0.732085\n",
      "trainer/Policy log std Std                              0.29273\n",
      "trainer/Policy log std Max                              0.236032\n",
      "trainer/Policy log std Min                             -2.3\n",
      "trainer/Alpha                                           0.0214951\n",
      "trainer/Alpha Loss                                     -1.69261\n",
      "exploration/num steps total                        187000\n",
      "exploration/num paths total                           187\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.218915\n",
      "exploration/Rewards Std                                 0.641952\n",
      "exploration/Rewards Max                                 4.22401\n",
      "exploration/Rewards Min                                -1.78574\n",
      "exploration/Returns Mean                             -218.915\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -218.915\n",
      "exploration/Returns Min                              -218.915\n",
      "exploration/Actions Mean                               -0.0531163\n",
      "exploration/Actions Std                                 0.717576\n",
      "exploration/Actions Max                                 0.999986\n",
      "exploration/Actions Min                                -0.999992\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -218.915\n",
      "exploration/env_infos/final/reward_run Mean             0.299207\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.299207\n",
      "exploration/env_infos/final/reward_run Min              0.299207\n",
      "exploration/env_infos/initial/reward_run Mean          -0.498656\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.498656\n",
      "exploration/env_infos/initial/reward_run Min           -0.498656\n",
      "exploration/env_infos/reward_run Mean                  -0.372868\n",
      "exploration/env_infos/reward_run Std                    1.42166\n",
      "exploration/env_infos/reward_run Max                    1.29375\n",
      "exploration/env_infos/reward_run Min                   -6.72327\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.35117\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.35117\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.35117\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.298647\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.298647\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.298647\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.310642\n",
      "exploration/env_infos/reward_ctrl Std                   0.0856618\n",
      "exploration/env_infos/reward_ctrl Max                  -0.105644\n",
      "exploration/env_infos/reward_ctrl Min                  -0.548205\n",
      "exploration/env_infos/final/height Mean                -0.556543\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.556543\n",
      "exploration/env_infos/final/height Min                 -0.556543\n",
      "exploration/env_infos/initial/height Mean              -0.0788097\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0788097\n",
      "exploration/env_infos/initial/height Min               -0.0788097\n",
      "exploration/env_infos/height Mean                      -0.481074\n",
      "exploration/env_infos/height Std                        0.193741\n",
      "exploration/env_infos/height Max                        0.363427\n",
      "exploration/env_infos/height Min                       -0.584457\n",
      "exploration/env_infos/final/reward_angular Mean        -0.0803141\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.0803141\n",
      "exploration/env_infos/final/reward_angular Min         -0.0803141\n",
      "exploration/env_infos/initial/reward_angular Mean       1.42941\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.42941\n",
      "exploration/env_infos/initial/reward_angular Min        1.42941\n",
      "exploration/env_infos/reward_angular Mean              -0.0428457\n",
      "exploration/env_infos/reward_angular Std                1.33214\n",
      "exploration/env_infos/reward_angular Max                7.97881\n",
      "exploration/env_infos/reward_angular Min               -4.85176\n",
      "evaluation/num steps total                              4.65e+06\n",
      "evaluation/num paths total                           4650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.608157\n",
      "evaluation/Rewards Std                                  1.54433\n",
      "evaluation/Rewards Max                                  8.94217\n",
      "evaluation/Rewards Min                                 -8.89331\n",
      "evaluation/Returns Mean                               608.157\n",
      "evaluation/Returns Std                                849.6\n",
      "evaluation/Returns Max                               2805.19\n",
      "evaluation/Returns Min                               -398.95\n",
      "evaluation/Actions Mean                                 0.0717806\n",
      "evaluation/Actions Std                                  0.781616\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            608.157\n",
      "evaluation/env_infos/final/reward_run Mean             -0.579387\n",
      "evaluation/env_infos/final/reward_run Std               2.52332\n",
      "evaluation/env_infos/final/reward_run Max               3.39829\n",
      "evaluation/env_infos/final/reward_run Min              -6.71599\n",
      "evaluation/env_infos/initial/reward_run Mean            0.180338\n",
      "evaluation/env_infos/initial/reward_run Std             0.593812\n",
      "evaluation/env_infos/initial/reward_run Max             1.17092\n",
      "evaluation/env_infos/initial/reward_run Min            -0.880961\n",
      "evaluation/env_infos/reward_run Mean                   -0.70762\n",
      "evaluation/env_infos/reward_run Std                     2.68059\n",
      "evaluation/env_infos/reward_run Max                     6.08875\n",
      "evaluation/env_infos/reward_run Min                    -8.07031\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.329183\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.114861\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0533478\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.472708\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.297292\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0810127\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.090469\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.42523\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.369646\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115203\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0263388\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.590625\n",
      "evaluation/env_infos/final/height Mean                 -0.281619\n",
      "evaluation/env_infos/final/height Std                   0.31287\n",
      "evaluation/env_infos/final/height Max                   0.378348\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0305869\n",
      "evaluation/env_infos/initial/height Std                 0.0521363\n",
      "evaluation/env_infos/initial/height Max                 0.0733259\n",
      "evaluation/env_infos/initial/height Min                -0.1142\n",
      "evaluation/env_infos/height Mean                       -0.212292\n",
      "evaluation/env_infos/height Std                         0.277802\n",
      "evaluation/env_infos/height Max                         0.568138\n",
      "evaluation/env_infos/height Min                        -0.588823\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0665837\n",
      "evaluation/env_infos/final/reward_angular Std           1.69444\n",
      "evaluation/env_infos/final/reward_angular Max           3.91907\n",
      "evaluation/env_infos/final/reward_angular Min          -3.78488\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.355167\n",
      "evaluation/env_infos/initial/reward_angular Std         0.78873\n",
      "evaluation/env_infos/initial/reward_angular Max         1.40722\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.95961\n",
      "evaluation/env_infos/reward_angular Mean                0.0539558\n",
      "evaluation/env_infos/reward_angular Std                 2.07644\n",
      "evaluation/env_infos/reward_angular Max                11.3641\n",
      "evaluation/env_infos/reward_angular Min                -6.97986\n",
      "time/data storing (s)                                   0.0166533\n",
      "time/evaluation sampling (s)                           22.9577\n",
      "time/exploration sampling (s)                           1.09225\n",
      "time/logging (s)                                        0.255528\n",
      "time/saving (s)                                         0.0292716\n",
      "time/training (s)                                       4.464\n",
      "time/epoch (s)                                         28.8154\n",
      "time/total (s)                                       5686.95\n",
      "Epoch                                                 185\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:47:24.070776 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 186 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 188000\n",
      "trainer/QF1 Loss                                        4.7112\n",
      "trainer/QF2 Loss                                        4.66615\n",
      "trainer/Policy Loss                                   -10.7512\n",
      "trainer/Q1 Predictions Mean                            16.9671\n",
      "trainer/Q1 Predictions Std                             30.3445\n",
      "trainer/Q1 Predictions Max                            128.021\n",
      "trainer/Q1 Predictions Min                            -20.0782\n",
      "trainer/Q2 Predictions Mean                            17.2627\n",
      "trainer/Q2 Predictions Std                             30.5552\n",
      "trainer/Q2 Predictions Max                            128.237\n",
      "trainer/Q2 Predictions Min                            -19.2902\n",
      "trainer/Q Targets Mean                                 17.1631\n",
      "trainer/Q Targets Std                                  30.6966\n",
      "trainer/Q Targets Max                                 132.191\n",
      "trainer/Q Targets Min                                 -19.6161\n",
      "trainer/Log Pis Mean                                    6.51621\n",
      "trainer/Log Pis Std                                     6.31175\n",
      "trainer/Log Pis Max                                    35.6577\n",
      "trainer/Log Pis Min                                    -8.41166\n",
      "trainer/Policy mu Mean                                  0.0380223\n",
      "trainer/Policy mu Std                                   1.66652\n",
      "trainer/Policy mu Max                                   6.02095\n",
      "trainer/Policy mu Min                                  -6.36528\n",
      "trainer/Policy log std Mean                            -0.696358\n",
      "trainer/Policy log std Std                              0.311766\n",
      "trainer/Policy log std Max                              0.369468\n",
      "trainer/Policy log std Min                             -2.09955\n",
      "trainer/Alpha                                           0.0226867\n",
      "trainer/Alpha Loss                                      1.95632\n",
      "exploration/num steps total                        188000\n",
      "exploration/num paths total                           188\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.119009\n",
      "exploration/Rewards Std                                 1.11116\n",
      "exploration/Rewards Max                                 6.69289\n",
      "exploration/Rewards Min                                -6.55278\n",
      "exploration/Returns Mean                              119.009\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               119.009\n",
      "exploration/Returns Min                               119.009\n",
      "exploration/Actions Mean                                0.114591\n",
      "exploration/Actions Std                                 0.756772\n",
      "exploration/Actions Max                                 0.999999\n",
      "exploration/Actions Min                                -0.999981\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           119.009\n",
      "exploration/env_infos/final/reward_run Mean            -0.172465\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.172465\n",
      "exploration/env_infos/final/reward_run Min             -0.172465\n",
      "exploration/env_infos/initial/reward_run Mean           0.463015\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.463015\n",
      "exploration/env_infos/initial/reward_run Min            0.463015\n",
      "exploration/env_infos/reward_run Mean                   0.0265167\n",
      "exploration/env_infos/reward_run Std                    0.380235\n",
      "exploration/env_infos/reward_run Max                    1.98222\n",
      "exploration/env_infos/reward_run Min                   -2.61804\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.345774\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.345774\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.345774\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.474271\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.474271\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.474271\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.351501\n",
      "exploration/env_infos/reward_ctrl Std                   0.084327\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0559378\n",
      "exploration/env_infos/reward_ctrl Min                  -0.572395\n",
      "exploration/env_infos/final/height Mean                -0.57611\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.57611\n",
      "exploration/env_infos/final/height Min                 -0.57611\n",
      "exploration/env_infos/initial/height Mean              -0.0791171\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0791171\n",
      "exploration/env_infos/initial/height Min               -0.0791171\n",
      "exploration/env_infos/height Mean                      -0.409564\n",
      "exploration/env_infos/height Std                        0.222926\n",
      "exploration/env_infos/height Max                        0.329021\n",
      "exploration/env_infos/height Min                       -0.578637\n",
      "exploration/env_infos/final/reward_angular Mean        -0.00165884\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.00165884\n",
      "exploration/env_infos/final/reward_angular Min         -0.00165884\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.97832\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.97832\n",
      "exploration/env_infos/initial/reward_angular Min       -1.97832\n",
      "exploration/env_infos/reward_angular Mean               0.198364\n",
      "exploration/env_infos/reward_angular Std                1.11424\n",
      "exploration/env_infos/reward_angular Max                6.97163\n",
      "exploration/env_infos/reward_angular Min               -6.50962\n",
      "evaluation/num steps total                              4.675e+06\n",
      "evaluation/num paths total                           4675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.795152\n",
      "evaluation/Rewards Std                                  1.68989\n",
      "evaluation/Rewards Max                                  8.47931\n",
      "evaluation/Rewards Min                                 -8.49547\n",
      "evaluation/Returns Mean                               795.152\n",
      "evaluation/Returns Std                                908.55\n",
      "evaluation/Returns Max                               2726.36\n",
      "evaluation/Returns Min                               -373.165\n",
      "evaluation/Actions Mean                                 0.198763\n",
      "evaluation/Actions Std                                  0.798502\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            795.152\n",
      "evaluation/env_infos/final/reward_run Mean             -1.52422\n",
      "evaluation/env_infos/final/reward_run Std               3.49144\n",
      "evaluation/env_infos/final/reward_run Max               3.9678\n",
      "evaluation/env_infos/final/reward_run Min              -6.65363\n",
      "evaluation/env_infos/initial/reward_run Mean            0.279889\n",
      "evaluation/env_infos/initial/reward_run Std             0.514469\n",
      "evaluation/env_infos/initial/reward_run Max             1.14835\n",
      "evaluation/env_infos/initial/reward_run Min            -0.637462\n",
      "evaluation/env_infos/reward_run Mean                   -1.20043\n",
      "evaluation/env_infos/reward_run Std                     3.08953\n",
      "evaluation/env_infos/reward_run Max                     5.84455\n",
      "evaluation/env_infos/reward_run Min                    -8.21097\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.419411\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0930797\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.224037\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.561432\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.343471\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.107061\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.126528\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.54449\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.406267\n",
      "evaluation/env_infos/reward_ctrl Std                    0.105689\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0319545\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597409\n",
      "evaluation/env_infos/final/height Mean                 -0.106856\n",
      "evaluation/env_infos/final/height Std                   0.270484\n",
      "evaluation/env_infos/final/height Max                   0.257083\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00536255\n",
      "evaluation/env_infos/initial/height Std                 0.0474402\n",
      "evaluation/env_infos/initial/height Max                 0.0774596\n",
      "evaluation/env_infos/initial/height Min                -0.080316\n",
      "evaluation/env_infos/height Mean                       -0.107038\n",
      "evaluation/env_infos/height Std                         0.229732\n",
      "evaluation/env_infos/height Max                         0.469032\n",
      "evaluation/env_infos/height Min                        -0.598746\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.905616\n",
      "evaluation/env_infos/final/reward_angular Std           1.95014\n",
      "evaluation/env_infos/final/reward_angular Max           3.00958\n",
      "evaluation/env_infos/final/reward_angular Min          -6.23298\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.495576\n",
      "evaluation/env_infos/initial/reward_angular Std         0.768708\n",
      "evaluation/env_infos/initial/reward_angular Max         1.93273\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.37082\n",
      "evaluation/env_infos/reward_angular Mean                0.0744373\n",
      "evaluation/env_infos/reward_angular Std                 2.38965\n",
      "evaluation/env_infos/reward_angular Max                 9.99429\n",
      "evaluation/env_infos/reward_angular Min                -7.3899\n",
      "time/data storing (s)                                   0.0151122\n",
      "time/evaluation sampling (s)                           26.8904\n",
      "time/exploration sampling (s)                           1.03028\n",
      "time/logging (s)                                        0.237595\n",
      "time/saving (s)                                         0.0298863\n",
      "time/training (s)                                       4.13628\n",
      "time/epoch (s)                                         32.3396\n",
      "time/total (s)                                       5720.98\n",
      "Epoch                                                 186\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:47:53.466491 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 187 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 189000\n",
      "trainer/QF1 Loss                                        1.76986\n",
      "trainer/QF2 Loss                                        2.05596\n",
      "trainer/Policy Loss                                   -11.2226\n",
      "trainer/Q1 Predictions Mean                            17.3451\n",
      "trainer/Q1 Predictions Std                             31.9163\n",
      "trainer/Q1 Predictions Max                            144.657\n",
      "trainer/Q1 Predictions Min                            -19.4828\n",
      "trainer/Q2 Predictions Mean                            17.4382\n",
      "trainer/Q2 Predictions Std                             31.9197\n",
      "trainer/Q2 Predictions Max                            145.207\n",
      "trainer/Q2 Predictions Min                            -19.4385\n",
      "trainer/Q Targets Mean                                 17.2703\n",
      "trainer/Q Targets Std                                  31.8416\n",
      "trainer/Q Targets Max                                 144.711\n",
      "trainer/Q Targets Min                                 -19.6634\n",
      "trainer/Log Pis Mean                                    6.34317\n",
      "trainer/Log Pis Std                                     5.83789\n",
      "trainer/Log Pis Max                                    24.5472\n",
      "trainer/Log Pis Min                                    -5.64832\n",
      "trainer/Policy mu Mean                                 -0.0716348\n",
      "trainer/Policy mu Std                                   1.62424\n",
      "trainer/Policy mu Max                                   4.09835\n",
      "trainer/Policy mu Min                                  -5.04668\n",
      "trainer/Policy log std Mean                            -0.642915\n",
      "trainer/Policy log std Std                              0.316744\n",
      "trainer/Policy log std Max                              0.397967\n",
      "trainer/Policy log std Min                             -2.04992\n",
      "trainer/Alpha                                           0.0229505\n",
      "trainer/Alpha Loss                                      1.29478\n",
      "exploration/num steps total                        189000\n",
      "exploration/num paths total                           189\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.545863\n",
      "exploration/Rewards Std                                 1.19038\n",
      "exploration/Rewards Max                                 3.88358\n",
      "exploration/Rewards Min                                -3.48382\n",
      "exploration/Returns Mean                              545.863\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               545.863\n",
      "exploration/Returns Min                               545.863\n",
      "exploration/Actions Mean                                0.161568\n",
      "exploration/Actions Std                                 0.776848\n",
      "exploration/Actions Max                                 0.999998\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           545.863\n",
      "exploration/env_infos/final/reward_run Mean             1.78683\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.78683\n",
      "exploration/env_infos/final/reward_run Min              1.78683\n",
      "exploration/env_infos/initial/reward_run Mean           0.793519\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.793519\n",
      "exploration/env_infos/initial/reward_run Min            0.793519\n",
      "exploration/env_infos/reward_run Mean                   1.83383\n",
      "exploration/env_infos/reward_run Std                    0.925684\n",
      "exploration/env_infos/reward_run Max                    4.49368\n",
      "exploration/env_infos/reward_run Min                   -0.751949\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.200657\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.200657\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.200657\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.371736\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.371736\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.371736\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.377758\n",
      "exploration/env_infos/reward_ctrl Std                   0.107341\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0678529\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593795\n",
      "exploration/env_infos/final/height Mean                -0.12091\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.12091\n",
      "exploration/env_infos/final/height Min                 -0.12091\n",
      "exploration/env_infos/initial/height Mean              -0.0776596\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0776596\n",
      "exploration/env_infos/initial/height Min               -0.0776596\n",
      "exploration/env_infos/height Mean                      -0.0619874\n",
      "exploration/env_infos/height Std                        0.120311\n",
      "exploration/env_infos/height Max                        0.37432\n",
      "exploration/env_infos/height Min                       -0.386632\n",
      "exploration/env_infos/final/reward_angular Mean        -1.13654\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.13654\n",
      "exploration/env_infos/final/reward_angular Min         -1.13654\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.20954\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.20954\n",
      "exploration/env_infos/initial/reward_angular Min       -1.20954\n",
      "exploration/env_infos/reward_angular Mean              -0.0576478\n",
      "exploration/env_infos/reward_angular Std                2.33224\n",
      "exploration/env_infos/reward_angular Max                6.1428\n",
      "exploration/env_infos/reward_angular Min               -6.58049\n",
      "evaluation/num steps total                              4.7e+06\n",
      "evaluation/num paths total                           4700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.461902\n",
      "evaluation/Rewards Std                                  1.48231\n",
      "evaluation/Rewards Max                                  8.99852\n",
      "evaluation/Rewards Min                                 -8.86594\n",
      "evaluation/Returns Mean                               461.902\n",
      "evaluation/Returns Std                                766.845\n",
      "evaluation/Returns Max                               2188.89\n",
      "evaluation/Returns Min                               -455.152\n",
      "evaluation/Actions Mean                                 0.0444015\n",
      "evaluation/Actions Std                                  0.766087\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            461.902\n",
      "evaluation/env_infos/final/reward_run Mean              0.466181\n",
      "evaluation/env_infos/final/reward_run Std               2.31661\n",
      "evaluation/env_infos/final/reward_run Max               5.29965\n",
      "evaluation/env_infos/final/reward_run Min              -6.34775\n",
      "evaluation/env_infos/initial/reward_run Mean            0.139969\n",
      "evaluation/env_infos/initial/reward_run Std             0.448087\n",
      "evaluation/env_infos/initial/reward_run Max             0.835285\n",
      "evaluation/env_infos/initial/reward_run Min            -0.700597\n",
      "evaluation/env_infos/reward_run Mean                   -0.320927\n",
      "evaluation/env_infos/reward_run Std                     2.4789\n",
      "evaluation/env_infos/reward_run Max                     5.60604\n",
      "evaluation/env_infos/reward_run Min                    -8.34717\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.329964\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.131535\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.11379\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.583082\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.281355\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.10037\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0687752\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.473142\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.353316\n",
      "evaluation/env_infos/reward_ctrl Std                    0.125277\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0417025\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596777\n",
      "evaluation/env_infos/final/height Mean                 -0.301967\n",
      "evaluation/env_infos/final/height Std                   0.229782\n",
      "evaluation/env_infos/final/height Max                   0.0923645\n",
      "evaluation/env_infos/final/height Min                  -0.580011\n",
      "evaluation/env_infos/initial/height Mean               -0.00426135\n",
      "evaluation/env_infos/initial/height Std                 0.0578783\n",
      "evaluation/env_infos/initial/height Max                 0.0755927\n",
      "evaluation/env_infos/initial/height Min                -0.110579\n",
      "evaluation/env_infos/height Mean                       -0.203865\n",
      "evaluation/env_infos/height Std                         0.248639\n",
      "evaluation/env_infos/height Max                         0.506645\n",
      "evaluation/env_infos/height Min                        -0.587337\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.211963\n",
      "evaluation/env_infos/final/reward_angular Std           1.29185\n",
      "evaluation/env_infos/final/reward_angular Max           1.64238\n",
      "evaluation/env_infos/final/reward_angular Min          -4.54368\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.149194\n",
      "evaluation/env_infos/initial/reward_angular Std         0.979095\n",
      "evaluation/env_infos/initial/reward_angular Max         2.81967\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.75404\n",
      "evaluation/env_infos/reward_angular Mean                0.020341\n",
      "evaluation/env_infos/reward_angular Std                 1.88728\n",
      "evaluation/env_infos/reward_angular Max                10.481\n",
      "evaluation/env_infos/reward_angular Min                -7.73456\n",
      "time/data storing (s)                                   0.0157107\n",
      "time/evaluation sampling (s)                           21.4346\n",
      "time/exploration sampling (s)                           1.13368\n",
      "time/logging (s)                                        0.242165\n",
      "time/saving (s)                                         0.0866996\n",
      "time/training (s)                                       5.05701\n",
      "time/epoch (s)                                         27.9699\n",
      "time/total (s)                                       5750.38\n",
      "Epoch                                                 187\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:48:22.353312 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 188 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 190000\n",
      "trainer/QF1 Loss                                        3.09884\n",
      "trainer/QF2 Loss                                        3.47703\n",
      "trainer/Policy Loss                                   -12.6916\n",
      "trainer/Q1 Predictions Mean                            20.0165\n",
      "trainer/Q1 Predictions Std                             32.733\n",
      "trainer/Q1 Predictions Max                            135.039\n",
      "trainer/Q1 Predictions Min                            -18.8278\n",
      "trainer/Q2 Predictions Mean                            19.6482\n",
      "trainer/Q2 Predictions Std                             32.5258\n",
      "trainer/Q2 Predictions Max                            136.27\n",
      "trainer/Q2 Predictions Min                            -19.2189\n",
      "trainer/Q Targets Mean                                 19.9266\n",
      "trainer/Q Targets Std                                  32.3444\n",
      "trainer/Q Targets Max                                 130.641\n",
      "trainer/Q Targets Min                                 -18.3379\n",
      "trainer/Log Pis Mean                                    7.36729\n",
      "trainer/Log Pis Std                                     7.14851\n",
      "trainer/Log Pis Max                                    41.0425\n",
      "trainer/Log Pis Min                                    -4.14525\n",
      "trainer/Policy mu Mean                                 -0.0255676\n",
      "trainer/Policy mu Std                                   1.75068\n",
      "trainer/Policy mu Max                                   5.45952\n",
      "trainer/Policy mu Min                                  -5.49221\n",
      "trainer/Policy log std Mean                            -0.70075\n",
      "trainer/Policy log std Std                              0.309191\n",
      "trainer/Policy log std Max                              0.358814\n",
      "trainer/Policy log std Min                             -2.16112\n",
      "trainer/Alpha                                           0.0222541\n",
      "trainer/Alpha Loss                                      5.20633\n",
      "exploration/num steps total                        190000\n",
      "exploration/num paths total                           190\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0721324\n",
      "exploration/Rewards Std                                 0.786655\n",
      "exploration/Rewards Max                                 3.55667\n",
      "exploration/Rewards Min                                -3.86548\n",
      "exploration/Returns Mean                              -72.1324\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -72.1324\n",
      "exploration/Returns Min                               -72.1324\n",
      "exploration/Actions Mean                                0.00143067\n",
      "exploration/Actions Std                                 0.749821\n",
      "exploration/Actions Max                                 0.99977\n",
      "exploration/Actions Min                                -0.999491\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -72.1324\n",
      "exploration/env_infos/final/reward_run Mean            -0.0567872\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.0567872\n",
      "exploration/env_infos/final/reward_run Min             -0.0567872\n",
      "exploration/env_infos/initial/reward_run Mean           0.377751\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.377751\n",
      "exploration/env_infos/initial/reward_run Min            0.377751\n",
      "exploration/env_infos/reward_run Mean                   0.0994634\n",
      "exploration/env_infos/reward_run Std                    0.338818\n",
      "exploration/env_infos/reward_run Max                    2.27914\n",
      "exploration/env_infos/reward_run Min                   -0.85812\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.251333\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.251333\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.251333\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.220083\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.220083\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.220083\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.33734\n",
      "exploration/env_infos/reward_ctrl Std                   0.0716419\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0792756\n",
      "exploration/env_infos/reward_ctrl Min                  -0.513243\n",
      "exploration/env_infos/final/height Mean                -0.123867\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.123867\n",
      "exploration/env_infos/final/height Min                 -0.123867\n",
      "exploration/env_infos/initial/height Mean              -0.0190994\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0190994\n",
      "exploration/env_infos/initial/height Min               -0.0190994\n",
      "exploration/env_infos/height Mean                      -0.174514\n",
      "exploration/env_infos/height Std                        0.0516333\n",
      "exploration/env_infos/height Max                        0.1711\n",
      "exploration/env_infos/height Min                       -0.245394\n",
      "exploration/env_infos/final/reward_angular Mean         0.0969929\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0969929\n",
      "exploration/env_infos/final/reward_angular Min          0.0969929\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.398695\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.398695\n",
      "exploration/env_infos/initial/reward_angular Min       -0.398695\n",
      "exploration/env_infos/reward_angular Mean               0.00287517\n",
      "exploration/env_infos/reward_angular Std                0.83251\n",
      "exploration/env_infos/reward_angular Max                4.05186\n",
      "exploration/env_infos/reward_angular Min               -3.76007\n",
      "evaluation/num steps total                              4.725e+06\n",
      "evaluation/num paths total                           4725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.715972\n",
      "evaluation/Rewards Std                                  1.56676\n",
      "evaluation/Rewards Max                                 10.0222\n",
      "evaluation/Rewards Min                                 -8.65874\n",
      "evaluation/Returns Mean                               715.972\n",
      "evaluation/Returns Std                                883.455\n",
      "evaluation/Returns Max                               2547.93\n",
      "evaluation/Returns Min                               -432.615\n",
      "evaluation/Actions Mean                                 0.090505\n",
      "evaluation/Actions Std                                  0.805473\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            715.972\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0426633\n",
      "evaluation/env_infos/final/reward_run Std               2.47477\n",
      "evaluation/env_infos/final/reward_run Max               4.94086\n",
      "evaluation/env_infos/final/reward_run Min              -5.21916\n",
      "evaluation/env_infos/initial/reward_run Mean            0.165973\n",
      "evaluation/env_infos/initial/reward_run Std             0.554719\n",
      "evaluation/env_infos/initial/reward_run Max             1.04652\n",
      "evaluation/env_infos/initial/reward_run Min            -0.88372\n",
      "evaluation/env_infos/reward_run Mean                   -0.614182\n",
      "evaluation/env_infos/reward_run Std                     2.88068\n",
      "evaluation/env_infos/reward_run Max                     5.55088\n",
      "evaluation/env_infos/reward_run Min                    -7.78178\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.348622\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.134413\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0860188\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.548605\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.28445\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0932328\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0932986\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.43459\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.394187\n",
      "evaluation/env_infos/reward_ctrl Std                    0.120269\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0341558\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598459\n",
      "evaluation/env_infos/final/height Mean                 -0.205237\n",
      "evaluation/env_infos/final/height Std                   0.293118\n",
      "evaluation/env_infos/final/height Max                   0.234037\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00733125\n",
      "evaluation/env_infos/initial/height Std                 0.0484797\n",
      "evaluation/env_infos/initial/height Max                 0.078877\n",
      "evaluation/env_infos/initial/height Min                -0.0919761\n",
      "evaluation/env_infos/height Mean                       -0.165988\n",
      "evaluation/env_infos/height Std                         0.260354\n",
      "evaluation/env_infos/height Max                         0.492283\n",
      "evaluation/env_infos/height Min                        -0.586541\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.121129\n",
      "evaluation/env_infos/final/reward_angular Std           1.59353\n",
      "evaluation/env_infos/final/reward_angular Max           4.18922\n",
      "evaluation/env_infos/final/reward_angular Min          -2.34744\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.507839\n",
      "evaluation/env_infos/initial/reward_angular Std         0.885415\n",
      "evaluation/env_infos/initial/reward_angular Max         2.21216\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.65548\n",
      "evaluation/env_infos/reward_angular Mean                0.034494\n",
      "evaluation/env_infos/reward_angular Std                 2.11646\n",
      "evaluation/env_infos/reward_angular Max                11.3628\n",
      "evaluation/env_infos/reward_angular Min                -6.70513\n",
      "time/data storing (s)                                   0.015842\n",
      "time/evaluation sampling (s)                           21.8667\n",
      "time/exploration sampling (s)                           1.05518\n",
      "time/logging (s)                                        0.236196\n",
      "time/saving (s)                                         0.0278484\n",
      "time/training (s)                                       4.1385\n",
      "time/epoch (s)                                         27.3403\n",
      "time/total (s)                                       5779.26\n",
      "Epoch                                                 188\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:48:53.455975 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 189 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 191000\n",
      "trainer/QF1 Loss                                        1.88508\n",
      "trainer/QF2 Loss                                        1.7499\n",
      "trainer/Policy Loss                                   -14.6655\n",
      "trainer/Q1 Predictions Mean                            20.6954\n",
      "trainer/Q1 Predictions Std                             32.5248\n",
      "trainer/Q1 Predictions Max                            142.592\n",
      "trainer/Q1 Predictions Min                            -18.9738\n",
      "trainer/Q2 Predictions Mean                            20.834\n",
      "trainer/Q2 Predictions Std                             32.5719\n",
      "trainer/Q2 Predictions Max                            145.546\n",
      "trainer/Q2 Predictions Min                            -18.8279\n",
      "trainer/Q Targets Mean                                 21.0372\n",
      "trainer/Q Targets Std                                  32.622\n",
      "trainer/Q Targets Max                                 143.884\n",
      "trainer/Q Targets Min                                 -18.6198\n",
      "trainer/Log Pis Mean                                    6.30891\n",
      "trainer/Log Pis Std                                     5.87878\n",
      "trainer/Log Pis Max                                    27.754\n",
      "trainer/Log Pis Min                                    -9.06394\n",
      "trainer/Policy mu Mean                                 -0.112659\n",
      "trainer/Policy mu Std                                   1.66986\n",
      "trainer/Policy mu Max                                   6.49215\n",
      "trainer/Policy mu Min                                  -6.2235\n",
      "trainer/Policy log std Mean                            -0.712657\n",
      "trainer/Policy log std Std                              0.268895\n",
      "trainer/Policy log std Max                              0.172517\n",
      "trainer/Policy log std Min                             -2.10166\n",
      "trainer/Alpha                                           0.0232882\n",
      "trainer/Alpha Loss                                      1.16186\n",
      "exploration/num steps total                        191000\n",
      "exploration/num paths total                           191\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.449375\n",
      "exploration/Rewards Std                                 1.1564\n",
      "exploration/Rewards Max                                 2.33932\n",
      "exploration/Rewards Min                                -4.70321\n",
      "exploration/Returns Mean                             -449.375\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -449.375\n",
      "exploration/Returns Min                              -449.375\n",
      "exploration/Actions Mean                                0.0917087\n",
      "exploration/Actions Std                                 0.765548\n",
      "exploration/Actions Max                                 0.999827\n",
      "exploration/Actions Min                                -0.999068\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -449.375\n",
      "exploration/env_infos/final/reward_run Mean            -5.20687\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -5.20687\n",
      "exploration/env_infos/final/reward_run Min             -5.20687\n",
      "exploration/env_infos/initial/reward_run Mean          -0.372198\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.372198\n",
      "exploration/env_infos/initial/reward_run Min           -0.372198\n",
      "exploration/env_infos/reward_run Mean                  -3.35465\n",
      "exploration/env_infos/reward_run Std                    1.79261\n",
      "exploration/env_infos/reward_run Max                    0.942683\n",
      "exploration/env_infos/reward_run Min                   -7.11545\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.547617\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.547617\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.547617\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.282039\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.282039\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.282039\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.356684\n",
      "exploration/env_infos/reward_ctrl Std                   0.0956591\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0561367\n",
      "exploration/env_infos/reward_ctrl Min                  -0.552958\n",
      "exploration/env_infos/final/height Mean                -0.173553\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.173553\n",
      "exploration/env_infos/final/height Min                 -0.173553\n",
      "exploration/env_infos/initial/height Mean               0.0588561\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0588561\n",
      "exploration/env_infos/initial/height Min                0.0588561\n",
      "exploration/env_infos/height Mean                      -0.0495631\n",
      "exploration/env_infos/height Std                        0.0997136\n",
      "exploration/env_infos/height Max                        0.325176\n",
      "exploration/env_infos/height Min                       -0.314082\n",
      "exploration/env_infos/final/reward_angular Mean        -2.83937\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.83937\n",
      "exploration/env_infos/final/reward_angular Min         -2.83937\n",
      "exploration/env_infos/initial/reward_angular Mean       0.281285\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.281285\n",
      "exploration/env_infos/initial/reward_angular Min        0.281285\n",
      "exploration/env_infos/reward_angular Mean               0.071006\n",
      "exploration/env_infos/reward_angular Std                2.40028\n",
      "exploration/env_infos/reward_angular Max                8.56224\n",
      "exploration/env_infos/reward_angular Min               -5.63803\n",
      "evaluation/num steps total                              4.75e+06\n",
      "evaluation/num paths total                           4750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.747195\n",
      "evaluation/Rewards Std                                  1.57323\n",
      "evaluation/Rewards Max                                  8.18062\n",
      "evaluation/Rewards Min                                 -9.05886\n",
      "evaluation/Returns Mean                               747.195\n",
      "evaluation/Returns Std                                945.631\n",
      "evaluation/Returns Max                               2735.52\n",
      "evaluation/Returns Min                               -437.864\n",
      "evaluation/Actions Mean                                -0.0329994\n",
      "evaluation/Actions Std                                  0.811358\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            747.195\n",
      "evaluation/env_infos/final/reward_run Mean             -0.465035\n",
      "evaluation/env_infos/final/reward_run Std               2.41848\n",
      "evaluation/env_infos/final/reward_run Max               3.73889\n",
      "evaluation/env_infos/final/reward_run Min              -5.94975\n",
      "evaluation/env_infos/initial/reward_run Mean            0.261694\n",
      "evaluation/env_infos/initial/reward_run Std             0.439761\n",
      "evaluation/env_infos/initial/reward_run Max             0.98168\n",
      "evaluation/env_infos/initial/reward_run Min            -0.688612\n",
      "evaluation/env_infos/reward_run Mean                   -0.658993\n",
      "evaluation/env_infos/reward_run Std                     2.88135\n",
      "evaluation/env_infos/reward_run Max                     5.74033\n",
      "evaluation/env_infos/reward_run Min                    -7.77689\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.382738\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.120783\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0909339\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.56387\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.268963\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0869175\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0962071\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.445668\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.395635\n",
      "evaluation/env_infos/reward_ctrl Std                    0.099103\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0251926\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596683\n",
      "evaluation/env_infos/final/height Mean                 -0.0985777\n",
      "evaluation/env_infos/final/height Std                   0.186525\n",
      "evaluation/env_infos/final/height Max                   0.205298\n",
      "evaluation/env_infos/final/height Min                  -0.577327\n",
      "evaluation/env_infos/initial/height Mean               -0.0068976\n",
      "evaluation/env_infos/initial/height Std                 0.0535026\n",
      "evaluation/env_infos/initial/height Max                 0.0849911\n",
      "evaluation/env_infos/initial/height Min                -0.11058\n",
      "evaluation/env_infos/height Mean                       -0.0973949\n",
      "evaluation/env_infos/height Std                         0.170187\n",
      "evaluation/env_infos/height Max                         0.544107\n",
      "evaluation/env_infos/height Min                        -0.585455\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.442724\n",
      "evaluation/env_infos/final/reward_angular Std           1.57126\n",
      "evaluation/env_infos/final/reward_angular Max           2.91697\n",
      "evaluation/env_infos/final/reward_angular Min          -4.18222\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.666395\n",
      "evaluation/env_infos/initial/reward_angular Std         0.575103\n",
      "evaluation/env_infos/initial/reward_angular Max         0.420139\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.74514\n",
      "evaluation/env_infos/reward_angular Mean                0.0399327\n",
      "evaluation/env_infos/reward_angular Std                 2.09183\n",
      "evaluation/env_infos/reward_angular Max                10.0254\n",
      "evaluation/env_infos/reward_angular Min                -7.39843\n",
      "time/data storing (s)                                   0.0161321\n",
      "time/evaluation sampling (s)                           22.8786\n",
      "time/exploration sampling (s)                           1.08781\n",
      "time/logging (s)                                        0.244311\n",
      "time/saving (s)                                         0.0352632\n",
      "time/training (s)                                       5.48522\n",
      "time/epoch (s)                                         29.7473\n",
      "time/total (s)                                       5810.36\n",
      "Epoch                                                 189\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:49:23.851275 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 190 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 192000\n",
      "trainer/QF1 Loss                                        1.82614\n",
      "trainer/QF2 Loss                                        1.66803\n",
      "trainer/Policy Loss                                   -13.2011\n",
      "trainer/Q1 Predictions Mean                            19.3031\n",
      "trainer/Q1 Predictions Std                             35.6258\n",
      "trainer/Q1 Predictions Max                            143.586\n",
      "trainer/Q1 Predictions Min                            -18.6673\n",
      "trainer/Q2 Predictions Mean                            19.148\n",
      "trainer/Q2 Predictions Std                             35.6233\n",
      "trainer/Q2 Predictions Max                            141.727\n",
      "trainer/Q2 Predictions Min                            -19.2875\n",
      "trainer/Q Targets Mean                                 19.067\n",
      "trainer/Q Targets Std                                  35.7043\n",
      "trainer/Q Targets Max                                 141.745\n",
      "trainer/Q Targets Min                                 -18.7836\n",
      "trainer/Log Pis Mean                                    6.24362\n",
      "trainer/Log Pis Std                                     5.97373\n",
      "trainer/Log Pis Max                                    21.5256\n",
      "trainer/Log Pis Min                                    -9.67269\n",
      "trainer/Policy mu Mean                                  0.0420871\n",
      "trainer/Policy mu Std                                   1.61668\n",
      "trainer/Policy mu Max                                   4.26849\n",
      "trainer/Policy mu Min                                  -6.03966\n",
      "trainer/Policy log std Mean                            -0.674228\n",
      "trainer/Policy log std Std                              0.304392\n",
      "trainer/Policy log std Max                              0.397714\n",
      "trainer/Policy log std Min                             -1.85951\n",
      "trainer/Alpha                                           0.0233878\n",
      "trainer/Alpha Loss                                      0.915388\n",
      "exploration/num steps total                        192000\n",
      "exploration/num paths total                           192\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                3.61819\n",
      "exploration/Rewards Std                                 1.45457\n",
      "exploration/Rewards Max                                 7.24126\n",
      "exploration/Rewards Min                                -1.86057\n",
      "exploration/Returns Mean                             3618.19\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              3618.19\n",
      "exploration/Returns Min                              3618.19\n",
      "exploration/Actions Mean                                0.119825\n",
      "exploration/Actions Std                                 0.901171\n",
      "exploration/Actions Max                                 0.99999\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          3618.19\n",
      "exploration/env_infos/final/reward_run Mean            -2.96281\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.96281\n",
      "exploration/env_infos/final/reward_run Min             -2.96281\n",
      "exploration/env_infos/initial/reward_run Mean           0.743677\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.743677\n",
      "exploration/env_infos/initial/reward_run Min            0.743677\n",
      "exploration/env_infos/reward_run Mean                  -4.13414\n",
      "exploration/env_infos/reward_run Std                    1.15631\n",
      "exploration/env_infos/reward_run Max                    1.32861\n",
      "exploration/env_infos/reward_run Min                   -7.01286\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.553446\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.553446\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.553446\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.350222\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.350222\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.350222\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.49588\n",
      "exploration/env_infos/reward_ctrl Std                   0.0715826\n",
      "exploration/env_infos/reward_ctrl Max                  -0.229681\n",
      "exploration/env_infos/reward_ctrl Min                  -0.597422\n",
      "exploration/env_infos/final/height Mean                 0.262086\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.262086\n",
      "exploration/env_infos/final/height Min                  0.262086\n",
      "exploration/env_infos/initial/height Mean              -0.014177\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.014177\n",
      "exploration/env_infos/initial/height Min               -0.014177\n",
      "exploration/env_infos/height Mean                       0.00770814\n",
      "exploration/env_infos/height Std                        0.143532\n",
      "exploration/env_infos/height Max                        0.47031\n",
      "exploration/env_infos/height Min                       -0.353086\n",
      "exploration/env_infos/final/reward_angular Mean         4.67654\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          4.67654\n",
      "exploration/env_infos/final/reward_angular Min          4.67654\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.884261\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.884261\n",
      "exploration/env_infos/initial/reward_angular Min       -0.884261\n",
      "exploration/env_infos/reward_angular Mean               0.122741\n",
      "exploration/env_infos/reward_angular Std                3.20761\n",
      "exploration/env_infos/reward_angular Max                9.97918\n",
      "exploration/env_infos/reward_angular Min               -5.70292\n",
      "evaluation/num steps total                              4.775e+06\n",
      "evaluation/num paths total                           4775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.760463\n",
      "evaluation/Rewards Std                                  1.63111\n",
      "evaluation/Rewards Max                                  8.90991\n",
      "evaluation/Rewards Min                                 -7.81674\n",
      "evaluation/Returns Mean                               760.463\n",
      "evaluation/Returns Std                                865.827\n",
      "evaluation/Returns Max                               2699.85\n",
      "evaluation/Returns Min                               -414.632\n",
      "evaluation/Actions Mean                                 0.155169\n",
      "evaluation/Actions Std                                  0.813494\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            760.463\n",
      "evaluation/env_infos/final/reward_run Mean             -1.40645\n",
      "evaluation/env_infos/final/reward_run Std               2.90889\n",
      "evaluation/env_infos/final/reward_run Max               4.79941\n",
      "evaluation/env_infos/final/reward_run Min              -5.31133\n",
      "evaluation/env_infos/initial/reward_run Mean            0.262488\n",
      "evaluation/env_infos/initial/reward_run Std             0.373611\n",
      "evaluation/env_infos/initial/reward_run Max             0.860305\n",
      "evaluation/env_infos/initial/reward_run Min            -0.484389\n",
      "evaluation/env_infos/reward_run Mean                   -1.15117\n",
      "evaluation/env_infos/reward_run Std                     2.8276\n",
      "evaluation/env_infos/reward_run Max                     5.76506\n",
      "evaluation/env_infos/reward_run Min                    -7.33695\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.422124\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.103087\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.260061\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.582523\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.245673\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0954111\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0465115\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.427697\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.41151\n",
      "evaluation/env_infos/reward_ctrl Std                    0.1078\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0083152\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.592917\n",
      "evaluation/env_infos/final/height Mean                 -0.124418\n",
      "evaluation/env_infos/final/height Std                   0.247053\n",
      "evaluation/env_infos/final/height Max                   0.31983\n",
      "evaluation/env_infos/final/height Min                  -0.577349\n",
      "evaluation/env_infos/initial/height Mean               -0.0163787\n",
      "evaluation/env_infos/initial/height Std                 0.0486178\n",
      "evaluation/env_infos/initial/height Max                 0.067015\n",
      "evaluation/env_infos/initial/height Min                -0.0849894\n",
      "evaluation/env_infos/height Mean                       -0.0729727\n",
      "evaluation/env_infos/height Std                         0.219841\n",
      "evaluation/env_infos/height Max                         0.634252\n",
      "evaluation/env_infos/height Min                        -0.58014\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.368808\n",
      "evaluation/env_infos/final/reward_angular Std           2.30096\n",
      "evaluation/env_infos/final/reward_angular Max           5.38084\n",
      "evaluation/env_infos/final/reward_angular Min          -4.70739\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.125784\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11784\n",
      "evaluation/env_infos/initial/reward_angular Max         2.49027\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.78318\n",
      "evaluation/env_infos/reward_angular Mean                0.080925\n",
      "evaluation/env_infos/reward_angular Std                 2.35329\n",
      "evaluation/env_infos/reward_angular Max                11.0881\n",
      "evaluation/env_infos/reward_angular Min                -6.92599\n",
      "time/data storing (s)                                   0.0160294\n",
      "time/evaluation sampling (s)                           22.9328\n",
      "time/exploration sampling (s)                           1.1246\n",
      "time/logging (s)                                        0.236725\n",
      "time/saving (s)                                         0.0301152\n",
      "time/training (s)                                       4.56362\n",
      "time/epoch (s)                                         28.9039\n",
      "time/total (s)                                       5840.74\n",
      "Epoch                                                 190\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:49:55.843327 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 191 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 193000\n",
      "trainer/QF1 Loss                                        2.10777\n",
      "trainer/QF2 Loss                                        2.08386\n",
      "trainer/Policy Loss                                   -12.4137\n",
      "trainer/Q1 Predictions Mean                            18.7214\n",
      "trainer/Q1 Predictions Std                             33.9581\n",
      "trainer/Q1 Predictions Max                            143.971\n",
      "trainer/Q1 Predictions Min                            -19.2183\n",
      "trainer/Q2 Predictions Mean                            18.5837\n",
      "trainer/Q2 Predictions Std                             34.0417\n",
      "trainer/Q2 Predictions Max                            144.391\n",
      "trainer/Q2 Predictions Min                            -18.9118\n",
      "trainer/Q Targets Mean                                 18.8795\n",
      "trainer/Q Targets Std                                  34.0542\n",
      "trainer/Q Targets Max                                 143.152\n",
      "trainer/Q Targets Min                                 -19.1678\n",
      "trainer/Log Pis Mean                                    6.42224\n",
      "trainer/Log Pis Std                                     5.61653\n",
      "trainer/Log Pis Max                                    23.7924\n",
      "trainer/Log Pis Min                                    -6.14135\n",
      "trainer/Policy mu Mean                                  0.0292795\n",
      "trainer/Policy mu Std                                   1.58516\n",
      "trainer/Policy mu Max                                   4.23205\n",
      "trainer/Policy mu Min                                  -5.07884\n",
      "trainer/Policy log std Mean                            -0.716623\n",
      "trainer/Policy log std Std                              0.279701\n",
      "trainer/Policy log std Max                              0.230358\n",
      "trainer/Policy log std Min                             -2.20507\n",
      "trainer/Alpha                                           0.0228296\n",
      "trainer/Alpha Loss                                      1.5958\n",
      "exploration/num steps total                        193000\n",
      "exploration/num paths total                           193\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.226769\n",
      "exploration/Rewards Std                                 0.426176\n",
      "exploration/Rewards Max                                 0.916393\n",
      "exploration/Rewards Min                                -1.97703\n",
      "exploration/Returns Mean                             -226.769\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -226.769\n",
      "exploration/Returns Min                              -226.769\n",
      "exploration/Actions Mean                               -0.0204892\n",
      "exploration/Actions Std                                 0.577858\n",
      "exploration/Actions Max                                 0.999502\n",
      "exploration/Actions Min                                -0.999956\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -226.769\n",
      "exploration/env_infos/final/reward_run Mean            -0.471859\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.471859\n",
      "exploration/env_infos/final/reward_run Min             -0.471859\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0124909\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0124909\n",
      "exploration/env_infos/initial/reward_run Min           -0.0124909\n",
      "exploration/env_infos/reward_run Mean                  -0.160726\n",
      "exploration/env_infos/reward_run Std                    0.62609\n",
      "exploration/env_infos/reward_run Max                    1.89018\n",
      "exploration/env_infos/reward_run Min                   -1.84754\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.250041\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.250041\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.250041\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0925192\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0925192\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0925192\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.200604\n",
      "exploration/env_infos/reward_ctrl Std                   0.0891493\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0139783\n",
      "exploration/env_infos/reward_ctrl Min                  -0.486322\n",
      "exploration/env_infos/final/height Mean                -0.125306\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.125306\n",
      "exploration/env_infos/final/height Min                 -0.125306\n",
      "exploration/env_infos/initial/height Mean              -0.0870846\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0870846\n",
      "exploration/env_infos/initial/height Min               -0.0870846\n",
      "exploration/env_infos/height Mean                      -0.142833\n",
      "exploration/env_infos/height Std                        0.0627378\n",
      "exploration/env_infos/height Max                        0.0433665\n",
      "exploration/env_infos/height Min                       -0.378191\n",
      "exploration/env_infos/final/reward_angular Mean         0.0733401\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0733401\n",
      "exploration/env_infos/final/reward_angular Min          0.0733401\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.222087\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.222087\n",
      "exploration/env_infos/initial/reward_angular Min       -0.222087\n",
      "exploration/env_infos/reward_angular Mean               0.00388629\n",
      "exploration/env_infos/reward_angular Std                1.47201\n",
      "exploration/env_infos/reward_angular Max                5.69563\n",
      "exploration/env_infos/reward_angular Min               -4.10066\n",
      "evaluation/num steps total                              4.8e+06\n",
      "evaluation/num paths total                           4800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.68775\n",
      "evaluation/Rewards Std                                  1.65676\n",
      "evaluation/Rewards Max                                  9.10393\n",
      "evaluation/Rewards Min                                 -7.74548\n",
      "evaluation/Returns Mean                               687.75\n",
      "evaluation/Returns Std                                859.502\n",
      "evaluation/Returns Max                               2596.9\n",
      "evaluation/Returns Min                               -425.408\n",
      "evaluation/Actions Mean                                 0.152689\n",
      "evaluation/Actions Std                                  0.805611\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            687.75\n",
      "evaluation/env_infos/final/reward_run Mean             -1.21428\n",
      "evaluation/env_infos/final/reward_run Std               2.25086\n",
      "evaluation/env_infos/final/reward_run Max               2.97823\n",
      "evaluation/env_infos/final/reward_run Min              -4.76228\n",
      "evaluation/env_infos/initial/reward_run Mean            0.201934\n",
      "evaluation/env_infos/initial/reward_run Std             0.588859\n",
      "evaluation/env_infos/initial/reward_run Max             1.16274\n",
      "evaluation/env_infos/initial/reward_run Min            -0.858728\n",
      "evaluation/env_infos/reward_run Mean                   -1.11425\n",
      "evaluation/env_infos/reward_run Std                     2.73675\n",
      "evaluation/env_infos/reward_run Max                     5.73651\n",
      "evaluation/env_infos/reward_run Min                    -7.91363\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.412192\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.126379\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0626811\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.566345\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.27073\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0842698\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0941839\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.416423\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.403393\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111582\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0172122\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.592479\n",
      "evaluation/env_infos/final/height Mean                 -0.151562\n",
      "evaluation/env_infos/final/height Std                   0.274206\n",
      "evaluation/env_infos/final/height Max                   0.150634\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0235918\n",
      "evaluation/env_infos/initial/height Std                 0.042891\n",
      "evaluation/env_infos/initial/height Max                 0.0728605\n",
      "evaluation/env_infos/initial/height Min                -0.0817427\n",
      "evaluation/env_infos/height Mean                       -0.140261\n",
      "evaluation/env_infos/height Std                         0.260435\n",
      "evaluation/env_infos/height Max                         0.5179\n",
      "evaluation/env_infos/height Min                        -0.586313\n",
      "evaluation/env_infos/final/reward_angular Mean          0.188416\n",
      "evaluation/env_infos/final/reward_angular Std           2.15967\n",
      "evaluation/env_infos/final/reward_angular Max           3.98416\n",
      "evaluation/env_infos/final/reward_angular Min          -4.73044\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.0423532\n",
      "evaluation/env_infos/initial/reward_angular Std         1.25497\n",
      "evaluation/env_infos/initial/reward_angular Max         2.21111\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.88493\n",
      "evaluation/env_infos/reward_angular Mean                0.0899857\n",
      "evaluation/env_infos/reward_angular Std                 2.32057\n",
      "evaluation/env_infos/reward_angular Max                10.1592\n",
      "evaluation/env_infos/reward_angular Min                -7.59738\n",
      "time/data storing (s)                                   0.0174268\n",
      "time/evaluation sampling (s)                           22.8763\n",
      "time/exploration sampling (s)                           1.16253\n",
      "time/logging (s)                                        0.257759\n",
      "time/saving (s)                                         0.0297671\n",
      "time/training (s)                                       6.14539\n",
      "time/epoch (s)                                         30.4892\n",
      "time/total (s)                                       5872.75\n",
      "Epoch                                                 191\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:50:28.402387 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 192 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 194000\n",
      "trainer/QF1 Loss                                        1.95774\n",
      "trainer/QF2 Loss                                        1.57727\n",
      "trainer/Policy Loss                                   -12.5635\n",
      "trainer/Q1 Predictions Mean                            19.0997\n",
      "trainer/Q1 Predictions Std                             34.7975\n",
      "trainer/Q1 Predictions Max                            136.017\n",
      "trainer/Q1 Predictions Min                            -18.975\n",
      "trainer/Q2 Predictions Mean                            18.9225\n",
      "trainer/Q2 Predictions Std                             34.9399\n",
      "trainer/Q2 Predictions Max                            136.636\n",
      "trainer/Q2 Predictions Min                            -18.9968\n",
      "trainer/Q Targets Mean                                 18.9092\n",
      "trainer/Q Targets Std                                  34.9555\n",
      "trainer/Q Targets Max                                 137.028\n",
      "trainer/Q Targets Min                                 -19.4793\n",
      "trainer/Log Pis Mean                                    6.64373\n",
      "trainer/Log Pis Std                                     7.17314\n",
      "trainer/Log Pis Max                                    39.0942\n",
      "trainer/Log Pis Min                                    -5.16629\n",
      "trainer/Policy mu Mean                                  0.227529\n",
      "trainer/Policy mu Std                                   1.69909\n",
      "trainer/Policy mu Max                                   7.02283\n",
      "trainer/Policy mu Min                                  -5.73133\n",
      "trainer/Policy log std Mean                            -0.705101\n",
      "trainer/Policy log std Std                              0.310688\n",
      "trainer/Policy log std Max                              0.309768\n",
      "trainer/Policy log std Min                             -2.46913\n",
      "trainer/Alpha                                           0.0229447\n",
      "trainer/Alpha Loss                                      2.42989\n",
      "exploration/num steps total                        194000\n",
      "exploration/num paths total                           194\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.37481\n",
      "exploration/Rewards Std                                 2.03497\n",
      "exploration/Rewards Max                                 6.00686\n",
      "exploration/Rewards Min                                -5.44517\n",
      "exploration/Returns Mean                             1374.81\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1374.81\n",
      "exploration/Returns Min                              1374.81\n",
      "exploration/Actions Mean                                0.248619\n",
      "exploration/Actions Std                                 0.851594\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.999977\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1374.81\n",
      "exploration/env_infos/final/reward_run Mean            -3.41481\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.41481\n",
      "exploration/env_infos/final/reward_run Min             -3.41481\n",
      "exploration/env_infos/initial/reward_run Mean          -0.230472\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.230472\n",
      "exploration/env_infos/initial/reward_run Min           -0.230472\n",
      "exploration/env_infos/reward_run Mean                  -3.42171\n",
      "exploration/env_infos/reward_run Std                    1.00393\n",
      "exploration/env_infos/reward_run Max                    0.473035\n",
      "exploration/env_infos/reward_run Min                   -6.19719\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.455782\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.455782\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.455782\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.252141\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.252141\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.252141\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.472214\n",
      "exploration/env_infos/reward_ctrl Std                   0.0724369\n",
      "exploration/env_infos/reward_ctrl Max                  -0.245051\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596551\n",
      "exploration/env_infos/final/height Mean                -0.0985421\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0985421\n",
      "exploration/env_infos/final/height Min                 -0.0985421\n",
      "exploration/env_infos/initial/height Mean               0.0714252\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0714252\n",
      "exploration/env_infos/initial/height Min                0.0714252\n",
      "exploration/env_infos/height Mean                      -0.0253539\n",
      "exploration/env_infos/height Std                        0.130261\n",
      "exploration/env_infos/height Max                        0.424115\n",
      "exploration/env_infos/height Min                       -0.343029\n",
      "exploration/env_infos/final/reward_angular Mean        -2.24304\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -2.24304\n",
      "exploration/env_infos/final/reward_angular Min         -2.24304\n",
      "exploration/env_infos/initial/reward_angular Mean       1.07819\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.07819\n",
      "exploration/env_infos/initial/reward_angular Min        1.07819\n",
      "exploration/env_infos/reward_angular Mean               0.122681\n",
      "exploration/env_infos/reward_angular Std                2.69007\n",
      "exploration/env_infos/reward_angular Max                8.44459\n",
      "exploration/env_infos/reward_angular Min               -5.47842\n",
      "evaluation/num steps total                              4.825e+06\n",
      "evaluation/num paths total                           4825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.775404\n",
      "evaluation/Rewards Std                                  1.54533\n",
      "evaluation/Rewards Max                                  9.59426\n",
      "evaluation/Rewards Min                                 -5.68673\n",
      "evaluation/Returns Mean                               775.404\n",
      "evaluation/Returns Std                                896.001\n",
      "evaluation/Returns Max                               2734.74\n",
      "evaluation/Returns Min                               -579.199\n",
      "evaluation/Actions Mean                                 0.146447\n",
      "evaluation/Actions Std                                  0.81965\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            775.404\n",
      "evaluation/env_infos/final/reward_run Mean             -0.582655\n",
      "evaluation/env_infos/final/reward_run Std               2.87237\n",
      "evaluation/env_infos/final/reward_run Max               5.03848\n",
      "evaluation/env_infos/final/reward_run Min              -4.8727\n",
      "evaluation/env_infos/initial/reward_run Mean            0.262688\n",
      "evaluation/env_infos/initial/reward_run Std             0.500173\n",
      "evaluation/env_infos/initial/reward_run Max             1.05427\n",
      "evaluation/env_infos/initial/reward_run Min            -0.579856\n",
      "evaluation/env_infos/reward_run Mean                   -0.610114\n",
      "evaluation/env_infos/reward_run Std                     2.84693\n",
      "evaluation/env_infos/reward_run Max                     6.24578\n",
      "evaluation/env_infos/reward_run Min                    -7.14734\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.441473\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.106696\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0973494\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.580339\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277442\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0909996\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0522348\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.447185\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.415964\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111859\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0372987\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597929\n",
      "evaluation/env_infos/final/height Mean                 -0.168908\n",
      "evaluation/env_infos/final/height Std                   0.225252\n",
      "evaluation/env_infos/final/height Max                   0.0837804\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0219568\n",
      "evaluation/env_infos/initial/height Std                 0.0491894\n",
      "evaluation/env_infos/initial/height Max                 0.0705811\n",
      "evaluation/env_infos/initial/height Min                -0.0975945\n",
      "evaluation/env_infos/height Mean                       -0.128513\n",
      "evaluation/env_infos/height Std                         0.239523\n",
      "evaluation/env_infos/height Max                         0.633838\n",
      "evaluation/env_infos/height Min                        -0.586097\n",
      "evaluation/env_infos/final/reward_angular Mean          0.298217\n",
      "evaluation/env_infos/final/reward_angular Std           2.14346\n",
      "evaluation/env_infos/final/reward_angular Max           4.12198\n",
      "evaluation/env_infos/final/reward_angular Min          -4.32667\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0441112\n",
      "evaluation/env_infos/initial/reward_angular Std         1.13917\n",
      "evaluation/env_infos/initial/reward_angular Max         1.96414\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.0124\n",
      "evaluation/env_infos/reward_angular Mean                0.0628739\n",
      "evaluation/env_infos/reward_angular Std                 2.14906\n",
      "evaluation/env_infos/reward_angular Max                10.1207\n",
      "evaluation/env_infos/reward_angular Min                -8.22113\n",
      "time/data storing (s)                                   0.0178947\n",
      "time/evaluation sampling (s)                           24.2888\n",
      "time/exploration sampling (s)                           1.48281\n",
      "time/logging (s)                                        0.241261\n",
      "time/saving (s)                                         0.0266569\n",
      "time/training (s)                                       4.85561\n",
      "time/epoch (s)                                         30.9131\n",
      "time/total (s)                                       5905.29\n",
      "Epoch                                                 192\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:51:01.537439 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 193 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 195000\n",
      "trainer/QF1 Loss                                        3.21941\n",
      "trainer/QF2 Loss                                        3.29847\n",
      "trainer/Policy Loss                                   -13.3237\n",
      "trainer/Q1 Predictions Mean                            18.569\n",
      "trainer/Q1 Predictions Std                             33.7192\n",
      "trainer/Q1 Predictions Max                            147.366\n",
      "trainer/Q1 Predictions Min                            -19.7645\n",
      "trainer/Q2 Predictions Mean                            18.8041\n",
      "trainer/Q2 Predictions Std                             34.1748\n",
      "trainer/Q2 Predictions Max                            150.662\n",
      "trainer/Q2 Predictions Min                            -20.1408\n",
      "trainer/Q Targets Mean                                 18.7929\n",
      "trainer/Q Targets Std                                  34.0023\n",
      "trainer/Q Targets Max                                 146.969\n",
      "trainer/Q Targets Min                                 -20.2304\n",
      "trainer/Log Pis Mean                                    5.61179\n",
      "trainer/Log Pis Std                                     5.09464\n",
      "trainer/Log Pis Max                                    21.4631\n",
      "trainer/Log Pis Min                                    -5.90093\n",
      "trainer/Policy mu Mean                                  0.0447677\n",
      "trainer/Policy mu Std                                   1.56005\n",
      "trainer/Policy mu Max                                   5.70426\n",
      "trainer/Policy mu Min                                  -5.04415\n",
      "trainer/Policy log std Mean                            -0.663572\n",
      "trainer/Policy log std Std                              0.290558\n",
      "trainer/Policy log std Max                              0.252738\n",
      "trainer/Policy log std Min                             -2.11284\n",
      "trainer/Alpha                                           0.0217807\n",
      "trainer/Alpha Loss                                     -1.48433\n",
      "exploration/num steps total                        195000\n",
      "exploration/num paths total                           195\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.0348117\n",
      "exploration/Rewards Std                                 0.882175\n",
      "exploration/Rewards Max                                 7.22937\n",
      "exploration/Rewards Min                                -4.53937\n",
      "exploration/Returns Mean                               34.8117\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                                34.8117\n",
      "exploration/Returns Min                                34.8117\n",
      "exploration/Actions Mean                                0.268919\n",
      "exploration/Actions Std                                 0.731863\n",
      "exploration/Actions Max                                 1\n",
      "exploration/Actions Min                                -0.99998\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                            34.8117\n",
      "exploration/env_infos/final/reward_run Mean             0.00973718\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.00973718\n",
      "exploration/env_infos/final/reward_run Min              0.00973718\n",
      "exploration/env_infos/initial/reward_run Mean           0.567941\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.567941\n",
      "exploration/env_infos/initial/reward_run Min            0.567941\n",
      "exploration/env_infos/reward_run Mean                   0.0460058\n",
      "exploration/env_infos/reward_run Std                    0.35233\n",
      "exploration/env_infos/reward_run Max                    2.17714\n",
      "exploration/env_infos/reward_run Min                   -2.18979\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.419825\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.419825\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.419825\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.306087\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.306087\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.306087\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.364765\n",
      "exploration/env_infos/reward_ctrl Std                   0.0840914\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0948116\n",
      "exploration/env_infos/reward_ctrl Min                  -0.58313\n",
      "exploration/env_infos/final/height Mean                -0.577292\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577292\n",
      "exploration/env_infos/final/height Min                 -0.577292\n",
      "exploration/env_infos/initial/height Mean               0.0444998\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0444998\n",
      "exploration/env_infos/initial/height Min                0.0444998\n",
      "exploration/env_infos/height Mean                      -0.409842\n",
      "exploration/env_infos/height Std                        0.227457\n",
      "exploration/env_infos/height Max                        0.376585\n",
      "exploration/env_infos/height Min                       -0.57868\n",
      "exploration/env_infos/final/reward_angular Mean         0.00156164\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.00156164\n",
      "exploration/env_infos/final/reward_angular Min          0.00156164\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.70318\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.70318\n",
      "exploration/env_infos/initial/reward_angular Min       -0.70318\n",
      "exploration/env_infos/reward_angular Mean               0.197667\n",
      "exploration/env_infos/reward_angular Std                0.980627\n",
      "exploration/env_infos/reward_angular Max                8.09584\n",
      "exploration/env_infos/reward_angular Min               -4.88197\n",
      "evaluation/num steps total                              4.85e+06\n",
      "evaluation/num paths total                           4850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.756359\n",
      "evaluation/Rewards Std                                  1.53962\n",
      "evaluation/Rewards Max                                  9.09981\n",
      "evaluation/Rewards Min                                 -7.20286\n",
      "evaluation/Returns Mean                               756.359\n",
      "evaluation/Returns Std                                896.711\n",
      "evaluation/Returns Max                               2461.96\n",
      "evaluation/Returns Min                               -525.089\n",
      "evaluation/Actions Mean                                 0.125059\n",
      "evaluation/Actions Std                                  0.771955\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            756.359\n",
      "evaluation/env_infos/final/reward_run Mean             -0.812884\n",
      "evaluation/env_infos/final/reward_run Std               2.72852\n",
      "evaluation/env_infos/final/reward_run Max               4.08229\n",
      "evaluation/env_infos/final/reward_run Min              -5.68289\n",
      "evaluation/env_infos/initial/reward_run Mean            0.253703\n",
      "evaluation/env_infos/initial/reward_run Std             0.466495\n",
      "evaluation/env_infos/initial/reward_run Max             1.31429\n",
      "evaluation/env_infos/initial/reward_run Min            -0.689974\n",
      "evaluation/env_infos/reward_run Mean                   -0.925757\n",
      "evaluation/env_infos/reward_run Std                     2.84758\n",
      "evaluation/env_infos/reward_run Max                     5.81619\n",
      "evaluation/env_infos/reward_run Min                    -7.68357\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.340867\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.108644\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0872731\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.514967\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.251408\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0902367\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0797153\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.415374\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.366933\n",
      "evaluation/env_infos/reward_ctrl Std                    0.111186\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0221501\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597731\n",
      "evaluation/env_infos/final/height Mean                 -0.136414\n",
      "evaluation/env_infos/final/height Std                   0.193859\n",
      "evaluation/env_infos/final/height Max                   0.177229\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00916062\n",
      "evaluation/env_infos/initial/height Std                 0.052911\n",
      "evaluation/env_infos/initial/height Max                 0.0828304\n",
      "evaluation/env_infos/initial/height Min                -0.0858274\n",
      "evaluation/env_infos/height Mean                       -0.119862\n",
      "evaluation/env_infos/height Std                         0.199256\n",
      "evaluation/env_infos/height Max                         0.487063\n",
      "evaluation/env_infos/height Min                        -0.593988\n",
      "evaluation/env_infos/final/reward_angular Mean          0.76635\n",
      "evaluation/env_infos/final/reward_angular Std           2.18733\n",
      "evaluation/env_infos/final/reward_angular Max           5.95301\n",
      "evaluation/env_infos/final/reward_angular Min          -3.21544\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.311685\n",
      "evaluation/env_infos/initial/reward_angular Std         0.917995\n",
      "evaluation/env_infos/initial/reward_angular Max         1.14834\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.8753\n",
      "evaluation/env_infos/reward_angular Mean                0.0720567\n",
      "evaluation/env_infos/reward_angular Std                 2.11858\n",
      "evaluation/env_infos/reward_angular Max                 9.73449\n",
      "evaluation/env_infos/reward_angular Min                -7.1315\n",
      "time/data storing (s)                                   0.0157522\n",
      "time/evaluation sampling (s)                           25.7513\n",
      "time/exploration sampling (s)                           1.11942\n",
      "time/logging (s)                                        0.238044\n",
      "time/saving (s)                                         0.0277756\n",
      "time/training (s)                                       4.33041\n",
      "time/epoch (s)                                         31.4828\n",
      "time/total (s)                                       5938.42\n",
      "Epoch                                                 193\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:51:29.858338 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 194 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 196000\n",
      "trainer/QF1 Loss                                        1.54283\n",
      "trainer/QF2 Loss                                        1.58506\n",
      "trainer/Policy Loss                                   -12.0976\n",
      "trainer/Q1 Predictions Mean                            17.6063\n",
      "trainer/Q1 Predictions Std                             32.1994\n",
      "trainer/Q1 Predictions Max                            135.417\n",
      "trainer/Q1 Predictions Min                            -18.6681\n",
      "trainer/Q2 Predictions Mean                            17.7393\n",
      "trainer/Q2 Predictions Std                             32.2173\n",
      "trainer/Q2 Predictions Max                            135.929\n",
      "trainer/Q2 Predictions Min                            -19.2256\n",
      "trainer/Q Targets Mean                                 17.5468\n",
      "trainer/Q Targets Std                                  32.3727\n",
      "trainer/Q Targets Max                                 137.076\n",
      "trainer/Q Targets Min                                 -19.3948\n",
      "trainer/Log Pis Mean                                    5.84463\n",
      "trainer/Log Pis Std                                     5.49044\n",
      "trainer/Log Pis Max                                    37.6335\n",
      "trainer/Log Pis Min                                    -4.46286\n",
      "trainer/Policy mu Mean                                  0.00433825\n",
      "trainer/Policy mu Std                                   1.57182\n",
      "trainer/Policy mu Max                                   5.87922\n",
      "trainer/Policy mu Min                                  -5.6875\n",
      "trainer/Policy log std Mean                            -0.686136\n",
      "trainer/Policy log std Std                              0.299262\n",
      "trainer/Policy log std Max                              0.429827\n",
      "trainer/Policy log std Min                             -2.18337\n",
      "trainer/Alpha                                           0.0223729\n",
      "trainer/Alpha Loss                                     -0.590246\n",
      "exploration/num steps total                        196000\n",
      "exploration/num paths total                           196\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.113244\n",
      "exploration/Rewards Std                                 1.11849\n",
      "exploration/Rewards Max                                 2.46628\n",
      "exploration/Rewards Min                                -4.35759\n",
      "exploration/Returns Mean                             -113.244\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -113.244\n",
      "exploration/Returns Min                              -113.244\n",
      "exploration/Actions Mean                               -0.00566355\n",
      "exploration/Actions Std                                 0.788332\n",
      "exploration/Actions Max                                 0.999426\n",
      "exploration/Actions Min                                -0.999911\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -113.244\n",
      "exploration/env_infos/final/reward_run Mean            -4.97659\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -4.97659\n",
      "exploration/env_infos/final/reward_run Min             -4.97659\n",
      "exploration/env_infos/initial/reward_run Mean          -0.292768\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.292768\n",
      "exploration/env_infos/initial/reward_run Min           -0.292768\n",
      "exploration/env_infos/reward_run Mean                  -3.18498\n",
      "exploration/env_infos/reward_run Std                    1.68766\n",
      "exploration/env_infos/reward_run Max                    0.83062\n",
      "exploration/env_infos/reward_run Min                   -6.94121\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.462945\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.462945\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.462945\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.251121\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.251121\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.251121\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.3729\n",
      "exploration/env_infos/reward_ctrl Std                   0.0834733\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0850009\n",
      "exploration/env_infos/reward_ctrl Min                  -0.557433\n",
      "exploration/env_infos/final/height Mean                -0.203174\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.203174\n",
      "exploration/env_infos/final/height Min                 -0.203174\n",
      "exploration/env_infos/initial/height Mean              -0.019479\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.019479\n",
      "exploration/env_infos/initial/height Min               -0.019479\n",
      "exploration/env_infos/height Mean                      -0.0486192\n",
      "exploration/env_infos/height Std                        0.132894\n",
      "exploration/env_infos/height Max                        0.37588\n",
      "exploration/env_infos/height Min                       -0.311854\n",
      "exploration/env_infos/final/reward_angular Mean         7.07967\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          7.07967\n",
      "exploration/env_infos/final/reward_angular Min          7.07967\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.199277\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.199277\n",
      "exploration/env_infos/initial/reward_angular Min       -0.199277\n",
      "exploration/env_infos/reward_angular Mean               0.0922655\n",
      "exploration/env_infos/reward_angular Std                2.5267\n",
      "exploration/env_infos/reward_angular Max                9.64776\n",
      "exploration/env_infos/reward_angular Min               -5.80954\n",
      "evaluation/num steps total                              4.875e+06\n",
      "evaluation/num paths total                           4875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.725605\n",
      "evaluation/Rewards Std                                  1.56054\n",
      "evaluation/Rewards Max                                  8.54\n",
      "evaluation/Rewards Min                                 -8.14227\n",
      "evaluation/Returns Mean                               725.605\n",
      "evaluation/Returns Std                                934.416\n",
      "evaluation/Returns Max                               2759.48\n",
      "evaluation/Returns Min                               -418.409\n",
      "evaluation/Actions Mean                                 0.0839641\n",
      "evaluation/Actions Std                                  0.806937\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            725.605\n",
      "evaluation/env_infos/final/reward_run Mean             -0.401813\n",
      "evaluation/env_infos/final/reward_run Std               2.88728\n",
      "evaluation/env_infos/final/reward_run Max               3.60013\n",
      "evaluation/env_infos/final/reward_run Min              -6.58167\n",
      "evaluation/env_infos/initial/reward_run Mean            0.324757\n",
      "evaluation/env_infos/initial/reward_run Std             0.46396\n",
      "evaluation/env_infos/initial/reward_run Max             0.942993\n",
      "evaluation/env_infos/initial/reward_run Min            -0.447642\n",
      "evaluation/env_infos/reward_run Mean                   -0.793204\n",
      "evaluation/env_infos/reward_run Std                     2.85898\n",
      "evaluation/env_infos/reward_run Max                     5.51285\n",
      "evaluation/env_infos/reward_run Min                    -8.14242\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.401243\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.110135\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0884843\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.542398\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.257603\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0647475\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.123552\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.391657\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.394919\n",
      "evaluation/env_infos/reward_ctrl Std                    0.118243\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0334802\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593599\n",
      "evaluation/env_infos/final/height Mean                 -0.1928\n",
      "evaluation/env_infos/final/height Std                   0.259555\n",
      "evaluation/env_infos/final/height Max                   0.292169\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.022132\n",
      "evaluation/env_infos/initial/height Std                 0.0528838\n",
      "evaluation/env_infos/initial/height Max                 0.0730454\n",
      "evaluation/env_infos/initial/height Min                -0.113171\n",
      "evaluation/env_infos/height Mean                       -0.155445\n",
      "evaluation/env_infos/height Std                         0.242633\n",
      "evaluation/env_infos/height Max                         0.454629\n",
      "evaluation/env_infos/height Min                        -0.582958\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.457083\n",
      "evaluation/env_infos/final/reward_angular Std           1.4554\n",
      "evaluation/env_infos/final/reward_angular Max           3.98795\n",
      "evaluation/env_infos/final/reward_angular Min          -3.94592\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.504613\n",
      "evaluation/env_infos/initial/reward_angular Std         1.01768\n",
      "evaluation/env_infos/initial/reward_angular Max         2.45691\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.17892\n",
      "evaluation/env_infos/reward_angular Mean                0.0677981\n",
      "evaluation/env_infos/reward_angular Std                 2.12364\n",
      "evaluation/env_infos/reward_angular Max                10.5021\n",
      "evaluation/env_infos/reward_angular Min                -7.02243\n",
      "time/data storing (s)                                   0.0261221\n",
      "time/evaluation sampling (s)                           20.7649\n",
      "time/exploration sampling (s)                           1.12505\n",
      "time/logging (s)                                        0.255571\n",
      "time/saving (s)                                         0.0360839\n",
      "time/training (s)                                       4.67749\n",
      "time/epoch (s)                                         26.8852\n",
      "time/total (s)                                       5966.75\n",
      "Epoch                                                 194\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:52:01.804116 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 195 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 197000\n",
      "trainer/QF1 Loss                                        2.08518\n",
      "trainer/QF2 Loss                                        2.73885\n",
      "trainer/Policy Loss                                    -9.15733\n",
      "trainer/Q1 Predictions Mean                            14.1552\n",
      "trainer/Q1 Predictions Std                             29.7868\n",
      "trainer/Q1 Predictions Max                            132.55\n",
      "trainer/Q1 Predictions Min                            -21.5332\n",
      "trainer/Q2 Predictions Mean                            14.4061\n",
      "trainer/Q2 Predictions Std                             30.0122\n",
      "trainer/Q2 Predictions Max                            134.443\n",
      "trainer/Q2 Predictions Min                            -21.6352\n",
      "trainer/Q Targets Mean                                 14.3495\n",
      "trainer/Q Targets Std                                  29.8921\n",
      "trainer/Q Targets Max                                 132.155\n",
      "trainer/Q Targets Min                                 -20.1995\n",
      "trainer/Log Pis Mean                                    5.21238\n",
      "trainer/Log Pis Std                                     5.27674\n",
      "trainer/Log Pis Max                                    20.5285\n",
      "trainer/Log Pis Min                                    -5.7042\n",
      "trainer/Policy mu Mean                                  0.0218469\n",
      "trainer/Policy mu Std                                   1.52079\n",
      "trainer/Policy mu Max                                   4.25932\n",
      "trainer/Policy mu Min                                  -4.35578\n",
      "trainer/Policy log std Mean                            -0.687706\n",
      "trainer/Policy log std Std                              0.288661\n",
      "trainer/Policy log std Max                              0.259491\n",
      "trainer/Policy log std Min                             -2.60532\n",
      "trainer/Alpha                                           0.0219337\n",
      "trainer/Alpha Loss                                     -3.00785\n",
      "exploration/num steps total                        197000\n",
      "exploration/num paths total                           197\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.92286\n",
      "exploration/Rewards Std                                 0.891838\n",
      "exploration/Rewards Max                                 4.29203\n",
      "exploration/Rewards Min                                -0.696248\n",
      "exploration/Returns Mean                             1922.86\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1922.86\n",
      "exploration/Returns Min                              1922.86\n",
      "exploration/Actions Mean                                0.243969\n",
      "exploration/Actions Std                                 0.813993\n",
      "exploration/Actions Max                                 0.999944\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1922.86\n",
      "exploration/env_infos/final/reward_run Mean             3.22662\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              3.22662\n",
      "exploration/env_infos/final/reward_run Min              3.22662\n",
      "exploration/env_infos/initial/reward_run Mean           0.895499\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.895499\n",
      "exploration/env_infos/initial/reward_run Min            0.895499\n",
      "exploration/env_infos/reward_run Mean                   2.7185\n",
      "exploration/env_infos/reward_run Std                    1.1454\n",
      "exploration/env_infos/reward_run Max                    5.84742\n",
      "exploration/env_infos/reward_run Min                   -0.976873\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.331536\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.331536\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.331536\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.446821\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.446821\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.446821\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.433263\n",
      "exploration/env_infos/reward_ctrl Std                   0.0815698\n",
      "exploration/env_infos/reward_ctrl Max                  -0.176941\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590604\n",
      "exploration/env_infos/final/height Mean                -0.191536\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.191536\n",
      "exploration/env_infos/final/height Min                 -0.191536\n",
      "exploration/env_infos/initial/height Mean               0.0815052\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0815052\n",
      "exploration/env_infos/initial/height Min                0.0815052\n",
      "exploration/env_infos/height Mean                      -0.0585594\n",
      "exploration/env_infos/height Std                        0.106283\n",
      "exploration/env_infos/height Max                        0.421612\n",
      "exploration/env_infos/height Min                       -0.346612\n",
      "exploration/env_infos/final/reward_angular Mean        -1.45697\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.45697\n",
      "exploration/env_infos/final/reward_angular Min         -1.45697\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.57292\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.57292\n",
      "exploration/env_infos/initial/reward_angular Min       -0.57292\n",
      "exploration/env_infos/reward_angular Mean              -0.0969978\n",
      "exploration/env_infos/reward_angular Std                2.12331\n",
      "exploration/env_infos/reward_angular Max                5.90228\n",
      "exploration/env_infos/reward_angular Min               -6.10295\n",
      "evaluation/num steps total                              4.9e+06\n",
      "evaluation/num paths total                           4900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.78833\n",
      "evaluation/Rewards Std                                  1.60836\n",
      "evaluation/Rewards Max                                  8.24245\n",
      "evaluation/Rewards Min                                 -8.41908\n",
      "evaluation/Returns Mean                               788.33\n",
      "evaluation/Returns Std                                916.951\n",
      "evaluation/Returns Max                               2643.37\n",
      "evaluation/Returns Min                               -436.378\n",
      "evaluation/Actions Mean                                 0.0880408\n",
      "evaluation/Actions Std                                  0.810965\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            788.33\n",
      "evaluation/env_infos/final/reward_run Mean             -1.29061\n",
      "evaluation/env_infos/final/reward_run Std               3.13305\n",
      "evaluation/env_infos/final/reward_run Max               4.294\n",
      "evaluation/env_infos/final/reward_run Min              -5.73632\n",
      "evaluation/env_infos/initial/reward_run Mean            0.31231\n",
      "evaluation/env_infos/initial/reward_run Std             0.566444\n",
      "evaluation/env_infos/initial/reward_run Max             1.21332\n",
      "evaluation/env_infos/initial/reward_run Min            -0.676725\n",
      "evaluation/env_infos/reward_run Mean                   -0.953326\n",
      "evaluation/env_infos/reward_run Std                     3.1427\n",
      "evaluation/env_infos/reward_run Max                     6.28741\n",
      "evaluation/env_infos/reward_run Min                    -8.19488\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.373839\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.101932\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.159421\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.532891\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.273648\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0789659\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0979932\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.497296\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.399249\n",
      "evaluation/env_infos/reward_ctrl Std                    0.102171\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0355194\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.594277\n",
      "evaluation/env_infos/final/height Mean                 -0.188866\n",
      "evaluation/env_infos/final/height Std                   0.250921\n",
      "evaluation/env_infos/final/height Max                   0.10761\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0214058\n",
      "evaluation/env_infos/initial/height Std                 0.0480997\n",
      "evaluation/env_infos/initial/height Max                 0.0733887\n",
      "evaluation/env_infos/initial/height Min                -0.0924259\n",
      "evaluation/env_infos/height Mean                       -0.154745\n",
      "evaluation/env_infos/height Std                         0.242118\n",
      "evaluation/env_infos/height Max                         0.496751\n",
      "evaluation/env_infos/height Min                        -0.587301\n",
      "evaluation/env_infos/final/reward_angular Mean          0.343372\n",
      "evaluation/env_infos/final/reward_angular Std           2.27198\n",
      "evaluation/env_infos/final/reward_angular Max           7.4459\n",
      "evaluation/env_infos/final/reward_angular Min          -5.33729\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.306327\n",
      "evaluation/env_infos/initial/reward_angular Std         0.693305\n",
      "evaluation/env_infos/initial/reward_angular Max         1.60922\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.15838\n",
      "evaluation/env_infos/reward_angular Mean                0.0579896\n",
      "evaluation/env_infos/reward_angular Std                 2.236\n",
      "evaluation/env_infos/reward_angular Max                10.7622\n",
      "evaluation/env_infos/reward_angular Min                -7.05545\n",
      "time/data storing (s)                                   0.0177104\n",
      "time/evaluation sampling (s)                           24.3933\n",
      "time/exploration sampling (s)                           1.185\n",
      "time/logging (s)                                        0.255737\n",
      "time/saving (s)                                         0.0292985\n",
      "time/training (s)                                       4.44534\n",
      "time/epoch (s)                                         30.3264\n",
      "time/total (s)                                       5998.69\n",
      "Epoch                                                 195\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:52:32.603955 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 196 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 198000\n",
      "trainer/QF1 Loss                                        2.36279\n",
      "trainer/QF2 Loss                                        1.92656\n",
      "trainer/Policy Loss                                   -12.9045\n",
      "trainer/Q1 Predictions Mean                            18.8542\n",
      "trainer/Q1 Predictions Std                             32.5667\n",
      "trainer/Q1 Predictions Max                            148.35\n",
      "trainer/Q1 Predictions Min                            -21.0071\n",
      "trainer/Q2 Predictions Mean                            18.8272\n",
      "trainer/Q2 Predictions Std                             32.458\n",
      "trainer/Q2 Predictions Max                            147.966\n",
      "trainer/Q2 Predictions Min                            -20.3288\n",
      "trainer/Q Targets Mean                                 18.9411\n",
      "trainer/Q Targets Std                                  32.6639\n",
      "trainer/Q Targets Max                                 148.757\n",
      "trainer/Q Targets Min                                 -19.8731\n",
      "trainer/Log Pis Mean                                    6.11332\n",
      "trainer/Log Pis Std                                     5.92726\n",
      "trainer/Log Pis Max                                    29.3976\n",
      "trainer/Log Pis Min                                    -6.92061\n",
      "trainer/Policy mu Mean                                 -0.0612022\n",
      "trainer/Policy mu Std                                   1.62676\n",
      "trainer/Policy mu Max                                   4.87758\n",
      "trainer/Policy mu Min                                  -5.50596\n",
      "trainer/Policy log std Mean                            -0.668831\n",
      "trainer/Policy log std Std                              0.296131\n",
      "trainer/Policy log std Max                              0.384002\n",
      "trainer/Policy log std Min                             -1.98948\n",
      "trainer/Alpha                                           0.0221751\n",
      "trainer/Alpha Loss                                      0.431444\n",
      "exploration/num steps total                        198000\n",
      "exploration/num paths total                           198\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.81861\n",
      "exploration/Rewards Std                                 0.979574\n",
      "exploration/Rewards Max                                 4.43338\n",
      "exploration/Rewards Min                                -2.35486\n",
      "exploration/Returns Mean                             1818.61\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1818.61\n",
      "exploration/Returns Min                              1818.61\n",
      "exploration/Actions Mean                                0.153689\n",
      "exploration/Actions Std                                 0.82879\n",
      "exploration/Actions Max                                 0.999958\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1818.61\n",
      "exploration/env_infos/final/reward_run Mean             4.37567\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              4.37567\n",
      "exploration/env_infos/final/reward_run Min              4.37567\n",
      "exploration/env_infos/initial/reward_run Mean           0.751203\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.751203\n",
      "exploration/env_infos/initial/reward_run Min            0.751203\n",
      "exploration/env_infos/reward_run Mean                   2.79302\n",
      "exploration/env_infos/reward_run Std                    1.13703\n",
      "exploration/env_infos/reward_run Max                    5.73857\n",
      "exploration/env_infos/reward_run Min                   -1.12978\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.279437\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.279437\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.279437\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.322343\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.322343\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.322343\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.426308\n",
      "exploration/env_infos/reward_ctrl Std                   0.0971968\n",
      "exploration/env_infos/reward_ctrl Max                  -0.153227\n",
      "exploration/env_infos/reward_ctrl Min                  -0.598312\n",
      "exploration/env_infos/final/height Mean                 0.0102343\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0102343\n",
      "exploration/env_infos/final/height Min                  0.0102343\n",
      "exploration/env_infos/initial/height Mean               0.0518804\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0518804\n",
      "exploration/env_infos/initial/height Min                0.0518804\n",
      "exploration/env_infos/height Mean                      -0.00104626\n",
      "exploration/env_infos/height Std                        0.122526\n",
      "exploration/env_infos/height Max                        0.500572\n",
      "exploration/env_infos/height Min                       -0.324786\n",
      "exploration/env_infos/final/reward_angular Mean         1.52856\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.52856\n",
      "exploration/env_infos/final/reward_angular Min          1.52856\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.50097\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.50097\n",
      "exploration/env_infos/initial/reward_angular Min       -1.50097\n",
      "exploration/env_infos/reward_angular Mean              -0.0858896\n",
      "exploration/env_infos/reward_angular Std                1.87394\n",
      "exploration/env_infos/reward_angular Max                6.81746\n",
      "exploration/env_infos/reward_angular Min               -6.44097\n",
      "evaluation/num steps total                              4.925e+06\n",
      "evaluation/num paths total                           4925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.741155\n",
      "evaluation/Rewards Std                                  1.68354\n",
      "evaluation/Rewards Max                                  9.046\n",
      "evaluation/Rewards Min                                 -7.48352\n",
      "evaluation/Returns Mean                               741.155\n",
      "evaluation/Returns Std                                929.705\n",
      "evaluation/Returns Max                               2778.38\n",
      "evaluation/Returns Min                               -580.531\n",
      "evaluation/Actions Mean                                 0.099959\n",
      "evaluation/Actions Std                                  0.808852\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            741.155\n",
      "evaluation/env_infos/final/reward_run Mean             -1.26178\n",
      "evaluation/env_infos/final/reward_run Std               3.22289\n",
      "evaluation/env_infos/final/reward_run Max               4.40514\n",
      "evaluation/env_infos/final/reward_run Min              -5.97391\n",
      "evaluation/env_infos/initial/reward_run Mean            0.176042\n",
      "evaluation/env_infos/initial/reward_run Std             0.522602\n",
      "evaluation/env_infos/initial/reward_run Max             1.02931\n",
      "evaluation/env_infos/initial/reward_run Min            -0.885414\n",
      "evaluation/env_infos/reward_run Mean                   -1.21594\n",
      "evaluation/env_infos/reward_run Std                     2.84836\n",
      "evaluation/env_infos/reward_run Max                     5.41627\n",
      "evaluation/env_infos/reward_run Min                    -7.80661\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.397662\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0915864\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0666461\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.553145\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.254197\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0745402\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0766792\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.424887\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.39854\n",
      "evaluation/env_infos/reward_ctrl Std                    0.105606\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0187265\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596872\n",
      "evaluation/env_infos/final/height Mean                 -0.193679\n",
      "evaluation/env_infos/final/height Std                   0.267489\n",
      "evaluation/env_infos/final/height Max                   0.370001\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0142709\n",
      "evaluation/env_infos/initial/height Std                 0.0606562\n",
      "evaluation/env_infos/initial/height Max                 0.0932413\n",
      "evaluation/env_infos/initial/height Min                -0.106561\n",
      "evaluation/env_infos/height Mean                       -0.136799\n",
      "evaluation/env_infos/height Std                         0.24932\n",
      "evaluation/env_infos/height Max                         0.635755\n",
      "evaluation/env_infos/height Min                        -0.592705\n",
      "evaluation/env_infos/final/reward_angular Mean          0.260213\n",
      "evaluation/env_infos/final/reward_angular Std           2.45809\n",
      "evaluation/env_infos/final/reward_angular Max          10.62\n",
      "evaluation/env_infos/final/reward_angular Min          -2.56443\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.333298\n",
      "evaluation/env_infos/initial/reward_angular Std         1.11564\n",
      "evaluation/env_infos/initial/reward_angular Max         1.77294\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.17064\n",
      "evaluation/env_infos/reward_angular Mean                0.0665717\n",
      "evaluation/env_infos/reward_angular Std                 2.23501\n",
      "evaluation/env_infos/reward_angular Max                10.6364\n",
      "evaluation/env_infos/reward_angular Min                -8.67651\n",
      "time/data storing (s)                                   0.0169358\n",
      "time/evaluation sampling (s)                           22.8975\n",
      "time/exploration sampling (s)                           1.11341\n",
      "time/logging (s)                                        0.294978\n",
      "time/saving (s)                                         0.02819\n",
      "time/training (s)                                       4.94741\n",
      "time/epoch (s)                                         29.2984\n",
      "time/total (s)                                       6029.52\n",
      "Epoch                                                 196\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:53:09.619777 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 197 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 199000\n",
      "trainer/QF1 Loss                                        2.68762\n",
      "trainer/QF2 Loss                                        2.01377\n",
      "trainer/Policy Loss                                   -15.0751\n",
      "trainer/Q1 Predictions Mean                            20.9069\n",
      "trainer/Q1 Predictions Std                             36.2375\n",
      "trainer/Q1 Predictions Max                            147.977\n",
      "trainer/Q1 Predictions Min                            -20.1839\n",
      "trainer/Q2 Predictions Mean                            21.0544\n",
      "trainer/Q2 Predictions Std                             36.4584\n",
      "trainer/Q2 Predictions Max                            148.31\n",
      "trainer/Q2 Predictions Min                            -19.7607\n",
      "trainer/Q Targets Mean                                 21.2458\n",
      "trainer/Q Targets Std                                  36.7128\n",
      "trainer/Q Targets Max                                 149.402\n",
      "trainer/Q Targets Min                                 -19.5755\n",
      "trainer/Log Pis Mean                                    6.00679\n",
      "trainer/Log Pis Std                                     5.89704\n",
      "trainer/Log Pis Max                                    29.7314\n",
      "trainer/Log Pis Min                                   -10.2519\n",
      "trainer/Policy mu Mean                                  0.244217\n",
      "trainer/Policy mu Std                                   1.59893\n",
      "trainer/Policy mu Max                                   5.36287\n",
      "trainer/Policy mu Min                                  -5.83793\n",
      "trainer/Policy log std Mean                            -0.660363\n",
      "trainer/Policy log std Std                              0.305398\n",
      "trainer/Policy log std Max                              0.574093\n",
      "trainer/Policy log std Min                             -2.73346\n",
      "trainer/Alpha                                           0.0238472\n",
      "trainer/Alpha Loss                                      0.0253557\n",
      "exploration/num steps total                        199000\n",
      "exploration/num paths total                           199\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.376296\n",
      "exploration/Rewards Std                                 0.927488\n",
      "exploration/Rewards Max                                 3.73738\n",
      "exploration/Rewards Min                                -1.5031\n",
      "exploration/Returns Mean                              376.296\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               376.296\n",
      "exploration/Returns Min                               376.296\n",
      "exploration/Actions Mean                                0.121174\n",
      "exploration/Actions Std                                 0.769646\n",
      "exploration/Actions Max                                 0.999987\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           376.296\n",
      "exploration/env_infos/final/reward_run Mean            -0.633287\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.633287\n",
      "exploration/env_infos/final/reward_run Min             -0.633287\n",
      "exploration/env_infos/initial/reward_run Mean           0.653556\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.653556\n",
      "exploration/env_infos/initial/reward_run Min            0.653556\n",
      "exploration/env_infos/reward_run Mean                   0.711276\n",
      "exploration/env_infos/reward_run Std                    1.04516\n",
      "exploration/env_infos/reward_run Max                    4.5422\n",
      "exploration/env_infos/reward_run Min                   -1.33724\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.389111\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.389111\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.389111\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.304594\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.304594\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.304594\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.364223\n",
      "exploration/env_infos/reward_ctrl Std                   0.0946712\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0864994\n",
      "exploration/env_infos/reward_ctrl Min                  -0.587519\n",
      "exploration/env_infos/final/height Mean                -0.420071\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.420071\n",
      "exploration/env_infos/final/height Min                 -0.420071\n",
      "exploration/env_infos/initial/height Mean              -0.0373813\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0373813\n",
      "exploration/env_infos/initial/height Min               -0.0373813\n",
      "exploration/env_infos/height Mean                      -0.389042\n",
      "exploration/env_infos/height Std                        0.174671\n",
      "exploration/env_infos/height Max                        0.291399\n",
      "exploration/env_infos/height Min                       -0.590219\n",
      "exploration/env_infos/final/reward_angular Mean         1.96472\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          1.96472\n",
      "exploration/env_infos/final/reward_angular Min          1.96472\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.996741\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.996741\n",
      "exploration/env_infos/initial/reward_angular Min       -0.996741\n",
      "exploration/env_infos/reward_angular Mean               0.0037121\n",
      "exploration/env_infos/reward_angular Std                2.07015\n",
      "exploration/env_infos/reward_angular Max                5.18445\n",
      "exploration/env_infos/reward_angular Min               -5.11396\n",
      "evaluation/num steps total                              4.95e+06\n",
      "evaluation/num paths total                           4950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.906055\n",
      "evaluation/Rewards Std                                  1.6264\n",
      "evaluation/Rewards Max                                 10.0761\n",
      "evaluation/Rewards Min                                 -6.5095\n",
      "evaluation/Returns Mean                               906.055\n",
      "evaluation/Returns Std                                949.711\n",
      "evaluation/Returns Max                               2945.85\n",
      "evaluation/Returns Min                               -523.658\n",
      "evaluation/Actions Mean                                 0.145881\n",
      "evaluation/Actions Std                                  0.798959\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            906.055\n",
      "evaluation/env_infos/final/reward_run Mean             -0.892902\n",
      "evaluation/env_infos/final/reward_run Std               3.33511\n",
      "evaluation/env_infos/final/reward_run Max               4.82033\n",
      "evaluation/env_infos/final/reward_run Min              -6.80608\n",
      "evaluation/env_infos/initial/reward_run Mean            0.169199\n",
      "evaluation/env_infos/initial/reward_run Std             0.436468\n",
      "evaluation/env_infos/initial/reward_run Max             0.933982\n",
      "evaluation/env_infos/initial/reward_run Min            -0.859238\n",
      "evaluation/env_infos/reward_run Mean                   -0.923684\n",
      "evaluation/env_infos/reward_run Std                     3.06172\n",
      "evaluation/env_infos/reward_run Max                     6.26194\n",
      "evaluation/env_infos/reward_run Min                    -7.52184\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.414176\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.107749\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0613796\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.55301\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.253334\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0903491\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0785792\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.458584\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.39577\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115969\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0091697\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598744\n",
      "evaluation/env_infos/final/height Mean                 -0.063655\n",
      "evaluation/env_infos/final/height Std                   0.260622\n",
      "evaluation/env_infos/final/height Max                   0.262556\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.0113766\n",
      "evaluation/env_infos/initial/height Std                 0.0490785\n",
      "evaluation/env_infos/initial/height Max                 0.0759702\n",
      "evaluation/env_infos/initial/height Min                -0.0845501\n",
      "evaluation/env_infos/height Mean                       -0.10292\n",
      "evaluation/env_infos/height Std                         0.234968\n",
      "evaluation/env_infos/height Max                         0.542036\n",
      "evaluation/env_infos/height Min                        -0.578002\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0679973\n",
      "evaluation/env_infos/final/reward_angular Std           2.14759\n",
      "evaluation/env_infos/final/reward_angular Max           4.9474\n",
      "evaluation/env_infos/final/reward_angular Min          -4.27968\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.135995\n",
      "evaluation/env_infos/initial/reward_angular Std         0.994595\n",
      "evaluation/env_infos/initial/reward_angular Max         2.77585\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.73879\n",
      "evaluation/env_infos/reward_angular Mean                0.053817\n",
      "evaluation/env_infos/reward_angular Std                 2.27791\n",
      "evaluation/env_infos/reward_angular Max                10.9522\n",
      "evaluation/env_infos/reward_angular Min                -8.08511\n",
      "time/data storing (s)                                   0.0165168\n",
      "time/evaluation sampling (s)                           28.9909\n",
      "time/exploration sampling (s)                           1.1382\n",
      "time/logging (s)                                        0.261574\n",
      "time/saving (s)                                         0.0326646\n",
      "time/training (s)                                       4.7985\n",
      "time/epoch (s)                                         35.2383\n",
      "time/total (s)                                       6066.5\n",
      "Epoch                                                 197\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:53:41.528345 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 198 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 200000\n",
      "trainer/QF1 Loss                                        2.17155\n",
      "trainer/QF2 Loss                                        1.82353\n",
      "trainer/Policy Loss                                   -11.2335\n",
      "trainer/Q1 Predictions Mean                            16.1181\n",
      "trainer/Q1 Predictions Std                             31.4842\n",
      "trainer/Q1 Predictions Max                            127.589\n",
      "trainer/Q1 Predictions Min                            -19.543\n",
      "trainer/Q2 Predictions Mean                            15.7808\n",
      "trainer/Q2 Predictions Std                             31.2073\n",
      "trainer/Q2 Predictions Max                            124.769\n",
      "trainer/Q2 Predictions Min                            -19.7344\n",
      "trainer/Q Targets Mean                                 15.8573\n",
      "trainer/Q Targets Std                                  31.3189\n",
      "trainer/Q Targets Max                                 125.53\n",
      "trainer/Q Targets Min                                 -19.6716\n",
      "trainer/Log Pis Mean                                    4.87893\n",
      "trainer/Log Pis Std                                     5.01087\n",
      "trainer/Log Pis Max                                    22.1858\n",
      "trainer/Log Pis Min                                    -5.45734\n",
      "trainer/Policy mu Mean                                  0.0247882\n",
      "trainer/Policy mu Std                                   1.49951\n",
      "trainer/Policy mu Max                                   4.56798\n",
      "trainer/Policy mu Min                                  -4.26339\n",
      "trainer/Policy log std Mean                            -0.681627\n",
      "trainer/Policy log std Std                              0.294218\n",
      "trainer/Policy log std Max                              0.402759\n",
      "trainer/Policy log std Min                             -1.81499\n",
      "trainer/Alpha                                           0.0242619\n",
      "trainer/Alpha Loss                                     -4.16701\n",
      "exploration/num steps total                        200000\n",
      "exploration/num paths total                           200\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.78602\n",
      "exploration/Rewards Std                                 1.0802\n",
      "exploration/Rewards Max                                 5.84925\n",
      "exploration/Rewards Min                                -1.87461\n",
      "exploration/Returns Mean                             2786.02\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2786.02\n",
      "exploration/Returns Min                              2786.02\n",
      "exploration/Actions Mean                                0.0974914\n",
      "exploration/Actions Std                                 0.882378\n",
      "exploration/Actions Max                                 0.999998\n",
      "exploration/Actions Min                                -0.999983\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2786.02\n",
      "exploration/env_infos/final/reward_run Mean            -2.62565\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.62565\n",
      "exploration/env_infos/final/reward_run Min             -2.62565\n",
      "exploration/env_infos/initial/reward_run Mean           0.870305\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.870305\n",
      "exploration/env_infos/initial/reward_run Min            0.870305\n",
      "exploration/env_infos/reward_run Mean                  -3.90267\n",
      "exploration/env_infos/reward_run Std                    1.15147\n",
      "exploration/env_infos/reward_run Max                    1.20408\n",
      "exploration/env_infos/reward_run Min                   -6.94658\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.522708\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.522708\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.522708\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.319151\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.319151\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.319151\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.472857\n",
      "exploration/env_infos/reward_ctrl Std                   0.0601566\n",
      "exploration/env_infos/reward_ctrl Max                  -0.23476\n",
      "exploration/env_infos/reward_ctrl Min                  -0.589811\n",
      "exploration/env_infos/final/height Mean                 0.0223932\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0223932\n",
      "exploration/env_infos/final/height Min                  0.0223932\n",
      "exploration/env_infos/initial/height Mean               0.0513037\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0513037\n",
      "exploration/env_infos/initial/height Min                0.0513037\n",
      "exploration/env_infos/height Mean                       0.0143279\n",
      "exploration/env_infos/height Std                        0.136886\n",
      "exploration/env_infos/height Max                        0.387756\n",
      "exploration/env_infos/height Min                       -0.329771\n",
      "exploration/env_infos/final/reward_angular Mean         3.99868\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.99868\n",
      "exploration/env_infos/final/reward_angular Min          3.99868\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.22721\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.22721\n",
      "exploration/env_infos/initial/reward_angular Min       -1.22721\n",
      "exploration/env_infos/reward_angular Mean               0.12044\n",
      "exploration/env_infos/reward_angular Std                3.08989\n",
      "exploration/env_infos/reward_angular Max               10.5288\n",
      "exploration/env_infos/reward_angular Min               -6.55704\n",
      "evaluation/num steps total                              4.975e+06\n",
      "evaluation/num paths total                           4975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.75654\n",
      "evaluation/Rewards Std                                  1.64784\n",
      "evaluation/Rewards Max                                  8.84883\n",
      "evaluation/Rewards Min                                 -7.38306\n",
      "evaluation/Returns Mean                               756.54\n",
      "evaluation/Returns Std                                939.777\n",
      "evaluation/Returns Max                               2659.56\n",
      "evaluation/Returns Min                               -634.491\n",
      "evaluation/Actions Mean                                 0.143705\n",
      "evaluation/Actions Std                                  0.81847\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            756.54\n",
      "evaluation/env_infos/final/reward_run Mean             -1.20243\n",
      "evaluation/env_infos/final/reward_run Std               2.98435\n",
      "evaluation/env_infos/final/reward_run Max               4.26509\n",
      "evaluation/env_infos/final/reward_run Min              -6.5267\n",
      "evaluation/env_infos/initial/reward_run Mean            0.136204\n",
      "evaluation/env_infos/initial/reward_run Std             0.555833\n",
      "evaluation/env_infos/initial/reward_run Max             1.08201\n",
      "evaluation/env_infos/initial/reward_run Min            -0.848709\n",
      "evaluation/env_infos/reward_run Mean                   -1.16593\n",
      "evaluation/env_infos/reward_run Std                     2.8243\n",
      "evaluation/env_infos/reward_run Max                     5.89768\n",
      "evaluation/env_infos/reward_run Min                    -7.38114\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.415738\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.117179\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0667531\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.563328\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.24854\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0866214\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0695325\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.404847\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.414327\n",
      "evaluation/env_infos/reward_ctrl Std                    0.108126\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0251123\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598432\n",
      "evaluation/env_infos/final/height Mean                 -0.182152\n",
      "evaluation/env_infos/final/height Std                   0.278455\n",
      "evaluation/env_infos/final/height Max                   0.340407\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0102238\n",
      "evaluation/env_infos/initial/height Std                 0.0481807\n",
      "evaluation/env_infos/initial/height Max                 0.0484241\n",
      "evaluation/env_infos/initial/height Min                -0.0982708\n",
      "evaluation/env_infos/height Mean                       -0.138537\n",
      "evaluation/env_infos/height Std                         0.264253\n",
      "evaluation/env_infos/height Max                         0.538919\n",
      "evaluation/env_infos/height Min                        -0.58632\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0700113\n",
      "evaluation/env_infos/final/reward_angular Std           2.59579\n",
      "evaluation/env_infos/final/reward_angular Max           6.73991\n",
      "evaluation/env_infos/final/reward_angular Min          -3.59063\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.167067\n",
      "evaluation/env_infos/initial/reward_angular Std         0.87483\n",
      "evaluation/env_infos/initial/reward_angular Max         2.10639\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.31689\n",
      "evaluation/env_infos/reward_angular Mean                0.0900035\n",
      "evaluation/env_infos/reward_angular Std                 2.22216\n",
      "evaluation/env_infos/reward_angular Max                10.9627\n",
      "evaluation/env_infos/reward_angular Min                -7.61904\n",
      "time/data storing (s)                                   0.0158192\n",
      "time/evaluation sampling (s)                           24.3844\n",
      "time/exploration sampling (s)                           1.08387\n",
      "time/logging (s)                                        0.238469\n",
      "time/saving (s)                                         0.0270414\n",
      "time/training (s)                                       4.50881\n",
      "time/epoch (s)                                         30.2584\n",
      "time/total (s)                                       6098.38\n",
      "Epoch                                                 198\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:54:13.859659 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 199 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 201000\n",
      "trainer/QF1 Loss                                        1.44954\n",
      "trainer/QF2 Loss                                        2.15867\n",
      "trainer/Policy Loss                                   -10.5123\n",
      "trainer/Q1 Predictions Mean                            15.9492\n",
      "trainer/Q1 Predictions Std                             34.134\n",
      "trainer/Q1 Predictions Max                            133.547\n",
      "trainer/Q1 Predictions Min                            -19.9846\n",
      "trainer/Q2 Predictions Mean                            16.1874\n",
      "trainer/Q2 Predictions Std                             34.6239\n",
      "trainer/Q2 Predictions Max                            135.874\n",
      "trainer/Q2 Predictions Min                            -20.1873\n",
      "trainer/Q Targets Mean                                 15.9858\n",
      "trainer/Q Targets Std                                  34.1245\n",
      "trainer/Q Targets Max                                 132.506\n",
      "trainer/Q Targets Min                                 -20.3278\n",
      "trainer/Log Pis Mean                                    5.64099\n",
      "trainer/Log Pis Std                                     5.96532\n",
      "trainer/Log Pis Max                                    33.8728\n",
      "trainer/Log Pis Min                                    -5.72023\n",
      "trainer/Policy mu Mean                                 -0.0142288\n",
      "trainer/Policy mu Std                                   1.57621\n",
      "trainer/Policy mu Max                                   5.11544\n",
      "trainer/Policy mu Min                                  -5.27893\n",
      "trainer/Policy log std Mean                            -0.706424\n",
      "trainer/Policy log std Std                              0.318031\n",
      "trainer/Policy log std Max                              0.271861\n",
      "trainer/Policy log std Min                             -2.34542\n",
      "trainer/Alpha                                           0.0231712\n",
      "trainer/Alpha Loss                                     -1.35209\n",
      "exploration/num steps total                        201000\n",
      "exploration/num paths total                           201\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.36283\n",
      "exploration/Rewards Std                                 1.18522\n",
      "exploration/Rewards Max                                 3.85798\n",
      "exploration/Rewards Min                                -3.686\n",
      "exploration/Returns Mean                              362.83\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               362.83\n",
      "exploration/Returns Min                               362.83\n",
      "exploration/Actions Mean                                0.27064\n",
      "exploration/Actions Std                                 0.740771\n",
      "exploration/Actions Max                                 0.999036\n",
      "exploration/Actions Min                                -0.999922\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           362.83\n",
      "exploration/env_infos/final/reward_run Mean             2.67464\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              2.67464\n",
      "exploration/env_infos/final/reward_run Min              2.67464\n",
      "exploration/env_infos/initial/reward_run Mean           0.875199\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.875199\n",
      "exploration/env_infos/initial/reward_run Min            0.875199\n",
      "exploration/env_infos/reward_run Mean                   2.13827\n",
      "exploration/env_infos/reward_run Std                    1.02407\n",
      "exploration/env_infos/reward_run Max                    4.68532\n",
      "exploration/env_infos/reward_run Min                   -1.07079\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.279157\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.279157\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.279157\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.297436\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.297436\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.297436\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.373192\n",
      "exploration/env_infos/reward_ctrl Std                   0.10242\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0660099\n",
      "exploration/env_infos/reward_ctrl Min                  -0.58587\n",
      "exploration/env_infos/final/height Mean                -0.0737786\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0737786\n",
      "exploration/env_infos/final/height Min                 -0.0737786\n",
      "exploration/env_infos/initial/height Mean              -0.0684984\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0684984\n",
      "exploration/env_infos/initial/height Min               -0.0684984\n",
      "exploration/env_infos/height Mean                      -0.0187821\n",
      "exploration/env_infos/height Std                        0.0944259\n",
      "exploration/env_infos/height Max                        0.242351\n",
      "exploration/env_infos/height Min                       -0.236727\n",
      "exploration/env_infos/final/reward_angular Mean         3.19471\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          3.19471\n",
      "exploration/env_infos/final/reward_angular Min          3.19471\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.248359\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.248359\n",
      "exploration/env_infos/initial/reward_angular Min       -0.248359\n",
      "exploration/env_infos/reward_angular Mean              -0.0721716\n",
      "exploration/env_infos/reward_angular Std                1.5885\n",
      "exploration/env_infos/reward_angular Max                5.28484\n",
      "exploration/env_infos/reward_angular Min               -4.68807\n",
      "evaluation/num steps total                              5e+06\n",
      "evaluation/num paths total                           5000\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.748383\n",
      "evaluation/Rewards Std                                  1.57548\n",
      "evaluation/Rewards Max                                  8.73447\n",
      "evaluation/Rewards Min                                 -8.37854\n",
      "evaluation/Returns Mean                               748.383\n",
      "evaluation/Returns Std                               1067.4\n",
      "evaluation/Returns Max                               3204.57\n",
      "evaluation/Returns Min                               -619.823\n",
      "evaluation/Actions Mean                                 0.0881865\n",
      "evaluation/Actions Std                                  0.797266\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -0.999998\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            748.383\n",
      "evaluation/env_infos/final/reward_run Mean             -0.250861\n",
      "evaluation/env_infos/final/reward_run Std               2.52363\n",
      "evaluation/env_infos/final/reward_run Max               4.32441\n",
      "evaluation/env_infos/final/reward_run Min              -5.95414\n",
      "evaluation/env_infos/initial/reward_run Mean            0.382147\n",
      "evaluation/env_infos/initial/reward_run Std             0.521235\n",
      "evaluation/env_infos/initial/reward_run Max             1.09337\n",
      "evaluation/env_infos/initial/reward_run Min            -0.745853\n",
      "evaluation/env_infos/reward_run Mean                   -0.596946\n",
      "evaluation/env_infos/reward_run Std                     2.84739\n",
      "evaluation/env_infos/reward_run Max                     6.09438\n",
      "evaluation/env_infos/reward_run Min                    -7.97934\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.348397\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.110692\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0890469\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.503406\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.288295\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.108426\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0549937\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.497953\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.386046\n",
      "evaluation/env_infos/reward_ctrl Std                    0.1226\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0184217\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593628\n",
      "evaluation/env_infos/final/height Mean                 -0.287015\n",
      "evaluation/env_infos/final/height Std                   0.276281\n",
      "evaluation/env_infos/final/height Max                   0.129257\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0241933\n",
      "evaluation/env_infos/initial/height Std                 0.0423752\n",
      "evaluation/env_infos/initial/height Max                 0.0523006\n",
      "evaluation/env_infos/initial/height Min                -0.0902512\n",
      "evaluation/env_infos/height Mean                       -0.223144\n",
      "evaluation/env_infos/height Std                         0.274537\n",
      "evaluation/env_infos/height Max                         0.41313\n",
      "evaluation/env_infos/height Min                        -0.592355\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.12099\n",
      "evaluation/env_infos/final/reward_angular Std           1.24847\n",
      "evaluation/env_infos/final/reward_angular Max           2.77563\n",
      "evaluation/env_infos/final/reward_angular Min          -2.99229\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.66686\n",
      "evaluation/env_infos/initial/reward_angular Std         0.865907\n",
      "evaluation/env_infos/initial/reward_angular Max         1.30602\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.93073\n",
      "evaluation/env_infos/reward_angular Mean                0.0512223\n",
      "evaluation/env_infos/reward_angular Std                 2.01499\n",
      "evaluation/env_infos/reward_angular Max                10.2122\n",
      "evaluation/env_infos/reward_angular Min                -7.42679\n",
      "time/data storing (s)                                   0.015931\n",
      "time/evaluation sampling (s)                           24.9923\n",
      "time/exploration sampling (s)                           1.07328\n",
      "time/logging (s)                                        0.254034\n",
      "time/saving (s)                                         0.0285331\n",
      "time/training (s)                                       4.4158\n",
      "time/epoch (s)                                         30.7799\n",
      "time/total (s)                                       6130.72\n",
      "Epoch                                                 199\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:54:42.980489 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 200 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 202000\n",
      "trainer/QF1 Loss                                        2.65504\n",
      "trainer/QF2 Loss                                        2.19267\n",
      "trainer/Policy Loss                                   -16.6408\n",
      "trainer/Q1 Predictions Mean                            23.7312\n",
      "trainer/Q1 Predictions Std                             38.4327\n",
      "trainer/Q1 Predictions Max                            153.425\n",
      "trainer/Q1 Predictions Min                            -19.6952\n",
      "trainer/Q2 Predictions Mean                            23.5445\n",
      "trainer/Q2 Predictions Std                             38.1343\n",
      "trainer/Q2 Predictions Max                            151.268\n",
      "trainer/Q2 Predictions Min                            -19.7384\n",
      "trainer/Q Targets Mean                                 23.6743\n",
      "trainer/Q Targets Std                                  38.0321\n",
      "trainer/Q Targets Max                                 153.946\n",
      "trainer/Q Targets Min                                 -19.7\n",
      "trainer/Log Pis Mean                                    7.14535\n",
      "trainer/Log Pis Std                                     5.96184\n",
      "trainer/Log Pis Max                                    23.191\n",
      "trainer/Log Pis Min                                    -7.32958\n",
      "trainer/Policy mu Mean                                  0.0561689\n",
      "trainer/Policy mu Std                                   1.67328\n",
      "trainer/Policy mu Max                                   4.80692\n",
      "trainer/Policy mu Min                                  -7.96622\n",
      "trainer/Policy log std Mean                            -0.735149\n",
      "trainer/Policy log std Std                              0.299168\n",
      "trainer/Policy log std Max                              0.498419\n",
      "trainer/Policy log std Min                             -2.18363\n",
      "trainer/Alpha                                           0.02206\n",
      "trainer/Alpha Loss                                      4.36886\n",
      "exploration/num steps total                        202000\n",
      "exploration/num paths total                           202\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.0320883\n",
      "exploration/Rewards Std                                 0.913973\n",
      "exploration/Rewards Max                                 5.27093\n",
      "exploration/Rewards Min                                -4.86703\n",
      "exploration/Returns Mean                              -32.0883\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               -32.0883\n",
      "exploration/Returns Min                               -32.0883\n",
      "exploration/Actions Mean                                0.118669\n",
      "exploration/Actions Std                                 0.766241\n",
      "exploration/Actions Max                                 0.999997\n",
      "exploration/Actions Min                                -0.999966\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           -32.0883\n",
      "exploration/env_infos/final/reward_run Mean            -0.757372\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.757372\n",
      "exploration/env_infos/final/reward_run Min             -0.757372\n",
      "exploration/env_infos/initial/reward_run Mean          -0.429968\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.429968\n",
      "exploration/env_infos/initial/reward_run Min           -0.429968\n",
      "exploration/env_infos/reward_run Mean                   0.075677\n",
      "exploration/env_infos/reward_run Std                    0.561672\n",
      "exploration/env_infos/reward_run Max                    1.93536\n",
      "exploration/env_infos/reward_run Min                   -2.03872\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.411905\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.411905\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.411905\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.21\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.21\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.21\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.360724\n",
      "exploration/env_infos/reward_ctrl Std                   0.0771476\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0842244\n",
      "exploration/env_infos/reward_ctrl Min                  -0.592489\n",
      "exploration/env_infos/final/height Mean                -0.565581\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.565581\n",
      "exploration/env_infos/final/height Min                 -0.565581\n",
      "exploration/env_infos/initial/height Mean               0.0692766\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0692766\n",
      "exploration/env_infos/initial/height Min                0.0692766\n",
      "exploration/env_infos/height Mean                      -0.445859\n",
      "exploration/env_infos/height Std                        0.233973\n",
      "exploration/env_infos/height Max                        0.393015\n",
      "exploration/env_infos/height Min                       -0.589199\n",
      "exploration/env_infos/final/reward_angular Mean        -1.0109\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.0109\n",
      "exploration/env_infos/final/reward_angular Min         -1.0109\n",
      "exploration/env_infos/initial/reward_angular Mean       1.39097\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.39097\n",
      "exploration/env_infos/initial/reward_angular Min        1.39097\n",
      "exploration/env_infos/reward_angular Mean               0.211559\n",
      "exploration/env_infos/reward_angular Std                1.2759\n",
      "exploration/env_infos/reward_angular Max                8.18987\n",
      "exploration/env_infos/reward_angular Min               -5.80216\n",
      "evaluation/num steps total                              5.025e+06\n",
      "evaluation/num paths total                           5025\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.839361\n",
      "evaluation/Rewards Std                                  1.73955\n",
      "evaluation/Rewards Max                                  8.9802\n",
      "evaluation/Rewards Min                                 -8.67519\n",
      "evaluation/Returns Mean                               839.361\n",
      "evaluation/Returns Std                                934.121\n",
      "evaluation/Returns Max                               2929.56\n",
      "evaluation/Returns Min                               -347.064\n",
      "evaluation/Actions Mean                                 0.165966\n",
      "evaluation/Actions Std                                  0.805916\n",
      "evaluation/Actions Max                                  0.999998\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            839.361\n",
      "evaluation/env_infos/final/reward_run Mean             -0.878241\n",
      "evaluation/env_infos/final/reward_run Std               3.22184\n",
      "evaluation/env_infos/final/reward_run Max               4.47215\n",
      "evaluation/env_infos/final/reward_run Min              -6.42588\n",
      "evaluation/env_infos/initial/reward_run Mean            0.350172\n",
      "evaluation/env_infos/initial/reward_run Std             0.488333\n",
      "evaluation/env_infos/initial/reward_run Max             1.18905\n",
      "evaluation/env_infos/initial/reward_run Min            -0.470167\n",
      "evaluation/env_infos/reward_run Mean                   -1.08124\n",
      "evaluation/env_infos/reward_run Std                     3.13833\n",
      "evaluation/env_infos/reward_run Max                     6.15523\n",
      "evaluation/env_infos/reward_run Min                    -8.56339\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.395961\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.11001\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.162837\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.55976\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.306428\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.105889\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0654648\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.484287\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.406227\n",
      "evaluation/env_infos/reward_ctrl Std                    0.094477\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0399372\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.593786\n",
      "evaluation/env_infos/final/height Mean                 -0.185418\n",
      "evaluation/env_infos/final/height Std                   0.267157\n",
      "evaluation/env_infos/final/height Max                   0.219598\n",
      "evaluation/env_infos/final/height Min                  -0.577541\n",
      "evaluation/env_infos/initial/height Mean               -0.00541662\n",
      "evaluation/env_infos/initial/height Std                 0.0511542\n",
      "evaluation/env_infos/initial/height Max                 0.097613\n",
      "evaluation/env_infos/initial/height Min                -0.0959757\n",
      "evaluation/env_infos/height Mean                       -0.126751\n",
      "evaluation/env_infos/height Std                         0.248207\n",
      "evaluation/env_infos/height Max                         0.519888\n",
      "evaluation/env_infos/height Min                        -0.586162\n",
      "evaluation/env_infos/final/reward_angular Mean          0.0316821\n",
      "evaluation/env_infos/final/reward_angular Std           1.95497\n",
      "evaluation/env_infos/final/reward_angular Max           4.22615\n",
      "evaluation/env_infos/final/reward_angular Min          -4.70724\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.0701625\n",
      "evaluation/env_infos/initial/reward_angular Std         1.07626\n",
      "evaluation/env_infos/initial/reward_angular Max         2.05783\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.70224\n",
      "evaluation/env_infos/reward_angular Mean                0.0683805\n",
      "evaluation/env_infos/reward_angular Std                 2.37438\n",
      "evaluation/env_infos/reward_angular Max                10.1494\n",
      "evaluation/env_infos/reward_angular Min                -6.60977\n",
      "time/data storing (s)                                   0.0162637\n",
      "time/evaluation sampling (s)                           21.4337\n",
      "time/exploration sampling (s)                           1.09865\n",
      "time/logging (s)                                        0.237989\n",
      "time/saving (s)                                         0.0538248\n",
      "time/training (s)                                       4.69507\n",
      "time/epoch (s)                                         27.5355\n",
      "time/total (s)                                       6159.82\n",
      "Epoch                                                 200\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:55:14.108195 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 201 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 203000\n",
      "trainer/QF1 Loss                                        1.72103\n",
      "trainer/QF2 Loss                                        2.21088\n",
      "trainer/Policy Loss                                   -14.6874\n",
      "trainer/Q1 Predictions Mean                            21.2243\n",
      "trainer/Q1 Predictions Std                             37.2989\n",
      "trainer/Q1 Predictions Max                            151.344\n",
      "trainer/Q1 Predictions Min                            -23.2963\n",
      "trainer/Q2 Predictions Mean                            21.07\n",
      "trainer/Q2 Predictions Std                             37.1925\n",
      "trainer/Q2 Predictions Max                            151.451\n",
      "trainer/Q2 Predictions Min                            -23.1323\n",
      "trainer/Q Targets Mean                                 21.0763\n",
      "trainer/Q Targets Std                                  37.186\n",
      "trainer/Q Targets Max                                 147.972\n",
      "trainer/Q Targets Min                                 -23.588\n",
      "trainer/Log Pis Mean                                    6.75913\n",
      "trainer/Log Pis Std                                     6.1552\n",
      "trainer/Log Pis Max                                    29.4895\n",
      "trainer/Log Pis Min                                    -6.83317\n",
      "trainer/Policy mu Mean                                 -0.0194849\n",
      "trainer/Policy mu Std                                   1.64943\n",
      "trainer/Policy mu Max                                   5.1606\n",
      "trainer/Policy mu Min                                  -5.09127\n",
      "trainer/Policy log std Mean                            -0.71502\n",
      "trainer/Policy log std Std                              0.280779\n",
      "trainer/Policy log std Max                              0.337257\n",
      "trainer/Policy log std Min                             -1.79019\n",
      "trainer/Alpha                                           0.02345\n",
      "trainer/Alpha Loss                                      2.8501\n",
      "exploration/num steps total                        203000\n",
      "exploration/num paths total                           203\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.656869\n",
      "exploration/Rewards Std                                 1.83598\n",
      "exploration/Rewards Max                                 8.11826\n",
      "exploration/Rewards Min                                -3.21748\n",
      "exploration/Returns Mean                              656.869\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               656.869\n",
      "exploration/Returns Min                               656.869\n",
      "exploration/Actions Mean                                0.240212\n",
      "exploration/Actions Std                                 0.71332\n",
      "exploration/Actions Max                                 0.999939\n",
      "exploration/Actions Min                                -0.999944\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           656.869\n",
      "exploration/env_infos/final/reward_run Mean            -0.284845\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.284845\n",
      "exploration/env_infos/final/reward_run Min             -0.284845\n",
      "exploration/env_infos/initial/reward_run Mean           0.445677\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.445677\n",
      "exploration/env_infos/initial/reward_run Min            0.445677\n",
      "exploration/env_infos/reward_run Mean                  -0.954755\n",
      "exploration/env_infos/reward_run Std                    1.72944\n",
      "exploration/env_infos/reward_run Max                    1.37407\n",
      "exploration/env_infos/reward_run Min                   -6.40213\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.36906\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.36906\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.36906\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.306512\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.306512\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.306512\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.339917\n",
      "exploration/env_infos/reward_ctrl Std                   0.107392\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0688139\n",
      "exploration/env_infos/reward_ctrl Min                  -0.580756\n",
      "exploration/env_infos/final/height Mean                -0.559853\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.559853\n",
      "exploration/env_infos/final/height Min                 -0.559853\n",
      "exploration/env_infos/initial/height Mean              -0.110162\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.110162\n",
      "exploration/env_infos/initial/height Min               -0.110162\n",
      "exploration/env_infos/height Mean                      -0.418411\n",
      "exploration/env_infos/height Std                        0.246929\n",
      "exploration/env_infos/height Max                        0.338844\n",
      "exploration/env_infos/height Min                       -0.584817\n",
      "exploration/env_infos/final/reward_angular Mean        -1.64155\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.64155\n",
      "exploration/env_infos/final/reward_angular Min         -1.64155\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.603728\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.603728\n",
      "exploration/env_infos/initial/reward_angular Min       -0.603728\n",
      "exploration/env_infos/reward_angular Mean              -0.0159368\n",
      "exploration/env_infos/reward_angular Std                1.86697\n",
      "exploration/env_infos/reward_angular Max                9.21773\n",
      "exploration/env_infos/reward_angular Min               -5.29755\n",
      "evaluation/num steps total                              5.05e+06\n",
      "evaluation/num paths total                           5050\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.671948\n",
      "evaluation/Rewards Std                                  1.61725\n",
      "evaluation/Rewards Max                                  8.80091\n",
      "evaluation/Rewards Min                                 -8.10694\n",
      "evaluation/Returns Mean                               671.948\n",
      "evaluation/Returns Std                                895.764\n",
      "evaluation/Returns Max                               3008.1\n",
      "evaluation/Returns Min                               -468.261\n",
      "evaluation/Actions Mean                                 0.102764\n",
      "evaluation/Actions Std                                  0.802415\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -0.999999\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            671.948\n",
      "evaluation/env_infos/final/reward_run Mean              0.0599885\n",
      "evaluation/env_infos/final/reward_run Std               2.65988\n",
      "evaluation/env_infos/final/reward_run Max               4.47751\n",
      "evaluation/env_infos/final/reward_run Min              -5.96885\n",
      "evaluation/env_infos/initial/reward_run Mean            0.304939\n",
      "evaluation/env_infos/initial/reward_run Std             0.488802\n",
      "evaluation/env_infos/initial/reward_run Max             1.07121\n",
      "evaluation/env_infos/initial/reward_run Min            -0.562104\n",
      "evaluation/env_infos/reward_run Mean                   -0.379542\n",
      "evaluation/env_infos/reward_run Std                     2.888\n",
      "evaluation/env_infos/reward_run Max                     5.74757\n",
      "evaluation/env_infos/reward_run Min                    -7.85699\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.374839\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.13714\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0640228\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.549467\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.28269\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.098137\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0306833\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.458476\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.392658\n",
      "evaluation/env_infos/reward_ctrl Std                    0.132947\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0306833\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.592193\n",
      "evaluation/env_infos/final/height Mean                 -0.222297\n",
      "evaluation/env_infos/final/height Std                   0.27359\n",
      "evaluation/env_infos/final/height Max                   0.25391\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.00914965\n",
      "evaluation/env_infos/initial/height Std                 0.0482929\n",
      "evaluation/env_infos/initial/height Max                 0.0686709\n",
      "evaluation/env_infos/initial/height Min                -0.10766\n",
      "evaluation/env_infos/height Mean                       -0.19861\n",
      "evaluation/env_infos/height Std                         0.248655\n",
      "evaluation/env_infos/height Max                         0.507564\n",
      "evaluation/env_infos/height Min                        -0.59281\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.22958\n",
      "evaluation/env_infos/final/reward_angular Std           1.54098\n",
      "evaluation/env_infos/final/reward_angular Max           2.51733\n",
      "evaluation/env_infos/final/reward_angular Min          -4.19825\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.192099\n",
      "evaluation/env_infos/initial/reward_angular Std         0.967698\n",
      "evaluation/env_infos/initial/reward_angular Max         2.70603\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.73968\n",
      "evaluation/env_infos/reward_angular Mean                0.023723\n",
      "evaluation/env_infos/reward_angular Std                 2.12715\n",
      "evaluation/env_infos/reward_angular Max                10.7793\n",
      "evaluation/env_infos/reward_angular Min                -6.86206\n",
      "time/data storing (s)                                   0.0157058\n",
      "time/evaluation sampling (s)                           23.9754\n",
      "time/exploration sampling (s)                           1.09656\n",
      "time/logging (s)                                        0.22463\n",
      "time/saving (s)                                         0.0276321\n",
      "time/training (s)                                       4.32539\n",
      "time/epoch (s)                                         29.6653\n",
      "time/total (s)                                       6190.93\n",
      "Epoch                                                 201\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:55:43.510016 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 202 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 204000\n",
      "trainer/QF1 Loss                                        3.33781\n",
      "trainer/QF2 Loss                                        2.92694\n",
      "trainer/Policy Loss                                   -15.0719\n",
      "trainer/Q1 Predictions Mean                            20.7567\n",
      "trainer/Q1 Predictions Std                             36.1265\n",
      "trainer/Q1 Predictions Max                            146.294\n",
      "trainer/Q1 Predictions Min                            -20.1061\n",
      "trainer/Q2 Predictions Mean                            20.6961\n",
      "trainer/Q2 Predictions Std                             36.0033\n",
      "trainer/Q2 Predictions Max                            144.437\n",
      "trainer/Q2 Predictions Min                            -19.9585\n",
      "trainer/Q Targets Mean                                 20.6809\n",
      "trainer/Q Targets Std                                  36.4378\n",
      "trainer/Q Targets Max                                 150.95\n",
      "trainer/Q Targets Min                                 -20.3082\n",
      "trainer/Log Pis Mean                                    5.89532\n",
      "trainer/Log Pis Std                                     5.78504\n",
      "trainer/Log Pis Max                                    30.7527\n",
      "trainer/Log Pis Min                                    -4.33895\n",
      "trainer/Policy mu Mean                                 -0.205894\n",
      "trainer/Policy mu Std                                   1.58358\n",
      "trainer/Policy mu Max                                   4.49136\n",
      "trainer/Policy mu Min                                  -4.99801\n",
      "trainer/Policy log std Mean                            -0.707456\n",
      "trainer/Policy log std Std                              0.270598\n",
      "trainer/Policy log std Max                              0.191325\n",
      "trainer/Policy log std Min                             -2.75368\n",
      "trainer/Alpha                                           0.0245456\n",
      "trainer/Alpha Loss                                     -0.388173\n",
      "exploration/num steps total                        204000\n",
      "exploration/num paths total                           204\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.229341\n",
      "exploration/Rewards Std                                 0.799375\n",
      "exploration/Rewards Max                                 2.95336\n",
      "exploration/Rewards Min                                -3.70672\n",
      "exploration/Returns Mean                             -229.341\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -229.341\n",
      "exploration/Returns Min                              -229.341\n",
      "exploration/Actions Mean                                0.0316243\n",
      "exploration/Actions Std                                 0.747173\n",
      "exploration/Actions Max                                 0.999926\n",
      "exploration/Actions Min                                -0.999999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -229.341\n",
      "exploration/env_infos/final/reward_run Mean            -0.511118\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -0.511118\n",
      "exploration/env_infos/final/reward_run Min             -0.511118\n",
      "exploration/env_infos/initial/reward_run Mean           1.07524\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            1.07524\n",
      "exploration/env_infos/initial/reward_run Min            1.07524\n",
      "exploration/env_infos/reward_run Mean                   0.230725\n",
      "exploration/env_infos/reward_run Std                    0.757137\n",
      "exploration/env_infos/reward_run Max                    3.66216\n",
      "exploration/env_infos/reward_run Min                   -2.2188\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.33179\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.33179\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.33179\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.250234\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.250234\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.250234\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.33556\n",
      "exploration/env_infos/reward_ctrl Std                   0.0820669\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0799832\n",
      "exploration/env_infos/reward_ctrl Min                  -0.584246\n",
      "exploration/env_infos/final/height Mean                -0.576667\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.576667\n",
      "exploration/env_infos/final/height Min                 -0.576667\n",
      "exploration/env_infos/initial/height Mean              -0.0798164\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0798164\n",
      "exploration/env_infos/initial/height Min               -0.0798164\n",
      "exploration/env_infos/height Mean                      -0.393157\n",
      "exploration/env_infos/height Std                        0.256548\n",
      "exploration/env_infos/height Max                        0.331227\n",
      "exploration/env_infos/height Min                       -0.584441\n",
      "exploration/env_infos/final/reward_angular Mean        -0.181949\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.181949\n",
      "exploration/env_infos/final/reward_angular Min         -0.181949\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.0805377\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.0805377\n",
      "exploration/env_infos/initial/reward_angular Min       -0.0805377\n",
      "exploration/env_infos/reward_angular Mean               0.204057\n",
      "exploration/env_infos/reward_angular Std                1.47938\n",
      "exploration/env_infos/reward_angular Max                6.55664\n",
      "exploration/env_infos/reward_angular Min               -6.71648\n",
      "evaluation/num steps total                              5.075e+06\n",
      "evaluation/num paths total                           5075\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.493448\n",
      "evaluation/Rewards Std                                  1.54375\n",
      "evaluation/Rewards Max                                  8.04361\n",
      "evaluation/Rewards Min                                 -8.91895\n",
      "evaluation/Returns Mean                               493.448\n",
      "evaluation/Returns Std                                751.728\n",
      "evaluation/Returns Max                               2651.97\n",
      "evaluation/Returns Min                               -411.494\n",
      "evaluation/Actions Mean                                 0.0186042\n",
      "evaluation/Actions Std                                  0.782905\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            493.448\n",
      "evaluation/env_infos/final/reward_run Mean             -0.0808619\n",
      "evaluation/env_infos/final/reward_run Std               1.96135\n",
      "evaluation/env_infos/final/reward_run Max               4.05512\n",
      "evaluation/env_infos/final/reward_run Min              -6.53961\n",
      "evaluation/env_infos/initial/reward_run Mean            0.307887\n",
      "evaluation/env_infos/initial/reward_run Std             0.53523\n",
      "evaluation/env_infos/initial/reward_run Max             1.07334\n",
      "evaluation/env_infos/initial/reward_run Min            -0.852611\n",
      "evaluation/env_infos/reward_run Mean                   -0.316751\n",
      "evaluation/env_infos/reward_run Std                     2.61198\n",
      "evaluation/env_infos/reward_run Max                     5.78368\n",
      "evaluation/env_infos/reward_run Min                    -8.06425\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.322659\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.12305\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0608927\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.494535\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.288355\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0978434\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0712815\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.433145\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.367972\n",
      "evaluation/env_infos/reward_ctrl Std                    0.115681\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0217091\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59724\n",
      "evaluation/env_infos/final/height Mean                 -0.313058\n",
      "evaluation/env_infos/final/height Std                   0.264501\n",
      "evaluation/env_infos/final/height Max                   0.0665826\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean                0.00995565\n",
      "evaluation/env_infos/initial/height Std                 0.0519945\n",
      "evaluation/env_infos/initial/height Max                 0.0727192\n",
      "evaluation/env_infos/initial/height Min                -0.121928\n",
      "evaluation/env_infos/height Mean                       -0.218316\n",
      "evaluation/env_infos/height Std                         0.273558\n",
      "evaluation/env_infos/height Max                         0.50026\n",
      "evaluation/env_infos/height Min                        -0.59456\n",
      "evaluation/env_infos/final/reward_angular Mean          0.475461\n",
      "evaluation/env_infos/final/reward_angular Std           1.66127\n",
      "evaluation/env_infos/final/reward_angular Max           4.46276\n",
      "evaluation/env_infos/final/reward_angular Min          -3.09659\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.4223\n",
      "evaluation/env_infos/initial/reward_angular Std         0.727299\n",
      "evaluation/env_infos/initial/reward_angular Max         1.0359\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.87829\n",
      "evaluation/env_infos/reward_angular Mean                0.0413139\n",
      "evaluation/env_infos/reward_angular Std                 2.08186\n",
      "evaluation/env_infos/reward_angular Max                10.3346\n",
      "evaluation/env_infos/reward_angular Min                -7.7861\n",
      "time/data storing (s)                                   0.0158043\n",
      "time/evaluation sampling (s)                           22.5393\n",
      "time/exploration sampling (s)                           1.13933\n",
      "time/logging (s)                                        0.239107\n",
      "time/saving (s)                                         0.0262906\n",
      "time/training (s)                                       4.20359\n",
      "time/epoch (s)                                         28.1634\n",
      "time/total (s)                                       6220.34\n",
      "Epoch                                                 202\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:56:19.505942 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 203 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 205000\n",
      "trainer/QF1 Loss                                        2.69537\n",
      "trainer/QF2 Loss                                        2.46142\n",
      "trainer/Policy Loss                                   -16.5545\n",
      "trainer/Q1 Predictions Mean                            22.5287\n",
      "trainer/Q1 Predictions Std                             37.4729\n",
      "trainer/Q1 Predictions Max                            162.261\n",
      "trainer/Q1 Predictions Min                            -20.7897\n",
      "trainer/Q2 Predictions Mean                            22.7237\n",
      "trainer/Q2 Predictions Std                             37.3016\n",
      "trainer/Q2 Predictions Max                            159.43\n",
      "trainer/Q2 Predictions Min                            -20.9461\n",
      "trainer/Q Targets Mean                                 22.6315\n",
      "trainer/Q Targets Std                                  37.4517\n",
      "trainer/Q Targets Max                                 158.204\n",
      "trainer/Q Targets Min                                 -20.2781\n",
      "trainer/Log Pis Mean                                    6.27738\n",
      "trainer/Log Pis Std                                     6.31787\n",
      "trainer/Log Pis Max                                    39.4391\n",
      "trainer/Log Pis Min                                    -4.93259\n",
      "trainer/Policy mu Mean                                 -0.201847\n",
      "trainer/Policy mu Std                                   1.65643\n",
      "trainer/Policy mu Max                                   6.2014\n",
      "trainer/Policy mu Min                                  -7.09526\n",
      "trainer/Policy log std Mean                            -0.680679\n",
      "trainer/Policy log std Std                              0.313814\n",
      "trainer/Policy log std Max                              0.625233\n",
      "trainer/Policy log std Min                             -2.31275\n",
      "trainer/Alpha                                           0.0223318\n",
      "trainer/Alpha Loss                                      1.0541\n",
      "exploration/num steps total                        205000\n",
      "exploration/num paths total                           205\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.199071\n",
      "exploration/Rewards Std                                 0.238768\n",
      "exploration/Rewards Max                                 0.820916\n",
      "exploration/Rewards Min                                -0.521762\n",
      "exploration/Returns Mean                              199.071\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               199.071\n",
      "exploration/Returns Min                               199.071\n",
      "exploration/Actions Mean                                0.0723613\n",
      "exploration/Actions Std                                 0.725572\n",
      "exploration/Actions Max                                 0.999507\n",
      "exploration/Actions Min                                -0.99999\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           199.071\n",
      "exploration/env_infos/final/reward_run Mean             1.38152\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              1.38152\n",
      "exploration/env_infos/final/reward_run Min              1.38152\n",
      "exploration/env_infos/initial/reward_run Mean           0.907385\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.907385\n",
      "exploration/env_infos/initial/reward_run Min            0.907385\n",
      "exploration/env_infos/reward_run Mean                   1.15893\n",
      "exploration/env_infos/reward_run Std                    1.096\n",
      "exploration/env_infos/reward_run Max                    3.88405\n",
      "exploration/env_infos/reward_run Min                   -1.49533\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0875088\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0875088\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0875088\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.191952\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.191952\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.191952\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.319015\n",
      "exploration/env_infos/reward_ctrl Std                   0.090149\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0762401\n",
      "exploration/env_infos/reward_ctrl Min                  -0.565531\n",
      "exploration/env_infos/final/height Mean                 0.132268\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.132268\n",
      "exploration/env_infos/final/height Min                  0.132268\n",
      "exploration/env_infos/initial/height Mean              -0.0828923\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0828923\n",
      "exploration/env_infos/initial/height Min               -0.0828923\n",
      "exploration/env_infos/height Mean                       0.0174045\n",
      "exploration/env_infos/height Std                        0.0877991\n",
      "exploration/env_infos/height Max                        0.352238\n",
      "exploration/env_infos/height Min                       -0.214269\n",
      "exploration/env_infos/final/reward_angular Mean         0.943856\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.943856\n",
      "exploration/env_infos/final/reward_angular Min          0.943856\n",
      "exploration/env_infos/initial/reward_angular Mean       0.109685\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.109685\n",
      "exploration/env_infos/initial/reward_angular Min        0.109685\n",
      "exploration/env_infos/reward_angular Mean              -0.0891698\n",
      "exploration/env_infos/reward_angular Std                1.96182\n",
      "exploration/env_infos/reward_angular Max                6.47427\n",
      "exploration/env_infos/reward_angular Min               -5.78198\n",
      "evaluation/num steps total                              5.1e+06\n",
      "evaluation/num paths total                           5100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.360145\n",
      "evaluation/Rewards Std                                  1.42773\n",
      "evaluation/Rewards Max                                  8.65958\n",
      "evaluation/Rewards Min                                 -7.58416\n",
      "evaluation/Returns Mean                               360.145\n",
      "evaluation/Returns Std                                636.909\n",
      "evaluation/Returns Max                               2194.46\n",
      "evaluation/Returns Min                               -437.9\n",
      "evaluation/Actions Mean                                 0.0644914\n",
      "evaluation/Actions Std                                  0.779984\n",
      "evaluation/Actions Max                                  0.999997\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            360.145\n",
      "evaluation/env_infos/final/reward_run Mean              0.171573\n",
      "evaluation/env_infos/final/reward_run Std               1.36514\n",
      "evaluation/env_infos/final/reward_run Max               3.60607\n",
      "evaluation/env_infos/final/reward_run Min              -3.03965\n",
      "evaluation/env_infos/initial/reward_run Mean            0.153472\n",
      "evaluation/env_infos/initial/reward_run Std             0.393261\n",
      "evaluation/env_infos/initial/reward_run Max             0.943498\n",
      "evaluation/env_infos/initial/reward_run Min            -0.630578\n",
      "evaluation/env_infos/reward_run Mean                   -0.558529\n",
      "evaluation/env_infos/reward_run Std                     2.30669\n",
      "evaluation/env_infos/reward_run Max                     5.25014\n",
      "evaluation/env_infos/reward_run Min                    -8.12159\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.328439\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.122433\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0879262\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.569633\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.259969\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.08743\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0821846\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.42486\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.36752\n",
      "evaluation/env_infos/reward_ctrl Std                    0.124451\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0376922\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59764\n",
      "evaluation/env_infos/final/height Mean                 -0.401435\n",
      "evaluation/env_infos/final/height Std                   0.224114\n",
      "evaluation/env_infos/final/height Max                   0.0249853\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean               -0.0231323\n",
      "evaluation/env_infos/initial/height Std                 0.0525033\n",
      "evaluation/env_infos/initial/height Max                 0.0816407\n",
      "evaluation/env_infos/initial/height Min                -0.114182\n",
      "evaluation/env_infos/height Mean                       -0.225924\n",
      "evaluation/env_infos/height Std                         0.27122\n",
      "evaluation/env_infos/height Max                         0.505545\n",
      "evaluation/env_infos/height Min                        -0.589816\n",
      "evaluation/env_infos/final/reward_angular Mean          0.236368\n",
      "evaluation/env_infos/final/reward_angular Std           2.14185\n",
      "evaluation/env_infos/final/reward_angular Max           9.58083\n",
      "evaluation/env_infos/final/reward_angular Min          -3.1392\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.359378\n",
      "evaluation/env_infos/initial/reward_angular Std         0.73199\n",
      "evaluation/env_infos/initial/reward_angular Max         1.436\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.5139\n",
      "evaluation/env_infos/reward_angular Mean                0.0487606\n",
      "evaluation/env_infos/reward_angular Std                 1.98959\n",
      "evaluation/env_infos/reward_angular Max                10.5356\n",
      "evaluation/env_infos/reward_angular Min                -8.51762\n",
      "time/data storing (s)                                   0.0183212\n",
      "time/evaluation sampling (s)                           26.9195\n",
      "time/exploration sampling (s)                           1.75683\n",
      "time/logging (s)                                        0.241732\n",
      "time/saving (s)                                         0.0270298\n",
      "time/training (s)                                       5.48772\n",
      "time/epoch (s)                                         34.4512\n",
      "time/total (s)                                       6256.33\n",
      "Epoch                                                 203\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:56:47.812192 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 204 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 206000\n",
      "trainer/QF1 Loss                                        2.48017\n",
      "trainer/QF2 Loss                                        1.9425\n",
      "trainer/Policy Loss                                   -15.0097\n",
      "trainer/Q1 Predictions Mean                            20.5121\n",
      "trainer/Q1 Predictions Std                             37.5728\n",
      "trainer/Q1 Predictions Max                            158.202\n",
      "trainer/Q1 Predictions Min                            -20.0775\n",
      "trainer/Q2 Predictions Mean                            20.4642\n",
      "trainer/Q2 Predictions Std                             37.468\n",
      "trainer/Q2 Predictions Max                            159.729\n",
      "trainer/Q2 Predictions Min                            -19.8131\n",
      "trainer/Q Targets Mean                                 20.6379\n",
      "trainer/Q Targets Std                                  37.5566\n",
      "trainer/Q Targets Max                                 158.021\n",
      "trainer/Q Targets Min                                 -20.9196\n",
      "trainer/Log Pis Mean                                    5.55479\n",
      "trainer/Log Pis Std                                     5.96868\n",
      "trainer/Log Pis Max                                    29.8102\n",
      "trainer/Log Pis Min                                    -5.52054\n",
      "trainer/Policy mu Mean                                  0.0935175\n",
      "trainer/Policy mu Std                                   1.57652\n",
      "trainer/Policy mu Max                                   3.95671\n",
      "trainer/Policy mu Min                                  -5.37497\n",
      "trainer/Policy log std Mean                            -0.666399\n",
      "trainer/Policy log std Std                              0.294944\n",
      "trainer/Policy log std Max                              0.372006\n",
      "trainer/Policy log std Min                             -2.37052\n",
      "trainer/Alpha                                           0.0228684\n",
      "trainer/Alpha Loss                                     -1.68203\n",
      "exploration/num steps total                        206000\n",
      "exploration/num paths total                           206\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.10787\n",
      "exploration/Rewards Std                                 0.514601\n",
      "exploration/Rewards Max                                 2.24172\n",
      "exploration/Rewards Min                                -0.607147\n",
      "exploration/Returns Mean                             1107.87\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1107.87\n",
      "exploration/Returns Min                              1107.87\n",
      "exploration/Actions Mean                                0.0867746\n",
      "exploration/Actions Std                                 0.827134\n",
      "exploration/Actions Max                                 0.999966\n",
      "exploration/Actions Min                                -0.999953\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1107.87\n",
      "exploration/env_infos/final/reward_run Mean            -5.81767\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -5.81767\n",
      "exploration/env_infos/final/reward_run Min             -5.81767\n",
      "exploration/env_infos/initial/reward_run Mean          -0.101229\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.101229\n",
      "exploration/env_infos/initial/reward_run Min           -0.101229\n",
      "exploration/env_infos/reward_run Mean                  -4.70393\n",
      "exploration/env_infos/reward_run Std                    1.123\n",
      "exploration/env_infos/reward_run Max                    0.108733\n",
      "exploration/env_infos/reward_run Min                   -7.52574\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.29541\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.29541\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.29541\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.295226\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.295226\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.295226\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.415008\n",
      "exploration/env_infos/reward_ctrl Std                   0.0716664\n",
      "exploration/env_infos/reward_ctrl Max                  -0.155508\n",
      "exploration/env_infos/reward_ctrl Min                  -0.581522\n",
      "exploration/env_infos/final/height Mean                 0.1673\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.1673\n",
      "exploration/env_infos/final/height Min                  0.1673\n",
      "exploration/env_infos/initial/height Mean              -0.0929816\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0929816\n",
      "exploration/env_infos/initial/height Min               -0.0929816\n",
      "exploration/env_infos/height Mean                      -0.0145083\n",
      "exploration/env_infos/height Std                        0.105991\n",
      "exploration/env_infos/height Max                        0.320767\n",
      "exploration/env_infos/height Min                       -0.285444\n",
      "exploration/env_infos/final/reward_angular Mean        -3.06166\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.06166\n",
      "exploration/env_infos/final/reward_angular Min         -3.06166\n",
      "exploration/env_infos/initial/reward_angular Mean       0.766623\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.766623\n",
      "exploration/env_infos/initial/reward_angular Min        0.766623\n",
      "exploration/env_infos/reward_angular Mean               0.0779388\n",
      "exploration/env_infos/reward_angular Std                3.13916\n",
      "exploration/env_infos/reward_angular Max                9.23017\n",
      "exploration/env_infos/reward_angular Min               -5.66449\n",
      "evaluation/num steps total                              5.125e+06\n",
      "evaluation/num paths total                           5125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.493938\n",
      "evaluation/Rewards Std                                  1.4672\n",
      "evaluation/Rewards Max                                  9.19549\n",
      "evaluation/Rewards Min                                 -8.44661\n",
      "evaluation/Returns Mean                               493.938\n",
      "evaluation/Returns Std                                821.587\n",
      "evaluation/Returns Max                               2393.74\n",
      "evaluation/Returns Min                               -418.485\n",
      "evaluation/Actions Mean                                 0.0633615\n",
      "evaluation/Actions Std                                  0.789213\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            493.938\n",
      "evaluation/env_infos/final/reward_run Mean              0.00559553\n",
      "evaluation/env_infos/final/reward_run Std               2.14805\n",
      "evaluation/env_infos/final/reward_run Max               3.60389\n",
      "evaluation/env_infos/final/reward_run Min              -5.11762\n",
      "evaluation/env_infos/initial/reward_run Mean            0.252223\n",
      "evaluation/env_infos/initial/reward_run Std             0.520531\n",
      "evaluation/env_infos/initial/reward_run Max             1.27645\n",
      "evaluation/env_infos/initial/reward_run Min            -0.691142\n",
      "evaluation/env_infos/reward_run Mean                   -0.366827\n",
      "evaluation/env_infos/reward_run Std                     2.63377\n",
      "evaluation/env_infos/reward_run Max                     6.13802\n",
      "evaluation/env_infos/reward_run Min                    -8.07732\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.352889\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.104762\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.114666\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.585128\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.294407\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0964589\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0675425\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.459853\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.376123\n",
      "evaluation/env_infos/reward_ctrl Std                    0.104352\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0199736\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.595504\n",
      "evaluation/env_infos/final/height Mean                 -0.284031\n",
      "evaluation/env_infos/final/height Std                   0.263986\n",
      "evaluation/env_infos/final/height Max                   0.197459\n",
      "evaluation/env_infos/final/height Min                  -0.577283\n",
      "evaluation/env_infos/initial/height Mean                0.00253045\n",
      "evaluation/env_infos/initial/height Std                 0.051131\n",
      "evaluation/env_infos/initial/height Max                 0.0851445\n",
      "evaluation/env_infos/initial/height Min                -0.0762703\n",
      "evaluation/env_infos/height Mean                       -0.203964\n",
      "evaluation/env_infos/height Std                         0.268821\n",
      "evaluation/env_infos/height Max                         0.523381\n",
      "evaluation/env_infos/height Min                        -0.594141\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.136564\n",
      "evaluation/env_infos/final/reward_angular Std           1.56809\n",
      "evaluation/env_infos/final/reward_angular Max           3.80151\n",
      "evaluation/env_infos/final/reward_angular Min          -3.97311\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.26966\n",
      "evaluation/env_infos/initial/reward_angular Std         0.865073\n",
      "evaluation/env_infos/initial/reward_angular Max         2.05841\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.01018\n",
      "evaluation/env_infos/reward_angular Mean                0.0517868\n",
      "evaluation/env_infos/reward_angular Std                 2.01094\n",
      "evaluation/env_infos/reward_angular Max                10.969\n",
      "evaluation/env_infos/reward_angular Min                -7.92407\n",
      "time/data storing (s)                                   0.0153013\n",
      "time/evaluation sampling (s)                           21.2265\n",
      "time/exploration sampling (s)                           1.07668\n",
      "time/logging (s)                                        0.241739\n",
      "time/saving (s)                                         0.0288344\n",
      "time/training (s)                                       4.16074\n",
      "time/epoch (s)                                         26.7498\n",
      "time/total (s)                                       6284.63\n",
      "Epoch                                                 204\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:57:19.401024 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 205 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 207000\n",
      "trainer/QF1 Loss                                        2.08717\n",
      "trainer/QF2 Loss                                        2.71548\n",
      "trainer/Policy Loss                                   -17.7826\n",
      "trainer/Q1 Predictions Mean                            23.2869\n",
      "trainer/Q1 Predictions Std                             39.9978\n",
      "trainer/Q1 Predictions Max                            155.073\n",
      "trainer/Q1 Predictions Min                            -20.7257\n",
      "trainer/Q2 Predictions Mean                            23.3125\n",
      "trainer/Q2 Predictions Std                             40.0415\n",
      "trainer/Q2 Predictions Max                            156.019\n",
      "trainer/Q2 Predictions Min                            -20.2987\n",
      "trainer/Q Targets Mean                                 23.2497\n",
      "trainer/Q Targets Std                                  40.0713\n",
      "trainer/Q Targets Max                                 154.24\n",
      "trainer/Q Targets Min                                 -21.0068\n",
      "trainer/Log Pis Mean                                    5.53291\n",
      "trainer/Log Pis Std                                     5.70696\n",
      "trainer/Log Pis Max                                    20.0308\n",
      "trainer/Log Pis Min                                    -6.37249\n",
      "trainer/Policy mu Mean                                  0.216308\n",
      "trainer/Policy mu Std                                   1.54813\n",
      "trainer/Policy mu Max                                   4.12635\n",
      "trainer/Policy mu Min                                  -4.55165\n",
      "trainer/Policy log std Mean                            -0.71649\n",
      "trainer/Policy log std Std                              0.284379\n",
      "trainer/Policy log std Max                              0.275835\n",
      "trainer/Policy log std Min                             -1.81559\n",
      "trainer/Alpha                                           0.0212527\n",
      "trainer/Alpha Loss                                     -1.79907\n",
      "exploration/num steps total                        207000\n",
      "exploration/num paths total                           207\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.75248\n",
      "exploration/Rewards Std                                 2.46102\n",
      "exploration/Rewards Max                                10.5976\n",
      "exploration/Rewards Min                                -3.43329\n",
      "exploration/Returns Mean                             2752.48\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2752.48\n",
      "exploration/Returns Min                              2752.48\n",
      "exploration/Actions Mean                                0.120367\n",
      "exploration/Actions Std                                 0.89995\n",
      "exploration/Actions Max                                 0.999997\n",
      "exploration/Actions Min                                -1\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2752.48\n",
      "exploration/env_infos/final/reward_run Mean            -4.39381\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -4.39381\n",
      "exploration/env_infos/final/reward_run Min             -4.39381\n",
      "exploration/env_infos/initial/reward_run Mean           0.248565\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.248565\n",
      "exploration/env_infos/initial/reward_run Min            0.248565\n",
      "exploration/env_infos/reward_run Mean                  -4.483\n",
      "exploration/env_infos/reward_run Std                    1.26699\n",
      "exploration/env_infos/reward_run Max                    0.675828\n",
      "exploration/env_infos/reward_run Min                   -7.42822\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.527068\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.527068\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.527068\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.245049\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.245049\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.245049\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.494639\n",
      "exploration/env_infos/reward_ctrl Std                   0.0690053\n",
      "exploration/env_infos/reward_ctrl Max                  -0.172504\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595569\n",
      "exploration/env_infos/final/height Mean                 0.0447867\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0447867\n",
      "exploration/env_infos/final/height Min                  0.0447867\n",
      "exploration/env_infos/initial/height Mean              -0.00307325\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00307325\n",
      "exploration/env_infos/initial/height Min               -0.00307325\n",
      "exploration/env_infos/height Mean                       0.0194136\n",
      "exploration/env_infos/height Std                        0.129626\n",
      "exploration/env_infos/height Max                        0.377154\n",
      "exploration/env_infos/height Min                       -0.30839\n",
      "exploration/env_infos/final/reward_angular Mean        -3.56944\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -3.56944\n",
      "exploration/env_infos/final/reward_angular Min         -3.56944\n",
      "exploration/env_infos/initial/reward_angular Mean       1.3899\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.3899\n",
      "exploration/env_infos/initial/reward_angular Min        1.3899\n",
      "exploration/env_infos/reward_angular Mean               0.0770777\n",
      "exploration/env_infos/reward_angular Std                3.40425\n",
      "exploration/env_infos/reward_angular Max                9.77545\n",
      "exploration/env_infos/reward_angular Min               -6.58555\n",
      "evaluation/num steps total                              5.15e+06\n",
      "evaluation/num paths total                           5150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.860421\n",
      "evaluation/Rewards Std                                  1.60857\n",
      "evaluation/Rewards Max                                  9.14942\n",
      "evaluation/Rewards Min                                 -6.42426\n",
      "evaluation/Returns Mean                               860.421\n",
      "evaluation/Returns Std                               1002.88\n",
      "evaluation/Returns Max                               3000.97\n",
      "evaluation/Returns Min                               -249.008\n",
      "evaluation/Actions Mean                                 0.146002\n",
      "evaluation/Actions Std                                  0.760964\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            860.421\n",
      "evaluation/env_infos/final/reward_run Mean             -0.728304\n",
      "evaluation/env_infos/final/reward_run Std               3.03387\n",
      "evaluation/env_infos/final/reward_run Max               3.48942\n",
      "evaluation/env_infos/final/reward_run Min              -6.92864\n",
      "evaluation/env_infos/initial/reward_run Mean            0.333577\n",
      "evaluation/env_infos/initial/reward_run Std             0.413661\n",
      "evaluation/env_infos/initial/reward_run Max             1.15515\n",
      "evaluation/env_infos/initial/reward_run Min            -0.425594\n",
      "evaluation/env_infos/reward_run Mean                   -0.624343\n",
      "evaluation/env_infos/reward_run Std                     3.04503\n",
      "evaluation/env_infos/reward_run Max                     5.88902\n",
      "evaluation/env_infos/reward_run Min                    -7.83251\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.397279\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.133957\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0967532\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.578541\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.289906\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0954412\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0963319\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.555411\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.360229\n",
      "evaluation/env_infos/reward_ctrl Std                    0.13658\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.030094\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597773\n",
      "evaluation/env_infos/final/height Mean                 -0.153788\n",
      "evaluation/env_infos/final/height Std                   0.234675\n",
      "evaluation/env_infos/final/height Max                   0.196763\n",
      "evaluation/env_infos/final/height Min                  -0.578551\n",
      "evaluation/env_infos/initial/height Mean               -0.00536152\n",
      "evaluation/env_infos/initial/height Std                 0.0630466\n",
      "evaluation/env_infos/initial/height Max                 0.0890826\n",
      "evaluation/env_infos/initial/height Min                -0.113226\n",
      "evaluation/env_infos/height Mean                       -0.117694\n",
      "evaluation/env_infos/height Std                         0.21235\n",
      "evaluation/env_infos/height Max                         0.489896\n",
      "evaluation/env_infos/height Min                        -0.602245\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.373555\n",
      "evaluation/env_infos/final/reward_angular Std           1.70472\n",
      "evaluation/env_infos/final/reward_angular Max           3.92759\n",
      "evaluation/env_infos/final/reward_angular Min          -4.53236\n",
      "evaluation/env_infos/initial/reward_angular Mean        0.320179\n",
      "evaluation/env_infos/initial/reward_angular Std         1.01732\n",
      "evaluation/env_infos/initial/reward_angular Max         2.49763\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.04987\n",
      "evaluation/env_infos/reward_angular Mean                0.0628477\n",
      "evaluation/env_infos/reward_angular Std                 2.18612\n",
      "evaluation/env_infos/reward_angular Max                10.4254\n",
      "evaluation/env_infos/reward_angular Min                -7.36849\n",
      "time/data storing (s)                                   0.0158853\n",
      "time/evaluation sampling (s)                           24.7799\n",
      "time/exploration sampling (s)                           0.99931\n",
      "time/logging (s)                                        0.246743\n",
      "time/saving (s)                                         0.0296248\n",
      "time/training (s)                                       4.05696\n",
      "time/epoch (s)                                         30.1284\n",
      "time/total (s)                                       6316.22\n",
      "Epoch                                                 205\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:57:54.845162 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 206 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 208000\n",
      "trainer/QF1 Loss                                        2.13204\n",
      "trainer/QF2 Loss                                        1.8674\n",
      "trainer/Policy Loss                                   -20.0697\n",
      "trainer/Q1 Predictions Mean                            26.6071\n",
      "trainer/Q1 Predictions Std                             39.8825\n",
      "trainer/Q1 Predictions Max                            150.545\n",
      "trainer/Q1 Predictions Min                            -20.2674\n",
      "trainer/Q2 Predictions Mean                            26.373\n",
      "trainer/Q2 Predictions Std                             39.8453\n",
      "trainer/Q2 Predictions Max                            150.762\n",
      "trainer/Q2 Predictions Min                            -20.4024\n",
      "trainer/Q Targets Mean                                 26.2519\n",
      "trainer/Q Targets Std                                  39.6947\n",
      "trainer/Q Targets Max                                 149.788\n",
      "trainer/Q Targets Min                                 -20.5188\n",
      "trainer/Log Pis Mean                                    6.45118\n",
      "trainer/Log Pis Std                                     5.90315\n",
      "trainer/Log Pis Max                                    30.6828\n",
      "trainer/Log Pis Min                                    -4.20081\n",
      "trainer/Policy mu Mean                                  0.0316225\n",
      "trainer/Policy mu Std                                   1.63169\n",
      "trainer/Policy mu Max                                   4.70968\n",
      "trainer/Policy mu Min                                  -6.49589\n",
      "trainer/Policy log std Mean                            -0.71325\n",
      "trainer/Policy log std Std                              0.318925\n",
      "trainer/Policy log std Max                              0.368343\n",
      "trainer/Policy log std Min                             -2.61212\n",
      "trainer/Alpha                                           0.0213157\n",
      "trainer/Alpha Loss                                      1.73537\n",
      "exploration/num steps total                        208000\n",
      "exploration/num paths total                           208\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                2.24078\n",
      "exploration/Rewards Std                                 1.23274\n",
      "exploration/Rewards Max                                 5.85551\n",
      "exploration/Rewards Min                                -0.685508\n",
      "exploration/Returns Mean                             2240.78\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              2240.78\n",
      "exploration/Returns Min                              2240.78\n",
      "exploration/Actions Mean                                0.0953514\n",
      "exploration/Actions Std                                 0.888996\n",
      "exploration/Actions Max                                 0.999967\n",
      "exploration/Actions Min                                -0.999993\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          2240.78\n",
      "exploration/env_infos/final/reward_run Mean            -3.17334\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.17334\n",
      "exploration/env_infos/final/reward_run Min             -3.17334\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0117855\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0117855\n",
      "exploration/env_infos/initial/reward_run Min           -0.0117855\n",
      "exploration/env_infos/reward_run Mean                  -4.35982\n",
      "exploration/env_infos/reward_run Std                    1.18679\n",
      "exploration/env_infos/reward_run Max                    0.490634\n",
      "exploration/env_infos/reward_run Min                   -7.44406\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.393011\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.393011\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.393011\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.355208\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.355208\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.355208\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.479643\n",
      "exploration/env_infos/reward_ctrl Std                   0.0713636\n",
      "exploration/env_infos/reward_ctrl Max                  -0.227199\n",
      "exploration/env_infos/reward_ctrl Min                  -0.596857\n",
      "exploration/env_infos/final/height Mean                -0.140581\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.140581\n",
      "exploration/env_infos/final/height Min                 -0.140581\n",
      "exploration/env_infos/initial/height Mean              -0.00186255\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00186255\n",
      "exploration/env_infos/initial/height Min               -0.00186255\n",
      "exploration/env_infos/height Mean                      -0.00315629\n",
      "exploration/env_infos/height Std                        0.143681\n",
      "exploration/env_infos/height Max                        0.478565\n",
      "exploration/env_infos/height Min                       -0.314664\n",
      "exploration/env_infos/final/reward_angular Mean        -0.641699\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -0.641699\n",
      "exploration/env_infos/final/reward_angular Min         -0.641699\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.282144\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.282144\n",
      "exploration/env_infos/initial/reward_angular Min       -0.282144\n",
      "exploration/env_infos/reward_angular Mean               0.0613935\n",
      "exploration/env_infos/reward_angular Std                3.20682\n",
      "exploration/env_infos/reward_angular Max               10.2492\n",
      "exploration/env_infos/reward_angular Min               -5.55332\n",
      "evaluation/num steps total                              5.175e+06\n",
      "evaluation/num paths total                           5175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.536345\n",
      "evaluation/Rewards Std                                  1.32929\n",
      "evaluation/Rewards Max                                  9.43802\n",
      "evaluation/Rewards Min                                 -6.84472\n",
      "evaluation/Returns Mean                               536.345\n",
      "evaluation/Returns Std                                758.769\n",
      "evaluation/Returns Max                               2411.16\n",
      "evaluation/Returns Min                               -474.56\n",
      "evaluation/Actions Mean                                 0.0513871\n",
      "evaluation/Actions Std                                  0.743794\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            536.345\n",
      "evaluation/env_infos/final/reward_run Mean             -0.192849\n",
      "evaluation/env_infos/final/reward_run Std               2.45231\n",
      "evaluation/env_infos/final/reward_run Max               3.95521\n",
      "evaluation/env_infos/final/reward_run Min              -5.61649\n",
      "evaluation/env_infos/initial/reward_run Mean            0.187486\n",
      "evaluation/env_infos/initial/reward_run Std             0.538296\n",
      "evaluation/env_infos/initial/reward_run Max             1.1464\n",
      "evaluation/env_infos/initial/reward_run Min            -1.00142\n",
      "evaluation/env_infos/reward_run Mean                   -0.0751207\n",
      "evaluation/env_infos/reward_run Std                     2.43967\n",
      "evaluation/env_infos/reward_run Max                     5.51466\n",
      "evaluation/env_infos/reward_run Min                    -7.64024\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.312409\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.121922\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0792113\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.537659\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.326864\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0847473\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0829508\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.426722\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.333522\n",
      "evaluation/env_infos/reward_ctrl Std                    0.136497\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0177534\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.598666\n",
      "evaluation/env_infos/final/height Mean                 -0.241412\n",
      "evaluation/env_infos/final/height Std                   0.295138\n",
      "evaluation/env_infos/final/height Max                   0.38269\n",
      "evaluation/env_infos/final/height Min                  -0.577288\n",
      "evaluation/env_infos/initial/height Mean               -0.0280395\n",
      "evaluation/env_infos/initial/height Std                 0.0582435\n",
      "evaluation/env_infos/initial/height Max                 0.0854976\n",
      "evaluation/env_infos/initial/height Min                -0.100933\n",
      "evaluation/env_infos/height Mean                       -0.194184\n",
      "evaluation/env_infos/height Std                         0.265069\n",
      "evaluation/env_infos/height Max                         0.582112\n",
      "evaluation/env_infos/height Min                        -0.589355\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.141928\n",
      "evaluation/env_infos/final/reward_angular Std           1.53082\n",
      "evaluation/env_infos/final/reward_angular Max           2.86393\n",
      "evaluation/env_infos/final/reward_angular Min          -4.96159\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.282555\n",
      "evaluation/env_infos/initial/reward_angular Std         0.517406\n",
      "evaluation/env_infos/initial/reward_angular Max         0.621428\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.30006\n",
      "evaluation/env_infos/reward_angular Mean                0.0215702\n",
      "evaluation/env_infos/reward_angular Std                 1.91686\n",
      "evaluation/env_infos/reward_angular Max                10.4155\n",
      "evaluation/env_infos/reward_angular Min                -8.69051\n",
      "time/data storing (s)                                   0.0166856\n",
      "time/evaluation sampling (s)                           26.5324\n",
      "time/exploration sampling (s)                           1.14536\n",
      "time/logging (s)                                        0.251717\n",
      "time/saving (s)                                         0.0277947\n",
      "time/training (s)                                       5.97288\n",
      "time/epoch (s)                                         33.9468\n",
      "time/total (s)                                       6351.66\n",
      "Epoch                                                 206\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:58:24.861902 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 207 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 209000\n",
      "trainer/QF1 Loss                                        2.86198\n",
      "trainer/QF2 Loss                                        2.66931\n",
      "trainer/Policy Loss                                   -15.9084\n",
      "trainer/Q1 Predictions Mean                            21.9628\n",
      "trainer/Q1 Predictions Std                             36.8281\n",
      "trainer/Q1 Predictions Max                            141.033\n",
      "trainer/Q1 Predictions Min                            -21.7042\n",
      "trainer/Q2 Predictions Mean                            22.0801\n",
      "trainer/Q2 Predictions Std                             36.9099\n",
      "trainer/Q2 Predictions Max                            142.446\n",
      "trainer/Q2 Predictions Min                            -20.7478\n",
      "trainer/Q Targets Mean                                 22.1101\n",
      "trainer/Q Targets Std                                  37.2536\n",
      "trainer/Q Targets Max                                 146.774\n",
      "trainer/Q Targets Min                                 -21.861\n",
      "trainer/Log Pis Mean                                    6.24894\n",
      "trainer/Log Pis Std                                     6.41995\n",
      "trainer/Log Pis Max                                    31.9937\n",
      "trainer/Log Pis Min                                    -5.04308\n",
      "trainer/Policy mu Mean                                  0.0811756\n",
      "trainer/Policy mu Std                                   1.63017\n",
      "trainer/Policy mu Max                                   5.35079\n",
      "trainer/Policy mu Min                                  -5.88047\n",
      "trainer/Policy log std Mean                            -0.723098\n",
      "trainer/Policy log std Std                              0.29874\n",
      "trainer/Policy log std Max                              0.381642\n",
      "trainer/Policy log std Min                             -2.43683\n",
      "trainer/Alpha                                           0.0228127\n",
      "trainer/Alpha Loss                                      0.941097\n",
      "exploration/num steps total                        209000\n",
      "exploration/num paths total                           209\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.72801\n",
      "exploration/Rewards Std                                 0.848675\n",
      "exploration/Rewards Max                                 4.08725\n",
      "exploration/Rewards Min                                -0.898218\n",
      "exploration/Returns Mean                             1728.01\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1728.01\n",
      "exploration/Returns Min                              1728.01\n",
      "exploration/Actions Mean                                0.120712\n",
      "exploration/Actions Std                                 0.881031\n",
      "exploration/Actions Max                                 0.999989\n",
      "exploration/Actions Min                                -0.999995\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1728.01\n",
      "exploration/env_infos/final/reward_run Mean            -3.78118\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -3.78118\n",
      "exploration/env_infos/final/reward_run Min             -3.78118\n",
      "exploration/env_infos/initial/reward_run Mean          -0.708883\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.708883\n",
      "exploration/env_infos/initial/reward_run Min           -0.708883\n",
      "exploration/env_infos/reward_run Mean                  -4.9688\n",
      "exploration/env_infos/reward_run Std                    1.22431\n",
      "exploration/env_infos/reward_run Max                    0.79281\n",
      "exploration/env_infos/reward_run Min                   -7.72149\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.409113\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.409113\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.409113\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.397389\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.397389\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.397389\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.474472\n",
      "exploration/env_infos/reward_ctrl Std                   0.0737357\n",
      "exploration/env_infos/reward_ctrl Max                  -0.113465\n",
      "exploration/env_infos/reward_ctrl Min                  -0.591297\n",
      "exploration/env_infos/final/height Mean                 0.0990788\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0990788\n",
      "exploration/env_infos/final/height Min                  0.0990788\n",
      "exploration/env_infos/initial/height Mean              -0.0702826\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0702826\n",
      "exploration/env_infos/initial/height Min               -0.0702826\n",
      "exploration/env_infos/height Mean                       0.00365775\n",
      "exploration/env_infos/height Std                        0.117793\n",
      "exploration/env_infos/height Max                        0.480094\n",
      "exploration/env_infos/height Min                       -0.315699\n",
      "exploration/env_infos/final/reward_angular Mean         4.54128\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          4.54128\n",
      "exploration/env_infos/final/reward_angular Min          4.54128\n",
      "exploration/env_infos/initial/reward_angular Mean       0.343962\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.343962\n",
      "exploration/env_infos/initial/reward_angular Min        0.343962\n",
      "exploration/env_infos/reward_angular Mean               0.084931\n",
      "exploration/env_infos/reward_angular Std                3.17142\n",
      "exploration/env_infos/reward_angular Max                9.63892\n",
      "exploration/env_infos/reward_angular Min               -6.80192\n",
      "evaluation/num steps total                              5.2e+06\n",
      "evaluation/num paths total                           5200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.891007\n",
      "evaluation/Rewards Std                                  1.71982\n",
      "evaluation/Rewards Max                                  9.62526\n",
      "evaluation/Rewards Min                                 -8.21297\n",
      "evaluation/Returns Mean                               891.007\n",
      "evaluation/Returns Std                               1069.81\n",
      "evaluation/Returns Max                               3012.57\n",
      "evaluation/Returns Min                               -458.748\n",
      "evaluation/Actions Mean                                 0.133242\n",
      "evaluation/Actions Std                                  0.78525\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            891.007\n",
      "evaluation/env_infos/final/reward_run Mean             -0.784063\n",
      "evaluation/env_infos/final/reward_run Std               2.97881\n",
      "evaluation/env_infos/final/reward_run Max               4.18333\n",
      "evaluation/env_infos/final/reward_run Min              -6.00274\n",
      "evaluation/env_infos/initial/reward_run Mean            0.171344\n",
      "evaluation/env_infos/initial/reward_run Std             0.517675\n",
      "evaluation/env_infos/initial/reward_run Max             0.873808\n",
      "evaluation/env_infos/initial/reward_run Min            -1.09879\n",
      "evaluation/env_infos/reward_run Mean                   -0.947925\n",
      "evaluation/env_infos/reward_run Std                     3.07344\n",
      "evaluation/env_infos/reward_run Max                     5.38062\n",
      "evaluation/env_infos/reward_run Min                    -8.3809\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.386503\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.123206\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.104998\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.584332\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.303872\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0873315\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0691726\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.425155\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.380622\n",
      "evaluation/env_infos/reward_ctrl Std                    0.119062\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.041455\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599063\n",
      "evaluation/env_infos/final/height Mean                 -0.156622\n",
      "evaluation/env_infos/final/height Std                   0.25519\n",
      "evaluation/env_infos/final/height Max                   0.244353\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00854156\n",
      "evaluation/env_infos/initial/height Std                 0.0577563\n",
      "evaluation/env_infos/initial/height Max                 0.0814975\n",
      "evaluation/env_infos/initial/height Min                -0.115471\n",
      "evaluation/env_infos/height Mean                       -0.120251\n",
      "evaluation/env_infos/height Std                         0.242037\n",
      "evaluation/env_infos/height Max                         0.506866\n",
      "evaluation/env_infos/height Min                        -0.579186\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.00505535\n",
      "evaluation/env_infos/final/reward_angular Std           1.9549\n",
      "evaluation/env_infos/final/reward_angular Max           3.69127\n",
      "evaluation/env_infos/final/reward_angular Min          -4.19879\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.16254\n",
      "evaluation/env_infos/initial/reward_angular Std         0.980768\n",
      "evaluation/env_infos/initial/reward_angular Max         2.29233\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.31073\n",
      "evaluation/env_infos/reward_angular Mean                0.0516945\n",
      "evaluation/env_infos/reward_angular Std                 2.23179\n",
      "evaluation/env_infos/reward_angular Max                11.3602\n",
      "evaluation/env_infos/reward_angular Min                -7.69427\n",
      "time/data storing (s)                                   0.0156511\n",
      "time/evaluation sampling (s)                           22.5982\n",
      "time/exploration sampling (s)                           1.02656\n",
      "time/logging (s)                                        0.241303\n",
      "time/saving (s)                                         0.0280147\n",
      "time/training (s)                                       4.45771\n",
      "time/epoch (s)                                         28.3675\n",
      "time/total (s)                                       6381.66\n",
      "Epoch                                                 207\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:58:52.157657 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 208 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 210000\n",
      "trainer/QF1 Loss                                        2.02754\n",
      "trainer/QF2 Loss                                        2.42905\n",
      "trainer/Policy Loss                                   -15.8447\n",
      "trainer/Q1 Predictions Mean                            21.9607\n",
      "trainer/Q1 Predictions Std                             37.1614\n",
      "trainer/Q1 Predictions Max                            157.403\n",
      "trainer/Q1 Predictions Min                            -21.6343\n",
      "trainer/Q2 Predictions Mean                            21.5284\n",
      "trainer/Q2 Predictions Std                             36.7989\n",
      "trainer/Q2 Predictions Max                            154.563\n",
      "trainer/Q2 Predictions Min                            -21.5287\n",
      "trainer/Q Targets Mean                                 21.8987\n",
      "trainer/Q Targets Std                                  37.3082\n",
      "trainer/Q Targets Max                                 159.395\n",
      "trainer/Q Targets Min                                 -21.903\n",
      "trainer/Log Pis Mean                                    5.96497\n",
      "trainer/Log Pis Std                                     6.12848\n",
      "trainer/Log Pis Max                                    36.656\n",
      "trainer/Log Pis Min                                    -4.46156\n",
      "trainer/Policy mu Mean                                  0.157767\n",
      "trainer/Policy mu Std                                   1.58144\n",
      "trainer/Policy mu Max                                   6.36768\n",
      "trainer/Policy mu Min                                  -4.88179\n",
      "trainer/Policy log std Mean                            -0.699972\n",
      "trainer/Policy log std Std                              0.294805\n",
      "trainer/Policy log std Max                              0.683484\n",
      "trainer/Policy log std Min                             -2.34577\n",
      "trainer/Alpha                                           0.023228\n",
      "trainer/Alpha Loss                                     -0.131735\n",
      "exploration/num steps total                        210000\n",
      "exploration/num paths total                           210\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.117042\n",
      "exploration/Rewards Std                                 0.846646\n",
      "exploration/Rewards Max                                 5.99325\n",
      "exploration/Rewards Min                                -3.57457\n",
      "exploration/Returns Mean                             -117.042\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -117.042\n",
      "exploration/Returns Min                              -117.042\n",
      "exploration/Actions Mean                                0.26688\n",
      "exploration/Actions Std                                 0.7051\n",
      "exploration/Actions Max                                 0.99996\n",
      "exploration/Actions Min                                -0.999996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -117.042\n",
      "exploration/env_infos/final/reward_run Mean             0.3868\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.3868\n",
      "exploration/env_infos/final/reward_run Min              0.3868\n",
      "exploration/env_infos/initial/reward_run Mean           0.139861\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.139861\n",
      "exploration/env_infos/initial/reward_run Min            0.139861\n",
      "exploration/env_infos/reward_run Mean                   0.0763441\n",
      "exploration/env_infos/reward_run Std                    0.404809\n",
      "exploration/env_infos/reward_run Max                    2.44308\n",
      "exploration/env_infos/reward_run Min                   -1.5057\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.333149\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.333149\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.333149\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.138678\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.138678\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.138678\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.341034\n",
      "exploration/env_infos/reward_ctrl Std                   0.0774577\n",
      "exploration/env_infos/reward_ctrl Max                  -0.119335\n",
      "exploration/env_infos/reward_ctrl Min                  -0.594969\n",
      "exploration/env_infos/final/height Mean                -0.574774\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.574774\n",
      "exploration/env_infos/final/height Min                 -0.574774\n",
      "exploration/env_infos/initial/height Mean               0.0139757\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0139757\n",
      "exploration/env_infos/initial/height Min                0.0139757\n",
      "exploration/env_infos/height Mean                      -0.506121\n",
      "exploration/env_infos/height Std                        0.168515\n",
      "exploration/env_infos/height Max                        0.318703\n",
      "exploration/env_infos/height Min                       -0.581468\n",
      "exploration/env_infos/final/reward_angular Mean         0.742802\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.742802\n",
      "exploration/env_infos/final/reward_angular Min          0.742802\n",
      "exploration/env_infos/initial/reward_angular Mean      -1.05434\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -1.05434\n",
      "exploration/env_infos/initial/reward_angular Min       -1.05434\n",
      "exploration/env_infos/reward_angular Mean               0.182045\n",
      "exploration/env_infos/reward_angular Std                1.13895\n",
      "exploration/env_infos/reward_angular Max                8.80007\n",
      "exploration/env_infos/reward_angular Min               -4.62631\n",
      "evaluation/num steps total                              5.225e+06\n",
      "evaluation/num paths total                           5225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.680396\n",
      "evaluation/Rewards Std                                  1.64126\n",
      "evaluation/Rewards Max                                  9.74321\n",
      "evaluation/Rewards Min                                 -7.93164\n",
      "evaluation/Returns Mean                               680.396\n",
      "evaluation/Returns Std                                945.095\n",
      "evaluation/Returns Max                               2945.67\n",
      "evaluation/Returns Min                               -239.626\n",
      "evaluation/Actions Mean                                 0.162665\n",
      "evaluation/Actions Std                                  0.745586\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            680.396\n",
      "evaluation/env_infos/final/reward_run Mean             -0.292846\n",
      "evaluation/env_infos/final/reward_run Std               2.45255\n",
      "evaluation/env_infos/final/reward_run Max               4.42741\n",
      "evaluation/env_infos/final/reward_run Min              -6.13103\n",
      "evaluation/env_infos/initial/reward_run Mean            0.190146\n",
      "evaluation/env_infos/initial/reward_run Std             0.398668\n",
      "evaluation/env_infos/initial/reward_run Max             0.944897\n",
      "evaluation/env_infos/initial/reward_run Min            -0.526683\n",
      "evaluation/env_infos/reward_run Mean                   -0.388481\n",
      "evaluation/env_infos/reward_run Std                     2.6627\n",
      "evaluation/env_infos/reward_run Max                     5.785\n",
      "evaluation/env_infos/reward_run Min                    -7.88941\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.348872\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.111752\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.100576\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.572518\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.291058\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0948914\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0365452\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.494627\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.349415\n",
      "evaluation/env_infos/reward_ctrl Std                    0.129204\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0227943\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596366\n",
      "evaluation/env_infos/final/height Mean                 -0.183378\n",
      "evaluation/env_infos/final/height Std                   0.24795\n",
      "evaluation/env_infos/final/height Max                   0.128754\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean                0.0105776\n",
      "evaluation/env_infos/initial/height Std                 0.0476665\n",
      "evaluation/env_infos/initial/height Max                 0.0823878\n",
      "evaluation/env_infos/initial/height Min                -0.0687507\n",
      "evaluation/env_infos/height Mean                       -0.153816\n",
      "evaluation/env_infos/height Std                         0.243502\n",
      "evaluation/env_infos/height Max                         0.513491\n",
      "evaluation/env_infos/height Min                        -0.595933\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.264638\n",
      "evaluation/env_infos/final/reward_angular Std           1.5391\n",
      "evaluation/env_infos/final/reward_angular Max           3.36967\n",
      "evaluation/env_infos/final/reward_angular Min          -3.32959\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.527332\n",
      "evaluation/env_infos/initial/reward_angular Std         0.688032\n",
      "evaluation/env_infos/initial/reward_angular Max         1.17294\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.61771\n",
      "evaluation/env_infos/reward_angular Mean                0.0382028\n",
      "evaluation/env_infos/reward_angular Std                 2.11399\n",
      "evaluation/env_infos/reward_angular Max                10.5635\n",
      "evaluation/env_infos/reward_angular Min                -8.39359\n",
      "time/data storing (s)                                   0.0155495\n",
      "time/evaluation sampling (s)                           20.3546\n",
      "time/exploration sampling (s)                           1.04857\n",
      "time/logging (s)                                        0.236264\n",
      "time/saving (s)                                         0.0284504\n",
      "time/training (s)                                       4.12274\n",
      "time/epoch (s)                                         25.8062\n",
      "time/total (s)                                       6408.94\n",
      "Epoch                                                 208\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:59:25.217021 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 209 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 211000\n",
      "trainer/QF1 Loss                                        2.78039\n",
      "trainer/QF2 Loss                                        2.95334\n",
      "trainer/Policy Loss                                   -15.2056\n",
      "trainer/Q1 Predictions Mean                            20.6455\n",
      "trainer/Q1 Predictions Std                             37.0002\n",
      "trainer/Q1 Predictions Max                            160.762\n",
      "trainer/Q1 Predictions Min                            -21.5538\n",
      "trainer/Q2 Predictions Mean                            20.9422\n",
      "trainer/Q2 Predictions Std                             36.8749\n",
      "trainer/Q2 Predictions Max                            160.242\n",
      "trainer/Q2 Predictions Min                            -21.0058\n",
      "trainer/Q Targets Mean                                 20.5588\n",
      "trainer/Q Targets Std                                  36.9265\n",
      "trainer/Q Targets Max                                 165.381\n",
      "trainer/Q Targets Min                                 -21.1765\n",
      "trainer/Log Pis Mean                                    5.64323\n",
      "trainer/Log Pis Std                                     5.46499\n",
      "trainer/Log Pis Max                                    25.4301\n",
      "trainer/Log Pis Min                                    -7.66302\n",
      "trainer/Policy mu Mean                                  0.100979\n",
      "trainer/Policy mu Std                                   1.58875\n",
      "trainer/Policy mu Max                                   4.57872\n",
      "trainer/Policy mu Min                                  -5.24456\n",
      "trainer/Policy log std Mean                            -0.684761\n",
      "trainer/Policy log std Std                              0.288735\n",
      "trainer/Policy log std Max                              0.872296\n",
      "trainer/Policy log std Min                             -2.73321\n",
      "trainer/Alpha                                           0.023884\n",
      "trainer/Alpha Loss                                     -1.3323\n",
      "exploration/num steps total                        211000\n",
      "exploration/num paths total                           211\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.46954\n",
      "exploration/Rewards Std                                 2.58673\n",
      "exploration/Rewards Max                                 8.68845\n",
      "exploration/Rewards Min                                -4.05745\n",
      "exploration/Returns Mean                             1469.54\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1469.54\n",
      "exploration/Returns Min                              1469.54\n",
      "exploration/Actions Mean                                0.0850992\n",
      "exploration/Actions Std                                 0.891643\n",
      "exploration/Actions Max                                 0.999997\n",
      "exploration/Actions Min                                -0.999931\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1469.54\n",
      "exploration/env_infos/final/reward_run Mean            -6.54778\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -6.54778\n",
      "exploration/env_infos/final/reward_run Min             -6.54778\n",
      "exploration/env_infos/initial/reward_run Mean           0.207094\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max            0.207094\n",
      "exploration/env_infos/initial/reward_run Min            0.207094\n",
      "exploration/env_infos/reward_run Mean                  -4.91734\n",
      "exploration/env_infos/reward_run Std                    1.37405\n",
      "exploration/env_infos/reward_run Max                    0.537409\n",
      "exploration/env_infos/reward_run Min                   -8.25463\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.494219\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.494219\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.494219\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.17434\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.17434\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.17434\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.481362\n",
      "exploration/env_infos/reward_ctrl Std                   0.0712015\n",
      "exploration/env_infos/reward_ctrl Max                  -0.17434\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590865\n",
      "exploration/env_infos/final/height Mean                -0.0466087\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.0466087\n",
      "exploration/env_infos/final/height Min                 -0.0466087\n",
      "exploration/env_infos/initial/height Mean               0.0855108\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max                0.0855108\n",
      "exploration/env_infos/initial/height Min                0.0855108\n",
      "exploration/env_infos/height Mean                      -0.0378279\n",
      "exploration/env_infos/height Std                        0.103573\n",
      "exploration/env_infos/height Max                        0.32756\n",
      "exploration/env_infos/height Min                       -0.287027\n",
      "exploration/env_infos/final/reward_angular Mean        -1.42004\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max         -1.42004\n",
      "exploration/env_infos/final/reward_angular Min         -1.42004\n",
      "exploration/env_infos/initial/reward_angular Mean      -0.460809\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max       -0.460809\n",
      "exploration/env_infos/initial/reward_angular Min       -0.460809\n",
      "exploration/env_infos/reward_angular Mean               0.113496\n",
      "exploration/env_infos/reward_angular Std                3.17945\n",
      "exploration/env_infos/reward_angular Max                9.0398\n",
      "exploration/env_infos/reward_angular Min               -5.94921\n",
      "evaluation/num steps total                              5.25e+06\n",
      "evaluation/num paths total                           5250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.882484\n",
      "evaluation/Rewards Std                                  1.68816\n",
      "evaluation/Rewards Max                                  8.69209\n",
      "evaluation/Rewards Min                                 -6.70983\n",
      "evaluation/Returns Mean                               882.484\n",
      "evaluation/Returns Std                               1011.14\n",
      "evaluation/Returns Max                               2932.91\n",
      "evaluation/Returns Min                               -344.377\n",
      "evaluation/Actions Mean                                 0.102173\n",
      "evaluation/Actions Std                                  0.765686\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            882.484\n",
      "evaluation/env_infos/final/reward_run Mean             -1.17722\n",
      "evaluation/env_infos/final/reward_run Std               3.08985\n",
      "evaluation/env_infos/final/reward_run Max               5.25758\n",
      "evaluation/env_infos/final/reward_run Min              -5.93141\n",
      "evaluation/env_infos/initial/reward_run Mean            0.337492\n",
      "evaluation/env_infos/initial/reward_run Std             0.3529\n",
      "evaluation/env_infos/initial/reward_run Max             0.869164\n",
      "evaluation/env_infos/initial/reward_run Min            -0.653085\n",
      "evaluation/env_infos/reward_run Mean                   -1.19466\n",
      "evaluation/env_infos/reward_run Std                     3.22029\n",
      "evaluation/env_infos/reward_run Max                     6.20621\n",
      "evaluation/env_infos/reward_run Min                    -8.09978\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.35007\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.144088\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0435804\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.550344\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.286296\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0991333\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0295525\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.456676\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.358028\n",
      "evaluation/env_infos/reward_ctrl Std                    0.140509\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0146441\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.59381\n",
      "evaluation/env_infos/final/height Mean                 -0.153518\n",
      "evaluation/env_infos/final/height Std                   0.197882\n",
      "evaluation/env_infos/final/height Max                   0.118826\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.015525\n",
      "evaluation/env_infos/initial/height Std                 0.0508734\n",
      "evaluation/env_infos/initial/height Max                 0.0707628\n",
      "evaluation/env_infos/initial/height Min                -0.125037\n",
      "evaluation/env_infos/height Mean                       -0.115161\n",
      "evaluation/env_infos/height Std                         0.181379\n",
      "evaluation/env_infos/height Max                         0.459591\n",
      "evaluation/env_infos/height Min                        -0.58672\n",
      "evaluation/env_infos/final/reward_angular Mean          0.30429\n",
      "evaluation/env_infos/final/reward_angular Std           2.58573\n",
      "evaluation/env_infos/final/reward_angular Max           7.4414\n",
      "evaluation/env_infos/final/reward_angular Min          -4.14362\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.880462\n",
      "evaluation/env_infos/initial/reward_angular Std         0.412814\n",
      "evaluation/env_infos/initial/reward_angular Max         0.315015\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.4784\n",
      "evaluation/env_infos/reward_angular Mean                0.0695947\n",
      "evaluation/env_infos/reward_angular Std                 2.25559\n",
      "evaluation/env_infos/reward_angular Max                 9.54863\n",
      "evaluation/env_infos/reward_angular Min                -7.31901\n",
      "time/data storing (s)                                   0.0147088\n",
      "time/evaluation sampling (s)                           25.5052\n",
      "time/exploration sampling (s)                           1.06356\n",
      "time/logging (s)                                        0.248505\n",
      "time/saving (s)                                         0.0275225\n",
      "time/training (s)                                       4.36099\n",
      "time/epoch (s)                                         31.2204\n",
      "time/total (s)                                       6442.01\n",
      "Epoch                                                 209\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 20:59:58.680905 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 210 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 212000\n",
      "trainer/QF1 Loss                                        1.81285\n",
      "trainer/QF2 Loss                                        1.95565\n",
      "trainer/Policy Loss                                   -14.4962\n",
      "trainer/Q1 Predictions Mean                            20.1805\n",
      "trainer/Q1 Predictions Std                             34.9852\n",
      "trainer/Q1 Predictions Max                            151.576\n",
      "trainer/Q1 Predictions Min                            -21.106\n",
      "trainer/Q2 Predictions Mean                            20.1693\n",
      "trainer/Q2 Predictions Std                             35.0305\n",
      "trainer/Q2 Predictions Max                            153.626\n",
      "trainer/Q2 Predictions Min                            -21.2806\n",
      "trainer/Q Targets Mean                                 19.9112\n",
      "trainer/Q Targets Std                                  34.8944\n",
      "trainer/Q Targets Max                                 153.203\n",
      "trainer/Q Targets Min                                 -21.0533\n",
      "trainer/Log Pis Mean                                    5.8558\n",
      "trainer/Log Pis Std                                     6.1752\n",
      "trainer/Log Pis Max                                    35.0988\n",
      "trainer/Log Pis Min                                    -5.83155\n",
      "trainer/Policy mu Mean                                  0.0780569\n",
      "trainer/Policy mu Std                                   1.59503\n",
      "trainer/Policy mu Max                                   4.623\n",
      "trainer/Policy mu Min                                  -6.83599\n",
      "trainer/Policy log std Mean                            -0.686495\n",
      "trainer/Policy log std Std                              0.281415\n",
      "trainer/Policy log std Max                              0.467341\n",
      "trainer/Policy log std Min                             -2.19917\n",
      "trainer/Alpha                                           0.0220132\n",
      "trainer/Alpha Loss                                     -0.550316\n",
      "exploration/num steps total                        212000\n",
      "exploration/num paths total                           212\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.10451\n",
      "exploration/Rewards Std                                 2.04158\n",
      "exploration/Rewards Max                                 5.61218\n",
      "exploration/Rewards Min                                -6.13021\n",
      "exploration/Returns Mean                             1104.51\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1104.51\n",
      "exploration/Returns Min                              1104.51\n",
      "exploration/Actions Mean                                0.254191\n",
      "exploration/Actions Std                                 0.818682\n",
      "exploration/Actions Max                                 0.999972\n",
      "exploration/Actions Min                                -0.999913\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1104.51\n",
      "exploration/env_infos/final/reward_run Mean            -2.83983\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max             -2.83983\n",
      "exploration/env_infos/final/reward_run Min             -2.83983\n",
      "exploration/env_infos/initial/reward_run Mean          -0.671784\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.671784\n",
      "exploration/env_infos/initial/reward_run Min           -0.671784\n",
      "exploration/env_infos/reward_run Mean                  -4.26472\n",
      "exploration/env_infos/reward_run Std                    1.22647\n",
      "exploration/env_infos/reward_run Max                    1.03245\n",
      "exploration/env_infos/reward_run Min                   -7.68631\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.425896\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.425896\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.425896\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.268118\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.268118\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.268118\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.440912\n",
      "exploration/env_infos/reward_ctrl Std                   0.0625223\n",
      "exploration/env_infos/reward_ctrl Max                  -0.218086\n",
      "exploration/env_infos/reward_ctrl Min                  -0.590905\n",
      "exploration/env_infos/final/height Mean                 0.0666909\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                  0.0666909\n",
      "exploration/env_infos/final/height Min                  0.0666909\n",
      "exploration/env_infos/initial/height Mean              -0.00754462\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.00754462\n",
      "exploration/env_infos/initial/height Min               -0.00754462\n",
      "exploration/env_infos/height Mean                       0.0329755\n",
      "exploration/env_infos/height Std                        0.130713\n",
      "exploration/env_infos/height Max                        0.392155\n",
      "exploration/env_infos/height Min                       -0.310801\n",
      "exploration/env_infos/final/reward_angular Mean         0.0911729\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0911729\n",
      "exploration/env_infos/final/reward_angular Min          0.0911729\n",
      "exploration/env_infos/initial/reward_angular Mean       1.35476\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        1.35476\n",
      "exploration/env_infos/initial/reward_angular Min        1.35476\n",
      "exploration/env_infos/reward_angular Mean               0.107183\n",
      "exploration/env_infos/reward_angular Std                3.04872\n",
      "exploration/env_infos/reward_angular Max               10.11\n",
      "exploration/env_infos/reward_angular Min               -5.8638\n",
      "evaluation/num steps total                              5.275e+06\n",
      "evaluation/num paths total                           5275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.902379\n",
      "evaluation/Rewards Std                                  1.74094\n",
      "evaluation/Rewards Max                                  8.80621\n",
      "evaluation/Rewards Min                                 -8.30067\n",
      "evaluation/Returns Mean                               902.379\n",
      "evaluation/Returns Std                                958.162\n",
      "evaluation/Returns Max                               2813.28\n",
      "evaluation/Returns Min                               -175.112\n",
      "evaluation/Actions Mean                                 0.111481\n",
      "evaluation/Actions Std                                  0.775766\n",
      "evaluation/Actions Max                                  1\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            902.379\n",
      "evaluation/env_infos/final/reward_run Mean             -1.42245\n",
      "evaluation/env_infos/final/reward_run Std               3.05532\n",
      "evaluation/env_infos/final/reward_run Max               3.98991\n",
      "evaluation/env_infos/final/reward_run Min              -6.33837\n",
      "evaluation/env_infos/initial/reward_run Mean            0.323359\n",
      "evaluation/env_infos/initial/reward_run Std             0.475231\n",
      "evaluation/env_infos/initial/reward_run Max             1.00837\n",
      "evaluation/env_infos/initial/reward_run Min            -0.745871\n",
      "evaluation/env_infos/reward_run Mean                   -1.09811\n",
      "evaluation/env_infos/reward_run Std                     3.09839\n",
      "evaluation/env_infos/reward_run Max                     5.59084\n",
      "evaluation/env_infos/reward_run Min                    -8.00569\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.363763\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.136755\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.116778\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.586599\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.271535\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0871126\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0457426\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.449443\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.368544\n",
      "evaluation/env_infos/reward_ctrl Std                    0.123181\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0382951\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.597833\n",
      "evaluation/env_infos/final/height Mean                 -0.0824313\n",
      "evaluation/env_infos/final/height Std                   0.226647\n",
      "evaluation/env_infos/final/height Max                   0.198275\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00721434\n",
      "evaluation/env_infos/initial/height Std                 0.0387215\n",
      "evaluation/env_infos/initial/height Max                 0.050653\n",
      "evaluation/env_infos/initial/height Min                -0.0864831\n",
      "evaluation/env_infos/height Mean                       -0.0609787\n",
      "evaluation/env_infos/height Std                         0.182525\n",
      "evaluation/env_infos/height Max                         0.492\n",
      "evaluation/env_infos/height Min                        -0.59162\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.348269\n",
      "evaluation/env_infos/final/reward_angular Std           1.96239\n",
      "evaluation/env_infos/final/reward_angular Max           4.3604\n",
      "evaluation/env_infos/final/reward_angular Min          -4.61739\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.590849\n",
      "evaluation/env_infos/initial/reward_angular Std         0.671664\n",
      "evaluation/env_infos/initial/reward_angular Max         1.32004\n",
      "evaluation/env_infos/initial/reward_angular Min        -1.4121\n",
      "evaluation/env_infos/reward_angular Mean                0.0769601\n",
      "evaluation/env_infos/reward_angular Std                 2.35948\n",
      "evaluation/env_infos/reward_angular Max                10.5673\n",
      "evaluation/env_infos/reward_angular Min                -7.83516\n",
      "time/data storing (s)                                   0.0161719\n",
      "time/evaluation sampling (s)                           24.5394\n",
      "time/exploration sampling (s)                           1.48839\n",
      "time/logging (s)                                        0.259171\n",
      "time/saving (s)                                         0.0304388\n",
      "time/training (s)                                       5.58903\n",
      "time/epoch (s)                                         31.9226\n",
      "time/total (s)                                       6475.48\n",
      "Epoch                                                 210\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 21:00:28.265246 PDT | [gher-halfcheetahhard-SAC-300e-1000s-disc0.99_2021_05_25_19_12_29_0000--s-10] Epoch 211 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 213000\n",
      "trainer/QF1 Loss                                        2.69823\n",
      "trainer/QF2 Loss                                        2.15579\n",
      "trainer/Policy Loss                                   -18.4975\n",
      "trainer/Q1 Predictions Mean                            24.2666\n",
      "trainer/Q1 Predictions Std                             38.3977\n",
      "trainer/Q1 Predictions Max                            157.129\n",
      "trainer/Q1 Predictions Min                            -20.915\n",
      "trainer/Q2 Predictions Mean                            24.3448\n",
      "trainer/Q2 Predictions Std                             38.4727\n",
      "trainer/Q2 Predictions Max                            155.417\n",
      "trainer/Q2 Predictions Min                            -20.8267\n",
      "trainer/Q Targets Mean                                 24.4899\n",
      "trainer/Q Targets Std                                  38.5334\n",
      "trainer/Q Targets Max                                 156.937\n",
      "trainer/Q Targets Min                                 -20.379\n",
      "trainer/Log Pis Mean                                    6.0049\n",
      "trainer/Log Pis Std                                     5.81609\n",
      "trainer/Log Pis Max                                    26.2755\n",
      "trainer/Log Pis Min                                    -4.46789\n",
      "trainer/Policy mu Mean                                 -0.0231575\n",
      "trainer/Policy mu Std                                   1.61327\n",
      "trainer/Policy mu Max                                   4.21341\n",
      "trainer/Policy mu Min                                  -6.09319\n",
      "trainer/Policy log std Mean                            -0.683818\n",
      "trainer/Policy log std Std                              0.289856\n",
      "trainer/Policy log std Max                              0.674656\n",
      "trainer/Policy log std Min                             -2.26151\n",
      "trainer/Alpha                                           0.0231563\n",
      "trainer/Alpha Loss                                      0.0184478\n",
      "exploration/num steps total                        213000\n",
      "exploration/num paths total                           213\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.576782\n",
      "exploration/Rewards Std                                 1.56657\n",
      "exploration/Rewards Max                                 5.74284\n",
      "exploration/Rewards Min                                -2.08524\n",
      "exploration/Returns Mean                              576.782\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               576.782\n",
      "exploration/Returns Min                               576.782\n",
      "exploration/Actions Mean                               -0.0402741\n",
      "exploration/Actions Std                                 0.762581\n",
      "exploration/Actions Max                                 0.999906\n",
      "exploration/Actions Min                                -0.999996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           576.782\n",
      "exploration/env_infos/final/reward_run Mean             0.131868\n",
      "exploration/env_infos/final/reward_run Std              0\n",
      "exploration/env_infos/final/reward_run Max              0.131868\n",
      "exploration/env_infos/final/reward_run Min              0.131868\n",
      "exploration/env_infos/initial/reward_run Mean          -0.0762419\n",
      "exploration/env_infos/initial/reward_run Std            0\n",
      "exploration/env_infos/initial/reward_run Max           -0.0762419\n",
      "exploration/env_infos/initial/reward_run Min           -0.0762419\n",
      "exploration/env_infos/reward_run Mean                  -1.35166\n",
      "exploration/env_infos/reward_run Std                    2.04786\n",
      "exploration/env_infos/reward_run Max                    1.53054\n",
      "exploration/env_infos/reward_run Min                   -8.07363\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.215246\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.215246\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.215246\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.233627\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.233627\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.233627\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.349891\n",
      "exploration/env_infos/reward_ctrl Std                   0.115651\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0705446\n",
      "exploration/env_infos/reward_ctrl Min                  -0.595655\n",
      "exploration/env_infos/final/height Mean                -0.577086\n",
      "exploration/env_infos/final/height Std                  0\n",
      "exploration/env_infos/final/height Max                 -0.577086\n",
      "exploration/env_infos/final/height Min                 -0.577086\n",
      "exploration/env_infos/initial/height Mean              -0.0142492\n",
      "exploration/env_infos/initial/height Std                0\n",
      "exploration/env_infos/initial/height Max               -0.0142492\n",
      "exploration/env_infos/initial/height Min               -0.0142492\n",
      "exploration/env_infos/height Mean                      -0.384635\n",
      "exploration/env_infos/height Std                        0.255514\n",
      "exploration/env_infos/height Max                        0.335192\n",
      "exploration/env_infos/height Min                       -0.589613\n",
      "exploration/env_infos/final/reward_angular Mean         0.0323445\n",
      "exploration/env_infos/final/reward_angular Std          0\n",
      "exploration/env_infos/final/reward_angular Max          0.0323445\n",
      "exploration/env_infos/final/reward_angular Min          0.0323445\n",
      "exploration/env_infos/initial/reward_angular Mean       0.0169769\n",
      "exploration/env_infos/initial/reward_angular Std        0\n",
      "exploration/env_infos/initial/reward_angular Max        0.0169769\n",
      "exploration/env_infos/initial/reward_angular Min        0.0169769\n",
      "exploration/env_infos/reward_angular Mean              -0.0469191\n",
      "exploration/env_infos/reward_angular Std                2.00438\n",
      "exploration/env_infos/reward_angular Max                9.13161\n",
      "exploration/env_infos/reward_angular Min               -6.01109\n",
      "evaluation/num steps total                              5.3e+06\n",
      "evaluation/num paths total                           5300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.717072\n",
      "evaluation/Rewards Std                                  1.62721\n",
      "evaluation/Rewards Max                                  8.66519\n",
      "evaluation/Rewards Min                                 -7.99951\n",
      "evaluation/Returns Mean                               717.072\n",
      "evaluation/Returns Std                                906.652\n",
      "evaluation/Returns Max                               3122.6\n",
      "evaluation/Returns Min                               -242.074\n",
      "evaluation/Actions Mean                                 0.0451809\n",
      "evaluation/Actions Std                                  0.776106\n",
      "evaluation/Actions Max                                  0.999999\n",
      "evaluation/Actions Min                                 -1\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            717.072\n",
      "evaluation/env_infos/final/reward_run Mean             -0.244012\n",
      "evaluation/env_infos/final/reward_run Std               2.34343\n",
      "evaluation/env_infos/final/reward_run Max               3.20114\n",
      "evaluation/env_infos/final/reward_run Min              -4.67153\n",
      "evaluation/env_infos/initial/reward_run Mean            0.333851\n",
      "evaluation/env_infos/initial/reward_run Std             0.45119\n",
      "evaluation/env_infos/initial/reward_run Max             1.02528\n",
      "evaluation/env_infos/initial/reward_run Min            -0.736952\n",
      "evaluation/env_infos/reward_run Mean                   -0.424412\n",
      "evaluation/env_infos/reward_run Std                     2.78741\n",
      "evaluation/env_infos/reward_run Max                     5.5951\n",
      "evaluation/env_infos/reward_run Min                    -8.41361\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.382026\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.132022\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0841898\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.559163\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.277161\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.101392\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0615624\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.397542\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.362629\n",
      "evaluation/env_infos/reward_ctrl Std                    0.122521\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0538967\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.596339\n",
      "evaluation/env_infos/final/height Mean                 -0.151553\n",
      "evaluation/env_infos/final/height Std                   0.229994\n",
      "evaluation/env_infos/final/height Max                   0.146346\n",
      "evaluation/env_infos/final/height Min                  -0.577282\n",
      "evaluation/env_infos/initial/height Mean               -0.00856766\n",
      "evaluation/env_infos/initial/height Std                 0.0541169\n",
      "evaluation/env_infos/initial/height Max                 0.0816201\n",
      "evaluation/env_infos/initial/height Min                -0.0994875\n",
      "evaluation/env_infos/height Mean                       -0.123056\n",
      "evaluation/env_infos/height Std                         0.19954\n",
      "evaluation/env_infos/height Max                         0.48243\n",
      "evaluation/env_infos/height Min                        -0.587198\n",
      "evaluation/env_infos/final/reward_angular Mean         -0.161709\n",
      "evaluation/env_infos/final/reward_angular Std           1.82943\n",
      "evaluation/env_infos/final/reward_angular Max           3.66375\n",
      "evaluation/env_infos/final/reward_angular Min          -4.65208\n",
      "evaluation/env_infos/initial/reward_angular Mean       -0.18236\n",
      "evaluation/env_infos/initial/reward_angular Std         0.955591\n",
      "evaluation/env_infos/initial/reward_angular Max         1.93124\n",
      "evaluation/env_infos/initial/reward_angular Min        -2.2733\n",
      "evaluation/env_infos/reward_angular Mean                0.0351059\n",
      "evaluation/env_infos/reward_angular Std                 2.10247\n",
      "evaluation/env_infos/reward_angular Max                10.6948\n",
      "evaluation/env_infos/reward_angular Min                -7.64003\n",
      "time/data storing (s)                                   0.01469\n",
      "time/evaluation sampling (s)                           22.4679\n",
      "time/exploration sampling (s)                           1.0639\n",
      "time/logging (s)                                        0.283432\n",
      "time/saving (s)                                         0.0275047\n",
      "time/training (s)                                       4.16192\n",
      "time/epoch (s)                                         28.0194\n",
      "time/total (s)                                       6505.08\n",
      "Epoch                                                 211\n",
      "-------------------------------------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 300 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 1 --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard\n",
    "!python launch_gher.py --epochs 300 --relabel --n_sampled_latents 100 --use_advantages --env halfcheetahhard --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e455ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_r = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-halfcheetahhard-SAC-100e-1000s-disc0.99/gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_10_24_55_0000--s-10/progress.csv')\n",
    "# df_rr = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-halfcheetahhard-SAC-100e-1000s-disc0.99/gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_11_12_17_0000--s-10/progress.csv')\n",
    "# df_air = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-halfcheetahhard-SAC-100e-1000s-disc0.99/gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_12_00_25_0000--s-10/progress.csv')\n",
    "# df_ar = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-halfcheetahhard-SAC-100e-1000s-disc0.99/gher-halfcheetahhard-SAC-100e-1000s-disc0.99_2021_05_25_12_47_41_0000--s-10/progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_r.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'evaluation/Average Returns'\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_no_r[name], label=\"No Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_rr[name], label=\"Random Relabeling\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_air[name], label=\"AIR\")\n",
    "# plt.plot(df_no_r['evaluation/num steps total'], df_ar[name], label=\"AR\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbb777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0b1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ea770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
