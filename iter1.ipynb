{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b881f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/shanliyu/School/CSE257/own/generalized-hindsight/data/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000/gher-antdirectionnewsparse-SAC-5e-1000s-disc0.99-horizon1000_2021_05_24_22_28_26_0000--s-10/progress.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[15504]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe740). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe768). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe7b8). One of the two will be used. Which one is undefined.\n",
      "objc[15504]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a2037d818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a203fe830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 10:24:53.694438 PDT | Variant:\n",
      "2021-05-25 10:24:53.694854 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": false,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 5,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 10:25:52.001442 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  2000\n",
      "trainer/QF1 Loss                                      22.946\n",
      "trainer/QF2 Loss                                      22.9294\n",
      "trainer/Policy Loss                                   -5.32959\n",
      "trainer/Q1 Predictions Mean                            0.00261926\n",
      "trainer/Q1 Predictions Std                             0.00363781\n",
      "trainer/Q1 Predictions Max                             0.0115209\n",
      "trainer/Q1 Predictions Min                            -0.0101821\n",
      "trainer/Q2 Predictions Mean                            0.00433422\n",
      "trainer/Q2 Predictions Std                             0.00328976\n",
      "trainer/Q2 Predictions Max                             0.0167003\n",
      "trainer/Q2 Predictions Min                            -0.00425816\n",
      "trainer/Q Targets Mean                                 4.73022\n",
      "trainer/Q Targets Std                                  0.771815\n",
      "trainer/Q Targets Max                                  7.34868\n",
      "trainer/Q Targets Min                                  2.72291\n",
      "trainer/Log Pis Mean                                  -5.32833\n",
      "trainer/Log Pis Std                                    0.609291\n",
      "trainer/Log Pis Max                                   -3.45657\n",
      "trainer/Log Pis Min                                   -7.65128\n",
      "trainer/Policy mu Mean                                -0.000311433\n",
      "trainer/Policy mu Std                                  0.00209994\n",
      "trainer/Policy mu Max                                  0.0058089\n",
      "trainer/Policy mu Min                                 -0.00728771\n",
      "trainer/Policy log std Mean                           -6.28851e-05\n",
      "trainer/Policy log std Std                             0.00222649\n",
      "trainer/Policy log std Max                             0.00915091\n",
      "trainer/Policy log std Min                            -0.00840893\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           17\n",
      "exploration/path length Mean                          90.9091\n",
      "exploration/path length Std                           93.571\n",
      "exploration/path length Max                          344\n",
      "exploration/path length Min                           18\n",
      "exploration/Rewards Mean                              -0.435322\n",
      "exploration/Rewards Std                                0.602718\n",
      "exploration/Rewards Max                                2.5889\n",
      "exploration/Rewards Min                               -1.89965\n",
      "exploration/Returns Mean                             -39.5747\n",
      "exploration/Returns Std                               41.2604\n",
      "exploration/Returns Max                               -4.61274\n",
      "exploration/Returns Min                             -129.239\n",
      "exploration/Actions Mean                               0.0046145\n",
      "exploration/Actions Std                                0.62243\n",
      "exploration/Actions Max                                0.999663\n",
      "exploration/Actions Min                               -0.999062\n",
      "exploration/Num Paths                                 11\n",
      "exploration/Average Returns                          -39.5747\n",
      "exploration/env_infos/final/reward_forward Mean       -0.395515\n",
      "exploration/env_infos/final/reward_forward Std         1.11989\n",
      "exploration/env_infos/final/reward_forward Max         1.57674\n",
      "exploration/env_infos/final/reward_forward Min        -2.39917\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0818824\n",
      "exploration/env_infos/initial/reward_forward Std       0.159423\n",
      "exploration/env_infos/initial/reward_forward Max       0.0975211\n",
      "exploration/env_infos/initial/reward_forward Min      -0.41539\n",
      "exploration/env_infos/reward_forward Mean              0.0971095\n",
      "exploration/env_infos/reward_forward Std               0.905224\n",
      "exploration/env_infos/reward_forward Max               3.17502\n",
      "exploration/env_infos/reward_forward Min              -2.58717\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.53577\n",
      "exploration/env_infos/final/reward_ctrl Std            0.451785\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.636608\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.37664\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.41591\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.388531\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.57685\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.95256\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54976\n",
      "exploration/env_infos/reward_ctrl Std                  0.438063\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477861\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89965\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.468836\n",
      "exploration/env_infos/final/torso_velocity Std         1.23118\n",
      "exploration/env_infos/final/torso_velocity Max         2.85086\n",
      "exploration/env_infos/final/torso_velocity Min        -2.39917\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0809147\n",
      "exploration/env_infos/initial/torso_velocity Std       0.290305\n",
      "exploration/env_infos/initial/torso_velocity Max       0.750508\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.41539\n",
      "exploration/env_infos/torso_velocity Mean              0.0768778\n",
      "exploration/env_infos/torso_velocity Std               0.877765\n",
      "exploration/env_infos/torso_velocity Max               3.97972\n",
      "exploration/env_infos/torso_velocity Min              -2.58717\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00143\n",
      "evaluation/Rewards Std                                 0.0218896\n",
      "evaluation/Rewards Max                                 2.39282\n",
      "evaluation/Rewards Min                                 0.999983\n",
      "evaluation/Returns Mean                             1001.43\n",
      "evaluation/Returns Std                                 1.64994\n",
      "evaluation/Returns Max                              1006.21\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000137594\n",
      "evaluation/Actions Std                                 0.00115534\n",
      "evaluation/Actions Max                                 0.00360492\n",
      "evaluation/Actions Min                                -0.00367134\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1001.43\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000312019\n",
      "evaluation/env_infos/final/reward_forward Std          0.000358591\n",
      "evaluation/env_infos/final/reward_forward Max          0.00109986\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000660512\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0346382\n",
      "evaluation/env_infos/initial/reward_forward Std        0.106539\n",
      "evaluation/env_infos/initial/reward_forward Max        0.241381\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.279468\n",
      "evaluation/env_infos/reward_forward Mean               0.0022557\n",
      "evaluation/env_infos/reward_forward Std                0.0433711\n",
      "evaluation/env_infos/reward_forward Max                1.09405\n",
      "evaluation/env_infos/reward_forward Min               -1.26008\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.39701e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             6.26989e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.74359e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.71496e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.34072e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           4.95771e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.46803e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.20333e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.41493e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.44184e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -2.55525e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.67815e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000315675\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000659196\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00257464\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000660512\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.157506\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.219636\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.654757\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.279468\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000627939\n",
      "evaluation/env_infos/torso_velocity Std                0.053504\n",
      "evaluation/env_infos/torso_velocity Max                1.16395\n",
      "evaluation/env_infos/torso_velocity Min               -2.14941\n",
      "time/data storing (s)                                  0.0183775\n",
      "time/evaluation sampling (s)                          50.3564\n",
      "time/exploration sampling (s)                          2.01676\n",
      "time/logging (s)                                       0.279635\n",
      "time/saving (s)                                        0.0688003\n",
      "time/training (s)                                      3.62625\n",
      "time/epoch (s)                                        56.3662\n",
      "time/total (s)                                        61.0745\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:26:49.070311 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  3000\n",
      "trainer/QF1 Loss                                       0.606542\n",
      "trainer/QF2 Loss                                       0.606524\n",
      "trainer/Policy Loss                                   -9.41089\n",
      "trainer/Q1 Predictions Mean                            3.96736\n",
      "trainer/Q1 Predictions Std                             0.390265\n",
      "trainer/Q1 Predictions Max                             5.04277\n",
      "trainer/Q1 Predictions Min                             2.48488\n",
      "trainer/Q2 Predictions Mean                            3.9624\n",
      "trainer/Q2 Predictions Std                             0.388732\n",
      "trainer/Q2 Predictions Max                             5.0606\n",
      "trainer/Q2 Predictions Min                             2.52054\n",
      "trainer/Q Targets Mean                                 4.02841\n",
      "trainer/Q Targets Std                                  0.740426\n",
      "trainer/Q Targets Max                                  6.75989\n",
      "trainer/Q Targets Min                                 -1.37664\n",
      "trainer/Log Pis Mean                                  -5.47178\n",
      "trainer/Log Pis Std                                    0.438132\n",
      "trainer/Log Pis Max                                   -4.60066\n",
      "trainer/Log Pis Min                                   -9.37588\n",
      "trainer/Policy mu Mean                                -6.78969e-06\n",
      "trainer/Policy mu Std                                  0.0250522\n",
      "trainer/Policy mu Max                                  0.116637\n",
      "trainer/Policy mu Min                                 -0.0710389\n",
      "trainer/Policy log std Mean                           -0.10286\n",
      "trainer/Policy log std Std                             0.0209839\n",
      "trainer/Policy log std Max                            -0.0505825\n",
      "trainer/Policy log std Min                            -0.175214\n",
      "trainer/Alpha                                          0.738558\n",
      "trainer/Alpha Loss                                    -4.04226\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           26\n",
      "exploration/path length Mean                         111.111\n",
      "exploration/path length Std                          149.161\n",
      "exploration/path length Max                          523\n",
      "exploration/path length Min                           16\n",
      "exploration/Rewards Mean                              -0.400784\n",
      "exploration/Rewards Std                                0.467987\n",
      "exploration/Rewards Max                                2.89302\n",
      "exploration/Rewards Min                               -1.78684\n",
      "exploration/Returns Mean                             -44.5316\n",
      "exploration/Returns Std                               60.8203\n",
      "exploration/Returns Max                               -6.8531\n",
      "exploration/Returns Min                             -212.038\n",
      "exploration/Actions Mean                              -0.00783071\n",
      "exploration/Actions Std                                0.600648\n",
      "exploration/Actions Max                                0.998586\n",
      "exploration/Actions Min                               -0.998832\n",
      "exploration/Num Paths                                  9\n",
      "exploration/Average Returns                          -44.5316\n",
      "exploration/env_infos/final/reward_forward Mean       -0.352974\n",
      "exploration/env_infos/final/reward_forward Std         1.16497\n",
      "exploration/env_infos/final/reward_forward Max         1.45881\n",
      "exploration/env_infos/final/reward_forward Min        -2.82235\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0843298\n",
      "exploration/env_infos/initial/reward_forward Std       0.191085\n",
      "exploration/env_infos/initial/reward_forward Max       0.298051\n",
      "exploration/env_infos/initial/reward_forward Min      -0.334575\n",
      "exploration/env_infos/reward_forward Mean             -0.04571\n",
      "exploration/env_infos/reward_forward Std               0.611672\n",
      "exploration/env_infos/reward_forward Max               2.17909\n",
      "exploration/env_infos/reward_forward Min              -2.83749\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.29347\n",
      "exploration/env_infos/final/reward_ctrl Std            0.335409\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.819668\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.85889\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.53383\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.182571\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.20589\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.75982\n",
      "exploration/env_infos/reward_ctrl Mean                -1.44336\n",
      "exploration/env_infos/reward_ctrl Std                  0.41207\n",
      "exploration/env_infos/reward_ctrl Max                 -0.422239\n",
      "exploration/env_infos/reward_ctrl Min                 -2.78684\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.279339\n",
      "exploration/env_infos/final/torso_velocity Std         1.20318\n",
      "exploration/env_infos/final/torso_velocity Max         2.37685\n",
      "exploration/env_infos/final/torso_velocity Min        -2.82235\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.112733\n",
      "exploration/env_infos/initial/torso_velocity Std       0.265205\n",
      "exploration/env_infos/initial/torso_velocity Max       0.504986\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.422856\n",
      "exploration/env_infos/torso_velocity Mean             -0.0163697\n",
      "exploration/env_infos/torso_velocity Std               0.669962\n",
      "exploration/env_infos/torso_velocity Max               3.60766\n",
      "exploration/env_infos/torso_velocity Min              -2.83749\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.998808\n",
      "evaluation/Rewards Std                                 0.0187674\n",
      "evaluation/Rewards Max                                 2.42227\n",
      "evaluation/Rewards Min                                 0.995599\n",
      "evaluation/Returns Mean                              998.808\n",
      "evaluation/Returns Std                                 0.703589\n",
      "evaluation/Returns Max                              1000.87\n",
      "evaluation/Returns Min                               998.095\n",
      "evaluation/Actions Mean                               -0.00120439\n",
      "evaluation/Actions Std                                 0.0212433\n",
      "evaluation/Actions Max                                 0.0646575\n",
      "evaluation/Actions Min                                -0.0455115\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           998.808\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.60225e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.30351e-07\n",
      "evaluation/env_infos/final/reward_forward Max          2.24301e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.80174e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0355255\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0901958\n",
      "evaluation/env_infos/initial/reward_forward Max        0.211949\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.116692\n",
      "evaluation/env_infos/reward_forward Mean               0.000970685\n",
      "evaluation/env_infos/reward_forward Std                0.0512219\n",
      "evaluation/env_infos/reward_forward Max                1.36977\n",
      "evaluation/env_infos/reward_forward Min               -1.08007\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00180807\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000247392\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00134118\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00204786\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00126574\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000216994\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000964923\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00169954\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00181091\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000247066\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000946125\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00457778\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.15327e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.0025e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.23411e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.4674e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137063\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.205487\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.513861\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.234168\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00101473\n",
      "evaluation/env_infos/torso_velocity Std                0.0537056\n",
      "evaluation/env_infos/torso_velocity Max                1.36977\n",
      "evaluation/env_infos/torso_velocity Min               -1.76539\n",
      "time/data storing (s)                                  0.0172376\n",
      "time/evaluation sampling (s)                          50.8696\n",
      "time/exploration sampling (s)                          1.8933\n",
      "time/logging (s)                                       0.27176\n",
      "time/saving (s)                                        0.0268071\n",
      "time/training (s)                                      3.70823\n",
      "time/epoch (s)                                        56.7869\n",
      "time/total (s)                                       118.135\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:27:44.128090 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                       0.360205\n",
      "trainer/QF2 Loss                                       0.366803\n",
      "trainer/Policy Loss                                   -9.01923\n",
      "trainer/Q1 Predictions Mean                            3.58762\n",
      "trainer/Q1 Predictions Std                             0.328466\n",
      "trainer/Q1 Predictions Max                             4.6006\n",
      "trainer/Q1 Predictions Min                             2.38658\n",
      "trainer/Q2 Predictions Mean                            3.58133\n",
      "trainer/Q2 Predictions Std                             0.33548\n",
      "trainer/Q2 Predictions Max                             4.65731\n",
      "trainer/Q2 Predictions Min                             2.40657\n",
      "trainer/Q Targets Mean                                 3.73994\n",
      "trainer/Q Targets Std                                  0.549587\n",
      "trainer/Q Targets Max                                  6.46728\n",
      "trainer/Q Targets Min                                 -0.324779\n",
      "trainer/Log Pis Mean                                  -5.45396\n",
      "trainer/Log Pis Std                                    0.350168\n",
      "trainer/Log Pis Max                                   -4.52936\n",
      "trainer/Log Pis Min                                   -7.76873\n",
      "trainer/Policy mu Mean                                -0.00781815\n",
      "trainer/Policy mu Std                                  0.0255865\n",
      "trainer/Policy mu Max                                  0.154751\n",
      "trainer/Policy mu Min                                 -0.102777\n",
      "trainer/Policy log std Mean                           -0.122764\n",
      "trainer/Policy log std Std                             0.0148932\n",
      "trainer/Policy log std Max                            -0.0606899\n",
      "trainer/Policy log std Min                            -0.21261\n",
      "trainer/Alpha                                          0.547088\n",
      "trainer/Alpha Loss                                    -8.07431\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           28\n",
      "exploration/path length Mean                         500\n",
      "exploration/path length Std                          443\n",
      "exploration/path length Max                          943\n",
      "exploration/path length Min                           57\n",
      "exploration/Rewards Mean                              -0.372951\n",
      "exploration/Rewards Std                                0.439135\n",
      "exploration/Rewards Max                                1.03478\n",
      "exploration/Rewards Min                               -1.74829\n",
      "exploration/Returns Mean                            -186.475\n",
      "exploration/Returns Std                              167.449\n",
      "exploration/Returns Max                              -19.027\n",
      "exploration/Returns Min                             -353.924\n",
      "exploration/Actions Mean                              -0.00413804\n",
      "exploration/Actions Std                                0.593206\n",
      "exploration/Actions Max                                0.998229\n",
      "exploration/Actions Min                               -0.996405\n",
      "exploration/Num Paths                                  2\n",
      "exploration/Average Returns                         -186.475\n",
      "exploration/env_infos/final/reward_forward Mean       -0.789552\n",
      "exploration/env_infos/final/reward_forward Std         0.848785\n",
      "exploration/env_infos/final/reward_forward Max         0.0592329\n",
      "exploration/env_infos/final/reward_forward Min        -1.63834\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0814731\n",
      "exploration/env_infos/initial/reward_forward Std       0.242425\n",
      "exploration/env_infos/initial/reward_forward Max       0.323898\n",
      "exploration/env_infos/initial/reward_forward Min      -0.160952\n",
      "exploration/env_infos/reward_forward Mean             -0.0120297\n",
      "exploration/env_infos/reward_forward Std               0.383437\n",
      "exploration/env_infos/reward_forward Max               1.00984\n",
      "exploration/env_infos/reward_forward Min              -1.63834\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.37739\n",
      "exploration/env_infos/final/reward_ctrl Std            0.0408099\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.33658\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.4182\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.65824\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.871096\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.787147\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.52934\n",
      "exploration/env_infos/reward_ctrl Mean                -1.40764\n",
      "exploration/env_infos/reward_ctrl Std                  0.422008\n",
      "exploration/env_infos/reward_ctrl Max                 -0.224481\n",
      "exploration/env_infos/reward_ctrl Min                 -2.74829\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0328022\n",
      "exploration/env_infos/final/torso_velocity Std         0.796553\n",
      "exploration/env_infos/final/torso_velocity Max         0.81515\n",
      "exploration/env_infos/final/torso_velocity Min        -1.63834\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.192908\n",
      "exploration/env_infos/initial/torso_velocity Std       0.211891\n",
      "exploration/env_infos/initial/torso_velocity Max       0.479942\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.160952\n",
      "exploration/env_infos/torso_velocity Mean              0.0108432\n",
      "exploration/env_infos/torso_velocity Std               0.405458\n",
      "exploration/env_infos/torso_velocity Max               2.1777\n",
      "exploration/env_infos/torso_velocity Min              -2.42281\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.998732\n",
      "evaluation/Rewards Std                                 0.0249507\n",
      "evaluation/Rewards Max                                 2.44904\n",
      "evaluation/Rewards Min                                 0.996024\n",
      "evaluation/Returns Mean                              998.732\n",
      "evaluation/Returns Std                                 1.32515\n",
      "evaluation/Returns Max                              1004.01\n",
      "evaluation/Returns Min                               997.716\n",
      "evaluation/Actions Mean                               -0.00639897\n",
      "evaluation/Actions Std                                 0.0212947\n",
      "evaluation/Actions Max                                 0.0456837\n",
      "evaluation/Actions Min                                -0.0518553\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           998.732\n",
      "evaluation/env_infos/final/reward_forward Mean         2.58422e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000156115\n",
      "evaluation/env_infos/final/reward_forward Max          0.000780033\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00013243\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0310224\n",
      "evaluation/env_infos/initial/reward_forward Std        0.122867\n",
      "evaluation/env_infos/initial/reward_forward Max        0.299604\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.28928\n",
      "evaluation/env_infos/reward_forward Mean              -0.00234073\n",
      "evaluation/env_infos/reward_forward Std                0.0442357\n",
      "evaluation/env_infos/reward_forward Max                1.03476\n",
      "evaluation/env_infos/reward_forward Min               -1.50949\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00197819\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000207011\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00157024\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00232892\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00161193\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000136903\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00134476\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0019431\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00197765\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000209496\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00114296\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0039764\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.40503e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00010606\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000780033\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000330413\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145656\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.242578\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.755381\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.28928\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00348627\n",
      "evaluation/env_infos/torso_velocity Std                0.0538398\n",
      "evaluation/env_infos/torso_velocity Max                1.03476\n",
      "evaluation/env_infos/torso_velocity Min               -2.20918\n",
      "time/data storing (s)                                  0.0149298\n",
      "time/evaluation sampling (s)                          49.1814\n",
      "time/exploration sampling (s)                          1.93628\n",
      "time/logging (s)                                       0.275844\n",
      "time/saving (s)                                        0.0267672\n",
      "time/training (s)                                      3.43627\n",
      "time/epoch (s)                                        54.8715\n",
      "time/total (s)                                       173.197\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:28:39.409369 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   5000\n",
      "trainer/QF1 Loss                                        0.371607\n",
      "trainer/QF2 Loss                                        0.368643\n",
      "trainer/Policy Loss                                    -9.07059\n",
      "trainer/Q1 Predictions Mean                             3.61325\n",
      "trainer/Q1 Predictions Std                              0.296767\n",
      "trainer/Q1 Predictions Max                              4.55949\n",
      "trainer/Q1 Predictions Min                              2.50059\n",
      "trainer/Q2 Predictions Mean                             3.62659\n",
      "trainer/Q2 Predictions Std                              0.297647\n",
      "trainer/Q2 Predictions Max                              4.43497\n",
      "trainer/Q2 Predictions Min                              2.42618\n",
      "trainer/Q Targets Mean                                  3.68909\n",
      "trainer/Q Targets Std                                   0.629181\n",
      "trainer/Q Targets Max                                   6.24423\n",
      "trainer/Q Targets Min                                  -0.419588\n",
      "trainer/Log Pis Mean                                   -5.47697\n",
      "trainer/Log Pis Std                                     0.427494\n",
      "trainer/Log Pis Max                                    -4.62317\n",
      "trainer/Log Pis Min                                    -9.53484\n",
      "trainer/Policy mu Mean                                 -0.0113808\n",
      "trainer/Policy mu Std                                   0.0332386\n",
      "trainer/Policy mu Max                                   0.135594\n",
      "trainer/Policy mu Min                                  -0.142924\n",
      "trainer/Policy log std Mean                            -0.140862\n",
      "trainer/Policy log std Std                              0.0188178\n",
      "trainer/Policy log std Max                             -0.0704517\n",
      "trainer/Policy log std Min                             -0.215702\n",
      "trainer/Alpha                                           0.405279\n",
      "trainer/Alpha Loss                                    -12.1317\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            30\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           461\n",
      "exploration/path length Max                           961\n",
      "exploration/path length Min                            39\n",
      "exploration/Rewards Mean                               -0.332221\n",
      "exploration/Rewards Std                                 0.430967\n",
      "exploration/Rewards Max                                 1.58069\n",
      "exploration/Rewards Min                                -1.69202\n",
      "exploration/Returns Mean                             -166.11\n",
      "exploration/Returns Std                               153.614\n",
      "exploration/Returns Max                               -12.4969\n",
      "exploration/Returns Min                              -319.724\n",
      "exploration/Actions Mean                               -0.00251528\n",
      "exploration/Actions Std                                 0.585622\n",
      "exploration/Actions Max                                 0.996782\n",
      "exploration/Actions Min                                -0.995335\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                          -166.11\n",
      "exploration/env_infos/final/reward_forward Mean         0.507931\n",
      "exploration/env_infos/final/reward_forward Std          1.16427\n",
      "exploration/env_infos/final/reward_forward Max          1.6722\n",
      "exploration/env_infos/final/reward_forward Min         -0.656336\n",
      "exploration/env_infos/initial/reward_forward Mean       0.165879\n",
      "exploration/env_infos/initial/reward_forward Std        0.1906\n",
      "exploration/env_infos/initial/reward_forward Max        0.356479\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0247208\n",
      "exploration/env_infos/reward_forward Mean              -0.00766838\n",
      "exploration/env_infos/reward_forward Std                0.410228\n",
      "exploration/env_infos/reward_forward Max                2.37215\n",
      "exploration/env_infos/reward_forward Min               -1.45037\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.31304\n",
      "exploration/env_infos/final/reward_ctrl Std             0.503332\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.809711\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.81637\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.40547\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0872137\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.31825\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.49268\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37184\n",
      "exploration/env_infos/reward_ctrl Std                   0.404715\n",
      "exploration/env_infos/reward_ctrl Max                  -0.32943\n",
      "exploration/env_infos/reward_ctrl Min                  -2.69202\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.325156\n",
      "exploration/env_infos/final/torso_velocity Std          0.754637\n",
      "exploration/env_infos/final/torso_velocity Max          1.6722\n",
      "exploration/env_infos/final/torso_velocity Min         -0.656336\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.176351\n",
      "exploration/env_infos/initial/torso_velocity Std        0.303801\n",
      "exploration/env_infos/initial/torso_velocity Max        0.559097\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.259612\n",
      "exploration/env_infos/torso_velocity Mean               0.0102447\n",
      "exploration/env_infos/torso_velocity Std                0.384042\n",
      "exploration/env_infos/torso_velocity Max                3.20139\n",
      "exploration/env_infos/torso_velocity Min               -2.89334\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.998007\n",
      "evaluation/Rewards Std                                  0.0263147\n",
      "evaluation/Rewards Max                                  2.4141\n",
      "evaluation/Rewards Min                                  0.994239\n",
      "evaluation/Returns Mean                               998.007\n",
      "evaluation/Returns Std                                  1.90108\n",
      "evaluation/Returns Max                               1003.81\n",
      "evaluation/Returns Min                                995.388\n",
      "evaluation/Actions Mean                                -0.0105852\n",
      "evaluation/Actions Std                                  0.0267141\n",
      "evaluation/Actions Max                                  0.0399429\n",
      "evaluation/Actions Min                                 -0.0655534\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            998.007\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000100683\n",
      "evaluation/env_infos/final/reward_forward Std           0.000523139\n",
      "evaluation/env_infos/final/reward_forward Max           0.00265589\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00015107\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0127757\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115911\n",
      "evaluation/env_infos/initial/reward_forward Max         0.162588\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.195974\n",
      "evaluation/env_infos/reward_forward Mean                0.00100653\n",
      "evaluation/env_infos/reward_forward Std                 0.0478308\n",
      "evaluation/env_infos/reward_forward Max                 1.09674\n",
      "evaluation/env_infos/reward_forward Min                -1.48288\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00332176\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000557678\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00260253\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00461706\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00330316\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000559152\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00245025\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00425841\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00330277\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000579204\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00197345\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0057613\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.62709e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000336944\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00265589\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00108266\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128671\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225013\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.544437\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.235771\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000811902\n",
      "evaluation/env_infos/torso_velocity Std                 0.0529041\n",
      "evaluation/env_infos/torso_velocity Max                 1.24046\n",
      "evaluation/env_infos/torso_velocity Min                -1.73329\n",
      "time/data storing (s)                                   0.0149866\n",
      "time/evaluation sampling (s)                           48.7667\n",
      "time/exploration sampling (s)                           1.99749\n",
      "time/logging (s)                                        0.305568\n",
      "time/saving (s)                                         0.0308998\n",
      "time/training (s)                                       3.99608\n",
      "time/epoch (s)                                         55.1117\n",
      "time/total (s)                                        228.508\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:29:35.200713 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   6000\n",
      "trainer/QF1 Loss                                        0.402287\n",
      "trainer/QF2 Loss                                        0.383144\n",
      "trainer/Policy Loss                                    -9.112\n",
      "trainer/Q1 Predictions Mean                             3.68166\n",
      "trainer/Q1 Predictions Std                              0.288223\n",
      "trainer/Q1 Predictions Max                              4.49122\n",
      "trainer/Q1 Predictions Min                              2.92403\n",
      "trainer/Q2 Predictions Mean                             3.68445\n",
      "trainer/Q2 Predictions Std                              0.287815\n",
      "trainer/Q2 Predictions Max                              4.62446\n",
      "trainer/Q2 Predictions Min                              2.73789\n",
      "trainer/Q Targets Mean                                  3.7325\n",
      "trainer/Q Targets Std                                   0.679184\n",
      "trainer/Q Targets Max                                   6.93174\n",
      "trainer/Q Targets Min                                  -0.920379\n",
      "trainer/Log Pis Mean                                   -5.44333\n",
      "trainer/Log Pis Std                                     0.387289\n",
      "trainer/Log Pis Max                                    -4.52185\n",
      "trainer/Log Pis Min                                    -7.28996\n",
      "trainer/Policy mu Mean                                 -0.0202597\n",
      "trainer/Policy mu Std                                   0.0570464\n",
      "trainer/Policy mu Max                                   0.158459\n",
      "trainer/Policy mu Min                                  -0.25169\n",
      "trainer/Policy log std Mean                            -0.132945\n",
      "trainer/Policy log std Std                              0.0160952\n",
      "trainer/Policy log std Max                             -0.0932732\n",
      "trainer/Policy log std Min                             -0.207179\n",
      "trainer/Alpha                                           0.300227\n",
      "trainer/Alpha Loss                                    -16.1349\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            40\n",
      "exploration/path length Mean                          100\n",
      "exploration/path length Std                            87.2055\n",
      "exploration/path length Max                           337\n",
      "exploration/path length Min                            17\n",
      "exploration/Rewards Mean                               -0.281422\n",
      "exploration/Rewards Std                                 0.558312\n",
      "exploration/Rewards Max                                 2.9086\n",
      "exploration/Rewards Min                                -1.81567\n",
      "exploration/Returns Mean                              -28.1422\n",
      "exploration/Returns Std                                31.6638\n",
      "exploration/Returns Max                                 7.93669\n",
      "exploration/Returns Min                              -114.698\n",
      "exploration/Actions Mean                               -0.0256338\n",
      "exploration/Actions Std                                 0.587125\n",
      "exploration/Actions Max                                 0.995734\n",
      "exploration/Actions Min                                -0.998942\n",
      "exploration/Num Paths                                  10\n",
      "exploration/Average Returns                           -28.1422\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0643964\n",
      "exploration/env_infos/final/reward_forward Std          1.16625\n",
      "exploration/env_infos/final/reward_forward Max          2.21745\n",
      "exploration/env_infos/final/reward_forward Min         -1.71145\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0345051\n",
      "exploration/env_infos/initial/reward_forward Std        0.168347\n",
      "exploration/env_infos/initial/reward_forward Max        0.217505\n",
      "exploration/env_infos/initial/reward_forward Min       -0.300126\n",
      "exploration/env_infos/reward_forward Mean               0.127907\n",
      "exploration/env_infos/reward_forward Std                0.77627\n",
      "exploration/env_infos/reward_forward Max                2.64447\n",
      "exploration/env_infos/reward_forward Min               -2.18675\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.43828\n",
      "exploration/env_infos/final/reward_ctrl Std             0.495388\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.697208\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.0034\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.34448\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.311116\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.892219\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.93271\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.38149\n",
      "exploration/env_infos/reward_ctrl Std                   0.404287\n",
      "exploration/env_infos/reward_ctrl Max                  -0.315324\n",
      "exploration/env_infos/reward_ctrl Min                  -2.81567\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.424337\n",
      "exploration/env_infos/final/torso_velocity Std          1.43934\n",
      "exploration/env_infos/final/torso_velocity Max          2.79744\n",
      "exploration/env_infos/final/torso_velocity Min         -2.10092\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.171009\n",
      "exploration/env_infos/initial/torso_velocity Std        0.242503\n",
      "exploration/env_infos/initial/torso_velocity Max        0.691015\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.300126\n",
      "exploration/env_infos/torso_velocity Mean               0.0618487\n",
      "exploration/env_infos/torso_velocity Std                0.758488\n",
      "exploration/env_infos/torso_velocity Max                3.59508\n",
      "exploration/env_infos/torso_velocity Min               -2.35663\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.991797\n",
      "evaluation/Rewards Std                                  0.026503\n",
      "evaluation/Rewards Max                                  2.39607\n",
      "evaluation/Rewards Min                                  0.963831\n",
      "evaluation/Returns Mean                               991.797\n",
      "evaluation/Returns Std                                  2.08117\n",
      "evaluation/Returns Max                                998.398\n",
      "evaluation/Returns Min                                988.368\n",
      "evaluation/Actions Mean                                -0.0165376\n",
      "evaluation/Actions Std                                  0.046131\n",
      "evaluation/Actions Max                                  0.095343\n",
      "evaluation/Actions Min                                 -0.201739\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            991.797\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.48914e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.85083e-06\n",
      "evaluation/env_infos/final/reward_forward Max           5.58847e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.17089e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.013998\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124553\n",
      "evaluation/env_infos/initial/reward_forward Max         0.201383\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.287535\n",
      "evaluation/env_infos/reward_forward Mean               -7.98269e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0535508\n",
      "evaluation/env_infos/reward_forward Max                 1.22884\n",
      "evaluation/env_infos/reward_forward Min                -1.43468\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00955983\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00142039\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00804991\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0126674\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00897681\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000991924\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00757956\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0109396\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00960626\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00158367\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00612077\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.036169\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.54486e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           5.04702e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.35499e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.36745e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.150064\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225791\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.762377\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.287535\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000934674\n",
      "evaluation/env_infos/torso_velocity Std                 0.0547097\n",
      "evaluation/env_infos/torso_velocity Max                 1.22884\n",
      "evaluation/env_infos/torso_velocity Min                -1.8519\n",
      "time/data storing (s)                                   0.0172952\n",
      "time/evaluation sampling (s)                           49.7293\n",
      "time/exploration sampling (s)                           1.89152\n",
      "time/logging (s)                                        0.278079\n",
      "time/saving (s)                                         0.0279566\n",
      "time/training (s)                                       3.57892\n",
      "time/epoch (s)                                         55.5231\n",
      "time/total (s)                                        284.271\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:30:27.697112 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   7000\n",
      "trainer/QF1 Loss                                        0.221621\n",
      "trainer/QF2 Loss                                        0.209144\n",
      "trainer/Policy Loss                                    -9.09589\n",
      "trainer/Q1 Predictions Mean                             3.65336\n",
      "trainer/Q1 Predictions Std                              0.327089\n",
      "trainer/Q1 Predictions Max                              5.17096\n",
      "trainer/Q1 Predictions Min                              2.85603\n",
      "trainer/Q2 Predictions Mean                             3.65715\n",
      "trainer/Q2 Predictions Std                              0.341538\n",
      "trainer/Q2 Predictions Max                              5.3743\n",
      "trainer/Q2 Predictions Min                              2.95015\n",
      "trainer/Q Targets Mean                                  3.77585\n",
      "trainer/Q Targets Std                                   0.511668\n",
      "trainer/Q Targets Max                                   6.47686\n",
      "trainer/Q Targets Min                                   2.62794\n",
      "trainer/Log Pis Mean                                   -5.45867\n",
      "trainer/Log Pis Std                                     0.606326\n",
      "trainer/Log Pis Max                                    -4.00132\n",
      "trainer/Log Pis Min                                    -9.88238\n",
      "trainer/Policy mu Mean                                 -0.0284535\n",
      "trainer/Policy mu Std                                   0.0945644\n",
      "trainer/Policy mu Max                                   0.261152\n",
      "trainer/Policy mu Min                                  -0.580878\n",
      "trainer/Policy log std Mean                            -0.133299\n",
      "trainer/Policy log std Std                              0.0189152\n",
      "trainer/Policy log std Max                             -0.0870391\n",
      "trainer/Policy log std Min                             -0.237341\n",
      "trainer/Alpha                                           0.222456\n",
      "trainer/Alpha Loss                                    -20.1884\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            42\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           443\n",
      "exploration/path length Max                           943\n",
      "exploration/path length Min                            57\n",
      "exploration/Rewards Mean                               -0.358604\n",
      "exploration/Rewards Std                                 0.453401\n",
      "exploration/Rewards Max                                 1.36623\n",
      "exploration/Rewards Min                                -2.07353\n",
      "exploration/Returns Mean                             -179.302\n",
      "exploration/Returns Std                               156.125\n",
      "exploration/Returns Max                               -23.1765\n",
      "exploration/Returns Min                              -335.427\n",
      "exploration/Actions Mean                               -0.0166429\n",
      "exploration/Actions Std                                 0.593811\n",
      "exploration/Actions Max                                 0.996267\n",
      "exploration/Actions Min                                -0.998074\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                          -179.302\n",
      "exploration/env_infos/final/reward_forward Mean         0.349272\n",
      "exploration/env_infos/final/reward_forward Std          0.127422\n",
      "exploration/env_infos/final/reward_forward Max          0.476694\n",
      "exploration/env_infos/final/reward_forward Min          0.22185\n",
      "exploration/env_infos/initial/reward_forward Mean       0.183299\n",
      "exploration/env_infos/initial/reward_forward Std        0.0184853\n",
      "exploration/env_infos/initial/reward_forward Max        0.201784\n",
      "exploration/env_infos/initial/reward_forward Min        0.164813\n",
      "exploration/env_infos/reward_forward Mean               0.0639577\n",
      "exploration/env_infos/reward_forward Std                0.572232\n",
      "exploration/env_infos/reward_forward Max                2.4896\n",
      "exploration/env_infos/reward_forward Min               -1.41921\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.01155\n",
      "exploration/env_infos/final/reward_ctrl Std             0.0970775\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.914473\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.10863\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.27046\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.0407788\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.22968\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.31123\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.41155\n",
      "exploration/env_infos/reward_ctrl Std                   0.42085\n",
      "exploration/env_infos/reward_ctrl Max                  -0.240669\n",
      "exploration/env_infos/reward_ctrl Min                  -3.07353\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.558905\n",
      "exploration/env_infos/final/torso_velocity Std          0.616584\n",
      "exploration/env_infos/final/torso_velocity Max          1.86683\n",
      "exploration/env_infos/final/torso_velocity Min          0.0121702\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.24887\n",
      "exploration/env_infos/initial/torso_velocity Std        0.166528\n",
      "exploration/env_infos/initial/torso_velocity Max        0.525795\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0336804\n",
      "exploration/env_infos/torso_velocity Mean              -0.00316633\n",
      "exploration/env_infos/torso_velocity Std                0.507407\n",
      "exploration/env_infos/torso_velocity Max                2.75422\n",
      "exploration/env_infos/torso_velocity Min               -2.56137\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985123\n",
      "evaluation/Rewards Std                                  0.0213754\n",
      "evaluation/Rewards Max                                  2.20886\n",
      "evaluation/Rewards Min                                  0.730694\n",
      "evaluation/Returns Mean                               985.123\n",
      "evaluation/Returns Std                                  2.98391\n",
      "evaluation/Returns Max                                988.657\n",
      "evaluation/Returns Min                                977.998\n",
      "evaluation/Actions Mean                                -0.0237353\n",
      "evaluation/Actions Std                                  0.0574785\n",
      "evaluation/Actions Max                                  0.186304\n",
      "evaluation/Actions Min                                 -0.474752\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.123\n",
      "evaluation/env_infos/final/reward_forward Mean          5.55279e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.6081e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.91896e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.58449e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.028553\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0866772\n",
      "evaluation/env_infos/initial/reward_forward Max         0.102369\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.288209\n",
      "evaluation/env_infos/reward_forward Mean               -0.00351224\n",
      "evaluation/env_infos/reward_forward Std                 0.0512872\n",
      "evaluation/env_infos/reward_forward Max                 0.715497\n",
      "evaluation/env_infos/reward_forward Min                -1.34734\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0151179\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00322443\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0116814\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0224275\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0145644\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00394058\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0100906\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0231439\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0154686\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0084715\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00655622\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.269306\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.59737e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.91445e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.14939e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.63863e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130896\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.223661\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.636787\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.288209\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00242834\n",
      "evaluation/env_infos/torso_velocity Std                 0.0519119\n",
      "evaluation/env_infos/torso_velocity Max                 1.15827\n",
      "evaluation/env_infos/torso_velocity Min                -1.74857\n",
      "time/data storing (s)                                   0.0156136\n",
      "time/evaluation sampling (s)                           46.3399\n",
      "time/exploration sampling (s)                           2.02711\n",
      "time/logging (s)                                        0.281815\n",
      "time/saving (s)                                         0.027766\n",
      "time/training (s)                                       3.592\n",
      "time/epoch (s)                                         52.2842\n",
      "time/total (s)                                        336.771\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:31:22.190322 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   8000\n",
      "trainer/QF1 Loss                                        0.314229\n",
      "trainer/QF2 Loss                                        0.302574\n",
      "trainer/Policy Loss                                    -9.02304\n",
      "trainer/Q1 Predictions Mean                             3.66675\n",
      "trainer/Q1 Predictions Std                              0.308303\n",
      "trainer/Q1 Predictions Max                              5.1721\n",
      "trainer/Q1 Predictions Min                              2.6906\n",
      "trainer/Q2 Predictions Mean                             3.61107\n",
      "trainer/Q2 Predictions Std                              0.325644\n",
      "trainer/Q2 Predictions Max                              5.2233\n",
      "trainer/Q2 Predictions Min                              2.54973\n",
      "trainer/Q Targets Mean                                  3.66661\n",
      "trainer/Q Targets Std                                   0.67492\n",
      "trainer/Q Targets Max                                   6.9872\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                   -5.43707\n",
      "trainer/Log Pis Std                                     0.561768\n",
      "trainer/Log Pis Max                                    -3.98195\n",
      "trainer/Log Pis Min                                   -10.062\n",
      "trainer/Policy mu Mean                                 -0.0303478\n",
      "trainer/Policy mu Std                                   0.10688\n",
      "trainer/Policy mu Max                                   0.375994\n",
      "trainer/Policy mu Min                                  -0.555145\n",
      "trainer/Policy log std Mean                            -0.155271\n",
      "trainer/Policy log std Std                              0.0242673\n",
      "trainer/Policy log std Max                             -0.0697164\n",
      "trainer/Policy log std Min                             -0.281619\n",
      "trainer/Alpha                                           0.164897\n",
      "trainer/Alpha Loss                                    -24.1793\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            43\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                               -0.304031\n",
      "exploration/Rewards Std                                 0.456544\n",
      "exploration/Rewards Max                                 1.49165\n",
      "exploration/Rewards Min                                -1.63852\n",
      "exploration/Returns Mean                             -304.031\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              -304.031\n",
      "exploration/Returns Min                              -304.031\n",
      "exploration/Actions Mean                               -0.0228873\n",
      "exploration/Actions Std                                 0.581405\n",
      "exploration/Actions Max                                 0.995112\n",
      "exploration/Actions Min                                -0.995053\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          -304.031\n",
      "exploration/env_infos/final/reward_forward Mean        -0.532301\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.532301\n",
      "exploration/env_infos/final/reward_forward Min         -0.532301\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0187557\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0187557\n",
      "exploration/env_infos/initial/reward_forward Min        0.0187557\n",
      "exploration/env_infos/reward_forward Mean              -0.0524543\n",
      "exploration/env_infos/reward_forward Std                0.4066\n",
      "exploration/env_infos/reward_forward Max                1.14828\n",
      "exploration/env_infos/reward_forward Min               -1.83081\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.96161\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -1.96161\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.96161\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -2.32447\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -2.32447\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.32447\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.35422\n",
      "exploration/env_infos/reward_ctrl Std                   0.420406\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0935119\n",
      "exploration/env_infos/reward_ctrl Min                  -2.65188\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.348302\n",
      "exploration/env_infos/final/torso_velocity Std          0.131704\n",
      "exploration/env_infos/final/torso_velocity Max         -0.231257\n",
      "exploration/env_infos/final/torso_velocity Min         -0.532301\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.135018\n",
      "exploration/env_infos/initial/torso_velocity Std        0.141688\n",
      "exploration/env_infos/initial/torso_velocity Max        0.334484\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0187557\n",
      "exploration/env_infos/torso_velocity Mean              -0.0258224\n",
      "exploration/env_infos/torso_velocity Std                0.373607\n",
      "exploration/env_infos/torso_velocity Max                2.57406\n",
      "exploration/env_infos/torso_velocity Min               -2.20525\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.984037\n",
      "evaluation/Rewards Std                                  0.0237064\n",
      "evaluation/Rewards Max                                  2.33516\n",
      "evaluation/Rewards Min                                  0.703186\n",
      "evaluation/Returns Mean                               984.037\n",
      "evaluation/Returns Std                                  2.13537\n",
      "evaluation/Returns Max                                989.852\n",
      "evaluation/Returns Min                                981.178\n",
      "evaluation/Actions Mean                                -0.0342791\n",
      "evaluation/Actions Std                                  0.0546808\n",
      "evaluation/Actions Max                                  0.279455\n",
      "evaluation/Actions Min                                 -0.53299\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            984.037\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00165872\n",
      "evaluation/env_infos/final/reward_forward Std           0.00822673\n",
      "evaluation/env_infos/final/reward_forward Max           0.000487743\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0419586\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0415104\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125001\n",
      "evaluation/env_infos/initial/reward_forward Max         0.238231\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.310529\n",
      "evaluation/env_infos/reward_forward Mean               -0.000789985\n",
      "evaluation/env_infos/reward_forward Std                 0.0469921\n",
      "evaluation/env_infos/reward_forward Max                 1.04829\n",
      "evaluation/env_infos/reward_forward Min                -1.70851\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0161793\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00162634\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0128574\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0186471\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.014198\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00239463\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0102258\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0202218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0166602\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00905586\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00894136\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.296814\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000732245\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00512864\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00197903\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0419586\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.133812\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253302\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.61694\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310529\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00156111\n",
      "evaluation/env_infos/torso_velocity Std                 0.0495662\n",
      "evaluation/env_infos/torso_velocity Max                 1.04829\n",
      "evaluation/env_infos/torso_velocity Min                -1.88016\n",
      "time/data storing (s)                                   0.0150738\n",
      "time/evaluation sampling (s)                           48.1821\n",
      "time/exploration sampling (s)                           1.98249\n",
      "time/logging (s)                                        0.272695\n",
      "time/saving (s)                                         0.0262913\n",
      "time/training (s)                                       3.78096\n",
      "time/epoch (s)                                         54.2596\n",
      "time/total (s)                                        391.255\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:32:17.234906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                   9000\n",
      "trainer/QF1 Loss                                        0.257678\n",
      "trainer/QF2 Loss                                        0.231392\n",
      "trainer/Policy Loss                                    -8.80985\n",
      "trainer/Q1 Predictions Mean                             3.56858\n",
      "trainer/Q1 Predictions Std                              0.354346\n",
      "trainer/Q1 Predictions Max                              5.88693\n",
      "trainer/Q1 Predictions Min                              2.50439\n",
      "trainer/Q2 Predictions Mean                             3.53768\n",
      "trainer/Q2 Predictions Std                              0.398111\n",
      "trainer/Q2 Predictions Max                              5.86132\n",
      "trainer/Q2 Predictions Min                              2.47642\n",
      "trainer/Q Targets Mean                                  3.69824\n",
      "trainer/Q Targets Std                                   0.621954\n",
      "trainer/Q Targets Max                                   7.72151\n",
      "trainer/Q Targets Min                                   2.28013\n",
      "trainer/Log Pis Mean                                   -5.29072\n",
      "trainer/Log Pis Std                                     0.63513\n",
      "trainer/Log Pis Max                                    -3.27\n",
      "trainer/Log Pis Min                                    -8.13879\n",
      "trainer/Policy mu Mean                                  0.0208486\n",
      "trainer/Policy mu Std                                   0.147495\n",
      "trainer/Policy mu Max                                   0.654932\n",
      "trainer/Policy mu Min                                  -0.513452\n",
      "trainer/Policy log std Mean                            -0.212497\n",
      "trainer/Policy log std Std                              0.0542292\n",
      "trainer/Policy log std Max                             -0.124873\n",
      "trainer/Policy log std Min                             -0.545487\n",
      "trainer/Alpha                                           0.122412\n",
      "trainer/Alpha Loss                                    -27.8758\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            59\n",
      "exploration/path length Mean                           62.5\n",
      "exploration/path length Std                            42.9287\n",
      "exploration/path length Max                           153\n",
      "exploration/path length Min                            14\n",
      "exploration/Rewards Mean                               -0.21643\n",
      "exploration/Rewards Std                                 0.52384\n",
      "exploration/Rewards Max                                 2.02484\n",
      "exploration/Rewards Min                                -1.61838\n",
      "exploration/Returns Mean                              -13.5269\n",
      "exploration/Returns Std                                10.4532\n",
      "exploration/Returns Max                                -0.85678\n",
      "exploration/Returns Min                               -40.6516\n",
      "exploration/Actions Mean                                0.0212915\n",
      "exploration/Actions Std                                 0.570425\n",
      "exploration/Actions Max                                 0.994923\n",
      "exploration/Actions Min                                -0.993371\n",
      "exploration/Num Paths                                  16\n",
      "exploration/Average Returns                           -13.5269\n",
      "exploration/env_infos/final/reward_forward Mean         0.019845\n",
      "exploration/env_infos/final/reward_forward Std          1.02553\n",
      "exploration/env_infos/final/reward_forward Max          1.97646\n",
      "exploration/env_infos/final/reward_forward Min         -2.37169\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0476197\n",
      "exploration/env_infos/initial/reward_forward Std        0.201681\n",
      "exploration/env_infos/initial/reward_forward Max        0.276643\n",
      "exploration/env_infos/initial/reward_forward Min       -0.578331\n",
      "exploration/env_infos/reward_forward Mean              -0.0468801\n",
      "exploration/env_infos/reward_forward Std                0.790114\n",
      "exploration/env_infos/reward_forward Max                2.92207\n",
      "exploration/env_infos/reward_forward Min               -2.37169\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.29066\n",
      "exploration/env_infos/final/reward_ctrl Std             0.37686\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.765889\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.05962\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.38139\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.300151\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.667486\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.02286\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.30335\n",
      "exploration/env_infos/reward_ctrl Std                   0.410146\n",
      "exploration/env_infos/reward_ctrl Max                  -0.209858\n",
      "exploration/env_infos/reward_ctrl Min                  -2.61838\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.444099\n",
      "exploration/env_infos/final/torso_velocity Std          1.17544\n",
      "exploration/env_infos/final/torso_velocity Max          3.56561\n",
      "exploration/env_infos/final/torso_velocity Min         -2.37169\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.100737\n",
      "exploration/env_infos/initial/torso_velocity Std        0.301984\n",
      "exploration/env_infos/initial/torso_velocity Max        0.746047\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.578331\n",
      "exploration/env_infos/torso_velocity Mean               0.0475794\n",
      "exploration/env_infos/torso_velocity Std                0.8308\n",
      "exploration/env_infos/torso_velocity Max                4.35868\n",
      "exploration/env_infos/torso_velocity Min               -2.75453\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969019\n",
      "evaluation/Rewards Std                                  0.0462153\n",
      "evaluation/Rewards Max                                  2.83456\n",
      "evaluation/Rewards Min                                  0.640964\n",
      "evaluation/Returns Mean                               969.019\n",
      "evaluation/Returns Std                                 14.6626\n",
      "evaluation/Returns Max                               1028.71\n",
      "evaluation/Returns Min                                962.544\n",
      "evaluation/Actions Mean                                -0.0186908\n",
      "evaluation/Actions Std                                  0.0925107\n",
      "evaluation/Actions Max                                  0.409873\n",
      "evaluation/Actions Min                                 -0.529335\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.019\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000232415\n",
      "evaluation/env_infos/final/reward_forward Std           0.057182\n",
      "evaluation/env_infos/final/reward_forward Max           0.205054\n",
      "evaluation/env_infos/final/reward_forward Min          -0.199246\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0413987\n",
      "evaluation/env_infos/initial/reward_forward Std         0.108145\n",
      "evaluation/env_infos/initial/reward_forward Max         0.225487\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.256417\n",
      "evaluation/env_infos/reward_forward Mean                0.00201227\n",
      "evaluation/env_infos/reward_forward Std                 0.115508\n",
      "evaluation/env_infos/reward_forward Max                 1.20803\n",
      "evaluation/env_infos/reward_forward Min                -1.88612\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0349419\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00150916\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0311085\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0372159\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0279435\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00753926\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0203082\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0567356\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0356303\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00728471\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0176843\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.359036\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00134413\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0353392\n",
      "evaluation/env_infos/final/torso_velocity Max           0.205054\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.199246\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126907\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219629\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.724475\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.256417\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00138794\n",
      "evaluation/env_infos/torso_velocity Std                 0.0854096\n",
      "evaluation/env_infos/torso_velocity Max                 1.20803\n",
      "evaluation/env_infos/torso_velocity Min                -1.92294\n",
      "time/data storing (s)                                   0.0181262\n",
      "time/evaluation sampling (s)                           48.3423\n",
      "time/exploration sampling (s)                           1.83827\n",
      "time/logging (s)                                        0.278388\n",
      "time/saving (s)                                         0.0288162\n",
      "time/training (s)                                       4.3154\n",
      "time/epoch (s)                                         54.8213\n",
      "time/total (s)                                        446.305\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:33:10.517156 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.349883\n",
      "trainer/QF2 Loss                                        0.352419\n",
      "trainer/Policy Loss                                    -8.39415\n",
      "trainer/Q1 Predictions Mean                             3.42134\n",
      "trainer/Q1 Predictions Std                              0.443943\n",
      "trainer/Q1 Predictions Max                              5.90547\n",
      "trainer/Q1 Predictions Min                              2.33824\n",
      "trainer/Q2 Predictions Mean                             3.43701\n",
      "trainer/Q2 Predictions Std                              0.47094\n",
      "trainer/Q2 Predictions Max                              5.827\n",
      "trainer/Q2 Predictions Min                              2.34889\n",
      "trainer/Q Targets Mean                                  3.61395\n",
      "trainer/Q Targets Std                                   0.755659\n",
      "trainer/Q Targets Max                                   8.20213\n",
      "trainer/Q Targets Min                                  -0.108628\n",
      "trainer/Log Pis Mean                                   -4.94132\n",
      "trainer/Log Pis Std                                     0.902321\n",
      "trainer/Log Pis Max                                    -2.48223\n",
      "trainer/Log Pis Min                                    -8.04894\n",
      "trainer/Policy mu Mean                                 -0.00491656\n",
      "trainer/Policy mu Std                                   0.196265\n",
      "trainer/Policy mu Max                                   0.830542\n",
      "trainer/Policy mu Min                                  -0.831553\n",
      "trainer/Policy log std Mean                            -0.373549\n",
      "trainer/Policy log std Std                              0.120603\n",
      "trainer/Policy log std Max                             -0.147991\n",
      "trainer/Policy log std Min                             -0.862397\n",
      "trainer/Alpha                                           0.0913216\n",
      "trainer/Alpha Loss                                    -30.9359\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            67\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                           161.556\n",
      "exploration/path length Max                           526\n",
      "exploration/path length Min                            15\n",
      "exploration/Rewards Mean                               -0.00761758\n",
      "exploration/Rewards Std                                 0.521642\n",
      "exploration/Rewards Max                                 3.08238\n",
      "exploration/Rewards Min                                -1.38404\n",
      "exploration/Returns Mean                               -0.952197\n",
      "exploration/Returns Std                                 3.10205\n",
      "exploration/Returns Max                                 3.29693\n",
      "exploration/Returns Min                                -7.2277\n",
      "exploration/Actions Mean                                0.00346605\n",
      "exploration/Actions Std                                 0.531144\n",
      "exploration/Actions Max                                 0.995364\n",
      "exploration/Actions Min                                -0.990604\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                            -0.952197\n",
      "exploration/env_infos/final/reward_forward Mean         0.0883616\n",
      "exploration/env_infos/final/reward_forward Std          0.425777\n",
      "exploration/env_infos/final/reward_forward Max          1.09196\n",
      "exploration/env_infos/final/reward_forward Min         -0.439071\n",
      "exploration/env_infos/initial/reward_forward Mean       0.063635\n",
      "exploration/env_infos/initial/reward_forward Std        0.113031\n",
      "exploration/env_infos/initial/reward_forward Max        0.249032\n",
      "exploration/env_infos/initial/reward_forward Min       -0.118255\n",
      "exploration/env_infos/reward_forward Mean              -0.0881416\n",
      "exploration/env_infos/reward_forward Std                0.667194\n",
      "exploration/env_infos/reward_forward Max                2.37919\n",
      "exploration/env_infos/reward_forward Min               -2.82616\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.09874\n",
      "exploration/env_infos/final/reward_ctrl Std             0.343954\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.522644\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.55996\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.07318\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.433999\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.279553\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.65\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.1285\n",
      "exploration/env_infos/reward_ctrl Std                   0.377221\n",
      "exploration/env_infos/reward_ctrl Max                  -0.133902\n",
      "exploration/env_infos/reward_ctrl Min                  -2.47988\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.65077\n",
      "exploration/env_infos/final/torso_velocity Std          0.822348\n",
      "exploration/env_infos/final/torso_velocity Max          3.06315\n",
      "exploration/env_infos/final/torso_velocity Min         -0.516348\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.203078\n",
      "exploration/env_infos/initial/torso_velocity Std        0.236715\n",
      "exploration/env_infos/initial/torso_velocity Max        0.594817\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.209315\n",
      "exploration/env_infos/torso_velocity Mean              -0.0355714\n",
      "exploration/env_infos/torso_velocity Std                0.649521\n",
      "exploration/env_infos/torso_velocity Max                3.26294\n",
      "exploration/env_infos/torso_velocity Min               -2.82616\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.955986\n",
      "evaluation/Rewards Std                                  0.0253807\n",
      "evaluation/Rewards Max                                  2.43765\n",
      "evaluation/Rewards Min                                  0.721917\n",
      "evaluation/Returns Mean                               955.986\n",
      "evaluation/Returns Std                                  9.93094\n",
      "evaluation/Returns Max                                971.552\n",
      "evaluation/Returns Min                                938.38\n",
      "evaluation/Actions Mean                                -0.00242458\n",
      "evaluation/Actions Std                                  0.106007\n",
      "evaluation/Actions Max                                  0.436962\n",
      "evaluation/Actions Min                                 -0.531794\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            955.986\n",
      "evaluation/env_infos/final/reward_forward Mean          7.12313e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.43334e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.73902e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -9.5483e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0324907\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0921489\n",
      "evaluation/env_infos/initial/reward_forward Max         0.136347\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.193912\n",
      "evaluation/env_infos/reward_forward Mean               -0.00447401\n",
      "evaluation/env_infos/reward_forward Std                 0.0676282\n",
      "evaluation/env_infos/reward_forward Max                 0.873419\n",
      "evaluation/env_infos/reward_forward Min                -1.70711\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0444427\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0103088\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0328478\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0631516\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.055407\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.014216\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0344417\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0968385\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0449732\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0122288\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0234079\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.278083\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.72233e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.91726e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.73902e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.3593e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.133936\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230082\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.626779\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.262438\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00310199\n",
      "evaluation/env_infos/torso_velocity Std                 0.0596079\n",
      "evaluation/env_infos/torso_velocity Max                 1.19611\n",
      "evaluation/env_infos/torso_velocity Min                -2.05693\n",
      "time/data storing (s)                                   0.0174526\n",
      "time/evaluation sampling (s)                           46.7055\n",
      "time/exploration sampling (s)                           1.92456\n",
      "time/logging (s)                                        0.269214\n",
      "time/saving (s)                                         0.0296992\n",
      "time/training (s)                                       4.08792\n",
      "time/epoch (s)                                         53.0343\n",
      "time/total (s)                                        499.577\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:06.234873 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  11000\n",
      "trainer/QF1 Loss                                        0.352693\n",
      "trainer/QF2 Loss                                        0.347967\n",
      "trainer/Policy Loss                                    -6.34144\n",
      "trainer/Q1 Predictions Mean                             3.39027\n",
      "trainer/Q1 Predictions Std                              0.486507\n",
      "trainer/Q1 Predictions Max                              4.89862\n",
      "trainer/Q1 Predictions Min                              1.7528\n",
      "trainer/Q2 Predictions Mean                             3.39143\n",
      "trainer/Q2 Predictions Std                              0.443038\n",
      "trainer/Q2 Predictions Max                              4.87118\n",
      "trainer/Q2 Predictions Min                              2.00957\n",
      "trainer/Q Targets Mean                                  3.4206\n",
      "trainer/Q Targets Std                                   0.771771\n",
      "trainer/Q Targets Max                                   5.53775\n",
      "trainer/Q Targets Min                                  -0.816375\n",
      "trainer/Log Pis Mean                                   -2.65963\n",
      "trainer/Log Pis Std                                     1.58084\n",
      "trainer/Log Pis Max                                     1.47672\n",
      "trainer/Log Pis Min                                    -8.36716\n",
      "trainer/Policy mu Mean                                 -0.00345547\n",
      "trainer/Policy mu Std                                   0.239321\n",
      "trainer/Policy mu Max                                   1.30353\n",
      "trainer/Policy mu Min                                  -1.12753\n",
      "trainer/Policy log std Mean                            -0.853317\n",
      "trainer/Policy log std Std                              0.266814\n",
      "trainer/Policy log std Max                             -0.323792\n",
      "trainer/Policy log std Min                             -1.56753\n",
      "trainer/Alpha                                           0.0695943\n",
      "trainer/Alpha Loss                                    -28.3821\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            71\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           299.883\n",
      "exploration/path length Max                           764\n",
      "exploration/path length Min                            16\n",
      "exploration/Rewards Mean                                0.369081\n",
      "exploration/Rewards Std                                 0.314746\n",
      "exploration/Rewards Max                                 2.02772\n",
      "exploration/Rewards Min                                -0.559494\n",
      "exploration/Returns Mean                               92.2702\n",
      "exploration/Returns Std                               110.139\n",
      "exploration/Returns Max                               280.327\n",
      "exploration/Returns Min                                 3.86671\n",
      "exploration/Actions Mean                               -0.0111593\n",
      "exploration/Actions Std                                 0.411224\n",
      "exploration/Actions Max                                 0.971445\n",
      "exploration/Actions Min                                -0.986502\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                            92.2702\n",
      "exploration/env_infos/final/reward_forward Mean         0.575067\n",
      "exploration/env_infos/final/reward_forward Std          0.513372\n",
      "exploration/env_infos/final/reward_forward Max          1.33167\n",
      "exploration/env_infos/final/reward_forward Min          0.0413152\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0616485\n",
      "exploration/env_infos/initial/reward_forward Std        0.171489\n",
      "exploration/env_infos/initial/reward_forward Max        0.307152\n",
      "exploration/env_infos/initial/reward_forward Min       -0.148834\n",
      "exploration/env_infos/reward_forward Mean              -0.00241924\n",
      "exploration/env_infos/reward_forward Std                0.431256\n",
      "exploration/env_infos/reward_forward Max                1.62958\n",
      "exploration/env_infos/reward_forward Min               -1.39156\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.83662\n",
      "exploration/env_infos/final/reward_ctrl Std             0.211713\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.544257\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.14275\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.911538\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.267806\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.673789\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.34984\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.676919\n",
      "exploration/env_infos/reward_ctrl Std                   0.270696\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0807195\n",
      "exploration/env_infos/reward_ctrl Min                  -1.82361\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.567232\n",
      "exploration/env_infos/final/torso_velocity Std          0.745174\n",
      "exploration/env_infos/final/torso_velocity Max          2.19648\n",
      "exploration/env_infos/final/torso_velocity Min         -0.371205\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.244703\n",
      "exploration/env_infos/initial/torso_velocity Std        0.222901\n",
      "exploration/env_infos/initial/torso_velocity Max        0.601478\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.148834\n",
      "exploration/env_infos/torso_velocity Mean               0.00317154\n",
      "exploration/env_infos/torso_velocity Std                0.450184\n",
      "exploration/env_infos/torso_velocity Max                3.3168\n",
      "exploration/env_infos/torso_velocity Min               -2.15897\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966421\n",
      "evaluation/Rewards Std                                  0.114168\n",
      "evaluation/Rewards Max                                  2.80369\n",
      "evaluation/Rewards Min                                  0.572154\n",
      "evaluation/Returns Mean                               966.421\n",
      "evaluation/Returns Std                                 31.2305\n",
      "evaluation/Returns Max                               1037.48\n",
      "evaluation/Returns Min                                935.651\n",
      "evaluation/Actions Mean                                -0.0516555\n",
      "evaluation/Actions Std                                  0.107625\n",
      "evaluation/Actions Max                                  0.643813\n",
      "evaluation/Actions Min                                 -0.713367\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.421\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0334652\n",
      "evaluation/env_infos/final/reward_forward Std           0.303698\n",
      "evaluation/env_infos/final/reward_forward Max           0.499537\n",
      "evaluation/env_infos/final/reward_forward Min          -0.811316\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00848573\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115545\n",
      "evaluation/env_infos/initial/reward_forward Max         0.154741\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.214416\n",
      "evaluation/env_infos/reward_forward Mean               -0.0169982\n",
      "evaluation/env_infos/reward_forward Std                 0.267676\n",
      "evaluation/env_infos/reward_forward Max                 1.30303\n",
      "evaluation/env_infos/reward_forward Min                -1.81804\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.048341\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0120882\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0175894\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0773558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0369476\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0103918\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0230847\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0734902\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0570061\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0254806\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.013657\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.427846\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0367619\n",
      "evaluation/env_infos/final/torso_velocity Std           0.297403\n",
      "evaluation/env_infos/final/torso_velocity Max           0.717423\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.928458\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135925\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244095\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.610949\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.377476\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0148728\n",
      "evaluation/env_infos/torso_velocity Std                 0.26997\n",
      "evaluation/env_infos/torso_velocity Max                 1.96004\n",
      "evaluation/env_infos/torso_velocity Min                -1.84354\n",
      "time/data storing (s)                                   0.0162019\n",
      "time/evaluation sampling (s)                           47.5281\n",
      "time/exploration sampling (s)                           1.90919\n",
      "time/logging (s)                                        0.299586\n",
      "time/saving (s)                                         0.0352908\n",
      "time/training (s)                                       5.71628\n",
      "time/epoch (s)                                         55.5047\n",
      "time/total (s)                                        555.325\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:34:59.930467 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.215878\n",
      "trainer/QF2 Loss                                        0.192199\n",
      "trainer/Policy Loss                                    -3.78472\n",
      "trainer/Q1 Predictions Mean                             3.62399\n",
      "trainer/Q1 Predictions Std                              0.558928\n",
      "trainer/Q1 Predictions Max                              5.21975\n",
      "trainer/Q1 Predictions Min                              1.77923\n",
      "trainer/Q2 Predictions Mean                             3.63444\n",
      "trainer/Q2 Predictions Std                              0.60472\n",
      "trainer/Q2 Predictions Max                              5.64152\n",
      "trainer/Q2 Predictions Min                              1.51761\n",
      "trainer/Q Targets Mean                                  3.61092\n",
      "trainer/Q Targets Std                                   0.770388\n",
      "trainer/Q Targets Max                                   6.65008\n",
      "trainer/Q Targets Min                                  -1.0034\n",
      "trainer/Log Pis Mean                                    0.39302\n",
      "trainer/Log Pis Std                                     1.91565\n",
      "trainer/Log Pis Max                                     4.61608\n",
      "trainer/Log Pis Min                                    -5.62366\n",
      "trainer/Policy mu Mean                                 -0.0234698\n",
      "trainer/Policy mu Std                                   0.191757\n",
      "trainer/Policy mu Max                                   0.908153\n",
      "trainer/Policy mu Min                                  -1.1868\n",
      "trainer/Policy log std Mean                            -1.35806\n",
      "trainer/Policy log std Std                              0.319369\n",
      "trainer/Policy log std Max                             -0.496717\n",
      "trainer/Policy log std Min                             -2.1242\n",
      "trainer/Alpha                                           0.0556006\n",
      "trainer/Alpha Loss                                    -21.9656\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            72\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.720513\n",
      "exploration/Rewards Std                                 0.251557\n",
      "exploration/Rewards Max                                 2.587\n",
      "exploration/Rewards Min                                -0.136134\n",
      "exploration/Returns Mean                              720.513\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               720.513\n",
      "exploration/Returns Min                               720.513\n",
      "exploration/Actions Mean                               -0.0319534\n",
      "exploration/Actions Std                                 0.283886\n",
      "exploration/Actions Max                                 0.905207\n",
      "exploration/Actions Min                                -0.963321\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           720.513\n",
      "exploration/env_infos/final/reward_forward Mean        -0.471251\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.471251\n",
      "exploration/env_infos/final/reward_forward Min         -0.471251\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0324961\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0324961\n",
      "exploration/env_infos/initial/reward_forward Min        0.0324961\n",
      "exploration/env_infos/reward_forward Mean              -0.00696577\n",
      "exploration/env_infos/reward_forward Std                0.468272\n",
      "exploration/env_infos/reward_forward Max                1.37853\n",
      "exploration/env_infos/reward_forward Min               -1.69239\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.534913\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.534913\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.534913\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.726958\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.726958\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.726958\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.326449\n",
      "exploration/env_infos/reward_ctrl Std                   0.15861\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0457677\n",
      "exploration/env_infos/reward_ctrl Min                  -1.13613\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.328372\n",
      "exploration/env_infos/final/torso_velocity Std          0.121933\n",
      "exploration/env_infos/final/torso_velocity Max         -0.173322\n",
      "exploration/env_infos/final/torso_velocity Min         -0.471251\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.179111\n",
      "exploration/env_infos/initial/torso_velocity Std        0.165761\n",
      "exploration/env_infos/initial/torso_velocity Max        0.410826\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0324961\n",
      "exploration/env_infos/torso_velocity Mean               0.00440372\n",
      "exploration/env_infos/torso_velocity Std                0.464254\n",
      "exploration/env_infos/torso_velocity Max                1.73107\n",
      "exploration/env_infos/torso_velocity Min               -1.69239\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.926768\n",
      "evaluation/Rewards Std                                  0.0704853\n",
      "evaluation/Rewards Max                                  2.3597\n",
      "evaluation/Rewards Min                                  0.778333\n",
      "evaluation/Returns Mean                               926.768\n",
      "evaluation/Returns Std                                 26.1957\n",
      "evaluation/Returns Max                                995.91\n",
      "evaluation/Returns Min                                901.219\n",
      "evaluation/Actions Mean                                -0.0600359\n",
      "evaluation/Actions Std                                  0.135648\n",
      "evaluation/Actions Max                                  0.511157\n",
      "evaluation/Actions Min                                 -0.603202\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            926.768\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0215659\n",
      "evaluation/env_infos/final/reward_forward Std           0.0952522\n",
      "evaluation/env_infos/final/reward_forward Max           0.225078\n",
      "evaluation/env_infos/final/reward_forward Min          -0.180616\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0616924\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127194\n",
      "evaluation/env_infos/initial/reward_forward Max         0.161244\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275544\n",
      "evaluation/env_infos/reward_forward Mean               -0.00268484\n",
      "evaluation/env_infos/reward_forward Std                 0.195863\n",
      "evaluation/env_infos/reward_forward Max                 1.31254\n",
      "evaluation/env_infos/reward_forward Min                -1.2294\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0927063\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0169106\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0539719\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.119484\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0502014\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0142895\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.028386\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.080296\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0880183\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0176444\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0189782\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.221667\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00600181\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0658692\n",
      "evaluation/env_infos/final/torso_velocity Max           0.225078\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.180616\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127336\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.255053\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.696897\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.275544\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000631481\n",
      "evaluation/env_infos/torso_velocity Std                 0.138266\n",
      "evaluation/env_infos/torso_velocity Max                 1.52596\n",
      "evaluation/env_infos/torso_velocity Min                -2.0674\n",
      "time/data storing (s)                                   0.0145912\n",
      "time/evaluation sampling (s)                           47.2138\n",
      "time/exploration sampling (s)                           1.90523\n",
      "time/logging (s)                                        0.271436\n",
      "time/saving (s)                                         0.0272231\n",
      "time/training (s)                                       3.9795\n",
      "time/epoch (s)                                         53.4118\n",
      "time/total (s)                                        608.992\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:35:54.543593 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  13000\n",
      "trainer/QF1 Loss                                        0.243459\n",
      "trainer/QF2 Loss                                        0.237247\n",
      "trainer/Policy Loss                                    -2.57451\n",
      "trainer/Q1 Predictions Mean                             3.6087\n",
      "trainer/Q1 Predictions Std                              0.581998\n",
      "trainer/Q1 Predictions Max                              5.07759\n",
      "trainer/Q1 Predictions Min                              2.33904\n",
      "trainer/Q2 Predictions Mean                             3.6397\n",
      "trainer/Q2 Predictions Std                              0.598441\n",
      "trainer/Q2 Predictions Max                              5.19104\n",
      "trainer/Q2 Predictions Min                              2.43091\n",
      "trainer/Q Targets Mean                                  3.67773\n",
      "trainer/Q Targets Std                                   0.763095\n",
      "trainer/Q Targets Max                                   7.20911\n",
      "trainer/Q Targets Min                                  -0.426959\n",
      "trainer/Log Pis Mean                                    1.84695\n",
      "trainer/Log Pis Std                                     2.15464\n",
      "trainer/Log Pis Max                                     5.49473\n",
      "trainer/Log Pis Min                                    -6.75053\n",
      "trainer/Policy mu Mean                                  0.0050273\n",
      "trainer/Policy mu Std                                   0.163882\n",
      "trainer/Policy mu Max                                   1.01827\n",
      "trainer/Policy mu Min                                  -0.793157\n",
      "trainer/Policy log std Mean                            -1.56881\n",
      "trainer/Policy log std Std                              0.259378\n",
      "trainer/Policy log std Max                             -0.599536\n",
      "trainer/Policy log std Min                             -2.07753\n",
      "trainer/Alpha                                           0.0468529\n",
      "trainer/Alpha Loss                                    -18.8233\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            73\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.862512\n",
      "exploration/Rewards Std                                 0.249739\n",
      "exploration/Rewards Max                                 2.30107\n",
      "exploration/Rewards Min                                 0.146662\n",
      "exploration/Returns Mean                              862.512\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               862.512\n",
      "exploration/Returns Min                               862.512\n",
      "exploration/Actions Mean                                0.00500763\n",
      "exploration/Actions Std                                 0.228138\n",
      "exploration/Actions Max                                 0.885198\n",
      "exploration/Actions Min                                -0.901981\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           862.512\n",
      "exploration/env_infos/final/reward_forward Mean        -0.180787\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.180787\n",
      "exploration/env_infos/final/reward_forward Min         -0.180787\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00749848\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.00749848\n",
      "exploration/env_infos/initial/reward_forward Min        0.00749848\n",
      "exploration/env_infos/reward_forward Mean               0.0520703\n",
      "exploration/env_infos/reward_forward Std                0.45317\n",
      "exploration/env_infos/reward_forward Max                2.07712\n",
      "exploration/env_infos/reward_forward Min               -1.21278\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.287324\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.287324\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.287324\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.163129\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.163129\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.163129\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.208288\n",
      "exploration/env_infos/reward_ctrl Std                   0.114275\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0282673\n",
      "exploration/env_infos/reward_ctrl Min                  -0.853338\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0311867\n",
      "exploration/env_infos/final/torso_velocity Std          0.225028\n",
      "exploration/env_infos/final/torso_velocity Max          0.342738\n",
      "exploration/env_infos/final/torso_velocity Min         -0.180787\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.149277\n",
      "exploration/env_infos/initial/torso_velocity Std        0.272194\n",
      "exploration/env_infos/initial/torso_velocity Max        0.530099\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0897657\n",
      "exploration/env_infos/torso_velocity Mean               0.00280963\n",
      "exploration/env_infos/torso_velocity Std                0.475657\n",
      "exploration/env_infos/torso_velocity Max                2.10983\n",
      "exploration/env_infos/torso_velocity Min               -1.62837\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.986195\n",
      "evaluation/Rewards Std                                  0.0258546\n",
      "evaluation/Rewards Max                                  2.28232\n",
      "evaluation/Rewards Min                                  0.655436\n",
      "evaluation/Returns Mean                               986.195\n",
      "evaluation/Returns Std                                  3.72361\n",
      "evaluation/Returns Max                               1000.34\n",
      "evaluation/Returns Min                                980.364\n",
      "evaluation/Actions Mean                                 0.0260267\n",
      "evaluation/Actions Std                                  0.0557579\n",
      "evaluation/Actions Max                                  0.48598\n",
      "evaluation/Actions Min                                 -0.557848\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            986.195\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00115915\n",
      "evaluation/env_infos/final/reward_forward Std           0.00329866\n",
      "evaluation/env_infos/final/reward_forward Max           0.000102558\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0127725\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0507181\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124131\n",
      "evaluation/env_infos/initial/reward_forward Max         0.32988\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.139938\n",
      "evaluation/env_infos/reward_forward Mean                0.00209979\n",
      "evaluation/env_infos/reward_forward Std                 0.0667948\n",
      "evaluation/env_infos/reward_forward Max                 1.67952\n",
      "evaluation/env_infos/reward_forward Min                -0.953756\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0146682\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00329014\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0074879\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0259166\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0287018\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0102685\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0104413\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0480664\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0151453\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00774719\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00159981\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.344564\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000289378\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00479232\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0336208\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0127725\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.155094\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228972\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.591313\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.364614\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00161576\n",
      "evaluation/env_infos/torso_velocity Std                 0.0678817\n",
      "evaluation/env_infos/torso_velocity Max                 1.67952\n",
      "evaluation/env_infos/torso_velocity Min                -2.09854\n",
      "time/data storing (s)                                   0.0148379\n",
      "time/evaluation sampling (s)                           48.1294\n",
      "time/exploration sampling (s)                           1.99618\n",
      "time/logging (s)                                        0.273341\n",
      "time/saving (s)                                         0.0270561\n",
      "time/training (s)                                       3.926\n",
      "time/epoch (s)                                         54.3668\n",
      "time/total (s)                                        663.607\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 10:36:47.687325 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.300468\n",
      "trainer/QF2 Loss                                        0.296651\n",
      "trainer/Policy Loss                                    -1.52853\n",
      "trainer/Q1 Predictions Mean                             4.11503\n",
      "trainer/Q1 Predictions Std                              0.73327\n",
      "trainer/Q1 Predictions Max                              6.66688\n",
      "trainer/Q1 Predictions Min                              2.37793\n",
      "trainer/Q2 Predictions Mean                             4.0977\n",
      "trainer/Q2 Predictions Std                              0.736257\n",
      "trainer/Q2 Predictions Max                              6.75353\n",
      "trainer/Q2 Predictions Min                              2.56236\n",
      "trainer/Q Targets Mean                                  4.0605\n",
      "trainer/Q Targets Std                                   0.954234\n",
      "trainer/Q Targets Max                                   8.24084\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                    3.36817\n",
      "trainer/Log Pis Std                                     2.22502\n",
      "trainer/Log Pis Max                                     7.39966\n",
      "trainer/Log Pis Min                                    -7.47284\n",
      "trainer/Policy mu Mean                                 -0.0220242\n",
      "trainer/Policy mu Std                                   0.134838\n",
      "trainer/Policy mu Max                                   0.941569\n",
      "trainer/Policy mu Min                                  -0.769819\n",
      "trainer/Policy log std Mean                            -1.82436\n",
      "trainer/Policy log std Std                              0.246232\n",
      "trainer/Policy log std Max                             -0.86002\n",
      "trainer/Policy log std Min                             -2.42762\n",
      "trainer/Alpha                                           0.0409187\n",
      "trainer/Alpha Loss                                    -14.7984\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            74\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.914988\n",
      "exploration/Rewards Std                                 0.151693\n",
      "exploration/Rewards Max                                 1.95695\n",
      "exploration/Rewards Min                                 0.552427\n",
      "exploration/Returns Mean                              914.988\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               914.988\n",
      "exploration/Returns Min                               914.988\n",
      "exploration/Actions Mean                               -0.0142025\n",
      "exploration/Actions Std                                 0.173921\n",
      "exploration/Actions Max                                 0.655557\n",
      "exploration/Actions Min                                -0.747532\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           914.988\n",
      "exploration/env_infos/final/reward_forward Mean        -0.219848\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.219848\n",
      "exploration/env_infos/final/reward_forward Min         -0.219848\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0483947\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0483947\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0483947\n",
      "exploration/env_infos/reward_forward Mean               0.0203978\n",
      "exploration/env_infos/reward_forward Std                0.377764\n",
      "exploration/env_infos/reward_forward Max                1.61244\n",
      "exploration/env_infos/reward_forward Min               -0.946183\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.180902\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.180902\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.180902\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.132742\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.132742\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.132742\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.1218\n",
      "exploration/env_infos/reward_ctrl Std                   0.0648566\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0135593\n",
      "exploration/env_infos/reward_ctrl Min                  -0.447573\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0301109\n",
      "exploration/env_infos/final/torso_velocity Std          0.224866\n",
      "exploration/env_infos/final/torso_velocity Max          0.325348\n",
      "exploration/env_infos/final/torso_velocity Min         -0.219848\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164383\n",
      "exploration/env_infos/initial/torso_velocity Std        0.226641\n",
      "exploration/env_infos/initial/torso_velocity Max        0.478361\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0483947\n",
      "exploration/env_infos/torso_velocity Mean               0.00824177\n",
      "exploration/env_infos/torso_velocity Std                0.382253\n",
      "exploration/env_infos/torso_velocity Max                2.0507\n",
      "exploration/env_infos/torso_velocity Min               -1.25689\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985334\n",
      "evaluation/Rewards Std                                  0.0213404\n",
      "evaluation/Rewards Max                                  2.2779\n",
      "evaluation/Rewards Min                                  0.815548\n",
      "evaluation/Returns Mean                               985.334\n",
      "evaluation/Returns Std                                  2.59865\n",
      "evaluation/Returns Max                                990.052\n",
      "evaluation/Returns Min                                980.815\n",
      "evaluation/Actions Mean                                -0.0328888\n",
      "evaluation/Actions Std                                  0.052962\n",
      "evaluation/Actions Max                                  0.239739\n",
      "evaluation/Actions Min                                 -0.465661\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.334\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000110973\n",
      "evaluation/env_infos/final/reward_forward Std           0.000543651\n",
      "evaluation/env_infos/final/reward_forward Max           0.00277431\n",
      "evaluation/env_infos/final/reward_forward Min          -4.10622e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00719238\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122251\n",
      "evaluation/env_infos/initial/reward_forward Max         0.259278\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.197411\n",
      "evaluation/env_infos/reward_forward Mean                0.00157888\n",
      "evaluation/env_infos/reward_forward Std                 0.0580156\n",
      "evaluation/env_infos/reward_forward Max                 1.61537\n",
      "evaluation/env_infos/reward_forward Min                -1.16446\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0152543\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00236734\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0118033\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0196578\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0213592\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00289406\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0158125\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.026607\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0155466\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00552782\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0027446\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.184452\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.10406e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000339731\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00277431\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000571466\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.131038\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225908\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.64102\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.231146\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00159961\n",
      "evaluation/env_infos/torso_velocity Std                 0.0557915\n",
      "evaluation/env_infos/torso_velocity Max                 1.61537\n",
      "evaluation/env_infos/torso_velocity Min                -1.81442\n",
      "time/data storing (s)                                   0.0167512\n",
      "time/evaluation sampling (s)                           45.3756\n",
      "time/exploration sampling (s)                           2.2438\n",
      "time/logging (s)                                        0.322074\n",
      "time/saving (s)                                         0.0287124\n",
      "time/training (s)                                       4.93237\n",
      "time/epoch (s)                                         52.9193\n",
      "time/total (s)                                        716.799\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:37:42.277171 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  15000\n",
      "trainer/QF1 Loss                                        0.185209\n",
      "trainer/QF2 Loss                                        0.181179\n",
      "trainer/Policy Loss                                    -1.03071\n",
      "trainer/Q1 Predictions Mean                             4.45719\n",
      "trainer/Q1 Predictions Std                              0.725891\n",
      "trainer/Q1 Predictions Max                              6.18492\n",
      "trainer/Q1 Predictions Min                              1.75962\n",
      "trainer/Q2 Predictions Mean                             4.43984\n",
      "trainer/Q2 Predictions Std                              0.731336\n",
      "trainer/Q2 Predictions Max                              6.00977\n",
      "trainer/Q2 Predictions Min                              1.81452\n",
      "trainer/Q Targets Mean                                  4.4691\n",
      "trainer/Q Targets Std                                   0.803048\n",
      "trainer/Q Targets Max                                   6.48117\n",
      "trainer/Q Targets Min                                  -0.858889\n",
      "trainer/Log Pis Mean                                    4.26323\n",
      "trainer/Log Pis Std                                     1.9543\n",
      "trainer/Log Pis Max                                     8.23921\n",
      "trainer/Log Pis Min                                    -2.79993\n",
      "trainer/Policy mu Mean                                  0.0198238\n",
      "trainer/Policy mu Std                                   0.130431\n",
      "trainer/Policy mu Max                                   1.12759\n",
      "trainer/Policy mu Min                                  -0.615862\n",
      "trainer/Policy log std Mean                            -1.90505\n",
      "trainer/Policy log std Std                              0.216125\n",
      "trainer/Policy log std Max                             -0.990436\n",
      "trainer/Policy log std Min                             -2.3052\n",
      "trainer/Alpha                                           0.0366923\n",
      "trainer/Alpha Loss                                    -12.3469\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            75\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.922072\n",
      "exploration/Rewards Std                                 0.110292\n",
      "exploration/Rewards Max                                 1.8072\n",
      "exploration/Rewards Min                                 0.643184\n",
      "exploration/Returns Mean                              922.072\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               922.072\n",
      "exploration/Returns Min                               922.072\n",
      "exploration/Actions Mean                                0.00573235\n",
      "exploration/Actions Std                                 0.156641\n",
      "exploration/Actions Max                                 0.687473\n",
      "exploration/Actions Min                                -0.589236\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           922.072\n",
      "exploration/env_infos/final/reward_forward Mean        -0.607085\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.607085\n",
      "exploration/env_infos/final/reward_forward Min         -0.607085\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0302671\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0302671\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0302671\n",
      "exploration/env_infos/reward_forward Mean               0.0611408\n",
      "exploration/env_infos/reward_forward Std                0.391528\n",
      "exploration/env_infos/reward_forward Max                1.36513\n",
      "exploration/env_infos/reward_forward Min               -0.959643\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.126824\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.126824\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.126824\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.22027\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.22027\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.22027\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0982765\n",
      "exploration/env_infos/reward_ctrl Std                   0.0526541\n",
      "exploration/env_infos/reward_ctrl Max                  -0.009272\n",
      "exploration/env_infos/reward_ctrl Min                  -0.356816\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.178295\n",
      "exploration/env_infos/final/torso_velocity Std          0.341166\n",
      "exploration/env_infos/final/torso_velocity Max          0.227661\n",
      "exploration/env_infos/final/torso_velocity Min         -0.607085\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.1393\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201578\n",
      "exploration/env_infos/initial/torso_velocity Max        0.422542\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0302671\n",
      "exploration/env_infos/torso_velocity Mean               0.0147313\n",
      "exploration/env_infos/torso_velocity Std                0.364402\n",
      "exploration/env_infos/torso_velocity Max                1.36513\n",
      "exploration/env_infos/torso_velocity Min               -1.3043\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.989537\n",
      "evaluation/Rewards Std                                  0.0245172\n",
      "evaluation/Rewards Max                                  2.55377\n",
      "evaluation/Rewards Min                                  0.872554\n",
      "evaluation/Returns Mean                               989.537\n",
      "evaluation/Returns Std                                  2.95296\n",
      "evaluation/Returns Max                                995.458\n",
      "evaluation/Returns Min                                982.806\n",
      "evaluation/Actions Mean                                 0.017543\n",
      "evaluation/Actions Std                                  0.0507965\n",
      "evaluation/Actions Max                                  0.301704\n",
      "evaluation/Actions Min                                 -0.420771\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            989.537\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0158729\n",
      "evaluation/env_infos/final/reward_forward Std           0.15556\n",
      "evaluation/env_infos/final/reward_forward Max           0.649281\n",
      "evaluation/env_infos/final/reward_forward Min          -0.407157\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00407104\n",
      "evaluation/env_infos/initial/reward_forward Std         0.135322\n",
      "evaluation/env_infos/initial/reward_forward Max         0.248514\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.258861\n",
      "evaluation/env_infos/reward_forward Mean                0.0273263\n",
      "evaluation/env_infos/reward_forward Std                 0.165314\n",
      "evaluation/env_infos/reward_forward Max                 1.52137\n",
      "evaluation/env_infos/reward_forward Min                -1.61311\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0109538\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00295752\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00822658\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0237274\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0191531\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00666971\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0101994\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0321908\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0115521\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0049176\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00330546\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.127446\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00552744\n",
      "evaluation/env_infos/final/torso_velocity Std           0.126031\n",
      "evaluation/env_infos/final/torso_velocity Max           0.649281\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.449693\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129023\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234818\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.64937\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.258861\n",
      "evaluation/env_infos/torso_velocity Mean                0.00761219\n",
      "evaluation/env_infos/torso_velocity Std                 0.135618\n",
      "evaluation/env_infos/torso_velocity Max                 1.52137\n",
      "evaluation/env_infos/torso_velocity Min                -1.8944\n",
      "time/data storing (s)                                   0.0154787\n",
      "time/evaluation sampling (s)                           47.9986\n",
      "time/exploration sampling (s)                           1.9326\n",
      "time/logging (s)                                        0.281245\n",
      "time/saving (s)                                         0.0275115\n",
      "time/training (s)                                       3.97222\n",
      "time/epoch (s)                                         54.2277\n",
      "time/total (s)                                        771.347\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:38:35.702259 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.130802\n",
      "trainer/QF2 Loss                                        0.124571\n",
      "trainer/Policy Loss                                    -0.838889\n",
      "trainer/Q1 Predictions Mean                             4.8411\n",
      "trainer/Q1 Predictions Std                              0.782695\n",
      "trainer/Q1 Predictions Max                              6.33688\n",
      "trainer/Q1 Predictions Min                              2.8736\n",
      "trainer/Q2 Predictions Mean                             4.91726\n",
      "trainer/Q2 Predictions Std                              0.801821\n",
      "trainer/Q2 Predictions Max                              6.54669\n",
      "trainer/Q2 Predictions Min                              2.93423\n",
      "trainer/Q Targets Mean                                  4.8445\n",
      "trainer/Q Targets Std                                   0.80284\n",
      "trainer/Q Targets Max                                   6.65752\n",
      "trainer/Q Targets Min                                   2.85324\n",
      "trainer/Log Pis Mean                                    4.8208\n",
      "trainer/Log Pis Std                                     2.30192\n",
      "trainer/Log Pis Max                                     8.84233\n",
      "trainer/Log Pis Min                                    -3.36251\n",
      "trainer/Policy mu Mean                                  0.00101184\n",
      "trainer/Policy mu Std                                   0.112643\n",
      "trainer/Policy mu Max                                   0.67307\n",
      "trainer/Policy mu Min                                  -0.54107\n",
      "trainer/Policy log std Mean                            -2.00882\n",
      "trainer/Policy log std Std                              0.190214\n",
      "trainer/Policy log std Max                             -1.07875\n",
      "trainer/Policy log std Min                             -2.45207\n",
      "trainer/Alpha                                           0.0334032\n",
      "trainer/Alpha Loss                                    -10.8036\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            76\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.975519\n",
      "exploration/Rewards Std                                 0.189857\n",
      "exploration/Rewards Max                                 2.43754\n",
      "exploration/Rewards Min                                 0.707243\n",
      "exploration/Returns Mean                              975.519\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               975.519\n",
      "exploration/Returns Min                               975.519\n",
      "exploration/Actions Mean                                0.013375\n",
      "exploration/Actions Std                                 0.14183\n",
      "exploration/Actions Max                                 0.536046\n",
      "exploration/Actions Min                                -0.529589\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           975.519\n",
      "exploration/env_infos/final/reward_forward Mean        -0.237179\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.237179\n",
      "exploration/env_infos/final/reward_forward Min         -0.237179\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.1217\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.1217\n",
      "exploration/env_infos/initial/reward_forward Min       -0.1217\n",
      "exploration/env_infos/reward_forward Mean               0.0347399\n",
      "exploration/env_infos/reward_forward Std                0.273248\n",
      "exploration/env_infos/reward_forward Max                1.39564\n",
      "exploration/env_infos/reward_forward Min               -1.19432\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.125972\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.125972\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.125972\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0511505\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0511505\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0511505\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0811781\n",
      "exploration/env_infos/reward_ctrl Std                   0.0427342\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00873468\n",
      "exploration/env_infos/reward_ctrl Min                  -0.292757\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.268492\n",
      "exploration/env_infos/final/torso_velocity Std          0.422998\n",
      "exploration/env_infos/final/torso_velocity Max          0.233205\n",
      "exploration/env_infos/final/torso_velocity Min         -0.801503\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.188337\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27152\n",
      "exploration/env_infos/initial/torso_velocity Max        0.53955\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.1217\n",
      "exploration/env_infos/torso_velocity Mean              -0.00095164\n",
      "exploration/env_infos/torso_velocity Std                0.330352\n",
      "exploration/env_infos/torso_velocity Max                1.39564\n",
      "exploration/env_infos/torso_velocity Min               -1.36185\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985021\n",
      "evaluation/Rewards Std                                  0.0338144\n",
      "evaluation/Rewards Max                                  2.74863\n",
      "evaluation/Rewards Min                                  0.812136\n",
      "evaluation/Returns Mean                               985.021\n",
      "evaluation/Returns Std                                  2.79721\n",
      "evaluation/Returns Max                                991.589\n",
      "evaluation/Returns Min                                976.449\n",
      "evaluation/Actions Mean                                 0.0117721\n",
      "evaluation/Actions Std                                  0.0630146\n",
      "evaluation/Actions Max                                  0.330055\n",
      "evaluation/Actions Min                                 -0.497803\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.021\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.9418e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.45403e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.67803e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.13928e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0349362\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0897274\n",
      "evaluation/env_infos/initial/reward_forward Max         0.191505\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.120284\n",
      "evaluation/env_infos/reward_forward Mean                0.00330436\n",
      "evaluation/env_infos/reward_forward Std                 0.0522433\n",
      "evaluation/env_infos/reward_forward Max                 1.77491\n",
      "evaluation/env_infos/reward_forward Min                -1.28353\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0159693\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00159391\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.014685\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0230681\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0134189\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00298189\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00837414\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0208537\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0164377\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00586036\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00771773\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.187864\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.16863e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.56356e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.77391e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.13928e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149566\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.240436\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614612\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.257483\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00158827\n",
      "evaluation/env_infos/torso_velocity Std                 0.0568573\n",
      "evaluation/env_infos/torso_velocity Max                 1.77491\n",
      "evaluation/env_infos/torso_velocity Min                -1.85948\n",
      "time/data storing (s)                                   0.0150403\n",
      "time/evaluation sampling (s)                           46.684\n",
      "time/exploration sampling (s)                           1.96855\n",
      "time/logging (s)                                        0.280103\n",
      "time/saving (s)                                         0.0263092\n",
      "time/training (s)                                       4.17045\n",
      "time/epoch (s)                                         53.1444\n",
      "time/total (s)                                        824.77\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:39:30.380389 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  17000\n",
      "trainer/QF1 Loss                                        0.166272\n",
      "trainer/QF2 Loss                                        0.159115\n",
      "trainer/Policy Loss                                    -0.784114\n",
      "trainer/Q1 Predictions Mean                             5.40836\n",
      "trainer/Q1 Predictions Std                              0.806916\n",
      "trainer/Q1 Predictions Max                              7.18874\n",
      "trainer/Q1 Predictions Min                              3.26447\n",
      "trainer/Q2 Predictions Mean                             5.18308\n",
      "trainer/Q2 Predictions Std                              0.799921\n",
      "trainer/Q2 Predictions Max                              6.94519\n",
      "trainer/Q2 Predictions Min                              3.10781\n",
      "trainer/Q Targets Mean                                  5.31786\n",
      "trainer/Q Targets Std                                   0.796345\n",
      "trainer/Q Targets Max                                   7.2086\n",
      "trainer/Q Targets Min                                   3.47602\n",
      "trainer/Log Pis Mean                                    5.20714\n",
      "trainer/Log Pis Std                                     2.21705\n",
      "trainer/Log Pis Max                                    10.1588\n",
      "trainer/Log Pis Min                                    -6.52256\n",
      "trainer/Policy mu Mean                                 -0.00785098\n",
      "trainer/Policy mu Std                                   0.106586\n",
      "trainer/Policy mu Max                                   0.616284\n",
      "trainer/Policy mu Min                                  -0.466132\n",
      "trainer/Policy log std Mean                            -2.02627\n",
      "trainer/Policy log std Std                              0.19584\n",
      "trainer/Policy log std Max                             -1.20825\n",
      "trainer/Policy log std Min                             -2.50255\n",
      "trainer/Alpha                                           0.0307896\n",
      "trainer/Alpha Loss                                     -9.71851\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            77\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.94589\n",
      "exploration/Rewards Std                                 0.133285\n",
      "exploration/Rewards Max                                 2.01303\n",
      "exploration/Rewards Min                                 0.730004\n",
      "exploration/Returns Mean                              945.89\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               945.89\n",
      "exploration/Returns Min                               945.89\n",
      "exploration/Actions Mean                                0.0143724\n",
      "exploration/Actions Std                                 0.143755\n",
      "exploration/Actions Max                                 0.524758\n",
      "exploration/Actions Min                                -0.574727\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           945.89\n",
      "exploration/env_infos/final/reward_forward Mean         0.433827\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.433827\n",
      "exploration/env_infos/final/reward_forward Min          0.433827\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0123029\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0123029\n",
      "exploration/env_infos/initial/reward_forward Min        0.0123029\n",
      "exploration/env_infos/reward_forward Mean               0.0473147\n",
      "exploration/env_infos/reward_forward Std                0.297906\n",
      "exploration/env_infos/reward_forward Max                0.948126\n",
      "exploration/env_infos/reward_forward Min               -0.955959\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.162745\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.162745\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.162745\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.125286\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.125286\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.125286\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0834886\n",
      "exploration/env_infos/reward_ctrl Std                   0.0421364\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00706801\n",
      "exploration/env_infos/reward_ctrl Min                  -0.269996\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.199796\n",
      "exploration/env_infos/final/torso_velocity Std          0.215118\n",
      "exploration/env_infos/final/torso_velocity Max          0.433827\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0855508\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.16628\n",
      "exploration/env_infos/initial/torso_velocity Std        0.266268\n",
      "exploration/env_infos/initial/torso_velocity Max        0.54087\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0543327\n",
      "exploration/env_infos/torso_velocity Mean               0.0101604\n",
      "exploration/env_infos/torso_velocity Std                0.300115\n",
      "exploration/env_infos/torso_velocity Max                1.51939\n",
      "exploration/env_infos/torso_velocity Min               -1.28003\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 1.00335\n",
      "evaluation/Rewards Std                                  0.105122\n",
      "evaluation/Rewards Max                                  2.19228\n",
      "evaluation/Rewards Min                                  0.884684\n",
      "evaluation/Returns Mean                              1003.35\n",
      "evaluation/Returns Std                                 33.2514\n",
      "evaluation/Returns Max                               1102.02\n",
      "evaluation/Returns Min                                980.52\n",
      "evaluation/Actions Mean                                -0.00255784\n",
      "evaluation/Actions Std                                  0.0597744\n",
      "evaluation/Actions Max                                  0.254072\n",
      "evaluation/Actions Min                                 -0.32197\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                           1003.35\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0217765\n",
      "evaluation/env_infos/final/reward_forward Std           0.104966\n",
      "evaluation/env_infos/final/reward_forward Max           0.466617\n",
      "evaluation/env_infos/final/reward_forward Min          -0.15279\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0466872\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139723\n",
      "evaluation/env_infos/initial/reward_forward Max         0.215661\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.475205\n",
      "evaluation/env_infos/reward_forward Mean                0.00133779\n",
      "evaluation/env_infos/reward_forward Std                 0.146115\n",
      "evaluation/env_infos/reward_forward Max                 1.73949\n",
      "evaluation/env_infos/reward_forward Min                -1.75147\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.01411\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00565973\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00532364\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0204546\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0186489\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00347631\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0132599\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.027103\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0143181\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00583307\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00220264\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.115316\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00991665\n",
      "evaluation/env_infos/final/torso_velocity Std           0.06405\n",
      "evaluation/env_infos/final/torso_velocity Max           0.466617\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.15279\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.124212\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.276162\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.738219\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.475205\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00311669\n",
      "evaluation/env_infos/torso_velocity Std                 0.130445\n",
      "evaluation/env_infos/torso_velocity Max                 1.73949\n",
      "evaluation/env_infos/torso_velocity Min                -1.75147\n",
      "time/data storing (s)                                   0.0161344\n",
      "time/evaluation sampling (s)                           47.5203\n",
      "time/exploration sampling (s)                           1.96075\n",
      "time/logging (s)                                        0.306688\n",
      "time/saving (s)                                         0.0295072\n",
      "time/training (s)                                       4.57248\n",
      "time/epoch (s)                                         54.4058\n",
      "time/total (s)                                        879.475\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:40:23.478867 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.203061\n",
      "trainer/QF2 Loss                                        0.21971\n",
      "trainer/Policy Loss                                    -0.0428114\n",
      "trainer/Q1 Predictions Mean                             5.59579\n",
      "trainer/Q1 Predictions Std                              0.824631\n",
      "trainer/Q1 Predictions Max                              7.64841\n",
      "trainer/Q1 Predictions Min                              3.43299\n",
      "trainer/Q2 Predictions Mean                             5.464\n",
      "trainer/Q2 Predictions Std                              0.810981\n",
      "trainer/Q2 Predictions Max                              7.41471\n",
      "trainer/Q2 Predictions Min                              3.41308\n",
      "trainer/Q Targets Mean                                  5.60681\n",
      "trainer/Q Targets Std                                   0.884628\n",
      "trainer/Q Targets Max                                   8.45715\n",
      "trainer/Q Targets Min                                   3.35453\n",
      "trainer/Log Pis Mean                                    6.29429\n",
      "trainer/Log Pis Std                                     2.23111\n",
      "trainer/Log Pis Max                                    10.2357\n",
      "trainer/Log Pis Min                                    -1.93423\n",
      "trainer/Policy mu Mean                                  0.00906414\n",
      "trainer/Policy mu Std                                   0.108144\n",
      "trainer/Policy mu Max                                   0.728702\n",
      "trainer/Policy mu Min                                  -0.571952\n",
      "trainer/Policy log std Mean                            -2.17361\n",
      "trainer/Policy log std Std                              0.166968\n",
      "trainer/Policy log std Max                             -1.55846\n",
      "trainer/Policy log std Min                             -2.68074\n",
      "trainer/Alpha                                           0.0285444\n",
      "trainer/Alpha Loss                                     -6.06491\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            78\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.958203\n",
      "exploration/Rewards Std                                 0.0801067\n",
      "exploration/Rewards Max                                 1.57134\n",
      "exploration/Rewards Min                                 0.799324\n",
      "exploration/Returns Mean                              958.203\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               958.203\n",
      "exploration/Returns Min                               958.203\n",
      "exploration/Actions Mean                                0.0150478\n",
      "exploration/Actions Std                                 0.119548\n",
      "exploration/Actions Max                                 0.487837\n",
      "exploration/Actions Min                                -0.413996\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           958.203\n",
      "exploration/env_infos/final/reward_forward Mean        -0.170263\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.170263\n",
      "exploration/env_infos/final/reward_forward Min         -0.170263\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0618226\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0618226\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0618226\n",
      "exploration/env_infos/reward_forward Mean               0.0606105\n",
      "exploration/env_infos/reward_forward Std                0.265782\n",
      "exploration/env_infos/reward_forward Max                1.35451\n",
      "exploration/env_infos/reward_forward Min               -0.748303\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0701259\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0701259\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0701259\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0530804\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0530804\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0530804\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0580725\n",
      "exploration/env_infos/reward_ctrl Std                   0.0297025\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00597254\n",
      "exploration/env_infos/reward_ctrl Min                  -0.200676\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.062547\n",
      "exploration/env_infos/final/torso_velocity Std          0.1813\n",
      "exploration/env_infos/final/torso_velocity Max          0.27198\n",
      "exploration/env_infos/final/torso_velocity Min         -0.170263\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0877003\n",
      "exploration/env_infos/initial/torso_velocity Std        0.270387\n",
      "exploration/env_infos/initial/torso_velocity Max        0.467251\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.142327\n",
      "exploration/env_infos/torso_velocity Mean               0.0161001\n",
      "exploration/env_infos/torso_velocity Std                0.247573\n",
      "exploration/env_infos/torso_velocity Max                1.37052\n",
      "exploration/env_infos/torso_velocity Min               -1.0082\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.985597\n",
      "evaluation/Rewards Std                                  0.0239132\n",
      "evaluation/Rewards Max                                  2.04386\n",
      "evaluation/Rewards Min                                  0.890213\n",
      "evaluation/Returns Mean                               985.597\n",
      "evaluation/Returns Std                                  6.65612\n",
      "evaluation/Returns Max                                996.633\n",
      "evaluation/Returns Min                                973.689\n",
      "evaluation/Actions Mean                                 0.0201553\n",
      "evaluation/Actions Std                                  0.0585781\n",
      "evaluation/Actions Max                                  0.324531\n",
      "evaluation/Actions Min                                 -0.351556\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            985.597\n",
      "evaluation/env_infos/final/reward_forward Mean         -9.56659e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.14753e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.17818e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.95387e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.015608\n",
      "evaluation/env_infos/initial/reward_forward Std         0.150774\n",
      "evaluation/env_infos/initial/reward_forward Max         0.21885\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.32919\n",
      "evaluation/env_infos/reward_forward Mean                0.00312828\n",
      "evaluation/env_infos/reward_forward Std                 0.058878\n",
      "evaluation/env_infos/reward_forward Max                 1.58114\n",
      "evaluation/env_infos/reward_forward Min                -1.3777\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0152344\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00669554\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00832847\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0266806\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.018612\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00502775\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00929848\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0285895\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0153505\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0073345\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0025484\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.109787\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.23547e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.77228e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.57651e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.27468e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14251\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.256094\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.691193\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.32919\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00025387\n",
      "evaluation/env_infos/torso_velocity Std                 0.0588284\n",
      "evaluation/env_infos/torso_velocity Max                 1.58114\n",
      "evaluation/env_infos/torso_velocity Min                -1.70171\n",
      "time/data storing (s)                                   0.0153778\n",
      "time/evaluation sampling (s)                           46.3802\n",
      "time/exploration sampling (s)                           1.99638\n",
      "time/logging (s)                                        0.278857\n",
      "time/saving (s)                                         0.0263206\n",
      "time/training (s)                                       4.04341\n",
      "time/epoch (s)                                         52.7406\n",
      "time/total (s)                                        932.544\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:41:18.229556 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  19000\n",
      "trainer/QF1 Loss                                        0.151247\n",
      "trainer/QF2 Loss                                        0.176433\n",
      "trainer/Policy Loss                                    -0.68299\n",
      "trainer/Q1 Predictions Mean                             5.9982\n",
      "trainer/Q1 Predictions Std                              0.848637\n",
      "trainer/Q1 Predictions Max                              7.55894\n",
      "trainer/Q1 Predictions Min                              3.9005\n",
      "trainer/Q2 Predictions Mean                             6.11699\n",
      "trainer/Q2 Predictions Std                              0.874239\n",
      "trainer/Q2 Predictions Max                              7.85137\n",
      "trainer/Q2 Predictions Min                              3.79253\n",
      "trainer/Q Targets Mean                                  6.00707\n",
      "trainer/Q Targets Std                                   0.872585\n",
      "trainer/Q Targets Max                                   9.30225\n",
      "trainer/Q Targets Min                                   3.83684\n",
      "trainer/Log Pis Mean                                    6.14315\n",
      "trainer/Log Pis Std                                     2.15839\n",
      "trainer/Log Pis Max                                    10.0054\n",
      "trainer/Log Pis Min                                    -0.644934\n",
      "trainer/Policy mu Mean                                  0.0217926\n",
      "trainer/Policy mu Std                                   0.106393\n",
      "trainer/Policy mu Max                                   0.779222\n",
      "trainer/Policy mu Min                                  -0.368409\n",
      "trainer/Policy log std Mean                            -2.16334\n",
      "trainer/Policy log std Std                              0.171663\n",
      "trainer/Policy log std Max                             -1.53026\n",
      "trainer/Policy log std Min                             -2.59302\n",
      "trainer/Alpha                                           0.0267339\n",
      "trainer/Alpha Loss                                     -6.72399\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            79\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.00325\n",
      "exploration/Rewards Std                                 0.187842\n",
      "exploration/Rewards Max                                 2.41487\n",
      "exploration/Rewards Min                                 0.784001\n",
      "exploration/Returns Mean                             1003.25\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1003.25\n",
      "exploration/Returns Min                              1003.25\n",
      "exploration/Actions Mean                                0.0197222\n",
      "exploration/Actions Std                                 0.115257\n",
      "exploration/Actions Max                                 0.426146\n",
      "exploration/Actions Min                                -0.419954\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1003.25\n",
      "exploration/env_infos/final/reward_forward Mean        -0.107534\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.107534\n",
      "exploration/env_infos/final/reward_forward Min         -0.107534\n",
      "exploration/env_infos/initial/reward_forward Mean       0.199427\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.199427\n",
      "exploration/env_infos/initial/reward_forward Min        0.199427\n",
      "exploration/env_infos/reward_forward Mean               0.0454501\n",
      "exploration/env_infos/reward_forward Std                0.313364\n",
      "exploration/env_infos/reward_forward Max                1.09376\n",
      "exploration/env_infos/reward_forward Min               -0.845157\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0854768\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0854768\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0854768\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0232508\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0232508\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0232508\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0546925\n",
      "exploration/env_infos/reward_ctrl Std                   0.0275588\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00699089\n",
      "exploration/env_infos/reward_ctrl Min                  -0.215999\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.301576\n",
      "exploration/env_infos/final/torso_velocity Std          0.138091\n",
      "exploration/env_infos/final/torso_velocity Max         -0.107534\n",
      "exploration/env_infos/final/torso_velocity Min         -0.417688\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.165693\n",
      "exploration/env_infos/initial/torso_velocity Std        0.124785\n",
      "exploration/env_infos/initial/torso_velocity Max        0.298838\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.001186\n",
      "exploration/env_infos/torso_velocity Mean               0.00543114\n",
      "exploration/env_infos/torso_velocity Std                0.293998\n",
      "exploration/env_infos/torso_velocity Max                1.09376\n",
      "exploration/env_infos/torso_velocity Min               -1.29488\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.991965\n",
      "evaluation/Rewards Std                                  0.0377137\n",
      "evaluation/Rewards Max                                  2.12795\n",
      "evaluation/Rewards Min                                  0.828643\n",
      "evaluation/Returns Mean                               991.965\n",
      "evaluation/Returns Std                                  6.65574\n",
      "evaluation/Returns Max                               1018.17\n",
      "evaluation/Returns Min                                980.859\n",
      "evaluation/Actions Mean                                 0.0146736\n",
      "evaluation/Actions Std                                  0.0508836\n",
      "evaluation/Actions Max                                  0.447186\n",
      "evaluation/Actions Min                                 -0.404657\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            991.965\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00043621\n",
      "evaluation/env_infos/final/reward_forward Std           0.0021371\n",
      "evaluation/env_infos/final/reward_forward Max           7.89278e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0109058\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00615173\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125314\n",
      "evaluation/env_infos/initial/reward_forward Max         0.231046\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.287025\n",
      "evaluation/env_infos/reward_forward Mean               -0.000631087\n",
      "evaluation/env_infos/reward_forward Std                 0.0693246\n",
      "evaluation/env_infos/reward_forward Max                 1.81483\n",
      "evaluation/env_infos/reward_forward Min                -1.23234\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0107332\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0036243\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00764131\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0191334\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0156052\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00322841\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0107824\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0263051\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0112178\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00708341\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00563658\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.171357\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000363568\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00299625\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0064896\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0228511\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.125447\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.243935\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.736556\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.378848\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00211126\n",
      "evaluation/env_infos/torso_velocity Std                 0.0774272\n",
      "evaluation/env_infos/torso_velocity Max                 1.81483\n",
      "evaluation/env_infos/torso_velocity Min                -2.06678\n",
      "time/data storing (s)                                   0.0148317\n",
      "time/evaluation sampling (s)                           47.7242\n",
      "time/exploration sampling (s)                           2.03922\n",
      "time/logging (s)                                        0.304257\n",
      "time/saving (s)                                         0.0281282\n",
      "time/training (s)                                       4.37017\n",
      "time/epoch (s)                                         54.4809\n",
      "time/total (s)                                        987.32\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:42:14.191315 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.156669\n",
      "trainer/QF2 Loss                                        0.173263\n",
      "trainer/Policy Loss                                    -1.03464\n",
      "trainer/Q1 Predictions Mean                             6.34256\n",
      "trainer/Q1 Predictions Std                              0.885872\n",
      "trainer/Q1 Predictions Max                             10.5212\n",
      "trainer/Q1 Predictions Min                              2.73862\n",
      "trainer/Q2 Predictions Mean                             6.36528\n",
      "trainer/Q2 Predictions Std                              0.838247\n",
      "trainer/Q2 Predictions Max                             10.0407\n",
      "trainer/Q2 Predictions Min                              3.08507\n",
      "trainer/Q Targets Mean                                  6.32404\n",
      "trainer/Q Targets Std                                   1.03717\n",
      "trainer/Q Targets Max                                  11.0683\n",
      "trainer/Q Targets Min                                  -0.932528\n",
      "trainer/Log Pis Mean                                    6.11748\n",
      "trainer/Log Pis Std                                     2.32076\n",
      "trainer/Log Pis Max                                    11.2881\n",
      "trainer/Log Pis Min                                    -5.86946\n",
      "trainer/Policy mu Mean                                 -0.00788347\n",
      "trainer/Policy mu Std                                   0.0982279\n",
      "trainer/Policy mu Max                                   0.749105\n",
      "trainer/Policy mu Min                                  -0.436146\n",
      "trainer/Policy log std Mean                            -2.16243\n",
      "trainer/Policy log std Std                              0.190731\n",
      "trainer/Policy log std Max                             -0.739961\n",
      "trainer/Policy log std Min                             -2.77097\n",
      "trainer/Alpha                                           0.0251939\n",
      "trainer/Alpha Loss                                     -6.92877\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                            80\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.964469\n",
      "exploration/Rewards Std                                 0.113537\n",
      "exploration/Rewards Max                                 1.94628\n",
      "exploration/Rewards Min                                 0.802883\n",
      "exploration/Returns Mean                              964.469\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               964.469\n",
      "exploration/Returns Min                               964.469\n",
      "exploration/Actions Mean                                0.00194075\n",
      "exploration/Actions Std                                 0.122947\n",
      "exploration/Actions Max                                 0.501287\n",
      "exploration/Actions Min                                -0.441796\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           964.469\n",
      "exploration/env_infos/final/reward_forward Mean         0.0654548\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0654548\n",
      "exploration/env_infos/final/reward_forward Min          0.0654548\n",
      "exploration/env_infos/initial/reward_forward Mean       0.100077\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.100077\n",
      "exploration/env_infos/initial/reward_forward Min        0.100077\n",
      "exploration/env_infos/reward_forward Mean               0.00410816\n",
      "exploration/env_infos/reward_forward Std                0.281621\n",
      "exploration/env_infos/reward_forward Max                1.14432\n",
      "exploration/env_infos/reward_forward Min               -1.1157\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.151937\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.151937\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.151937\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0381735\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0381735\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0381735\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0604788\n",
      "exploration/env_infos/reward_ctrl Std                   0.0313509\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00388727\n",
      "exploration/env_infos/reward_ctrl Min                  -0.197117\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0667523\n",
      "exploration/env_infos/final/torso_velocity Std          0.056774\n",
      "exploration/env_infos/final/torso_velocity Max          0.136926\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00212346\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.202524\n",
      "exploration/env_infos/initial/torso_velocity Std        0.207715\n",
      "exploration/env_infos/initial/torso_velocity Max        0.492173\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0153205\n",
      "exploration/env_infos/torso_velocity Mean              -0.00599991\n",
      "exploration/env_infos/torso_velocity Std                0.277353\n",
      "exploration/env_infos/torso_velocity Max                1.14432\n",
      "exploration/env_infos/torso_velocity Min               -1.57645\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.994093\n",
      "evaluation/Rewards Std                                  0.0379564\n",
      "evaluation/Rewards Max                                  2.02366\n",
      "evaluation/Rewards Min                                  0.875131\n",
      "evaluation/Returns Mean                               994.093\n",
      "evaluation/Returns Std                                  4.28357\n",
      "evaluation/Returns Max                               1007.82\n",
      "evaluation/Returns Min                                987.947\n",
      "evaluation/Actions Mean                                -0.000824484\n",
      "evaluation/Actions Std                                  0.0485254\n",
      "evaluation/Actions Max                                  0.3677\n",
      "evaluation/Actions Min                                 -0.355755\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            994.093\n",
      "evaluation/env_infos/final/reward_forward Mean          1.83564e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.79518e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.77428e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.81151e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0431362\n",
      "evaluation/env_infos/initial/reward_forward Std         0.100063\n",
      "evaluation/env_infos/initial/reward_forward Max         0.207983\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.175084\n",
      "evaluation/env_infos/reward_forward Mean                0.00077596\n",
      "evaluation/env_infos/reward_forward Std                 0.0797887\n",
      "evaluation/env_infos/reward_forward Max                 1.62056\n",
      "evaluation/env_infos/reward_forward Min                -1.06793\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00905715\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00231264\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00601847\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0139622\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.018494\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0032199\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0127409\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0245568\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00942158\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00502224\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00200752\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.124869\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.69818e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.31327e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.8305e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.01436e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137491\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221178\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.646839\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.234181\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00282147\n",
      "evaluation/env_infos/torso_velocity Std                 0.0749749\n",
      "evaluation/env_infos/torso_velocity Max                 1.62056\n",
      "evaluation/env_infos/torso_velocity Min                -1.86457\n",
      "time/data storing (s)                                   0.0154551\n",
      "time/evaluation sampling (s)                           49.0058\n",
      "time/exploration sampling (s)                           1.92965\n",
      "time/logging (s)                                        0.274494\n",
      "time/saving (s)                                         0.0257203\n",
      "time/training (s)                                       4.32854\n",
      "time/epoch (s)                                         55.5796\n",
      "time/total (s)                                       1043.25\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:43:06.008227 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  21000\n",
      "trainer/QF1 Loss                                        0.196079\n",
      "trainer/QF2 Loss                                        0.201213\n",
      "trainer/Policy Loss                                    -0.716621\n",
      "trainer/Q1 Predictions Mean                             6.86831\n",
      "trainer/Q1 Predictions Std                              0.896105\n",
      "trainer/Q1 Predictions Max                              9.11559\n",
      "trainer/Q1 Predictions Min                              4.32219\n",
      "trainer/Q2 Predictions Mean                             6.79004\n",
      "trainer/Q2 Predictions Std                              0.888049\n",
      "trainer/Q2 Predictions Max                              8.84663\n",
      "trainer/Q2 Predictions Min                              4.34276\n",
      "trainer/Q Targets Mean                                  6.76662\n",
      "trainer/Q Targets Std                                   0.940395\n",
      "trainer/Q Targets Max                                   8.53755\n",
      "trainer/Q Targets Min                                   3.02203\n",
      "trainer/Log Pis Mean                                    6.8045\n",
      "trainer/Log Pis Std                                     2.24854\n",
      "trainer/Log Pis Max                                    11.4273\n",
      "trainer/Log Pis Min                                    -2.85579\n",
      "trainer/Policy mu Mean                                 -0.00408153\n",
      "trainer/Policy mu Std                                   0.11861\n",
      "trainer/Policy mu Max                                   0.803765\n",
      "trainer/Policy mu Min                                  -0.546342\n",
      "trainer/Policy log std Mean                            -2.23796\n",
      "trainer/Policy log std Std                              0.153504\n",
      "trainer/Policy log std Max                             -1.46757\n",
      "trainer/Policy log std Min                             -2.6898\n",
      "trainer/Alpha                                           0.0238765\n",
      "trainer/Alpha Loss                                     -4.46437\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                            81\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.950453\n",
      "exploration/Rewards Std                                 0.0813213\n",
      "exploration/Rewards Max                                 1.67545\n",
      "exploration/Rewards Min                                 0.784331\n",
      "exploration/Returns Mean                              950.453\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               950.453\n",
      "exploration/Returns Min                               950.453\n",
      "exploration/Actions Mean                                0.0168676\n",
      "exploration/Actions Std                                 0.128483\n",
      "exploration/Actions Max                                 0.483262\n",
      "exploration/Actions Min                                -0.458161\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           950.453\n",
      "exploration/env_infos/final/reward_forward Mean         0.212459\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.212459\n",
      "exploration/env_infos/final/reward_forward Min          0.212459\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0116771\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0116771\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0116771\n",
      "exploration/env_infos/reward_forward Mean               0.0342875\n",
      "exploration/env_infos/reward_forward Std                0.196118\n",
      "exploration/env_infos/reward_forward Max                1.06256\n",
      "exploration/env_infos/reward_forward Min               -0.589658\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0618768\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0618768\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0618768\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.100154\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.100154\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.100154\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0671694\n",
      "exploration/env_infos/reward_ctrl Std                   0.0324573\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00688148\n",
      "exploration/env_infos/reward_ctrl Min                  -0.215669\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.178673\n",
      "exploration/env_infos/final/torso_velocity Std          0.28048\n",
      "exploration/env_infos/final/torso_velocity Max          0.212459\n",
      "exploration/env_infos/final/torso_velocity Min         -0.431379\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.093646\n",
      "exploration/env_infos/initial/torso_velocity Std        0.202674\n",
      "exploration/env_infos/initial/torso_velocity Max        0.377166\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0845504\n",
      "exploration/env_infos/torso_velocity Mean               0.00232911\n",
      "exploration/env_infos/torso_velocity Std                0.208714\n",
      "exploration/env_infos/torso_velocity Max                1.07859\n",
      "exploration/env_infos/torso_velocity Min               -1.29643\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.981572\n",
      "evaluation/Rewards Std                                  0.0176068\n",
      "evaluation/Rewards Max                                  2.09737\n",
      "evaluation/Rewards Min                                  0.828269\n",
      "evaluation/Returns Mean                               981.572\n",
      "evaluation/Returns Std                                  3.98042\n",
      "evaluation/Returns Max                                987.623\n",
      "evaluation/Returns Min                                973.875\n",
      "evaluation/Actions Mean                                 0.00471741\n",
      "evaluation/Actions Std                                  0.0687366\n",
      "evaluation/Actions Max                                  0.385648\n",
      "evaluation/Actions Min                                 -0.450907\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            981.572\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.61609e-06\n",
      "evaluation/env_infos/final/reward_forward Std           7.26968e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000236317\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000276292\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0548278\n",
      "evaluation/env_infos/initial/reward_forward Std         0.105391\n",
      "evaluation/env_infos/initial/reward_forward Max         0.31802\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.115527\n",
      "evaluation/env_infos/reward_forward Mean                0.00189505\n",
      "evaluation/env_infos/reward_forward Std                 0.0464728\n",
      "evaluation/env_infos/reward_forward Max                 1.45061\n",
      "evaluation/env_infos/reward_forward Min                -0.956103\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.018666\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00506746\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0123292\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0340491\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0403335\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00326464\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0354014\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.048614\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0189879\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00770344\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00942625\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.171731\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.09401e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           6.60453e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000236317\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000297155\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.167991\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.210689\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.680309\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.221432\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00115206\n",
      "evaluation/env_infos/torso_velocity Std                 0.053094\n",
      "evaluation/env_infos/torso_velocity Max                 1.45061\n",
      "evaluation/env_infos/torso_velocity Min                -1.79103\n",
      "time/data storing (s)                                   0.0154487\n",
      "time/evaluation sampling (s)                           45.1392\n",
      "time/exploration sampling (s)                           1.94652\n",
      "time/logging (s)                                        0.266001\n",
      "time/saving (s)                                         0.0249726\n",
      "time/training (s)                                       4.11508\n",
      "time/epoch (s)                                         51.5072\n",
      "time/total (s)                                       1095.06\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:43:58.317709 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.213344\n",
      "trainer/QF2 Loss                                        0.161959\n",
      "trainer/Policy Loss                                    -0.867362\n",
      "trainer/Q1 Predictions Mean                             6.99955\n",
      "trainer/Q1 Predictions Std                              0.838975\n",
      "trainer/Q1 Predictions Max                              8.6145\n",
      "trainer/Q1 Predictions Min                              4.41942\n",
      "trainer/Q2 Predictions Mean                             7.17641\n",
      "trainer/Q2 Predictions Std                              0.865543\n",
      "trainer/Q2 Predictions Max                              9.06692\n",
      "trainer/Q2 Predictions Min                              4.60381\n",
      "trainer/Q Targets Mean                                  7.21864\n",
      "trainer/Q Targets Std                                   0.872294\n",
      "trainer/Q Targets Max                                   9.61555\n",
      "trainer/Q Targets Min                                   4.73587\n",
      "trainer/Log Pis Mean                                    6.78821\n",
      "trainer/Log Pis Std                                     2.11825\n",
      "trainer/Log Pis Max                                    11.6123\n",
      "trainer/Log Pis Min                                    -1.09665\n",
      "trainer/Policy mu Mean                                  0.0131153\n",
      "trainer/Policy mu Std                                   0.115817\n",
      "trainer/Policy mu Max                                   0.788571\n",
      "trainer/Policy mu Min                                  -0.38345\n",
      "trainer/Policy log std Mean                            -2.25376\n",
      "trainer/Policy log std Std                              0.174447\n",
      "trainer/Policy log std Max                             -1.17377\n",
      "trainer/Policy log std Min                             -2.63605\n",
      "trainer/Alpha                                           0.0226859\n",
      "trainer/Alpha Loss                                     -4.58733\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                            82\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.936306\n",
      "exploration/Rewards Std                                 0.0395724\n",
      "exploration/Rewards Max                                 1.26722\n",
      "exploration/Rewards Min                                 0.796001\n",
      "exploration/Returns Mean                              936.306\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               936.306\n",
      "exploration/Returns Min                               936.306\n",
      "exploration/Actions Mean                                0.0309808\n",
      "exploration/Actions Std                                 0.126208\n",
      "exploration/Actions Max                                 0.419293\n",
      "exploration/Actions Min                                -0.489874\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           936.306\n",
      "exploration/env_infos/final/reward_forward Mean         0.135075\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.135075\n",
      "exploration/env_infos/final/reward_forward Min          0.135075\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.249656\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.249656\n",
      "exploration/env_infos/initial/reward_forward Min       -0.249656\n",
      "exploration/env_infos/reward_forward Mean               0.0463916\n",
      "exploration/env_infos/reward_forward Std                0.175298\n",
      "exploration/env_infos/reward_forward Max                1.26607\n",
      "exploration/env_infos/reward_forward Min               -0.431196\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0792317\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0792317\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0792317\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0659419\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0659419\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0659419\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0675534\n",
      "exploration/env_infos/reward_ctrl Std                   0.0293744\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0105439\n",
      "exploration/env_infos/reward_ctrl Min                  -0.203999\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0415655\n",
      "exploration/env_infos/final/torso_velocity Std          0.113323\n",
      "exploration/env_infos/final/torso_velocity Max          0.135075\n",
      "exploration/env_infos/final/torso_velocity Min         -0.117907\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0247322\n",
      "exploration/env_infos/initial/torso_velocity Std        0.237854\n",
      "exploration/env_infos/initial/torso_velocity Max        0.304338\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.249656\n",
      "exploration/env_infos/torso_velocity Mean               0.0224127\n",
      "exploration/env_infos/torso_velocity Std                0.171654\n",
      "exploration/env_infos/torso_velocity Max                1.26607\n",
      "exploration/env_infos/torso_velocity Min               -0.997354\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.975507\n",
      "evaluation/Rewards Std                                  0.022476\n",
      "evaluation/Rewards Max                                  2.463\n",
      "evaluation/Rewards Min                                  0.844325\n",
      "evaluation/Returns Mean                               975.507\n",
      "evaluation/Returns Std                                  3.37032\n",
      "evaluation/Returns Max                                980.011\n",
      "evaluation/Returns Min                                968.403\n",
      "evaluation/Actions Mean                                 0.0217894\n",
      "evaluation/Actions Std                                  0.0762032\n",
      "evaluation/Actions Max                                  0.394082\n",
      "evaluation/Actions Min                                 -0.324791\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            975.507\n",
      "evaluation/env_infos/final/reward_forward Mean          4.33632e-09\n",
      "evaluation/env_infos/final/reward_forward Std           4.46652e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.60982e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.30143e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0170513\n",
      "evaluation/env_infos/initial/reward_forward Std         0.103646\n",
      "evaluation/env_infos/initial/reward_forward Max         0.291888\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.180051\n",
      "evaluation/env_infos/reward_forward Mean                0.00276352\n",
      "evaluation/env_infos/reward_forward Std                 0.0524218\n",
      "evaluation/env_infos/reward_forward Max                 1.77878\n",
      "evaluation/env_infos/reward_forward Min                -1.22829\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0247668\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00296254\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0209108\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0312748\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0400342\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00466308\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0335938\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0509318\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0251268\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00554102\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00808916\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.155675\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          5.76132e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.7169e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.80154e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.30143e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144914\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224991\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.631839\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.27487\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0016842\n",
      "evaluation/env_infos/torso_velocity Std                 0.0566362\n",
      "evaluation/env_infos/torso_velocity Max                 1.77878\n",
      "evaluation/env_infos/torso_velocity Min                -2.0499\n",
      "time/data storing (s)                                   0.0146651\n",
      "time/evaluation sampling (s)                           45.5757\n",
      "time/exploration sampling (s)                           1.99469\n",
      "time/logging (s)                                        0.274393\n",
      "time/saving (s)                                         0.0258194\n",
      "time/training (s)                                       4.12959\n",
      "time/epoch (s)                                         52.0149\n",
      "time/total (s)                                       1147.38\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:44:50.839768 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  23000\n",
      "trainer/QF1 Loss                                        0.185341\n",
      "trainer/QF2 Loss                                        0.150052\n",
      "trainer/Policy Loss                                    -1.04197\n",
      "trainer/Q1 Predictions Mean                             7.29455\n",
      "trainer/Q1 Predictions Std                              0.772731\n",
      "trainer/Q1 Predictions Max                              8.69812\n",
      "trainer/Q1 Predictions Min                              5.18897\n",
      "trainer/Q2 Predictions Mean                             7.44236\n",
      "trainer/Q2 Predictions Std                              0.820118\n",
      "trainer/Q2 Predictions Max                              9.14057\n",
      "trainer/Q2 Predictions Min                              5.14706\n",
      "trainer/Q Targets Mean                                  7.51357\n",
      "trainer/Q Targets Std                                   0.833994\n",
      "trainer/Q Targets Max                                   8.99633\n",
      "trainer/Q Targets Min                                   4.98795\n",
      "trainer/Log Pis Mean                                    6.91327\n",
      "trainer/Log Pis Std                                     2.21272\n",
      "trainer/Log Pis Max                                    11.5927\n",
      "trainer/Log Pis Min                                    -4.73205\n",
      "trainer/Policy mu Mean                                  0.015018\n",
      "trainer/Policy mu Std                                   0.105446\n",
      "trainer/Policy mu Max                                   0.614064\n",
      "trainer/Policy mu Min                                  -0.385255\n",
      "trainer/Policy log std Mean                            -2.26574\n",
      "trainer/Policy log std Std                              0.177698\n",
      "trainer/Policy log std Max                             -1.30933\n",
      "trainer/Policy log std Min                             -2.64388\n",
      "trainer/Alpha                                           0.0216915\n",
      "trainer/Alpha Loss                                     -4.16258\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                            83\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.954961\n",
      "exploration/Rewards Std                                 0.0815204\n",
      "exploration/Rewards Max                                 1.63684\n",
      "exploration/Rewards Min                                 0.813415\n",
      "exploration/Returns Mean                              954.961\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               954.961\n",
      "exploration/Returns Min                               954.961\n",
      "exploration/Actions Mean                                0.0292958\n",
      "exploration/Actions Std                                 0.123636\n",
      "exploration/Actions Max                                 0.550376\n",
      "exploration/Actions Min                                -0.47412\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           954.961\n",
      "exploration/env_infos/final/reward_forward Mean         0.383992\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.383992\n",
      "exploration/env_infos/final/reward_forward Min          0.383992\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.173197\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.173197\n",
      "exploration/env_infos/initial/reward_forward Min       -0.173197\n",
      "exploration/env_infos/reward_forward Mean               0.0394099\n",
      "exploration/env_infos/reward_forward Std                0.177649\n",
      "exploration/env_infos/reward_forward Max                0.697723\n",
      "exploration/env_infos/reward_forward Min               -0.692117\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0661234\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0661234\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0661234\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0835717\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0835717\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0835717\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0645764\n",
      "exploration/env_infos/reward_ctrl Std                   0.0325091\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00398939\n",
      "exploration/env_infos/reward_ctrl Min                  -0.223799\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.157048\n",
      "exploration/env_infos/final/torso_velocity Std          0.16049\n",
      "exploration/env_infos/final/torso_velocity Max          0.383992\n",
      "exploration/env_infos/final/torso_velocity Min          0.0407844\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0803907\n",
      "exploration/env_infos/initial/torso_velocity Std        0.238527\n",
      "exploration/env_infos/initial/torso_velocity Max        0.399832\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.173197\n",
      "exploration/env_infos/torso_velocity Mean               0.0142568\n",
      "exploration/env_infos/torso_velocity Std                0.20783\n",
      "exploration/env_infos/torso_velocity Max                0.907291\n",
      "exploration/env_infos/torso_velocity Min               -1.23095\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978998\n",
      "evaluation/Rewards Std                                  0.026033\n",
      "evaluation/Rewards Max                                  2.17775\n",
      "evaluation/Rewards Min                                  0.838669\n",
      "evaluation/Returns Mean                               978.998\n",
      "evaluation/Returns Std                                  4.09322\n",
      "evaluation/Returns Max                                987.847\n",
      "evaluation/Returns Min                                974.27\n",
      "evaluation/Actions Mean                                 0.0390153\n",
      "evaluation/Actions Std                                  0.0630736\n",
      "evaluation/Actions Max                                  0.492504\n",
      "evaluation/Actions Min                                 -0.417367\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.998\n",
      "evaluation/env_infos/final/reward_forward Mean          6.67484e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.24459e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.63096e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -7.92125e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0276608\n",
      "evaluation/env_infos/initial/reward_forward Std         0.147759\n",
      "evaluation/env_infos/initial/reward_forward Max         0.309701\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.377908\n",
      "evaluation/env_infos/reward_forward Mean                0.0039922\n",
      "evaluation/env_infos/reward_forward Std                 0.0585321\n",
      "evaluation/env_infos/reward_forward Max                 1.83138\n",
      "evaluation/env_infos/reward_forward Min                -0.803811\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0215944\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00371713\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0136106\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0255538\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0209341\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00286708\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0167018\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0273675\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0220019\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0064727\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00796633\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.161331\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.95688e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           1.66855e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           6.12818e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000124275\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.15207\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244295\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.653046\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.377908\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000581523\n",
      "evaluation/env_infos/torso_velocity Std                 0.0570929\n",
      "evaluation/env_infos/torso_velocity Max                 1.83138\n",
      "evaluation/env_infos/torso_velocity Min                -1.81216\n",
      "time/data storing (s)                                   0.0151116\n",
      "time/evaluation sampling (s)                           45.385\n",
      "time/exploration sampling (s)                           1.95444\n",
      "time/logging (s)                                        0.274559\n",
      "time/saving (s)                                         0.0256394\n",
      "time/training (s)                                       4.54557\n",
      "time/epoch (s)                                         52.2004\n",
      "time/total (s)                                       1199.9\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:45:43.737208 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  24000\n",
      "trainer/QF1 Loss                                        0.331188\n",
      "trainer/QF2 Loss                                        0.397773\n",
      "trainer/Policy Loss                                    -1.91055\n",
      "trainer/Q1 Predictions Mean                             8.14816\n",
      "trainer/Q1 Predictions Std                              0.928722\n",
      "trainer/Q1 Predictions Max                              9.93201\n",
      "trainer/Q1 Predictions Min                              5.26476\n",
      "trainer/Q2 Predictions Mean                             8.24997\n",
      "trainer/Q2 Predictions Std                              0.915347\n",
      "trainer/Q2 Predictions Max                             10.0838\n",
      "trainer/Q2 Predictions Min                              5.42148\n",
      "trainer/Q Targets Mean                                  7.9017\n",
      "trainer/Q Targets Std                                   1.03979\n",
      "trainer/Q Targets Max                                  10.3347\n",
      "trainer/Q Targets Min                                   0.168907\n",
      "trainer/Log Pis Mean                                    6.88302\n",
      "trainer/Log Pis Std                                     2.14172\n",
      "trainer/Log Pis Max                                    11.6542\n",
      "trainer/Log Pis Min                                     0.0224281\n",
      "trainer/Policy mu Mean                                  0.00283106\n",
      "trainer/Policy mu Std                                   0.110673\n",
      "trainer/Policy mu Max                                   0.576153\n",
      "trainer/Policy mu Min                                  -0.442311\n",
      "trainer/Policy log std Mean                            -2.25096\n",
      "trainer/Policy log std Std                              0.178656\n",
      "trainer/Policy log std Max                             -1.32287\n",
      "trainer/Policy log std Min                             -2.80507\n",
      "trainer/Alpha                                           0.0208283\n",
      "trainer/Alpha Loss                                     -4.32374\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                            84\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.937545\n",
      "exploration/Rewards Std                                 0.067575\n",
      "exploration/Rewards Max                                 1.56387\n",
      "exploration/Rewards Min                                 0.720581\n",
      "exploration/Returns Mean                              937.545\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               937.545\n",
      "exploration/Returns Min                               937.545\n",
      "exploration/Actions Mean                                0.0203426\n",
      "exploration/Actions Std                                 0.133915\n",
      "exploration/Actions Max                                 0.509772\n",
      "exploration/Actions Min                                -0.590304\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           937.545\n",
      "exploration/env_infos/final/reward_forward Mean        -0.235481\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.235481\n",
      "exploration/env_infos/final/reward_forward Min         -0.235481\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.235649\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.235649\n",
      "exploration/env_infos/initial/reward_forward Min       -0.235649\n",
      "exploration/env_infos/reward_forward Mean               0.0220778\n",
      "exploration/env_infos/reward_forward Std                0.229075\n",
      "exploration/env_infos/reward_forward Max                0.831362\n",
      "exploration/env_infos/reward_forward Min               -0.764749\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0669529\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0669529\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0669529\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0518224\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0518224\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0518224\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0733884\n",
      "exploration/env_infos/reward_ctrl Std                   0.0335561\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00822628\n",
      "exploration/env_infos/reward_ctrl Min                  -0.279419\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.240794\n",
      "exploration/env_infos/final/torso_velocity Std          0.00429423\n",
      "exploration/env_infos/final/torso_velocity Max         -0.235481\n",
      "exploration/env_infos/final/torso_velocity Min         -0.245998\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0376621\n",
      "exploration/env_infos/initial/torso_velocity Std        0.315509\n",
      "exploration/env_infos/initial/torso_velocity Max        0.47976\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.235649\n",
      "exploration/env_infos/torso_velocity Mean               0.0115972\n",
      "exploration/env_infos/torso_velocity Std                0.251604\n",
      "exploration/env_infos/torso_velocity Max                1.002\n",
      "exploration/env_infos/torso_velocity Min               -1.12877\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.972727\n",
      "evaluation/Rewards Std                                  0.0257926\n",
      "evaluation/Rewards Max                                  2.1895\n",
      "evaluation/Rewards Min                                  0.665738\n",
      "evaluation/Returns Mean                               972.727\n",
      "evaluation/Returns Std                                  8.74379\n",
      "evaluation/Returns Max                                987.059\n",
      "evaluation/Returns Min                                957.065\n",
      "evaluation/Actions Mean                                -0.00282658\n",
      "evaluation/Actions Std                                  0.0842397\n",
      "evaluation/Actions Max                                  0.466227\n",
      "evaluation/Actions Min                                 -0.477009\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            972.727\n",
      "evaluation/env_infos/final/reward_forward Mean          2.7476e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000203358\n",
      "evaluation/env_infos/final/reward_forward Max           0.00048763\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000706085\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0258737\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128312\n",
      "evaluation/env_infos/initial/reward_forward Max         0.201045\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.398687\n",
      "evaluation/env_infos/reward_forward Mean                0.00232115\n",
      "evaluation/env_infos/reward_forward Std                 0.0605236\n",
      "evaluation/env_infos/reward_forward Max                 1.67052\n",
      "evaluation/env_infos/reward_forward Min                -1.18156\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0279007\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00786025\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0170389\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0421941\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0327627\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00675243\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0203134\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0478728\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0284173\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0103749\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00920552\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.334262\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.56354e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000214291\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00048763\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000975039\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127669\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244102\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.644071\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.398687\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00149415\n",
      "evaluation/env_infos/torso_velocity Std                 0.0596437\n",
      "evaluation/env_infos/torso_velocity Max                 1.67052\n",
      "evaluation/env_infos/torso_velocity Min                -1.75081\n",
      "time/data storing (s)                                   0.0149794\n",
      "time/evaluation sampling (s)                           45.888\n",
      "time/exploration sampling (s)                           2.01279\n",
      "time/logging (s)                                        0.270785\n",
      "time/saving (s)                                         0.0283164\n",
      "time/training (s)                                       4.34234\n",
      "time/epoch (s)                                         52.5572\n",
      "time/total (s)                                       1252.79\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:46:36.157328 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  25000\n",
      "trainer/QF1 Loss                                        0.310064\n",
      "trainer/QF2 Loss                                        0.305273\n",
      "trainer/Policy Loss                                    -1.96096\n",
      "trainer/Q1 Predictions Mean                             8.34658\n",
      "trainer/Q1 Predictions Std                              0.945241\n",
      "trainer/Q1 Predictions Max                             10.9864\n",
      "trainer/Q1 Predictions Min                              4.41573\n",
      "trainer/Q2 Predictions Mean                             8.38908\n",
      "trainer/Q2 Predictions Std                              0.936829\n",
      "trainer/Q2 Predictions Max                             10.8433\n",
      "trainer/Q2 Predictions Min                              3.94993\n",
      "trainer/Q Targets Mean                                  8.25922\n",
      "trainer/Q Targets Std                                   1.13299\n",
      "trainer/Q Targets Max                                  13.31\n",
      "trainer/Q Targets Min                                  -0.674402\n",
      "trainer/Log Pis Mean                                    6.94971\n",
      "trainer/Log Pis Std                                     2.00886\n",
      "trainer/Log Pis Max                                    10.8503\n",
      "trainer/Log Pis Min                                     0.567214\n",
      "trainer/Policy mu Mean                                 -0.0274151\n",
      "trainer/Policy mu Std                                   0.144378\n",
      "trainer/Policy mu Max                                   1.01237\n",
      "trainer/Policy mu Min                                  -0.721528\n",
      "trainer/Policy log std Mean                            -2.24465\n",
      "trainer/Policy log std Std                              0.178388\n",
      "trainer/Policy log std Max                             -1.2544\n",
      "trainer/Policy log std Min                             -2.95671\n",
      "trainer/Alpha                                           0.019892\n",
      "trainer/Alpha Loss                                     -4.11392\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.940464\n",
      "exploration/Rewards Std                                 0.0860137\n",
      "exploration/Rewards Max                                 1.68121\n",
      "exploration/Rewards Min                                 0.501748\n",
      "exploration/Returns Mean                              940.464\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               940.464\n",
      "exploration/Returns Min                               940.464\n",
      "exploration/Actions Mean                               -0.00715683\n",
      "exploration/Actions Std                                 0.140495\n",
      "exploration/Actions Max                                 0.591468\n",
      "exploration/Actions Min                                -0.596519\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           940.464\n",
      "exploration/env_infos/final/reward_forward Mean         0.171972\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.171972\n",
      "exploration/env_infos/final/reward_forward Min          0.171972\n",
      "exploration/env_infos/initial/reward_forward Mean       0.120189\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.120189\n",
      "exploration/env_infos/initial/reward_forward Min        0.120189\n",
      "exploration/env_infos/reward_forward Mean              -0.000934558\n",
      "exploration/env_infos/reward_forward Std                0.205006\n",
      "exploration/env_infos/reward_forward Max                0.765227\n",
      "exploration/env_infos/reward_forward Min               -1.09639\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0553069\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0553069\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0553069\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0743452\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0743452\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0743452\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0791608\n",
      "exploration/env_infos/reward_ctrl Std                   0.0398843\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00436417\n",
      "exploration/env_infos/reward_ctrl Min                  -0.498252\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0979167\n",
      "exploration/env_infos/final/torso_velocity Std          0.112367\n",
      "exploration/env_infos/final/torso_velocity Max          0.182653\n",
      "exploration/env_infos/final/torso_velocity Min         -0.060875\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0898303\n",
      "exploration/env_infos/initial/torso_velocity Std        0.163465\n",
      "exploration/env_infos/initial/torso_velocity Max        0.27312\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.123818\n",
      "exploration/env_infos/torso_velocity Mean              -0.0163827\n",
      "exploration/env_infos/torso_velocity Std                0.198833\n",
      "exploration/env_infos/torso_velocity Max                1.25587\n",
      "exploration/env_infos/torso_velocity Min               -1.09639\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.960331\n",
      "evaluation/Rewards Std                                  0.0241902\n",
      "evaluation/Rewards Max                                  2.24135\n",
      "evaluation/Rewards Min                                  0.399181\n",
      "evaluation/Returns Mean                               960.331\n",
      "evaluation/Returns Std                                  4.35533\n",
      "evaluation/Returns Max                                967.847\n",
      "evaluation/Returns Min                                949.946\n",
      "evaluation/Actions Mean                                -0.0133974\n",
      "evaluation/Actions Std                                  0.0995704\n",
      "evaluation/Actions Max                                  0.639822\n",
      "evaluation/Actions Min                                 -0.60555\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            960.331\n",
      "evaluation/env_infos/final/reward_forward Mean          1.14844e-05\n",
      "evaluation/env_infos/final/reward_forward Std           3.99131e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000162096\n",
      "evaluation/env_infos/final/reward_forward Min          -6.54411e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0215098\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132191\n",
      "evaluation/env_infos/initial/reward_forward Max         0.252492\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.189064\n",
      "evaluation/env_infos/reward_forward Mean                0.00491143\n",
      "evaluation/env_infos/reward_forward Std                 0.0541116\n",
      "evaluation/env_infos/reward_forward Max                 1.52091\n",
      "evaluation/env_infos/reward_forward Min                -1.13478\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0390419\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00470334\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0302549\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0493777\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0428522\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00721623\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0308658\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0558842\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.040375\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0164196\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0235446\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.600819\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.70245e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.88738e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000162096\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000217937\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.147668\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244881\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.778378\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.220556\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000979046\n",
      "evaluation/env_infos/torso_velocity Std                 0.056478\n",
      "evaluation/env_infos/torso_velocity Max                 1.52091\n",
      "evaluation/env_infos/torso_velocity Min                -1.95127\n",
      "time/data storing (s)                                   0.0158023\n",
      "time/evaluation sampling (s)                           45.4114\n",
      "time/exploration sampling (s)                           2.14324\n",
      "time/logging (s)                                        0.280584\n",
      "time/saving (s)                                         0.03252\n",
      "time/training (s)                                       4.21438\n",
      "time/epoch (s)                                         52.0979\n",
      "time/total (s)                                       1305.22\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:47:28.082581 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  26000\n",
      "trainer/QF1 Loss                                        0.159475\n",
      "trainer/QF2 Loss                                        0.183442\n",
      "trainer/Policy Loss                                    -2.05584\n",
      "trainer/Q1 Predictions Mean                             8.53576\n",
      "trainer/Q1 Predictions Std                              0.875552\n",
      "trainer/Q1 Predictions Max                             10.3029\n",
      "trainer/Q1 Predictions Min                              5.76484\n",
      "trainer/Q2 Predictions Mean                             8.47014\n",
      "trainer/Q2 Predictions Std                              0.896637\n",
      "trainer/Q2 Predictions Max                             10.6277\n",
      "trainer/Q2 Predictions Min                              5.97351\n",
      "trainer/Q Targets Mean                                  8.73856\n",
      "trainer/Q Targets Std                                   0.861692\n",
      "trainer/Q Targets Max                                  10.6412\n",
      "trainer/Q Targets Min                                   5.91386\n",
      "trainer/Log Pis Mean                                    6.98491\n",
      "trainer/Log Pis Std                                     2.24344\n",
      "trainer/Log Pis Max                                    12.389\n",
      "trainer/Log Pis Min                                    -2.44784\n",
      "trainer/Policy mu Mean                                  0.011918\n",
      "trainer/Policy mu Std                                   0.12143\n",
      "trainer/Policy mu Max                                   0.95836\n",
      "trainer/Policy mu Min                                  -0.714772\n",
      "trainer/Policy log std Mean                            -2.25958\n",
      "trainer/Policy log std Std                              0.184917\n",
      "trainer/Policy log std Max                             -1.19589\n",
      "trainer/Policy log std Min                             -2.73881\n",
      "trainer/Alpha                                           0.0190368\n",
      "trainer/Alpha Loss                                     -4.02071\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                            86\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.970776\n",
      "exploration/Rewards Std                                 0.118685\n",
      "exploration/Rewards Max                                 2.02196\n",
      "exploration/Rewards Min                                 0.753691\n",
      "exploration/Returns Mean                              970.776\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               970.776\n",
      "exploration/Returns Min                               970.776\n",
      "exploration/Actions Mean                                0.0268812\n",
      "exploration/Actions Std                                 0.115729\n",
      "exploration/Actions Max                                 0.474431\n",
      "exploration/Actions Min                                -0.418699\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           970.776\n",
      "exploration/env_infos/final/reward_forward Mean        -0.167222\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.167222\n",
      "exploration/env_infos/final/reward_forward Min         -0.167222\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0192168\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0192168\n",
      "exploration/env_infos/initial/reward_forward Min        0.0192168\n",
      "exploration/env_infos/reward_forward Mean               0.04866\n",
      "exploration/env_infos/reward_forward Std                0.311829\n",
      "exploration/env_infos/reward_forward Max                1.31727\n",
      "exploration/env_infos/reward_forward Min               -0.97704\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0561263\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0561263\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0561263\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0807113\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0807113\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0807113\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0564628\n",
      "exploration/env_infos/reward_ctrl Std                   0.0304909\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00321654\n",
      "exploration/env_infos/reward_ctrl Min                  -0.246309\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00115741\n",
      "exploration/env_infos/final/torso_velocity Std          0.134698\n",
      "exploration/env_infos/final/torso_velocity Max          0.162494\n",
      "exploration/env_infos/final/torso_velocity Min         -0.167222\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.181454\n",
      "exploration/env_infos/initial/torso_velocity Std        0.22672\n",
      "exploration/env_infos/initial/torso_velocity Max        0.502077\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0192168\n",
      "exploration/env_infos/torso_velocity Mean               0.0201165\n",
      "exploration/env_infos/torso_velocity Std                0.280576\n",
      "exploration/env_infos/torso_velocity Max                1.31727\n",
      "exploration/env_infos/torso_velocity Min               -1.52976\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.98111\n",
      "evaluation/Rewards Std                                  0.0246908\n",
      "evaluation/Rewards Max                                  2.41916\n",
      "evaluation/Rewards Min                                  0.835837\n",
      "evaluation/Returns Mean                               981.11\n",
      "evaluation/Returns Std                                  7.31747\n",
      "evaluation/Returns Max                                992.644\n",
      "evaluation/Returns Min                                969.154\n",
      "evaluation/Actions Mean                                 0.0191863\n",
      "evaluation/Actions Std                                  0.0675293\n",
      "evaluation/Actions Max                                  0.45797\n",
      "evaluation/Actions Min                                 -0.375248\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            981.11\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.000134621\n",
      "evaluation/env_infos/final/reward_forward Std           0.000547963\n",
      "evaluation/env_infos/final/reward_forward Max           1.23336e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00279037\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0132381\n",
      "evaluation/env_infos/initial/reward_forward Std         0.140241\n",
      "evaluation/env_infos/initial/reward_forward Max         0.243431\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.280176\n",
      "evaluation/env_infos/reward_forward Mean                0.00252098\n",
      "evaluation/env_infos/reward_forward Std                 0.0573119\n",
      "evaluation/env_infos/reward_forward Max                 1.51712\n",
      "evaluation/env_infos/reward_forward Min                -0.953121\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0194754\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00709812\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0109148\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0307742\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0209754\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00506589\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0100387\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0309987\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0197133\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00836745\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00321296\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.164163\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.19851e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000401646\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000632544\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00279037\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.156377\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.214322\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.588962\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.280176\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00103461\n",
      "evaluation/env_infos/torso_velocity Std                 0.0575401\n",
      "evaluation/env_infos/torso_velocity Max                 1.51712\n",
      "evaluation/env_infos/torso_velocity Min                -1.94561\n",
      "time/data storing (s)                                   0.0147655\n",
      "time/evaluation sampling (s)                           45.084\n",
      "time/exploration sampling (s)                           1.9101\n",
      "time/logging (s)                                        0.270516\n",
      "time/saving (s)                                         0.0251353\n",
      "time/training (s)                                       4.27313\n",
      "time/epoch (s)                                         51.5776\n",
      "time/total (s)                                       1357.13\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:48:19.460283 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  27000\n",
      "trainer/QF1 Loss                                        0.208794\n",
      "trainer/QF2 Loss                                        0.191067\n",
      "trainer/Policy Loss                                    -2.41976\n",
      "trainer/Q1 Predictions Mean                             9.31499\n",
      "trainer/Q1 Predictions Std                              0.930757\n",
      "trainer/Q1 Predictions Max                             13.0081\n",
      "trainer/Q1 Predictions Min                              5.90335\n",
      "trainer/Q2 Predictions Mean                             9.14622\n",
      "trainer/Q2 Predictions Std                              0.934873\n",
      "trainer/Q2 Predictions Max                             12.6485\n",
      "trainer/Q2 Predictions Min                              6.08346\n",
      "trainer/Q Targets Mean                                  9.1127\n",
      "trainer/Q Targets Std                                   0.977721\n",
      "trainer/Q Targets Max                                  14.4954\n",
      "trainer/Q Targets Min                                   4.82416\n",
      "trainer/Log Pis Mean                                    7.29649\n",
      "trainer/Log Pis Std                                     2.20602\n",
      "trainer/Log Pis Max                                    13.0682\n",
      "trainer/Log Pis Min                                    -1.18567\n",
      "trainer/Policy mu Mean                                  0.0121854\n",
      "trainer/Policy mu Std                                   0.0963128\n",
      "trainer/Policy mu Max                                   0.470905\n",
      "trainer/Policy mu Min                                  -0.435171\n",
      "trainer/Policy log std Mean                            -2.28273\n",
      "trainer/Policy log std Std                              0.192319\n",
      "trainer/Policy log std Max                             -1.42134\n",
      "trainer/Policy log std Min                             -3.14272\n",
      "trainer/Alpha                                           0.0183342\n",
      "trainer/Alpha Loss                                     -2.81308\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                            87\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.964472\n",
      "exploration/Rewards Std                                 0.100312\n",
      "exploration/Rewards Max                                 1.97724\n",
      "exploration/Rewards Min                                 0.758649\n",
      "exploration/Returns Mean                              964.472\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               964.472\n",
      "exploration/Returns Min                               964.472\n",
      "exploration/Actions Mean                                0.0152943\n",
      "exploration/Actions Std                                 0.118739\n",
      "exploration/Actions Max                                 0.490008\n",
      "exploration/Actions Min                                -0.41705\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           964.472\n",
      "exploration/env_infos/final/reward_forward Mean        -0.336198\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.336198\n",
      "exploration/env_infos/final/reward_forward Min         -0.336198\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.158668\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.158668\n",
      "exploration/env_infos/initial/reward_forward Min       -0.158668\n",
      "exploration/env_infos/reward_forward Mean              -0.0059197\n",
      "exploration/env_infos/reward_forward Std                0.221566\n",
      "exploration/env_infos/reward_forward Max                1.57474\n",
      "exploration/env_infos/reward_forward Min               -0.845758\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.02539\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.02539\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.02539\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0374086\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0374086\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0374086\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0573319\n",
      "exploration/env_infos/reward_ctrl Std                   0.0302243\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00365375\n",
      "exploration/env_infos/reward_ctrl Min                  -0.241351\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.107286\n",
      "exploration/env_infos/final/torso_velocity Std          0.180749\n",
      "exploration/env_infos/final/torso_velocity Max          0.105682\n",
      "exploration/env_infos/final/torso_velocity Min         -0.336198\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0546219\n",
      "exploration/env_infos/initial/torso_velocity Std        0.214269\n",
      "exploration/env_infos/initial/torso_velocity Max        0.347674\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.158668\n",
      "exploration/env_infos/torso_velocity Mean               0.000852018\n",
      "exploration/env_infos/torso_velocity Std                0.243722\n",
      "exploration/env_infos/torso_velocity Max                1.57474\n",
      "exploration/env_infos/torso_velocity Min               -1.45453\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.981984\n",
      "evaluation/Rewards Std                                  0.0199384\n",
      "evaluation/Rewards Max                                  1.72385\n",
      "evaluation/Rewards Min                                  0.767716\n",
      "evaluation/Returns Mean                               981.984\n",
      "evaluation/Returns Std                                  8.46681\n",
      "evaluation/Returns Max                                993.255\n",
      "evaluation/Returns Min                                965.259\n",
      "evaluation/Actions Mean                                 0.0213707\n",
      "evaluation/Actions Std                                  0.0656879\n",
      "evaluation/Actions Max                                  0.447141\n",
      "evaluation/Actions Min                                 -0.457645\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            981.984\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000151274\n",
      "evaluation/env_infos/final/reward_forward Std           0.000702201\n",
      "evaluation/env_infos/final/reward_forward Max           0.00358885\n",
      "evaluation/env_infos/final/reward_forward Min          -6.38669e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0208639\n",
      "evaluation/env_infos/initial/reward_forward Std         0.123701\n",
      "evaluation/env_infos/initial/reward_forward Max         0.417241\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.163442\n",
      "evaluation/env_infos/reward_forward Mean                0.00108985\n",
      "evaluation/env_infos/reward_forward Std                 0.0651441\n",
      "evaluation/env_infos/reward_forward Max                 1.71126\n",
      "evaluation/env_infos/reward_forward Min                -1.47428\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0187315\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00815447\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00883782\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0345215\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0281656\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0090592\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00903707\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0512026\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0190864\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00944877\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00348624\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.232284\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.27976e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000561096\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00358885\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00309436\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.150519\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.210847\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.559917\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.234416\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00082135\n",
      "evaluation/env_infos/torso_velocity Std                 0.0662675\n",
      "evaluation/env_infos/torso_velocity Max                 1.71126\n",
      "evaluation/env_infos/torso_velocity Min                -1.81899\n",
      "time/data storing (s)                                   0.0143919\n",
      "time/evaluation sampling (s)                           44.6177\n",
      "time/exploration sampling (s)                           1.89737\n",
      "time/logging (s)                                        0.277415\n",
      "time/saving (s)                                         0.0309168\n",
      "time/training (s)                                       4.20221\n",
      "time/epoch (s)                                         51.04\n",
      "time/total (s)                                       1408.52\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:49:10.615512 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 26 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  28000\n",
      "trainer/QF1 Loss                                        0.232553\n",
      "trainer/QF2 Loss                                        0.195908\n",
      "trainer/Policy Loss                                    -2.06697\n",
      "trainer/Q1 Predictions Mean                             9.13237\n",
      "trainer/Q1 Predictions Std                              1.01349\n",
      "trainer/Q1 Predictions Max                             10.7305\n",
      "trainer/Q1 Predictions Min                              1.66596\n",
      "trainer/Q2 Predictions Mean                             9.31998\n",
      "trainer/Q2 Predictions Std                              1.12219\n",
      "trainer/Q2 Predictions Max                             10.8245\n",
      "trainer/Q2 Predictions Min                             -0.51693\n",
      "trainer/Q Targets Mean                                  9.41374\n",
      "trainer/Q Targets Std                                   1.09699\n",
      "trainer/Q Targets Max                                  11.7612\n",
      "trainer/Q Targets Min                                   0.180332\n",
      "trainer/Log Pis Mean                                    7.57142\n",
      "trainer/Log Pis Std                                     2.14736\n",
      "trainer/Log Pis Max                                    13.2951\n",
      "trainer/Log Pis Min                                     0.302965\n",
      "trainer/Policy mu Mean                                 -0.00293593\n",
      "trainer/Policy mu Std                                   0.136248\n",
      "trainer/Policy mu Max                                   2.48508\n",
      "trainer/Policy mu Min                                  -0.82831\n",
      "trainer/Policy log std Mean                            -2.34071\n",
      "trainer/Policy log std Std                              0.176425\n",
      "trainer/Policy log std Max                             -1.28822\n",
      "trainer/Policy log std Min                             -3.02069\n",
      "trainer/Alpha                                           0.0175628\n",
      "trainer/Alpha Loss                                     -1.73207\n",
      "exploration/num steps total                         28000\n",
      "exploration/num paths total                            88\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                1.00091\n",
      "exploration/Rewards Std                                 0.180331\n",
      "exploration/Rewards Max                                 2.10068\n",
      "exploration/Rewards Min                                 0.691674\n",
      "exploration/Returns Mean                             1000.91\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                              1000.91\n",
      "exploration/Returns Min                              1000.91\n",
      "exploration/Actions Mean                                0.00781612\n",
      "exploration/Actions Std                                 0.12743\n",
      "exploration/Actions Max                                 0.465299\n",
      "exploration/Actions Min                                -0.543893\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                          1000.91\n",
      "exploration/env_infos/final/reward_forward Mean        -0.262324\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.262324\n",
      "exploration/env_infos/final/reward_forward Min         -0.262324\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0473271\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0473271\n",
      "exploration/env_infos/initial/reward_forward Min        0.0473271\n",
      "exploration/env_infos/reward_forward Mean              -0.0177527\n",
      "exploration/env_infos/reward_forward Std                0.220346\n",
      "exploration/env_infos/reward_forward Max                0.683646\n",
      "exploration/env_infos/reward_forward Min               -1.5296\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.046035\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.046035\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.046035\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0323126\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0323126\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0323126\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0651984\n",
      "exploration/env_infos/reward_ctrl Std                   0.0343928\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00595207\n",
      "exploration/env_infos/reward_ctrl Min                  -0.308326\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.170849\n",
      "exploration/env_infos/final/torso_velocity Std          0.0790307\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0694969\n",
      "exploration/env_infos/final/torso_velocity Min         -0.262324\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.00625189\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0934705\n",
      "exploration/env_infos/initial/torso_velocity Max        0.0945247\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.123096\n",
      "exploration/env_infos/torso_velocity Mean              -0.0244295\n",
      "exploration/env_infos/torso_velocity Std                0.244668\n",
      "exploration/env_infos/torso_velocity Max                0.872508\n",
      "exploration/env_infos/torso_velocity Min               -1.5296\n",
      "evaluation/num steps total                         675000\n",
      "evaluation/num paths total                            675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.965129\n",
      "evaluation/Rewards Std                                  0.0223183\n",
      "evaluation/Rewards Max                                  1.66108\n",
      "evaluation/Rewards Min                                  0.679672\n",
      "evaluation/Returns Mean                               965.129\n",
      "evaluation/Returns Std                                  8.6568\n",
      "evaluation/Returns Max                                982.324\n",
      "evaluation/Returns Min                                953.484\n",
      "evaluation/Actions Mean                                 0.00738942\n",
      "evaluation/Actions Std                                  0.0944007\n",
      "evaluation/Actions Max                                  0.586361\n",
      "evaluation/Actions Min                                 -0.502408\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            965.129\n",
      "evaluation/env_infos/final/reward_forward Mean          1.23193e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.70394e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.7416e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.35681e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00867331\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0780302\n",
      "evaluation/env_infos/initial/reward_forward Max         0.116408\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.139112\n",
      "evaluation/env_infos/reward_forward Mean               -0.000566168\n",
      "evaluation/env_infos/reward_forward Std                 0.0503614\n",
      "evaluation/env_infos/reward_forward Max                 1.54755\n",
      "evaluation/env_infos/reward_forward Min                -1.49497\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0353983\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00890258\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0179393\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0459717\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0278777\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00526268\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0162292\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0404704\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0358644\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0128776\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0087484\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.320328\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.29295e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           8.01245e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.75732e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.282e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.13765\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.20555\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.674922\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.157128\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000403\n",
      "evaluation/env_infos/torso_velocity Std                 0.05505\n",
      "evaluation/env_infos/torso_velocity Max                 1.54755\n",
      "evaluation/env_infos/torso_velocity Min                -1.93757\n",
      "time/data storing (s)                                   0.0156891\n",
      "time/evaluation sampling (s)                           44.3476\n",
      "time/exploration sampling (s)                           1.91703\n",
      "time/logging (s)                                        0.270394\n",
      "time/saving (s)                                         0.0261032\n",
      "time/training (s)                                       4.21162\n",
      "time/epoch (s)                                         50.7884\n",
      "time/total (s)                                       1459.67\n",
      "Epoch                                                  26\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:03.298766 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 27 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  29000\n",
      "trainer/QF1 Loss                                        0.168297\n",
      "trainer/QF2 Loss                                        0.150719\n",
      "trainer/Policy Loss                                    -3.59628\n",
      "trainer/Q1 Predictions Mean                             9.89655\n",
      "trainer/Q1 Predictions Std                              0.828488\n",
      "trainer/Q1 Predictions Max                             11.5914\n",
      "trainer/Q1 Predictions Min                              7.32823\n",
      "trainer/Q2 Predictions Mean                             9.97433\n",
      "trainer/Q2 Predictions Std                              0.833155\n",
      "trainer/Q2 Predictions Max                             11.7257\n",
      "trainer/Q2 Predictions Min                              7.44702\n",
      "trainer/Q Targets Mean                                  9.87691\n",
      "trainer/Q Targets Std                                   0.876996\n",
      "trainer/Q Targets Max                                  12.5216\n",
      "trainer/Q Targets Min                                   5.82284\n",
      "trainer/Log Pis Mean                                    6.75578\n",
      "trainer/Log Pis Std                                     2.27775\n",
      "trainer/Log Pis Max                                    12.2132\n",
      "trainer/Log Pis Min                                    -5.46506\n",
      "trainer/Policy mu Mean                                  0.0146325\n",
      "trainer/Policy mu Std                                   0.0993173\n",
      "trainer/Policy mu Max                                   0.579123\n",
      "trainer/Policy mu Min                                  -0.428556\n",
      "trainer/Policy log std Mean                            -2.24881\n",
      "trainer/Policy log std Std                              0.181439\n",
      "trainer/Policy log std Max                             -1.285\n",
      "trainer/Policy log std Min                             -2.98409\n",
      "trainer/Alpha                                           0.0168244\n",
      "trainer/Alpha Loss                                     -5.08191\n",
      "exploration/num steps total                         29000\n",
      "exploration/num paths total                            89\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.965802\n",
      "exploration/Rewards Std                                 0.100076\n",
      "exploration/Rewards Max                                 1.67568\n",
      "exploration/Rewards Min                                 0.769276\n",
      "exploration/Returns Mean                              965.802\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               965.802\n",
      "exploration/Returns Min                               965.802\n",
      "exploration/Actions Mean                                0.00308654\n",
      "exploration/Actions Std                                 0.123717\n",
      "exploration/Actions Max                                 0.494142\n",
      "exploration/Actions Min                                -0.468107\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           965.802\n",
      "exploration/env_infos/final/reward_forward Mean         0.0111671\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0111671\n",
      "exploration/env_infos/final/reward_forward Min          0.0111671\n",
      "exploration/env_infos/initial/reward_forward Mean       0.156497\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.156497\n",
      "exploration/env_infos/initial/reward_forward Min        0.156497\n",
      "exploration/env_infos/reward_forward Mean               0.0353904\n",
      "exploration/env_infos/reward_forward Std                0.225121\n",
      "exploration/env_infos/reward_forward Max                0.840048\n",
      "exploration/env_infos/reward_forward Min               -1.27888\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0880685\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0880685\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0880685\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0588513\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0588513\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0588513\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0612621\n",
      "exploration/env_infos/reward_ctrl Std                   0.0282279\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0077485\n",
      "exploration/env_infos/reward_ctrl Min                  -0.230724\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0394878\n",
      "exploration/env_infos/final/torso_velocity Std          0.0557444\n",
      "exploration/env_infos/final/torso_velocity Max          0.117363\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0100669\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.203846\n",
      "exploration/env_infos/initial/torso_velocity Std        0.288858\n",
      "exploration/env_infos/initial/torso_velocity Max        0.578913\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.123872\n",
      "exploration/env_infos/torso_velocity Mean               0.0044318\n",
      "exploration/env_infos/torso_velocity Std                0.225961\n",
      "exploration/env_infos/torso_velocity Max                1.07341\n",
      "exploration/env_infos/torso_velocity Min               -1.27888\n",
      "evaluation/num steps total                         700000\n",
      "evaluation/num paths total                            700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.977338\n",
      "evaluation/Rewards Std                                  0.0336077\n",
      "evaluation/Rewards Max                                  2.1429\n",
      "evaluation/Rewards Min                                  0.831504\n",
      "evaluation/Returns Mean                               977.338\n",
      "evaluation/Returns Std                                  8.60176\n",
      "evaluation/Returns Max                                988.38\n",
      "evaluation/Returns Min                                961.428\n",
      "evaluation/Actions Mean                                 0.010518\n",
      "evaluation/Actions Std                                  0.0774702\n",
      "evaluation/Actions Max                                  0.467157\n",
      "evaluation/Actions Min                                 -0.283465\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            977.338\n",
      "evaluation/env_infos/final/reward_forward Mean          6.93988e-06\n",
      "evaluation/env_infos/final/reward_forward Std           3.0326e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000155193\n",
      "evaluation/env_infos/final/reward_forward Min          -1.52559e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.047877\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0923249\n",
      "evaluation/env_infos/initial/reward_forward Max         0.189194\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.165712\n",
      "evaluation/env_infos/reward_forward Mean                0.000880485\n",
      "evaluation/env_infos/reward_forward Std                 0.0668869\n",
      "evaluation/env_infos/reward_forward Max                 1.5021\n",
      "evaluation/env_infos/reward_forward Min                -1.40407\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0243017\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00854279\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0128969\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0401139\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0313232\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00315006\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0262903\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.037623\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.024449\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00962606\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00209316\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.168496\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.93636e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.50991e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000155193\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000259445\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.174235\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.212675\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.617593\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.289577\n",
      "evaluation/env_infos/torso_velocity Mean               -0.002756\n",
      "evaluation/env_infos/torso_velocity Std                 0.0680847\n",
      "evaluation/env_infos/torso_velocity Max                 1.5021\n",
      "evaluation/env_infos/torso_velocity Min                -1.69621\n",
      "time/data storing (s)                                   0.0155967\n",
      "time/evaluation sampling (s)                           45.9693\n",
      "time/exploration sampling (s)                           1.93442\n",
      "time/logging (s)                                        0.282597\n",
      "time/saving (s)                                         0.0262606\n",
      "time/training (s)                                       4.09598\n",
      "time/epoch (s)                                         52.3241\n",
      "time/total (s)                                       1512.36\n",
      "Epoch                                                  27\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:50:55.318156 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 28 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  30000\n",
      "trainer/QF1 Loss                                        0.219624\n",
      "trainer/QF2 Loss                                        0.207886\n",
      "trainer/Policy Loss                                    -3.01972\n",
      "trainer/Q1 Predictions Mean                             9.97659\n",
      "trainer/Q1 Predictions Std                              1.00062\n",
      "trainer/Q1 Predictions Max                             11.7034\n",
      "trainer/Q1 Predictions Min                              5.23265\n",
      "trainer/Q2 Predictions Mean                            10.0299\n",
      "trainer/Q2 Predictions Std                              0.974408\n",
      "trainer/Q2 Predictions Max                             11.6174\n",
      "trainer/Q2 Predictions Min                              5.91546\n",
      "trainer/Q Targets Mean                                 10.0346\n",
      "trainer/Q Targets Std                                   1.01609\n",
      "trainer/Q Targets Max                                  12.0761\n",
      "trainer/Q Targets Min                                   5.22728\n",
      "trainer/Log Pis Mean                                    7.4846\n",
      "trainer/Log Pis Std                                     2.27235\n",
      "trainer/Log Pis Max                                    12.444\n",
      "trainer/Log Pis Min                                     0.665703\n",
      "trainer/Policy mu Mean                                 -0.0173169\n",
      "trainer/Policy mu Std                                   0.136117\n",
      "trainer/Policy mu Max                                   1.39791\n",
      "trainer/Policy mu Min                                  -0.743234\n",
      "trainer/Policy log std Mean                            -2.32805\n",
      "trainer/Policy log std Std                              0.197435\n",
      "trainer/Policy log std Max                             -1.57434\n",
      "trainer/Policy log std Min                             -3.00989\n",
      "trainer/Alpha                                           0.0160791\n",
      "trainer/Alpha Loss                                     -2.12858\n",
      "exploration/num steps total                         30000\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.93543\n",
      "exploration/Rewards Std                                 0.0506291\n",
      "exploration/Rewards Max                                 1.35483\n",
      "exploration/Rewards Min                                 0.743446\n",
      "exploration/Returns Mean                              935.43\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.43\n",
      "exploration/Returns Min                               935.43\n",
      "exploration/Actions Mean                               -0.0102795\n",
      "exploration/Actions Std                                 0.132436\n",
      "exploration/Actions Max                                 0.377414\n",
      "exploration/Actions Min                                -0.473982\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.43\n",
      "exploration/env_infos/final/reward_forward Mean         0.0129813\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0129813\n",
      "exploration/env_infos/final/reward_forward Min          0.0129813\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0622004\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0622004\n",
      "exploration/env_infos/initial/reward_forward Min        0.0622004\n",
      "exploration/env_infos/reward_forward Mean              -0.00163519\n",
      "exploration/env_infos/reward_forward Std                0.132035\n",
      "exploration/env_infos/reward_forward Max                0.953327\n",
      "exploration/env_infos/reward_forward Min               -0.53477\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0572927\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0572927\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0572927\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0322089\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0322089\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0322089\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0705793\n",
      "exploration/env_infos/reward_ctrl Std                   0.0356898\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00641937\n",
      "exploration/env_infos/reward_ctrl Min                  -0.256554\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.062456\n",
      "exploration/env_infos/final/torso_velocity Std          0.116029\n",
      "exploration/env_infos/final/torso_velocity Max          0.222687\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0483001\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.254426\n",
      "exploration/env_infos/initial/torso_velocity Std        0.154225\n",
      "exploration/env_infos/initial/torso_velocity Max        0.439785\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0622004\n",
      "exploration/env_infos/torso_velocity Mean              -0.00128614\n",
      "exploration/env_infos/torso_velocity Std                0.177509\n",
      "exploration/env_infos/torso_velocity Max                0.953327\n",
      "exploration/env_infos/torso_velocity Min               -0.983854\n",
      "evaluation/num steps total                         725000\n",
      "evaluation/num paths total                            725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969838\n",
      "evaluation/Rewards Std                                  0.0241283\n",
      "evaluation/Rewards Max                                  2.42531\n",
      "evaluation/Rewards Min                                  0.661581\n",
      "evaluation/Returns Mean                               969.838\n",
      "evaluation/Returns Std                                 11.0443\n",
      "evaluation/Returns Max                                984.724\n",
      "evaluation/Returns Min                                950.176\n",
      "evaluation/Actions Mean                                -0.00170602\n",
      "evaluation/Actions Std                                  0.0879397\n",
      "evaluation/Actions Max                                  0.368639\n",
      "evaluation/Actions Min                                 -0.541235\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.838\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.000187043\n",
      "evaluation/env_infos/final/reward_forward Std           0.000828584\n",
      "evaluation/env_infos/final/reward_forward Max           0.000167435\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00422293\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0568186\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122594\n",
      "evaluation/env_infos/initial/reward_forward Max         0.286316\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.22934\n",
      "evaluation/env_infos/reward_forward Mean                0.00329947\n",
      "evaluation/env_infos/reward_forward Std                 0.0564461\n",
      "evaluation/env_infos/reward_forward Max                 1.71412\n",
      "evaluation/env_infos/reward_forward Min                -1.17826\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0307521\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0106682\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0150848\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0493024\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0333572\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0080145\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0223124\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0456072\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0309452\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0127576\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0131671\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.338419\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000104145\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000544816\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000167435\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00422293\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151095\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225435\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.569231\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.236001\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000909074\n",
      "evaluation/env_infos/torso_velocity Std                 0.0563618\n",
      "evaluation/env_infos/torso_velocity Max                 1.71412\n",
      "evaluation/env_infos/torso_velocity Min                -1.68125\n",
      "time/data storing (s)                                   0.0147395\n",
      "time/evaluation sampling (s)                           44.9669\n",
      "time/exploration sampling (s)                           2.03621\n",
      "time/logging (s)                                        0.279729\n",
      "time/saving (s)                                         0.0277073\n",
      "time/training (s)                                       4.32018\n",
      "time/epoch (s)                                         51.6455\n",
      "time/total (s)                                       1564.38\n",
      "Epoch                                                  28\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 10:51:47.395311 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 29 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  31000\n",
      "trainer/QF1 Loss                                        0.501727\n",
      "trainer/QF2 Loss                                        0.443805\n",
      "trainer/Policy Loss                                    -3.64234\n",
      "trainer/Q1 Predictions Mean                            10.3089\n",
      "trainer/Q1 Predictions Std                              0.94257\n",
      "trainer/Q1 Predictions Max                             12.7255\n",
      "trainer/Q1 Predictions Min                              6.67461\n",
      "trainer/Q2 Predictions Mean                            10.3609\n",
      "trainer/Q2 Predictions Std                              0.963431\n",
      "trainer/Q2 Predictions Max                             12.6649\n",
      "trainer/Q2 Predictions Min                              7.1619\n",
      "trainer/Q Targets Mean                                 10.4445\n",
      "trainer/Q Targets Std                                   1.20499\n",
      "trainer/Q Targets Max                                  13.0745\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                    7.10703\n",
      "trainer/Log Pis Std                                     2.62681\n",
      "trainer/Log Pis Max                                    17.0877\n",
      "trainer/Log Pis Min                                    -3.15816\n",
      "trainer/Policy mu Mean                                 -0.0166954\n",
      "trainer/Policy mu Std                                   0.125976\n",
      "trainer/Policy mu Max                                   0.67949\n",
      "trainer/Policy mu Min                                  -0.681235\n",
      "trainer/Policy log std Mean                            -2.31348\n",
      "trainer/Policy log std Std                              0.231168\n",
      "trainer/Policy log std Max                             -1.50015\n",
      "trainer/Policy log std Min                             -4.30742\n",
      "trainer/Alpha                                           0.0155543\n",
      "trainer/Alpha Loss                                     -3.71751\n",
      "exploration/num steps total                         31000\n",
      "exploration/num paths total                            91\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938357\n",
      "exploration/Rewards Std                                 0.0335165\n",
      "exploration/Rewards Max                                 1.12483\n",
      "exploration/Rewards Min                                 0.823096\n",
      "exploration/Returns Mean                              938.357\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.357\n",
      "exploration/Returns Min                               938.357\n",
      "exploration/Actions Mean                               -0.00733295\n",
      "exploration/Actions Std                                 0.127734\n",
      "exploration/Actions Max                                 0.402652\n",
      "exploration/Actions Min                                -0.525252\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.357\n",
      "exploration/env_infos/final/reward_forward Mean         0.0966437\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0966437\n",
      "exploration/env_infos/final/reward_forward Min          0.0966437\n",
      "exploration/env_infos/initial/reward_forward Mean       0.010336\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.010336\n",
      "exploration/env_infos/initial/reward_forward Min        0.010336\n",
      "exploration/env_infos/reward_forward Mean               0.0197112\n",
      "exploration/env_infos/reward_forward Std                0.120361\n",
      "exploration/env_infos/reward_forward Max                1.12837\n",
      "exploration/env_infos/reward_forward Min               -0.320605\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0383154\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0383154\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0383154\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0623941\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0623941\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0623941\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0654788\n",
      "exploration/env_infos/reward_ctrl Std                   0.0273435\n",
      "exploration/env_infos/reward_ctrl Max                  -0.01102\n",
      "exploration/env_infos/reward_ctrl Min                  -0.176904\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0158823\n",
      "exploration/env_infos/final/torso_velocity Std          0.0624852\n",
      "exploration/env_infos/final/torso_velocity Max          0.0966437\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0555595\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.115617\n",
      "exploration/env_infos/initial/torso_velocity Std        0.226746\n",
      "exploration/env_infos/initial/torso_velocity Max        0.43057\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0940538\n",
      "exploration/env_infos/torso_velocity Mean              -0.000400853\n",
      "exploration/env_infos/torso_velocity Std                0.123199\n",
      "exploration/env_infos/torso_velocity Max                1.12837\n",
      "exploration/env_infos/torso_velocity Min               -1.62998\n",
      "evaluation/num steps total                         750000\n",
      "evaluation/num paths total                            750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.969338\n",
      "evaluation/Rewards Std                                  0.02241\n",
      "evaluation/Rewards Max                                  2.03365\n",
      "evaluation/Rewards Min                                  0.696579\n",
      "evaluation/Returns Mean                               969.338\n",
      "evaluation/Returns Std                                 10.1752\n",
      "evaluation/Returns Max                                981.921\n",
      "evaluation/Returns Min                                947.673\n",
      "evaluation/Actions Mean                                -0.00150914\n",
      "evaluation/Actions Std                                  0.0886078\n",
      "evaluation/Actions Max                                  0.331866\n",
      "evaluation/Actions Min                                 -0.460766\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            969.338\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.000363326\n",
      "evaluation/env_infos/final/reward_forward Std           0.00185028\n",
      "evaluation/env_infos/final/reward_forward Max           0.000322237\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00942236\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0356701\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129443\n",
      "evaluation/env_infos/initial/reward_forward Max         0.237219\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.180933\n",
      "evaluation/env_infos/reward_forward Mean                0.00354852\n",
      "evaluation/env_infos/reward_forward Std                 0.0551361\n",
      "evaluation/env_infos/reward_forward Max                 1.34831\n",
      "evaluation/env_infos/reward_forward Min                -1.08296\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0310939\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00990425\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0180825\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0522192\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0286141\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00635052\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0189645\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0469728\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0314145\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0110086\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00948471\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.303421\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.23161e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00109904\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00119654\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00942236\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.166491\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238887\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.642616\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.339071\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00111026\n",
      "evaluation/env_infos/torso_velocity Std                 0.0568436\n",
      "evaluation/env_infos/torso_velocity Max                 1.44944\n",
      "evaluation/env_infos/torso_velocity Min                -1.85064\n",
      "time/data storing (s)                                   0.015114\n",
      "time/evaluation sampling (s)                           45.1287\n",
      "time/exploration sampling (s)                           1.98835\n",
      "time/logging (s)                                        0.295566\n",
      "time/saving (s)                                         0.030329\n",
      "time/training (s)                                       4.25514\n",
      "time/epoch (s)                                         51.7132\n",
      "time/total (s)                                       1616.47\n",
      "Epoch                                                  29\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:52:38.967675 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 30 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  32000\n",
      "trainer/QF1 Loss                                        0.310905\n",
      "trainer/QF2 Loss                                        0.381557\n",
      "trainer/Policy Loss                                    -3.8615\n",
      "trainer/Q1 Predictions Mean                            10.9241\n",
      "trainer/Q1 Predictions Std                              0.859924\n",
      "trainer/Q1 Predictions Max                             13.0505\n",
      "trainer/Q1 Predictions Min                              6.06144\n",
      "trainer/Q2 Predictions Mean                            11.0438\n",
      "trainer/Q2 Predictions Std                              1.05968\n",
      "trainer/Q2 Predictions Max                             13.0071\n",
      "trainer/Q2 Predictions Min                              3.50558\n",
      "trainer/Q Targets Mean                                 10.8419\n",
      "trainer/Q Targets Std                                   1.0947\n",
      "trainer/Q Targets Max                                  12.6351\n",
      "trainer/Q Targets Min                                  -0.794723\n",
      "trainer/Log Pis Mean                                    7.46446\n",
      "trainer/Log Pis Std                                     2.32079\n",
      "trainer/Log Pis Max                                    14.9724\n",
      "trainer/Log Pis Min                                    -0.452629\n",
      "trainer/Policy mu Mean                                 -0.00995733\n",
      "trainer/Policy mu Std                                   0.133517\n",
      "trainer/Policy mu Max                                   2.11009\n",
      "trainer/Policy mu Min                                  -0.963298\n",
      "trainer/Policy log std Mean                            -2.33606\n",
      "trainer/Policy log std Std                              0.193884\n",
      "trainer/Policy log std Max                             -1.40692\n",
      "trainer/Policy log std Min                             -3.37028\n",
      "trainer/Alpha                                           0.0150254\n",
      "trainer/Alpha Loss                                     -2.24806\n",
      "exploration/num steps total                         32000\n",
      "exploration/num paths total                            92\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.959068\n",
      "exploration/Rewards Std                                 0.101549\n",
      "exploration/Rewards Max                                 1.80532\n",
      "exploration/Rewards Min                                 0.477368\n",
      "exploration/Returns Mean                              959.068\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               959.068\n",
      "exploration/Returns Min                               959.068\n",
      "exploration/Actions Mean                               -0.0149957\n",
      "exploration/Actions Std                                 0.130226\n",
      "exploration/Actions Max                                 0.546975\n",
      "exploration/Actions Min                                -0.653182\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           959.068\n",
      "exploration/env_infos/final/reward_forward Mean         0.0896348\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0896348\n",
      "exploration/env_infos/final/reward_forward Min          0.0896348\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0449749\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0449749\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0449749\n",
      "exploration/env_infos/reward_forward Mean              -0.00161074\n",
      "exploration/env_infos/reward_forward Std                0.20277\n",
      "exploration/env_infos/reward_forward Max                0.864209\n",
      "exploration/env_infos/reward_forward Min               -0.753245\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0354196\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0354196\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0354196\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0482567\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0482567\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0482567\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0687346\n",
      "exploration/env_infos/reward_ctrl Std                   0.0361333\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00660176\n",
      "exploration/env_infos/reward_ctrl Min                  -0.522632\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0701198\n",
      "exploration/env_infos/final/torso_velocity Std          0.184334\n",
      "exploration/env_infos/final/torso_velocity Max          0.0896348\n",
      "exploration/env_infos/final/torso_velocity Min         -0.328399\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.173625\n",
      "exploration/env_infos/initial/torso_velocity Std        0.170003\n",
      "exploration/env_infos/initial/torso_velocity Max        0.369598\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0449749\n",
      "exploration/env_infos/torso_velocity Mean              -0.00362925\n",
      "exploration/env_infos/torso_velocity Std                0.179336\n",
      "exploration/env_infos/torso_velocity Max                0.864209\n",
      "exploration/env_infos/torso_velocity Min               -1.21569\n",
      "evaluation/num steps total                         775000\n",
      "evaluation/num paths total                            775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.971395\n",
      "evaluation/Rewards Std                                  0.0262831\n",
      "evaluation/Rewards Max                                  1.82236\n",
      "evaluation/Rewards Min                                  0.475327\n",
      "evaluation/Returns Mean                               971.395\n",
      "evaluation/Returns Std                                 14.6976\n",
      "evaluation/Returns Max                                986.03\n",
      "evaluation/Returns Min                                943.16\n",
      "evaluation/Actions Mean                                -0.00706772\n",
      "evaluation/Actions Std                                  0.085279\n",
      "evaluation/Actions Max                                  0.606668\n",
      "evaluation/Actions Min                                 -0.607968\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            971.395\n",
      "evaluation/env_infos/final/reward_forward Mean         -8.09753e-06\n",
      "evaluation/env_infos/final/reward_forward Std           9.2474e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000251178\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000302865\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0222024\n",
      "evaluation/env_infos/initial/reward_forward Std         0.107626\n",
      "evaluation/env_infos/initial/reward_forward Max         0.231443\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.153302\n",
      "evaluation/env_infos/reward_forward Mean                0.00235351\n",
      "evaluation/env_infos/reward_forward Std                 0.0419124\n",
      "evaluation/env_infos/reward_forward Max                 1.34799\n",
      "evaluation/env_infos/reward_forward Min                -1.06938\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0284792\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0151936\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0130836\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0578543\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0373469\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00892354\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0234025\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0599826\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0292898\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0224619\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00736601\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.524673\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.70381e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           9.82679e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000474172\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000302865\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152065\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221691\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.610386\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.189369\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00143062\n",
      "evaluation/env_infos/torso_velocity Std                 0.0521512\n",
      "evaluation/env_infos/torso_velocity Max                 1.34799\n",
      "evaluation/env_infos/torso_velocity Min                -1.96874\n",
      "time/data storing (s)                                   0.0164889\n",
      "time/evaluation sampling (s)                           44.7046\n",
      "time/exploration sampling (s)                           1.97364\n",
      "time/logging (s)                                        0.274056\n",
      "time/saving (s)                                         0.0254452\n",
      "time/training (s)                                       4.15721\n",
      "time/epoch (s)                                         51.1514\n",
      "time/total (s)                                       1668.02\n",
      "Epoch                                                  30\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:53:30.961906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 31 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  33000\n",
      "trainer/QF1 Loss                                        0.271679\n",
      "trainer/QF2 Loss                                        0.192977\n",
      "trainer/Policy Loss                                    -4.52347\n",
      "trainer/Q1 Predictions Mean                            11.0392\n",
      "trainer/Q1 Predictions Std                              0.914055\n",
      "trainer/Q1 Predictions Max                             13.4984\n",
      "trainer/Q1 Predictions Min                              6.95838\n",
      "trainer/Q2 Predictions Mean                            11.1554\n",
      "trainer/Q2 Predictions Std                              0.891733\n",
      "trainer/Q2 Predictions Max                             12.7228\n",
      "trainer/Q2 Predictions Min                              6.68302\n",
      "trainer/Q Targets Mean                                 11.2105\n",
      "trainer/Q Targets Std                                   0.933602\n",
      "trainer/Q Targets Max                                  15.2193\n",
      "trainer/Q Targets Min                                   6.03492\n",
      "trainer/Log Pis Mean                                    6.90773\n",
      "trainer/Log Pis Std                                     2.28431\n",
      "trainer/Log Pis Max                                    12.5607\n",
      "trainer/Log Pis Min                                    -0.376815\n",
      "trainer/Policy mu Mean                                 -0.0200665\n",
      "trainer/Policy mu Std                                   0.108932\n",
      "trainer/Policy mu Max                                   0.665089\n",
      "trainer/Policy mu Min                                  -0.635904\n",
      "trainer/Policy log std Mean                            -2.25109\n",
      "trainer/Policy log std Std                              0.218917\n",
      "trainer/Policy log std Max                             -1.34629\n",
      "trainer/Policy log std Min                             -3.13235\n",
      "trainer/Alpha                                           0.0141997\n",
      "trainer/Alpha Loss                                     -4.64639\n",
      "exploration/num steps total                         33000\n",
      "exploration/num paths total                            93\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.959052\n",
      "exploration/Rewards Std                                 0.106975\n",
      "exploration/Rewards Max                                 2.18623\n",
      "exploration/Rewards Min                                 0.761635\n",
      "exploration/Returns Mean                              959.052\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               959.052\n",
      "exploration/Returns Min                               959.052\n",
      "exploration/Actions Mean                               -0.0101585\n",
      "exploration/Actions Std                                 0.127432\n",
      "exploration/Actions Max                                 0.373918\n",
      "exploration/Actions Min                                -0.480159\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           959.052\n",
      "exploration/env_infos/final/reward_forward Mean         0.106272\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.106272\n",
      "exploration/env_infos/final/reward_forward Min          0.106272\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0878131\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0878131\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0878131\n",
      "exploration/env_infos/reward_forward Mean               0.024047\n",
      "exploration/env_infos/reward_forward Std                0.210795\n",
      "exploration/env_infos/reward_forward Max                1.31325\n",
      "exploration/env_infos/reward_forward Min               -0.718988\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0309001\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0309001\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0309001\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0689338\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0689338\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0689338\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0653679\n",
      "exploration/env_infos/reward_ctrl Std                   0.0329614\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00694081\n",
      "exploration/env_infos/reward_ctrl Min                  -0.238365\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0137006\n",
      "exploration/env_infos/final/torso_velocity Std          0.106326\n",
      "exploration/env_infos/final/torso_velocity Max          0.106272\n",
      "exploration/env_infos/final/torso_velocity Min         -0.152189\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.162722\n",
      "exploration/env_infos/initial/torso_velocity Std        0.28845\n",
      "exploration/env_infos/initial/torso_velocity Max        0.56679\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0878131\n",
      "exploration/env_infos/torso_velocity Mean              -0.00501841\n",
      "exploration/env_infos/torso_velocity Std                0.227652\n",
      "exploration/env_infos/torso_velocity Max                1.31325\n",
      "exploration/env_infos/torso_velocity Min               -1.47055\n",
      "evaluation/num steps total                         800000\n",
      "evaluation/num paths total                            800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.984566\n",
      "evaluation/Rewards Std                                  0.024932\n",
      "evaluation/Rewards Max                                  2.45487\n",
      "evaluation/Rewards Min                                  0.768856\n",
      "evaluation/Returns Mean                               984.566\n",
      "evaluation/Returns Std                                  2.84119\n",
      "evaluation/Returns Max                                990.096\n",
      "evaluation/Returns Min                                978.267\n",
      "evaluation/Actions Mean                                -0.0112855\n",
      "evaluation/Actions Std                                  0.0639146\n",
      "evaluation/Actions Max                                  0.282765\n",
      "evaluation/Actions Min                                 -0.499486\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            984.566\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.66862e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000122578\n",
      "evaluation/env_infos/final/reward_forward Max           5.92606e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000504898\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0092711\n",
      "evaluation/env_infos/initial/reward_forward Std         0.101481\n",
      "evaluation/env_infos/initial/reward_forward Max         0.266357\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.151548\n",
      "evaluation/env_infos/reward_forward Mean                0.00186222\n",
      "evaluation/env_infos/reward_forward Std                 0.0460246\n",
      "evaluation/env_infos/reward_forward Max                 1.08267\n",
      "evaluation/env_infos/reward_forward Min                -1.32887\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0168137\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00278264\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0101699\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0219446\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0205268\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00631066\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.012908\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0411176\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0168497\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0055718\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00504858\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.231144\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.34637e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000161566\n",
      "evaluation/env_infos/final/torso_velocity Max           7.45144e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000926641\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137914\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235183\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.59616\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.32475\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00190078\n",
      "evaluation/env_infos/torso_velocity Std                 0.0533629\n",
      "evaluation/env_infos/torso_velocity Max                 1.1676\n",
      "evaluation/env_infos/torso_velocity Min                -2.04507\n",
      "time/data storing (s)                                   0.0145993\n",
      "time/evaluation sampling (s)                           45.2532\n",
      "time/exploration sampling (s)                           1.90392\n",
      "time/logging (s)                                        0.271797\n",
      "time/saving (s)                                         0.0280617\n",
      "time/training (s)                                       4.14167\n",
      "time/epoch (s)                                         51.6133\n",
      "time/total (s)                                       1720.01\n",
      "Epoch                                                  31\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:54:22.462423 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 32 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  34000\n",
      "trainer/QF1 Loss                                        0.385701\n",
      "trainer/QF2 Loss                                        0.270914\n",
      "trainer/Policy Loss                                    -3.26208\n",
      "trainer/Q1 Predictions Mean                            11.2398\n",
      "trainer/Q1 Predictions Std                              1.10647\n",
      "trainer/Q1 Predictions Max                             14.7866\n",
      "trainer/Q1 Predictions Min                              4.55917\n",
      "trainer/Q2 Predictions Mean                            11.4448\n",
      "trainer/Q2 Predictions Std                              1.11167\n",
      "trainer/Q2 Predictions Max                             15.0223\n",
      "trainer/Q2 Predictions Min                              3.3625\n",
      "trainer/Q Targets Mean                                 11.5459\n",
      "trainer/Q Targets Std                                   0.941977\n",
      "trainer/Q Targets Max                                  15.9059\n",
      "trainer/Q Targets Min                                   6.16177\n",
      "trainer/Log Pis Mean                                    8.48409\n",
      "trainer/Log Pis Std                                     2.37978\n",
      "trainer/Log Pis Max                                    13.9369\n",
      "trainer/Log Pis Min                                    -4.70293\n",
      "trainer/Policy mu Mean                                  0.00912088\n",
      "trainer/Policy mu Std                                   0.124903\n",
      "trainer/Policy mu Max                                   1.40217\n",
      "trainer/Policy mu Min                                  -0.60767\n",
      "trainer/Policy log std Mean                            -2.44359\n",
      "trainer/Policy log std Std                              0.235468\n",
      "trainer/Policy log std Max                             -1.04282\n",
      "trainer/Policy log std Min                             -3.2398\n",
      "trainer/Alpha                                           0.0134948\n",
      "trainer/Alpha Loss                                      2.08411\n",
      "exploration/num steps total                         34000\n",
      "exploration/num paths total                            94\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.95699\n",
      "exploration/Rewards Std                                 0.0477123\n",
      "exploration/Rewards Max                                 1.24804\n",
      "exploration/Rewards Min                                 0.80163\n",
      "exploration/Returns Mean                              956.99\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               956.99\n",
      "exploration/Returns Min                               956.99\n",
      "exploration/Actions Mean                                0.00938596\n",
      "exploration/Actions Std                                 0.114667\n",
      "exploration/Actions Max                                 0.435798\n",
      "exploration/Actions Min                                -0.431585\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           956.99\n",
      "exploration/env_infos/final/reward_forward Mean         0.0270793\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0270793\n",
      "exploration/env_infos/final/reward_forward Min          0.0270793\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.144235\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.144235\n",
      "exploration/env_infos/initial/reward_forward Min       -0.144235\n",
      "exploration/env_infos/reward_forward Mean               0.0195942\n",
      "exploration/env_infos/reward_forward Std                0.131937\n",
      "exploration/env_infos/reward_forward Max                1.17426\n",
      "exploration/env_infos/reward_forward Min               -0.4476\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0258807\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0258807\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0258807\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0401578\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0401578\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0401578\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0529461\n",
      "exploration/env_infos/reward_ctrl Std                   0.0264548\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00473849\n",
      "exploration/env_infos/reward_ctrl Min                  -0.19837\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0189141\n",
      "exploration/env_infos/final/torso_velocity Std          0.0102064\n",
      "exploration/env_infos/final/torso_velocity Max          0.0270793\n",
      "exploration/env_infos/final/torso_velocity Min          0.00452358\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0693192\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27486\n",
      "exploration/env_infos/initial/torso_velocity Max        0.457375\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.144235\n",
      "exploration/env_infos/torso_velocity Mean               0.00524657\n",
      "exploration/env_infos/torso_velocity Std                0.132948\n",
      "exploration/env_infos/torso_velocity Max                1.17426\n",
      "exploration/env_infos/torso_velocity Min               -1.07504\n",
      "evaluation/num steps total                         825000\n",
      "evaluation/num paths total                            825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.971818\n",
      "evaluation/Rewards Std                                  0.0230815\n",
      "evaluation/Rewards Max                                  2.60583\n",
      "evaluation/Rewards Min                                  0.889587\n",
      "evaluation/Returns Mean                               971.818\n",
      "evaluation/Returns Std                                  7.25479\n",
      "evaluation/Returns Max                                981.557\n",
      "evaluation/Returns Min                                955.887\n",
      "evaluation/Actions Mean                                 0.0178645\n",
      "evaluation/Actions Std                                  0.0833174\n",
      "evaluation/Actions Max                                  0.335435\n",
      "evaluation/Actions Min                                 -0.398948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            971.818\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.12192e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.98432e-06\n",
      "evaluation/env_infos/final/reward_forward Max           7.59434e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.21381e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0208214\n",
      "evaluation/env_infos/initial/reward_forward Std         0.134461\n",
      "evaluation/env_infos/initial/reward_forward Max         0.252233\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.297466\n",
      "evaluation/env_infos/reward_forward Mean                0.00150871\n",
      "evaluation/env_infos/reward_forward Std                 0.0421178\n",
      "evaluation/env_infos/reward_forward Max                 1.5968\n",
      "evaluation/env_infos/reward_forward Min                -1.02135\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0290627\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00712803\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0202988\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0444697\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0301893\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0101647\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0151266\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0507498\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0290437\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00747817\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0127329\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.110413\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.19796e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           1.56157e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           7.59434e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000106926\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134359\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224455\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.62711\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.297466\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00162275\n",
      "evaluation/env_infos/torso_velocity Std                 0.0486256\n",
      "evaluation/env_infos/torso_velocity Max                 1.5968\n",
      "evaluation/env_infos/torso_velocity Min                -1.84972\n",
      "time/data storing (s)                                   0.0148648\n",
      "time/evaluation sampling (s)                           44.7336\n",
      "time/exploration sampling (s)                           1.99213\n",
      "time/logging (s)                                        0.283211\n",
      "time/saving (s)                                         0.026535\n",
      "time/training (s)                                       4.06705\n",
      "time/epoch (s)                                         51.1174\n",
      "time/total (s)                                       1771.52\n",
      "Epoch                                                  32\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 10:55:15.141117 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 33 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  35000\n",
      "trainer/QF1 Loss                                        0.197389\n",
      "trainer/QF2 Loss                                        0.160454\n",
      "trainer/Policy Loss                                    -4.92208\n",
      "trainer/Q1 Predictions Mean                            12.1506\n",
      "trainer/Q1 Predictions Std                              0.935162\n",
      "trainer/Q1 Predictions Max                             13.9756\n",
      "trainer/Q1 Predictions Min                              7.79163\n",
      "trainer/Q2 Predictions Mean                            11.9277\n",
      "trainer/Q2 Predictions Std                              0.903289\n",
      "trainer/Q2 Predictions Max                             13.9878\n",
      "trainer/Q2 Predictions Min                              8.18362\n",
      "trainer/Q Targets Mean                                 11.9492\n",
      "trainer/Q Targets Std                                   0.974144\n",
      "trainer/Q Targets Max                                  14.1139\n",
      "trainer/Q Targets Min                                   8.1842\n",
      "trainer/Log Pis Mean                                    7.38247\n",
      "trainer/Log Pis Std                                     2.0406\n",
      "trainer/Log Pis Max                                    12.4978\n",
      "trainer/Log Pis Min                                     0.536787\n",
      "trainer/Policy mu Mean                                  0.00416415\n",
      "trainer/Policy mu Std                                   0.106425\n",
      "trainer/Policy mu Max                                   1.49937\n",
      "trainer/Policy mu Min                                  -0.574755\n",
      "trainer/Policy log std Mean                            -2.31468\n",
      "trainer/Policy log std Std                              0.205794\n",
      "trainer/Policy log std Max                             -1.56851\n",
      "trainer/Policy log std Min                             -2.99524\n",
      "trainer/Alpha                                           0.0130957\n",
      "trainer/Alpha Loss                                     -2.67711\n",
      "exploration/num steps total                         35000\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.971548\n",
      "exploration/Rewards Std                                 0.0865019\n",
      "exploration/Rewards Max                                 1.67271\n",
      "exploration/Rewards Min                                 0.708849\n",
      "exploration/Returns Mean                              971.548\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               971.548\n",
      "exploration/Returns Min                               971.548\n",
      "exploration/Actions Mean                               -0.00692708\n",
      "exploration/Actions Std                                 0.114469\n",
      "exploration/Actions Max                                 0.548211\n",
      "exploration/Actions Min                                -0.400109\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           971.548\n",
      "exploration/env_infos/final/reward_forward Mean         0.07642\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.07642\n",
      "exploration/env_infos/final/reward_forward Min          0.07642\n",
      "exploration/env_infos/initial/reward_forward Mean       0.144779\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.144779\n",
      "exploration/env_infos/initial/reward_forward Min        0.144779\n",
      "exploration/env_infos/reward_forward Mean              -0.00739821\n",
      "exploration/env_infos/reward_forward Std                0.153857\n",
      "exploration/env_infos/reward_forward Max                0.54201\n",
      "exploration/env_infos/reward_forward Min               -0.532588\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.113505\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.113505\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.113505\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0544922\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0544922\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0544922\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0526048\n",
      "exploration/env_infos/reward_ctrl Std                   0.0289018\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00384864\n",
      "exploration/env_infos/reward_ctrl Min                  -0.291151\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0417148\n",
      "exploration/env_infos/final/torso_velocity Std          0.025533\n",
      "exploration/env_infos/final/torso_velocity Max          0.07642\n",
      "exploration/env_infos/final/torso_velocity Min          0.0157269\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.153621\n",
      "exploration/env_infos/initial/torso_velocity Std        0.221268\n",
      "exploration/env_infos/initial/torso_velocity Max        0.42893\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.112847\n",
      "exploration/env_infos/torso_velocity Mean               0.0027569\n",
      "exploration/env_infos/torso_velocity Std                0.180989\n",
      "exploration/env_infos/torso_velocity Max                1.26183\n",
      "exploration/env_infos/torso_velocity Min               -0.994275\n",
      "evaluation/num steps total                         850000\n",
      "evaluation/num paths total                            850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.981709\n",
      "evaluation/Rewards Std                                  0.0302813\n",
      "evaluation/Rewards Max                                  2.00424\n",
      "evaluation/Rewards Min                                  0.772161\n",
      "evaluation/Returns Mean                               981.709\n",
      "evaluation/Returns Std                                  6.07089\n",
      "evaluation/Returns Max                                990.595\n",
      "evaluation/Returns Min                                969.24\n",
      "evaluation/Actions Mean                                 0.0118827\n",
      "evaluation/Actions Std                                  0.0701341\n",
      "evaluation/Actions Max                                  0.496285\n",
      "evaluation/Actions Min                                 -0.468331\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            981.709\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000612017\n",
      "evaluation/env_infos/final/reward_forward Std           0.00299361\n",
      "evaluation/env_infos/final/reward_forward Max           0.0152601\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000498939\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00933379\n",
      "evaluation/env_infos/initial/reward_forward Std         0.146249\n",
      "evaluation/env_infos/initial/reward_forward Max         0.330847\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.336109\n",
      "evaluation/env_infos/reward_forward Mean                0.00227092\n",
      "evaluation/env_infos/reward_forward Std                 0.0550414\n",
      "evaluation/env_infos/reward_forward Max                 1.46057\n",
      "evaluation/env_infos/reward_forward Min                -0.978054\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0198301\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00631218\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.013589\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0328681\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0254871\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00769206\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.01259\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0427098\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.02024\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00916568\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00641804\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.227839\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00108711\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00625865\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0502551\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00134921\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.13657\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.24432\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.600634\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.336109\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00070454\n",
      "evaluation/env_infos/torso_velocity Std                 0.0601449\n",
      "evaluation/env_infos/torso_velocity Max                 1.46057\n",
      "evaluation/env_infos/torso_velocity Min                -1.91451\n",
      "time/data storing (s)                                   0.0154669\n",
      "time/evaluation sampling (s)                           45.3071\n",
      "time/exploration sampling (s)                           2.13475\n",
      "time/logging (s)                                        0.281581\n",
      "time/saving (s)                                         0.0271441\n",
      "time/training (s)                                       4.48911\n",
      "time/epoch (s)                                         52.2551\n",
      "time/total (s)                                       1824.2\n",
      "Epoch                                                  33\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:56:07.950432 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 34 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  36000\n",
      "trainer/QF1 Loss                                        0.163239\n",
      "trainer/QF2 Loss                                        0.149574\n",
      "trainer/Policy Loss                                    -5.11703\n",
      "trainer/Q1 Predictions Mean                            12.3293\n",
      "trainer/Q1 Predictions Std                              0.955566\n",
      "trainer/Q1 Predictions Max                             13.6262\n",
      "trainer/Q1 Predictions Min                              6.89377\n",
      "trainer/Q2 Predictions Mean                            12.3884\n",
      "trainer/Q2 Predictions Std                              0.957502\n",
      "trainer/Q2 Predictions Max                             14.4344\n",
      "trainer/Q2 Predictions Min                              8.68914\n",
      "trainer/Q Targets Mean                                 12.2577\n",
      "trainer/Q Targets Std                                   0.965272\n",
      "trainer/Q Targets Max                                  13.9811\n",
      "trainer/Q Targets Min                                   8.52567\n",
      "trainer/Log Pis Mean                                    7.59005\n",
      "trainer/Log Pis Std                                     2.02156\n",
      "trainer/Log Pis Max                                    13.1968\n",
      "trainer/Log Pis Min                                     1.40826\n",
      "trainer/Policy mu Mean                                 -0.00116755\n",
      "trainer/Policy mu Std                                   0.115584\n",
      "trainer/Policy mu Max                                   0.605556\n",
      "trainer/Policy mu Min                                  -1.01715\n",
      "trainer/Policy log std Mean                            -2.32219\n",
      "trainer/Policy log std Std                              0.206005\n",
      "trainer/Policy log std Max                             -1.5975\n",
      "trainer/Policy log std Min                             -3.08814\n",
      "trainer/Alpha                                           0.0126879\n",
      "trainer/Alpha Loss                                     -1.79025\n",
      "exploration/num steps total                         36000\n",
      "exploration/num paths total                            96\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.923816\n",
      "exploration/Rewards Std                                 0.0720577\n",
      "exploration/Rewards Max                                 1.48903\n",
      "exploration/Rewards Min                                 0.743022\n",
      "exploration/Returns Mean                              923.816\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               923.816\n",
      "exploration/Returns Min                               923.816\n",
      "exploration/Actions Mean                                0.0255599\n",
      "exploration/Actions Std                                 0.145561\n",
      "exploration/Actions Max                                 0.501888\n",
      "exploration/Actions Min                                -0.525781\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           923.816\n",
      "exploration/env_infos/final/reward_forward Mean         0.0934746\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0934746\n",
      "exploration/env_infos/final/reward_forward Min          0.0934746\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0123651\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0123651\n",
      "exploration/env_infos/initial/reward_forward Min        0.0123651\n",
      "exploration/env_infos/reward_forward Mean               0.0195184\n",
      "exploration/env_infos/reward_forward Std                0.142271\n",
      "exploration/env_infos/reward_forward Max                1.67611\n",
      "exploration/env_infos/reward_forward Min               -0.643003\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0618502\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0618502\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0618502\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0782429\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0782429\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0782429\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0873652\n",
      "exploration/env_infos/reward_ctrl Std                   0.0406547\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00479188\n",
      "exploration/env_infos/reward_ctrl Min                  -0.274224\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0523003\n",
      "exploration/env_infos/final/torso_velocity Std          0.0302473\n",
      "exploration/env_infos/final/torso_velocity Max          0.0934746\n",
      "exploration/env_infos/final/torso_velocity Min          0.0216704\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0819537\n",
      "exploration/env_infos/initial/torso_velocity Std        0.20064\n",
      "exploration/env_infos/initial/torso_velocity Max        0.354976\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.12148\n",
      "exploration/env_infos/torso_velocity Mean              -0.000711531\n",
      "exploration/env_infos/torso_velocity Std                0.161539\n",
      "exploration/env_infos/torso_velocity Max                1.67611\n",
      "exploration/env_infos/torso_velocity Min               -1.02279\n",
      "evaluation/num steps total                         875000\n",
      "evaluation/num paths total                            875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.959457\n",
      "evaluation/Rewards Std                                  0.0290639\n",
      "evaluation/Rewards Max                                  2.25498\n",
      "evaluation/Rewards Min                                  0.851306\n",
      "evaluation/Returns Mean                               959.457\n",
      "evaluation/Returns Std                                 12.68\n",
      "evaluation/Returns Max                                981.109\n",
      "evaluation/Returns Min                                941.505\n",
      "evaluation/Actions Mean                                -0.00536177\n",
      "evaluation/Actions Std                                  0.101849\n",
      "evaluation/Actions Max                                  0.369905\n",
      "evaluation/Actions Min                                 -0.346013\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            959.457\n",
      "evaluation/env_infos/final/reward_forward Mean          3.40006e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.532e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.29806e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.02025e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0155994\n",
      "evaluation/env_infos/initial/reward_forward Std         0.148303\n",
      "evaluation/env_infos/initial/reward_forward Max         0.294456\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.253687\n",
      "evaluation/env_infos/reward_forward Mean                7.80952e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0576752\n",
      "evaluation/env_infos/reward_forward Max                 1.4042\n",
      "evaluation/env_infos/reward_forward Min                -1.33688\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0414784\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0128902\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0210189\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0631852\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0544024\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.013055\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0254707\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0800809\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0416079\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0135008\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00950912\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.148694\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.91067e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.9453e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.14662e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.27953e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14051\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244306\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.588916\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.253687\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000862253\n",
      "evaluation/env_infos/torso_velocity Std                 0.0643986\n",
      "evaluation/env_infos/torso_velocity Max                 1.4042\n",
      "evaluation/env_infos/torso_velocity Min                -1.80913\n",
      "time/data storing (s)                                   0.0161937\n",
      "time/evaluation sampling (s)                           45.5033\n",
      "time/exploration sampling (s)                           2.14834\n",
      "time/logging (s)                                        0.284491\n",
      "time/saving (s)                                         0.028609\n",
      "time/training (s)                                       4.41695\n",
      "time/epoch (s)                                         52.3979\n",
      "time/total (s)                                       1877.01\n",
      "Epoch                                                  34\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:56:59.614965 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 35 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  37000\n",
      "trainer/QF1 Loss                                        0.178634\n",
      "trainer/QF2 Loss                                        0.213673\n",
      "trainer/Policy Loss                                    -5.52708\n",
      "trainer/Q1 Predictions Mean                            12.9074\n",
      "trainer/Q1 Predictions Std                              0.947254\n",
      "trainer/Q1 Predictions Max                             18.4524\n",
      "trainer/Q1 Predictions Min                              8.86015\n",
      "trainer/Q2 Predictions Mean                            12.7586\n",
      "trainer/Q2 Predictions Std                              0.962607\n",
      "trainer/Q2 Predictions Max                             17.5217\n",
      "trainer/Q2 Predictions Min                              8.50187\n",
      "trainer/Q Targets Mean                                 12.7766\n",
      "trainer/Q Targets Std                                   1.03758\n",
      "trainer/Q Targets Max                                  20.9701\n",
      "trainer/Q Targets Min                                   9.11806\n",
      "trainer/Log Pis Mean                                    7.612\n",
      "trainer/Log Pis Std                                     2.48768\n",
      "trainer/Log Pis Max                                    14.2787\n",
      "trainer/Log Pis Min                                    -1.61548\n",
      "trainer/Policy mu Mean                                  0.00824234\n",
      "trainer/Policy mu Std                                   0.117481\n",
      "trainer/Policy mu Max                                   1.87924\n",
      "trainer/Policy mu Min                                  -0.549072\n",
      "trainer/Policy log std Mean                            -2.33967\n",
      "trainer/Policy log std Std                              0.234471\n",
      "trainer/Policy log std Max                             -1.26849\n",
      "trainer/Policy log std Min                             -3.24415\n",
      "trainer/Alpha                                           0.0123415\n",
      "trainer/Alpha Loss                                     -1.70495\n",
      "exploration/num steps total                         37000\n",
      "exploration/num paths total                            97\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.946202\n",
      "exploration/Rewards Std                                 0.0879842\n",
      "exploration/Rewards Max                                 1.70244\n",
      "exploration/Rewards Min                                 0.735406\n",
      "exploration/Returns Mean                              946.202\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               946.202\n",
      "exploration/Returns Min                               946.202\n",
      "exploration/Actions Mean                                0.0107233\n",
      "exploration/Actions Std                                 0.134765\n",
      "exploration/Actions Max                                 0.521983\n",
      "exploration/Actions Min                                -0.493371\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           946.202\n",
      "exploration/env_infos/final/reward_forward Mean         0.153267\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.153267\n",
      "exploration/env_infos/final/reward_forward Min          0.153267\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.207471\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.207471\n",
      "exploration/env_infos/initial/reward_forward Min       -0.207471\n",
      "exploration/env_infos/reward_forward Mean               0.00266672\n",
      "exploration/env_infos/reward_forward Std                0.171783\n",
      "exploration/env_infos/reward_forward Max                0.688105\n",
      "exploration/env_infos/reward_forward Min               -0.93857\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0267611\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0267611\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0267611\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0807425\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0807425\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0807425\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0731065\n",
      "exploration/env_infos/reward_ctrl Std                   0.0357913\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00797478\n",
      "exploration/env_infos/reward_ctrl Min                  -0.264594\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.209678\n",
      "exploration/env_infos/final/torso_velocity Std          0.20529\n",
      "exploration/env_infos/final/torso_velocity Max          0.484519\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00875242\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0165727\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254413\n",
      "exploration/env_infos/initial/torso_velocity Max        0.372403\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.207471\n",
      "exploration/env_infos/torso_velocity Mean              -0.00717857\n",
      "exploration/env_infos/torso_velocity Std                0.216114\n",
      "exploration/env_infos/torso_velocity Max                0.930737\n",
      "exploration/env_infos/torso_velocity Min               -1.08666\n",
      "evaluation/num steps total                         900000\n",
      "evaluation/num paths total                            900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.955322\n",
      "evaluation/Rewards Std                                  0.0370804\n",
      "evaluation/Rewards Max                                  2.14576\n",
      "evaluation/Rewards Min                                  0.845381\n",
      "evaluation/Returns Mean                               955.322\n",
      "evaluation/Returns Std                                 28.3522\n",
      "evaluation/Returns Max                                987.049\n",
      "evaluation/Returns Min                                915.014\n",
      "evaluation/Actions Mean                                 0.00702591\n",
      "evaluation/Actions Std                                  0.106691\n",
      "evaluation/Actions Max                                  0.500472\n",
      "evaluation/Actions Min                                 -0.271908\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            955.322\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.09796e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.56372e-05\n",
      "evaluation/env_infos/final/reward_forward Max           8.0193e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000130683\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0397514\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121646\n",
      "evaluation/env_infos/initial/reward_forward Max         0.266248\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.125125\n",
      "evaluation/env_infos/reward_forward Mean               -0.00174472\n",
      "evaluation/env_infos/reward_forward Std                 0.0566358\n",
      "evaluation/env_infos/reward_forward Max                 1.40272\n",
      "evaluation/env_infos/reward_forward Min                -1.51948\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0457444\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0283007\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.016279\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0850234\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0537228\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0133272\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0299086\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0914239\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0457295\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0284612\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0090015\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.154619\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.46783e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           4.32442e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000350414\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000130683\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151084\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221895\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.579164\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.282293\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00227187\n",
      "evaluation/env_infos/torso_velocity Std                 0.064434\n",
      "evaluation/env_infos/torso_velocity Max                 1.40272\n",
      "evaluation/env_infos/torso_velocity Min                -1.77725\n",
      "time/data storing (s)                                   0.0207614\n",
      "time/evaluation sampling (s)                           44.7725\n",
      "time/exploration sampling (s)                           1.94135\n",
      "time/logging (s)                                        0.278892\n",
      "time/saving (s)                                         0.026007\n",
      "time/training (s)                                       4.1945\n",
      "time/epoch (s)                                         51.234\n",
      "time/total (s)                                       1928.67\n",
      "Epoch                                                  35\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:57:53.121314 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 36 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  38000\n",
      "trainer/QF1 Loss                                        0.174573\n",
      "trainer/QF2 Loss                                        0.208717\n",
      "trainer/Policy Loss                                    -5.38856\n",
      "trainer/Q1 Predictions Mean                            12.8397\n",
      "trainer/Q1 Predictions Std                              1.00596\n",
      "trainer/Q1 Predictions Max                             14.5226\n",
      "trainer/Q1 Predictions Min                              7.70689\n",
      "trainer/Q2 Predictions Mean                            12.9778\n",
      "trainer/Q2 Predictions Std                              1.01968\n",
      "trainer/Q2 Predictions Max                             14.9523\n",
      "trainer/Q2 Predictions Min                              6.19938\n",
      "trainer/Q Targets Mean                                 12.8141\n",
      "trainer/Q Targets Std                                   0.955072\n",
      "trainer/Q Targets Max                                  14.7898\n",
      "trainer/Q Targets Min                                   8.78935\n",
      "trainer/Log Pis Mean                                    7.88387\n",
      "trainer/Log Pis Std                                     2.40455\n",
      "trainer/Log Pis Max                                    12.9633\n",
      "trainer/Log Pis Min                                    -2.52887\n",
      "trainer/Policy mu Mean                                 -0.0244252\n",
      "trainer/Policy mu Std                                   0.151147\n",
      "trainer/Policy mu Max                                   1.66571\n",
      "trainer/Policy mu Min                                  -0.704207\n",
      "trainer/Policy log std Mean                            -2.36267\n",
      "trainer/Policy log std Std                              0.238771\n",
      "trainer/Policy log std Max                             -1.33186\n",
      "trainer/Policy log std Min                             -3.19031\n",
      "trainer/Alpha                                           0.0120889\n",
      "trainer/Alpha Loss                                     -0.512742\n",
      "exploration/num steps total                         38000\n",
      "exploration/num paths total                            98\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.902521\n",
      "exploration/Rewards Std                                 0.0646502\n",
      "exploration/Rewards Max                                 1.42489\n",
      "exploration/Rewards Min                                 0.742118\n",
      "exploration/Returns Mean                              902.521\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               902.521\n",
      "exploration/Returns Min                               902.521\n",
      "exploration/Actions Mean                               -0.0201214\n",
      "exploration/Actions Std                                 0.164277\n",
      "exploration/Actions Max                                 0.500276\n",
      "exploration/Actions Min                                -0.458042\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           902.521\n",
      "exploration/env_infos/final/reward_forward Mean        -0.137461\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.137461\n",
      "exploration/env_infos/final/reward_forward Min         -0.137461\n",
      "exploration/env_infos/initial/reward_forward Mean       0.051602\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.051602\n",
      "exploration/env_infos/initial/reward_forward Min        0.051602\n",
      "exploration/env_infos/reward_forward Mean              -0.00633369\n",
      "exploration/env_infos/reward_forward Std                0.206806\n",
      "exploration/env_infos/reward_forward Max                1.0619\n",
      "exploration/env_infos/reward_forward Min               -0.694993\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.095557\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.095557\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.095557\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.161904\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.161904\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.161904\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109568\n",
      "exploration/env_infos/reward_ctrl Std                   0.0398396\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0227663\n",
      "exploration/env_infos/reward_ctrl Min                  -0.257882\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0169474\n",
      "exploration/env_infos/final/torso_velocity Std          0.0890203\n",
      "exploration/env_infos/final/torso_velocity Max          0.0748422\n",
      "exploration/env_infos/final/torso_velocity Min         -0.137461\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146577\n",
      "exploration/env_infos/initial/torso_velocity Std        0.177016\n",
      "exploration/env_infos/initial/torso_velocity Max        0.394656\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00652638\n",
      "exploration/env_infos/torso_velocity Mean               0.00510871\n",
      "exploration/env_infos/torso_velocity Std                0.188392\n",
      "exploration/env_infos/torso_velocity Max                1.50036\n",
      "exploration/env_infos/torso_velocity Min               -0.890169\n",
      "evaluation/num steps total                         925000\n",
      "evaluation/num paths total                            925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.916343\n",
      "evaluation/Rewards Std                                  0.0482332\n",
      "evaluation/Rewards Max                                  1.98284\n",
      "evaluation/Rewards Min                                 -0.821536\n",
      "evaluation/Returns Mean                               916.343\n",
      "evaluation/Returns Std                                 37.5229\n",
      "evaluation/Returns Max                                978.395\n",
      "evaluation/Returns Min                                869.52\n",
      "evaluation/Actions Mean                                -0.0247471\n",
      "evaluation/Actions Std                                  0.143561\n",
      "evaluation/Actions Max                                  0.943393\n",
      "evaluation/Actions Min                                 -0.695523\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            916.343\n",
      "evaluation/env_infos/final/reward_forward Mean          6.9001e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.64704e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.11092e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.378e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00446997\n",
      "evaluation/env_infos/initial/reward_forward Std         0.143665\n",
      "evaluation/env_infos/initial/reward_forward Max         0.273502\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.281665\n",
      "evaluation/env_infos/reward_forward Mean               -0.00199112\n",
      "evaluation/env_infos/reward_forward Std                 0.0579376\n",
      "evaluation/env_infos/reward_forward Max                 1.20191\n",
      "evaluation/env_infos/reward_forward Min                -1.38419\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0843532\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.036531\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0223927\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.132087\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.135007\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0874726\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0464425\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.369643\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0848887\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0422372\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0109896\n",
      "evaluation/env_infos/reward_ctrl Min                   -1.82154\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.62957e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.52302e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.71946e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.04072e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127847\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.223251\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.584124\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.281665\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00203106\n",
      "evaluation/env_infos/torso_velocity Std                 0.0609919\n",
      "evaluation/env_infos/torso_velocity Max                 1.54933\n",
      "evaluation/env_infos/torso_velocity Min                -1.83847\n",
      "time/data storing (s)                                   0.0151559\n",
      "time/evaluation sampling (s)                           46.2604\n",
      "time/exploration sampling (s)                           2.0335\n",
      "time/logging (s)                                        0.280544\n",
      "time/saving (s)                                         0.0252344\n",
      "time/training (s)                                       4.46309\n",
      "time/epoch (s)                                         53.0779\n",
      "time/total (s)                                       1982.17\n",
      "Epoch                                                  36\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:58:45.269787 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 37 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  39000\n",
      "trainer/QF1 Loss                                        0.229121\n",
      "trainer/QF2 Loss                                        0.222845\n",
      "trainer/Policy Loss                                    -5.67709\n",
      "trainer/Q1 Predictions Mean                            13.372\n",
      "trainer/Q1 Predictions Std                              1.07132\n",
      "trainer/Q1 Predictions Max                             15.7997\n",
      "trainer/Q1 Predictions Min                              8.2093\n",
      "trainer/Q2 Predictions Mean                            13.3844\n",
      "trainer/Q2 Predictions Std                              0.994205\n",
      "trainer/Q2 Predictions Max                             15.591\n",
      "trainer/Q2 Predictions Min                              9.8773\n",
      "trainer/Q Targets Mean                                 13.3095\n",
      "trainer/Q Targets Std                                   1.06523\n",
      "trainer/Q Targets Max                                  15.351\n",
      "trainer/Q Targets Min                                   9.50608\n",
      "trainer/Log Pis Mean                                    8.0515\n",
      "trainer/Log Pis Std                                     2.57612\n",
      "trainer/Log Pis Max                                    14.2873\n",
      "trainer/Log Pis Min                                    -0.236129\n",
      "trainer/Policy mu Mean                                  0.0112567\n",
      "trainer/Policy mu Std                                   0.161034\n",
      "trainer/Policy mu Max                                   2.21256\n",
      "trainer/Policy mu Min                                  -0.617108\n",
      "trainer/Policy log std Mean                            -2.4042\n",
      "trainer/Policy log std Std                              0.247988\n",
      "trainer/Policy log std Max                             -0.473922\n",
      "trainer/Policy log std Min                             -3.13446\n",
      "trainer/Alpha                                           0.0116795\n",
      "trainer/Alpha Loss                                      0.22915\n",
      "exploration/num steps total                         39000\n",
      "exploration/num paths total                            99\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.940157\n",
      "exploration/Rewards Std                                 0.10371\n",
      "exploration/Rewards Max                                 1.83813\n",
      "exploration/Rewards Min                                 0.758156\n",
      "exploration/Returns Mean                              940.157\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               940.157\n",
      "exploration/Returns Min                               940.157\n",
      "exploration/Actions Mean                                0.0153154\n",
      "exploration/Actions Std                                 0.144806\n",
      "exploration/Actions Max                                 0.559611\n",
      "exploration/Actions Min                                -0.429928\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           940.157\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00879076\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00879076\n",
      "exploration/env_infos/final/reward_forward Min         -0.00879076\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.245971\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.245971\n",
      "exploration/env_infos/initial/reward_forward Min       -0.245971\n",
      "exploration/env_infos/reward_forward Mean               0.0321005\n",
      "exploration/env_infos/reward_forward Std                0.224511\n",
      "exploration/env_infos/reward_forward Max                1.24256\n",
      "exploration/env_infos/reward_forward Min               -0.92319\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0920648\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0920648\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0920648\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0946875\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0946875\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0946875\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0848136\n",
      "exploration/env_infos/reward_ctrl Std                   0.030377\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0123974\n",
      "exploration/env_infos/reward_ctrl Min                  -0.241844\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.129863\n",
      "exploration/env_infos/final/torso_velocity Std          0.32693\n",
      "exploration/env_infos/final/torso_velocity Max          0.581167\n",
      "exploration/env_infos/final/torso_velocity Min         -0.182786\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.088094\n",
      "exploration/env_infos/initial/torso_velocity Std        0.25206\n",
      "exploration/env_infos/initial/torso_velocity Max        0.362839\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.245971\n",
      "exploration/env_infos/torso_velocity Mean               0.00471684\n",
      "exploration/env_infos/torso_velocity Std                0.175141\n",
      "exploration/env_infos/torso_velocity Max                1.24256\n",
      "exploration/env_infos/torso_velocity Min               -1.15681\n",
      "evaluation/num steps total                         950000\n",
      "evaluation/num paths total                            950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.963992\n",
      "evaluation/Rewards Std                                  0.0308181\n",
      "evaluation/Rewards Max                                  2.231\n",
      "evaluation/Rewards Min                                  0.730357\n",
      "evaluation/Returns Mean                               963.992\n",
      "evaluation/Returns Std                                 20.9907\n",
      "evaluation/Returns Max                                985.633\n",
      "evaluation/Returns Min                                916.816\n",
      "evaluation/Actions Mean                                 0.0151386\n",
      "evaluation/Actions Std                                  0.0951027\n",
      "evaluation/Actions Max                                  0.571903\n",
      "evaluation/Actions Min                                 -0.465596\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            963.992\n",
      "evaluation/env_infos/final/reward_forward Mean          1.45064e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.52715e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.9738e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.07329e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.017504\n",
      "evaluation/env_infos/initial/reward_forward Std         0.101078\n",
      "evaluation/env_infos/initial/reward_forward Max         0.186111\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.179022\n",
      "evaluation/env_infos/reward_forward Mean                0.00322719\n",
      "evaluation/env_infos/reward_forward Std                 0.0564368\n",
      "evaluation/env_infos/reward_forward Max                 1.61293\n",
      "evaluation/env_infos/reward_forward Min                -1.28735\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.036905\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0212123\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0150661\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0837528\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0829206\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0538872\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0387717\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.269643\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0370948\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0221267\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0110147\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.269643\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.05097e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.94816e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.9738e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.07329e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.140463\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224665\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.583518\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.259966\n",
      "evaluation/env_infos/torso_velocity Mean               -2.24015e-05\n",
      "evaluation/env_infos/torso_velocity Std                 0.0557955\n",
      "evaluation/env_infos/torso_velocity Max                 1.61293\n",
      "evaluation/env_infos/torso_velocity Min                -1.87195\n",
      "time/data storing (s)                                   0.0148405\n",
      "time/evaluation sampling (s)                           45.1372\n",
      "time/exploration sampling (s)                           2.03851\n",
      "time/logging (s)                                        0.268068\n",
      "time/saving (s)                                         0.0253401\n",
      "time/training (s)                                       4.23347\n",
      "time/epoch (s)                                         51.7175\n",
      "time/total (s)                                       2034.31\n",
      "Epoch                                                  37\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 10:59:37.844383 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 38 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  40000\n",
      "trainer/QF1 Loss                                        0.455778\n",
      "trainer/QF2 Loss                                        0.424364\n",
      "trainer/Policy Loss                                    -6.37814\n",
      "trainer/Q1 Predictions Mean                            13.742\n",
      "trainer/Q1 Predictions Std                              1.49159\n",
      "trainer/Q1 Predictions Max                             28.1839\n",
      "trainer/Q1 Predictions Min                              7.57311\n",
      "trainer/Q2 Predictions Mean                            13.8386\n",
      "trainer/Q2 Predictions Std                              1.50845\n",
      "trainer/Q2 Predictions Max                             28.648\n",
      "trainer/Q2 Predictions Min                              7.5071\n",
      "trainer/Q Targets Mean                                 13.7698\n",
      "trainer/Q Targets Std                                   1.49951\n",
      "trainer/Q Targets Max                                  27.0996\n",
      "trainer/Q Targets Min                                   7.54557\n",
      "trainer/Log Pis Mean                                    7.80454\n",
      "trainer/Log Pis Std                                     5.50205\n",
      "trainer/Log Pis Max                                    73.3793\n",
      "trainer/Log Pis Min                                    -1.31972\n",
      "trainer/Policy mu Mean                                  0.0136358\n",
      "trainer/Policy mu Std                                   0.452453\n",
      "trainer/Policy mu Max                                   9.93786\n",
      "trainer/Policy mu Min                                  -3.67769\n",
      "trainer/Policy log std Mean                            -2.32487\n",
      "trainer/Policy log std Std                              0.288969\n",
      "trainer/Policy log std Max                             -0.778433\n",
      "trainer/Policy log std Min                             -4.95263\n",
      "trainer/Alpha                                           0.0116525\n",
      "trainer/Alpha Loss                                     -0.870147\n",
      "exploration/num steps total                         40000\n",
      "exploration/num paths total                           100\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.907465\n",
      "exploration/Rewards Std                                 0.047396\n",
      "exploration/Rewards Max                                 1.52628\n",
      "exploration/Rewards Min                                 0.715073\n",
      "exploration/Returns Mean                              907.465\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               907.465\n",
      "exploration/Returns Min                               907.465\n",
      "exploration/Actions Mean                               -0.0479953\n",
      "exploration/Actions Std                                 0.146496\n",
      "exploration/Actions Max                                 0.614212\n",
      "exploration/Actions Min                                -0.616192\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           907.465\n",
      "exploration/env_infos/final/reward_forward Mean        -0.000753051\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.000753051\n",
      "exploration/env_infos/final/reward_forward Min         -0.000753051\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0701264\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0701264\n",
      "exploration/env_infos/initial/reward_forward Min        0.0701264\n",
      "exploration/env_infos/reward_forward Mean              -0.0035303\n",
      "exploration/env_infos/reward_forward Std                0.0860138\n",
      "exploration/env_infos/reward_forward Max                0.786909\n",
      "exploration/env_infos/reward_forward Min               -0.371505\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.110075\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.110075\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.110075\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.153992\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.153992\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.153992\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0950588\n",
      "exploration/env_infos/reward_ctrl Std                   0.0365161\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00984817\n",
      "exploration/env_infos/reward_ctrl Min                  -0.284927\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.000325675\n",
      "exploration/env_infos/final/torso_velocity Std          0.00133532\n",
      "exploration/env_infos/final/torso_velocity Max          0.00220738\n",
      "exploration/env_infos/final/torso_velocity Min         -0.000753051\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.128991\n",
      "exploration/env_infos/initial/torso_velocity Std        0.138038\n",
      "exploration/env_infos/initial/torso_velocity Max        0.319616\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0027689\n",
      "exploration/env_infos/torso_velocity Mean              -0.00307387\n",
      "exploration/env_infos/torso_velocity Std                0.0906434\n",
      "exploration/env_infos/torso_velocity Max                0.786909\n",
      "exploration/env_infos/torso_velocity Min               -1.4863\n",
      "evaluation/num steps total                         975000\n",
      "evaluation/num paths total                            975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.944296\n",
      "evaluation/Rewards Std                                  0.0359964\n",
      "evaluation/Rewards Max                                  2.48146\n",
      "evaluation/Rewards Min                                  0.47368\n",
      "evaluation/Returns Mean                               944.296\n",
      "evaluation/Returns Std                                  9.91898\n",
      "evaluation/Returns Max                                969.052\n",
      "evaluation/Returns Min                                931.734\n",
      "evaluation/Actions Mean                                -0.0369019\n",
      "evaluation/Actions Std                                  0.114101\n",
      "evaluation/Actions Max                                  0.704273\n",
      "evaluation/Actions Min                                 -0.362651\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            944.296\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.76317e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.85063e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.43193e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.41306e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0393087\n",
      "evaluation/env_infos/initial/reward_forward Std         0.156384\n",
      "evaluation/env_infos/initial/reward_forward Max         0.276732\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.39841\n",
      "evaluation/env_infos/reward_forward Mean                0.00308509\n",
      "evaluation/env_infos/reward_forward Std                 0.0667834\n",
      "evaluation/env_infos/reward_forward Max                 1.51187\n",
      "evaluation/env_infos/reward_forward Min                -1.21063\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0579047\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0100412\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0321101\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0684945\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.124935\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.149198\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0249663\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.588733\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0575234\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0128339\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00984995\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.588733\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.20519e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.79982e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.2894e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.41306e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.15436\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.255673\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.657496\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.39841\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00184831\n",
      "evaluation/env_infos/torso_velocity Std                 0.06569\n",
      "evaluation/env_infos/torso_velocity Max                 1.51187\n",
      "evaluation/env_infos/torso_velocity Min                -1.74265\n",
      "time/data storing (s)                                   0.0147089\n",
      "time/evaluation sampling (s)                           45.4868\n",
      "time/exploration sampling (s)                           2.12054\n",
      "time/logging (s)                                        0.268307\n",
      "time/saving (s)                                         0.0263924\n",
      "time/training (s)                                       4.22906\n",
      "time/epoch (s)                                         52.1458\n",
      "time/total (s)                                       2086.88\n",
      "Epoch                                                  38\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:00:31.235901 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 39 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 41000\n",
      "trainer/QF1 Loss                                       0.657358\n",
      "trainer/QF2 Loss                                       0.46613\n",
      "trainer/Policy Loss                                   -6.85678\n",
      "trainer/Q1 Predictions Mean                           14.2659\n",
      "trainer/Q1 Predictions Std                             1.17522\n",
      "trainer/Q1 Predictions Max                            16.3717\n",
      "trainer/Q1 Predictions Min                             8.92438\n",
      "trainer/Q2 Predictions Mean                           14.0671\n",
      "trainer/Q2 Predictions Std                             1.19865\n",
      "trainer/Q2 Predictions Max                            16.4227\n",
      "trainer/Q2 Predictions Min                             7.31701\n",
      "trainer/Q Targets Mean                                13.9935\n",
      "trainer/Q Targets Std                                  1.39774\n",
      "trainer/Q Targets Max                                 17.3275\n",
      "trainer/Q Targets Min                                 -0.426959\n",
      "trainer/Log Pis Mean                                   7.54583\n",
      "trainer/Log Pis Std                                    2.36685\n",
      "trainer/Log Pis Max                                   13.7043\n",
      "trainer/Log Pis Min                                    0.602465\n",
      "trainer/Policy mu Mean                                 0.00529712\n",
      "trainer/Policy mu Std                                  0.134801\n",
      "trainer/Policy mu Max                                  0.76448\n",
      "trainer/Policy mu Min                                 -0.428967\n",
      "trainer/Policy log std Mean                           -2.33115\n",
      "trainer/Policy log std Std                             0.232229\n",
      "trainer/Policy log std Max                            -1.42769\n",
      "trainer/Policy log std Min                            -3.28423\n",
      "trainer/Alpha                                          0.0111441\n",
      "trainer/Alpha Loss                                    -2.04198\n",
      "exploration/num steps total                        41000\n",
      "exploration/num paths total                          101\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.786902\n",
      "exploration/Rewards Std                                0.0468416\n",
      "exploration/Rewards Max                                1.00755\n",
      "exploration/Rewards Min                                0.62328\n",
      "exploration/Returns Mean                             786.902\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              786.902\n",
      "exploration/Returns Min                              786.902\n",
      "exploration/Actions Mean                              -0.0215855\n",
      "exploration/Actions Std                                0.22987\n",
      "exploration/Actions Max                                0.589382\n",
      "exploration/Actions Min                               -0.496415\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          786.902\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0124184\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0124184\n",
      "exploration/env_infos/final/reward_forward Min        -0.0124184\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.151749\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.151749\n",
      "exploration/env_infos/initial/reward_forward Min      -0.151749\n",
      "exploration/env_infos/reward_forward Mean             -0.00319125\n",
      "exploration/env_infos/reward_forward Std               0.0525562\n",
      "exploration/env_infos/reward_forward Max               1.21589\n",
      "exploration/env_infos/reward_forward Min              -0.252956\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.305842\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.305842\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.305842\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.086704\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.086704\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.086704\n",
      "exploration/env_infos/reward_ctrl Mean                -0.213225\n",
      "exploration/env_infos/reward_ctrl Std                  0.046517\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0772746\n",
      "exploration/env_infos/reward_ctrl Min                 -0.37672\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00229226\n",
      "exploration/env_infos/final/torso_velocity Std         0.0421706\n",
      "exploration/env_infos/final/torso_velocity Max         0.0536691\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0481275\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.094475\n",
      "exploration/env_infos/initial/torso_velocity Std       0.208082\n",
      "exploration/env_infos/initial/torso_velocity Max       0.357149\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.151749\n",
      "exploration/env_infos/torso_velocity Mean              0.000917339\n",
      "exploration/env_infos/torso_velocity Std               0.0858267\n",
      "exploration/env_infos/torso_velocity Max               1.21589\n",
      "exploration/env_infos/torso_velocity Min              -1.2853\n",
      "evaluation/num steps total                             1e+06\n",
      "evaluation/num paths total                          1000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.915741\n",
      "evaluation/Rewards Std                                 0.0674912\n",
      "evaluation/Rewards Max                                 2.27218\n",
      "evaluation/Rewards Min                                -1.56384\n",
      "evaluation/Returns Mean                              915.741\n",
      "evaluation/Returns Std                                53.6474\n",
      "evaluation/Returns Max                               990.597\n",
      "evaluation/Returns Min                               810.538\n",
      "evaluation/Actions Mean                                0.0021476\n",
      "evaluation/Actions Std                                 0.145978\n",
      "evaluation/Actions Max                                 0.999123\n",
      "evaluation/Actions Min                                -0.830657\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           915.741\n",
      "evaluation/env_infos/final/reward_forward Mean         4.48298e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.59017e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.46093e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.62207e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0971457\n",
      "evaluation/env_infos/initial/reward_forward Std        0.124854\n",
      "evaluation/env_infos/initial/reward_forward Max        0.388216\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.225751\n",
      "evaluation/env_infos/reward_forward Mean              -0.00149419\n",
      "evaluation/env_infos/reward_forward Std                0.0494306\n",
      "evaluation/env_infos/reward_forward Max                1.47243\n",
      "evaluation/env_infos/reward_forward Min               -1.29105\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0851517\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0547943\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0102459\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.195642\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.423248\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.36138\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0772772\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.19317\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0852567\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0619453\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0042442\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.56384\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.16947e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.59401e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.11163e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.05865e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.183379\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.203316\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.640341\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.24265\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00198589\n",
      "evaluation/env_infos/torso_velocity Std                0.0589473\n",
      "evaluation/env_infos/torso_velocity Max                1.47243\n",
      "evaluation/env_infos/torso_velocity Min               -1.70294\n",
      "time/data storing (s)                                  0.0164249\n",
      "time/evaluation sampling (s)                          46.2169\n",
      "time/exploration sampling (s)                          2.15096\n",
      "time/logging (s)                                       0.270583\n",
      "time/saving (s)                                        0.026511\n",
      "time/training (s)                                      4.28401\n",
      "time/epoch (s)                                        52.9654\n",
      "time/total (s)                                      2140.27\n",
      "Epoch                                                 39\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:01:25.038222 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 40 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 42000\n",
      "trainer/QF1 Loss                                       0.181007\n",
      "trainer/QF2 Loss                                       0.195844\n",
      "trainer/Policy Loss                                   -7.07054\n",
      "trainer/Q1 Predictions Mean                           14.4195\n",
      "trainer/Q1 Predictions Std                             1.13366\n",
      "trainer/Q1 Predictions Max                            16.8008\n",
      "trainer/Q1 Predictions Min                             6.95536\n",
      "trainer/Q2 Predictions Mean                           14.2852\n",
      "trainer/Q2 Predictions Std                             1.13685\n",
      "trainer/Q2 Predictions Max                            16.6948\n",
      "trainer/Q2 Predictions Min                             7.7421\n",
      "trainer/Q Targets Mean                                14.4175\n",
      "trainer/Q Targets Std                                  1.22725\n",
      "trainer/Q Targets Max                                 16.6722\n",
      "trainer/Q Targets Min                                  5.59552\n",
      "trainer/Log Pis Mean                                   7.50507\n",
      "trainer/Log Pis Std                                    2.32733\n",
      "trainer/Log Pis Max                                   14.0785\n",
      "trainer/Log Pis Min                                   -0.524253\n",
      "trainer/Policy mu Mean                                -0.0480531\n",
      "trainer/Policy mu Std                                  0.148337\n",
      "trainer/Policy mu Max                                  2.21311\n",
      "trainer/Policy mu Min                                 -0.952004\n",
      "trainer/Policy log std Mean                           -2.34204\n",
      "trainer/Policy log std Std                             0.227921\n",
      "trainer/Policy log std Max                            -0.450663\n",
      "trainer/Policy log std Min                            -3.46384\n",
      "trainer/Alpha                                          0.0106597\n",
      "trainer/Alpha Loss                                    -2.24718\n",
      "exploration/num steps total                        42000\n",
      "exploration/num paths total                          102\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.970076\n",
      "exploration/Rewards Std                                0.099959\n",
      "exploration/Rewards Max                                1.64735\n",
      "exploration/Rewards Min                                0.793291\n",
      "exploration/Returns Mean                             970.076\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              970.076\n",
      "exploration/Returns Min                              970.076\n",
      "exploration/Actions Mean                              -0.0290889\n",
      "exploration/Actions Std                                0.124599\n",
      "exploration/Actions Max                                0.407053\n",
      "exploration/Actions Min                               -0.428182\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          970.076\n",
      "exploration/env_infos/final/reward_forward Mean        0.110977\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.110977\n",
      "exploration/env_infos/final/reward_forward Min         0.110977\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0259468\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0259468\n",
      "exploration/env_infos/initial/reward_forward Min       0.0259468\n",
      "exploration/env_infos/reward_forward Mean             -0.0176998\n",
      "exploration/env_infos/reward_forward Std               0.203179\n",
      "exploration/env_infos/reward_forward Max               0.785198\n",
      "exploration/env_infos/reward_forward Min              -0.990301\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0658055\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0658055\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0658055\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0253651\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0253651\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0253651\n",
      "exploration/env_infos/reward_ctrl Mean                -0.065484\n",
      "exploration/env_infos/reward_ctrl Std                  0.0277043\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00912748\n",
      "exploration/env_infos/reward_ctrl Min                 -0.206709\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.049058\n",
      "exploration/env_infos/final/torso_velocity Std         0.114803\n",
      "exploration/env_infos/final/torso_velocity Max         0.110977\n",
      "exploration/env_infos/final/torso_velocity Min        -0.152764\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.25911\n",
      "exploration/env_infos/initial/torso_velocity Std       0.196829\n",
      "exploration/env_infos/initial/torso_velocity Max       0.507367\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0259468\n",
      "exploration/env_infos/torso_velocity Mean             -0.00972151\n",
      "exploration/env_infos/torso_velocity Std               0.172272\n",
      "exploration/env_infos/torso_velocity Max               0.913743\n",
      "exploration/env_infos/torso_velocity Min              -1.23807\n",
      "evaluation/num steps total                             1.025e+06\n",
      "evaluation/num paths total                          1025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.95328\n",
      "evaluation/Rewards Std                                 0.0516326\n",
      "evaluation/Rewards Max                                 2.41346\n",
      "evaluation/Rewards Min                                -0.636795\n",
      "evaluation/Returns Mean                              953.28\n",
      "evaluation/Returns Std                                24.0094\n",
      "evaluation/Returns Max                               989.232\n",
      "evaluation/Returns Min                               890.663\n",
      "evaluation/Actions Mean                               -0.0494365\n",
      "evaluation/Actions Std                                 0.0988727\n",
      "evaluation/Actions Max                                 0.939983\n",
      "evaluation/Actions Min                                -0.568903\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           953.28\n",
      "evaluation/env_infos/final/reward_forward Mean        -3.94758e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000101117\n",
      "evaluation/env_infos/final/reward_forward Max          8.80183e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00040084\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0392827\n",
      "evaluation/env_infos/initial/reward_forward Std        0.133143\n",
      "evaluation/env_infos/initial/reward_forward Max        0.309294\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.187872\n",
      "evaluation/env_infos/reward_forward Mean              -0.00301922\n",
      "evaluation/env_infos/reward_forward Std                0.0587214\n",
      "evaluation/env_infos/reward_forward Max                1.09931\n",
      "evaluation/env_infos/reward_forward Min               -1.07855\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0485898\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.023361\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00883199\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.105735\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.631944\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.537069\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0479086\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.63679\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0488791\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0355896\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00706873\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.63679\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.293e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          7.07037e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000241897\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00040084\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.141111\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.214854\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.536537\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.20571\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00230848\n",
      "evaluation/env_infos/torso_velocity Std                0.0616838\n",
      "evaluation/env_infos/torso_velocity Max                1.25936\n",
      "evaluation/env_infos/torso_velocity Min               -1.66715\n",
      "time/data storing (s)                                  0.0159601\n",
      "time/evaluation sampling (s)                          46.568\n",
      "time/exploration sampling (s)                          2.16949\n",
      "time/logging (s)                                       0.289959\n",
      "time/saving (s)                                        0.0273685\n",
      "time/training (s)                                      4.30867\n",
      "time/epoch (s)                                        53.3794\n",
      "time/total (s)                                      2194.09\n",
      "Epoch                                                 40\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:02:18.776063 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 41 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 43000\n",
      "trainer/QF1 Loss                                       0.408836\n",
      "trainer/QF2 Loss                                       0.236989\n",
      "trainer/Policy Loss                                   -7.24025\n",
      "trainer/Q1 Predictions Mean                           15.1859\n",
      "trainer/Q1 Predictions Std                             1.64154\n",
      "trainer/Q1 Predictions Max                            33.9832\n",
      "trainer/Q1 Predictions Min                             9.84251\n",
      "trainer/Q2 Predictions Mean                           15.0775\n",
      "trainer/Q2 Predictions Std                             1.67367\n",
      "trainer/Q2 Predictions Max                            34.9338\n",
      "trainer/Q2 Predictions Min                            10.5672\n",
      "trainer/Q Targets Mean                                14.8748\n",
      "trainer/Q Targets Std                                  1.68332\n",
      "trainer/Q Targets Max                                 36.3821\n",
      "trainer/Q Targets Min                                 11.1916\n",
      "trainer/Log Pis Mean                                   8.22084\n",
      "trainer/Log Pis Std                                    4.59344\n",
      "trainer/Log Pis Max                                   66.3088\n",
      "trainer/Log Pis Min                                   -3.93088\n",
      "trainer/Policy mu Mean                                 0.0366059\n",
      "trainer/Policy mu Std                                  0.383151\n",
      "trainer/Policy mu Max                                 10.4884\n",
      "trainer/Policy mu Min                                 -2.15592\n",
      "trainer/Policy log std Mean                           -2.38742\n",
      "trainer/Policy log std Std                             0.255523\n",
      "trainer/Policy log std Max                            -0.922431\n",
      "trainer/Policy log std Min                            -5.3379\n",
      "trainer/Alpha                                          0.0104334\n",
      "trainer/Alpha Loss                                     1.00768\n",
      "exploration/num steps total                        43000\n",
      "exploration/num paths total                          103\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.907629\n",
      "exploration/Rewards Std                                0.0504997\n",
      "exploration/Rewards Max                                1.24317\n",
      "exploration/Rewards Min                                0.782484\n",
      "exploration/Returns Mean                             907.629\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              907.629\n",
      "exploration/Returns Min                              907.629\n",
      "exploration/Actions Mean                               0.0336683\n",
      "exploration/Actions Std                                0.156651\n",
      "exploration/Actions Max                                0.480735\n",
      "exploration/Actions Min                               -0.410619\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          907.629\n",
      "exploration/env_infos/final/reward_forward Mean       -0.134829\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.134829\n",
      "exploration/env_infos/final/reward_forward Min        -0.134829\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0549744\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0549744\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0549744\n",
      "exploration/env_infos/reward_forward Mean             -0.0220217\n",
      "exploration/env_infos/reward_forward Std               0.0777294\n",
      "exploration/env_infos/reward_forward Max               0.364362\n",
      "exploration/env_infos/reward_forward Min              -0.341696\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.167554\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.167554\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.167554\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.135756\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.135756\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.135756\n",
      "exploration/env_infos/reward_ctrl Mean                -0.102692\n",
      "exploration/env_infos/reward_ctrl Std                  0.0306327\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0209763\n",
      "exploration/env_infos/reward_ctrl Min                 -0.217516\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00216132\n",
      "exploration/env_infos/final/torso_velocity Std         0.139023\n",
      "exploration/env_infos/final/torso_velocity Max         0.189834\n",
      "exploration/env_infos/final/torso_velocity Min        -0.134829\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.154822\n",
      "exploration/env_infos/initial/torso_velocity Std       0.26353\n",
      "exploration/env_infos/initial/torso_velocity Max       0.52648\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0549744\n",
      "exploration/env_infos/torso_velocity Mean             -0.00520853\n",
      "exploration/env_infos/torso_velocity Std               0.0918958\n",
      "exploration/env_infos/torso_velocity Max               0.733978\n",
      "exploration/env_infos/torso_velocity Min              -1.00593\n",
      "evaluation/num steps total                             1.05e+06\n",
      "evaluation/num paths total                          1050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.944096\n",
      "evaluation/Rewards Std                                 0.0620179\n",
      "evaluation/Rewards Max                                 2.34978\n",
      "evaluation/Rewards Min                                -1.73017\n",
      "evaluation/Returns Mean                              944.096\n",
      "evaluation/Returns Std                                20.6183\n",
      "evaluation/Returns Max                               975.962\n",
      "evaluation/Returns Min                               913.789\n",
      "evaluation/Actions Mean                                0.0362934\n",
      "evaluation/Actions Std                                 0.114216\n",
      "evaluation/Actions Max                                 0.998506\n",
      "evaluation/Actions Min                                -0.710928\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           944.096\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.000634125\n",
      "evaluation/env_infos/final/reward_forward Std          0.0031878\n",
      "evaluation/env_infos/final/reward_forward Max          0.000382196\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0162468\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0921845\n",
      "evaluation/env_infos/initial/reward_forward Std        0.118375\n",
      "evaluation/env_infos/initial/reward_forward Max        0.337251\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.0661409\n",
      "evaluation/env_infos/reward_forward Mean              -0.000977549\n",
      "evaluation/env_infos/reward_forward Std                0.0538229\n",
      "evaluation/env_infos/reward_forward Max                1.01652\n",
      "evaluation/env_infos/reward_forward Min               -1.40808\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.056313\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0209552\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0212685\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0864029\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.2879\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.930131\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0644234\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -2.73017\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.05745\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0535402\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00680319\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.73017\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.61135e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00247765\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0140118\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.0162468\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.164315\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.228959\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.545279\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.350031\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00155716\n",
      "evaluation/env_infos/torso_velocity Std                0.0617927\n",
      "evaluation/env_infos/torso_velocity Max                1.41239\n",
      "evaluation/env_infos/torso_velocity Min               -2.16175\n",
      "time/data storing (s)                                  0.0153698\n",
      "time/evaluation sampling (s)                          46.4852\n",
      "time/exploration sampling (s)                          2.11569\n",
      "time/logging (s)                                       0.270946\n",
      "time/saving (s)                                        0.026581\n",
      "time/training (s)                                      4.33139\n",
      "time/epoch (s)                                        53.2452\n",
      "time/total (s)                                      2247.81\n",
      "Epoch                                                 41\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:03:11.783721 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 42 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 44000\n",
      "trainer/QF1 Loss                                       0.330602\n",
      "trainer/QF2 Loss                                       0.448055\n",
      "trainer/Policy Loss                                   -6.71675\n",
      "trainer/Q1 Predictions Mean                           14.841\n",
      "trainer/Q1 Predictions Std                             1.16482\n",
      "trainer/Q1 Predictions Max                            16.9755\n",
      "trainer/Q1 Predictions Min                            10.3936\n",
      "trainer/Q2 Predictions Mean                           15.3527\n",
      "trainer/Q2 Predictions Std                             1.18462\n",
      "trainer/Q2 Predictions Max                            17.788\n",
      "trainer/Q2 Predictions Min                            10.5082\n",
      "trainer/Q Targets Mean                                15.0084\n",
      "trainer/Q Targets Std                                  1.07607\n",
      "trainer/Q Targets Max                                 17.2995\n",
      "trainer/Q Targets Min                                 10.57\n",
      "trainer/Log Pis Mean                                   8.54367\n",
      "trainer/Log Pis Std                                    3.82478\n",
      "trainer/Log Pis Max                                   38.0256\n",
      "trainer/Log Pis Min                                    1.32486\n",
      "trainer/Policy mu Mean                                 0.0491735\n",
      "trainer/Policy mu Std                                  0.404645\n",
      "trainer/Policy mu Max                                  5.85051\n",
      "trainer/Policy mu Min                                 -2.19318\n",
      "trainer/Policy log std Mean                           -2.35091\n",
      "trainer/Policy log std Std                             0.271205\n",
      "trainer/Policy log std Max                            -0.661529\n",
      "trainer/Policy log std Min                            -3.89227\n",
      "trainer/Alpha                                          0.0103008\n",
      "trainer/Alpha Loss                                     2.48789\n",
      "exploration/num steps total                        44000\n",
      "exploration/num paths total                          104\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.841928\n",
      "exploration/Rewards Std                                0.475736\n",
      "exploration/Rewards Max                                1.2895\n",
      "exploration/Rewards Min                               -2.9949\n",
      "exploration/Returns Mean                             841.928\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              841.928\n",
      "exploration/Returns Min                              841.928\n",
      "exploration/Actions Mean                               0.0323065\n",
      "exploration/Actions Std                                0.197948\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -0.999971\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          841.928\n",
      "exploration/env_infos/final/reward_forward Mean        0.964224\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.964224\n",
      "exploration/env_infos/final/reward_forward Min         0.964224\n",
      "exploration/env_infos/initial/reward_forward Mean      0.568794\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.568794\n",
      "exploration/env_infos/initial/reward_forward Min       0.568794\n",
      "exploration/env_infos/reward_forward Mean              0.000178663\n",
      "exploration/env_infos/reward_forward Std               0.341895\n",
      "exploration/env_infos/reward_forward Max               1.30629\n",
      "exploration/env_infos/reward_forward Min              -1.02241\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.304373\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.304373\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.304373\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -3.58475\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -3.58475\n",
      "exploration/env_infos/initial/reward_ctrl Min         -3.58475\n",
      "exploration/env_infos/reward_ctrl Mean                -0.160908\n",
      "exploration/env_infos/reward_ctrl Std                  0.474741\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0147284\n",
      "exploration/env_infos/reward_ctrl Min                 -3.9949\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.160823\n",
      "exploration/env_infos/final/torso_velocity Std         0.795782\n",
      "exploration/env_infos/final/torso_velocity Max         0.964224\n",
      "exploration/env_infos/final/torso_velocity Min        -0.747965\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.232738\n",
      "exploration/env_infos/initial/torso_velocity Std       0.292645\n",
      "exploration/env_infos/initial/torso_velocity Max       0.568794\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.144482\n",
      "exploration/env_infos/torso_velocity Mean              0.0126243\n",
      "exploration/env_infos/torso_velocity Std               0.274458\n",
      "exploration/env_infos/torso_velocity Max               1.30629\n",
      "exploration/env_infos/torso_velocity Min              -2.13929\n",
      "evaluation/num steps total                             1.075e+06\n",
      "evaluation/num paths total                          1075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.912314\n",
      "evaluation/Rewards Std                                 0.150674\n",
      "evaluation/Rewards Max                                 2.11978\n",
      "evaluation/Rewards Min                                -2.96662\n",
      "evaluation/Returns Mean                              912.314\n",
      "evaluation/Returns Std                                13.3235\n",
      "evaluation/Returns Max                               938.662\n",
      "evaluation/Returns Min                               876.999\n",
      "evaluation/Actions Mean                                0.029417\n",
      "evaluation/Actions Std                                 0.146328\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999175\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           912.314\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.70847e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.23925e-06\n",
      "evaluation/env_infos/final/reward_forward Max          5.50708e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -1.12582e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.202036\n",
      "evaluation/env_infos/initial/reward_forward Std        0.150419\n",
      "evaluation/env_infos/initial/reward_forward Max        0.45447\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.0402955\n",
      "evaluation/env_infos/reward_forward Mean              -0.00234906\n",
      "evaluation/env_infos/reward_forward Std                0.0518102\n",
      "evaluation/env_infos/reward_forward Max                0.846227\n",
      "evaluation/env_infos/reward_forward Min               -1.29688\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0799435\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0121556\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0517114\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.103725\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -2.674\n",
      "evaluation/env_infos/initial/reward_ctrl Std           1.10062\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.129463\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -3.74567\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0891094\n",
      "evaluation/env_infos/reward_ctrl Std                   0.148173\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0195535\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.96662\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.30544e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          4.96683e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          1.27936e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -3.945e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.124984\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.271931\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.558767\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.452777\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00118154\n",
      "evaluation/env_infos/torso_velocity Std                0.0871119\n",
      "evaluation/env_infos/torso_velocity Max                1.48992\n",
      "evaluation/env_infos/torso_velocity Min               -2.44427\n",
      "time/data storing (s)                                  0.0142669\n",
      "time/evaluation sampling (s)                          45.6606\n",
      "time/exploration sampling (s)                          2.07223\n",
      "time/logging (s)                                       0.284533\n",
      "time/saving (s)                                        0.0263532\n",
      "time/training (s)                                      4.49436\n",
      "time/epoch (s)                                        52.5523\n",
      "time/total (s)                                      2300.83\n",
      "Epoch                                                 42\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:04.797719 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 43 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 45000\n",
      "trainer/QF1 Loss                                       0.222346\n",
      "trainer/QF2 Loss                                       0.278454\n",
      "trainer/Policy Loss                                   -8.25772\n",
      "trainer/Q1 Predictions Mean                           15.2396\n",
      "trainer/Q1 Predictions Std                             1.12555\n",
      "trainer/Q1 Predictions Max                            17.2908\n",
      "trainer/Q1 Predictions Min                            10.5843\n",
      "trainer/Q2 Predictions Mean                           15.2643\n",
      "trainer/Q2 Predictions Std                             1.10434\n",
      "trainer/Q2 Predictions Max                            17.2474\n",
      "trainer/Q2 Predictions Min                            10.5104\n",
      "trainer/Q Targets Mean                                15.3036\n",
      "trainer/Q Targets Std                                  1.18033\n",
      "trainer/Q Targets Max                                 17.7251\n",
      "trainer/Q Targets Min                                  9.6109\n",
      "trainer/Log Pis Mean                                   7.29643\n",
      "trainer/Log Pis Std                                    2.82618\n",
      "trainer/Log Pis Max                                   22.9398\n",
      "trainer/Log Pis Min                                   -2.29089\n",
      "trainer/Policy mu Mean                                 0.0906247\n",
      "trainer/Policy mu Std                                  0.235711\n",
      "trainer/Policy mu Max                                  4.74814\n",
      "trainer/Policy mu Min                                 -1.69887\n",
      "trainer/Policy log std Mean                           -2.29596\n",
      "trainer/Policy log std Std                             0.261674\n",
      "trainer/Policy log std Max                            -0.0739334\n",
      "trainer/Policy log std Min                            -3.36453\n",
      "trainer/Alpha                                          0.0101481\n",
      "trainer/Alpha Loss                                    -3.22946\n",
      "exploration/num steps total                        45000\n",
      "exploration/num paths total                          105\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.923087\n",
      "exploration/Rewards Std                                0.0676761\n",
      "exploration/Rewards Max                                2.0305\n",
      "exploration/Rewards Min                                0.230062\n",
      "exploration/Returns Mean                             923.087\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              923.087\n",
      "exploration/Returns Min                              923.087\n",
      "exploration/Actions Mean                               0.0732077\n",
      "exploration/Actions Std                                0.125674\n",
      "exploration/Actions Max                                0.711937\n",
      "exploration/Actions Min                               -0.489193\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          923.087\n",
      "exploration/env_infos/final/reward_forward Mean       -0.112441\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.112441\n",
      "exploration/env_infos/final/reward_forward Min        -0.112441\n",
      "exploration/env_infos/initial/reward_forward Mean      0.178217\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.178217\n",
      "exploration/env_infos/initial/reward_forward Min       0.178217\n",
      "exploration/env_infos/reward_forward Mean             -0.00278138\n",
      "exploration/env_infos/reward_forward Std               0.133072\n",
      "exploration/env_infos/reward_forward Max               0.529138\n",
      "exploration/env_infos/reward_forward Min              -0.825769\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.1097\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.1097\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.1097\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.769938\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.769938\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.769938\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0846131\n",
      "exploration/env_infos/reward_ctrl Std                  0.0407927\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0116299\n",
      "exploration/env_infos/reward_ctrl Min                 -0.769938\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0332651\n",
      "exploration/env_infos/final/torso_velocity Std         0.108875\n",
      "exploration/env_infos/final/torso_velocity Max         0.149222\n",
      "exploration/env_infos/final/torso_velocity Min        -0.112441\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.254181\n",
      "exploration/env_infos/initial/torso_velocity Std       0.102051\n",
      "exploration/env_infos/initial/torso_velocity Max       0.398434\n",
      "exploration/env_infos/initial/torso_velocity Min       0.178217\n",
      "exploration/env_infos/torso_velocity Mean             -0.00190703\n",
      "exploration/env_infos/torso_velocity Std               0.131552\n",
      "exploration/env_infos/torso_velocity Max               1.15379\n",
      "exploration/env_infos/torso_velocity Min              -1.15584\n",
      "evaluation/num steps total                             1.1e+06\n",
      "evaluation/num paths total                          1100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.953586\n",
      "evaluation/Rewards Std                                 0.0709576\n",
      "evaluation/Rewards Max                                 1.73233\n",
      "evaluation/Rewards Min                                -2.39424\n",
      "evaluation/Returns Mean                              953.586\n",
      "evaluation/Returns Std                                 5.13371\n",
      "evaluation/Returns Max                               965.323\n",
      "evaluation/Returns Min                               938.194\n",
      "evaluation/Actions Mean                                0.0763648\n",
      "evaluation/Actions Std                                 0.0770972\n",
      "evaluation/Actions Max                                 0.999995\n",
      "evaluation/Actions Min                                -0.97849\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           953.586\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.84733e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.93637e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.49912e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.77944e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0722634\n",
      "evaluation/env_infos/initial/reward_forward Std        0.13276\n",
      "evaluation/env_infos/initial/reward_forward Max        0.371973\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.188547\n",
      "evaluation/env_infos/reward_forward Mean              -0.00219908\n",
      "evaluation/env_infos/reward_forward Std                0.045983\n",
      "evaluation/env_infos/reward_forward Max                1.13318\n",
      "evaluation/env_infos/reward_forward Min               -1.07465\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0442698\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00449148\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0313114\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0523798\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.77139\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.658711\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.213517\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -2.68128\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0471022\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0703076\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0185984\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.39424\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -6.72682e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.28079e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.77742e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.10769e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137602\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.22252\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.630355\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.274491\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00195003\n",
      "evaluation/env_infos/torso_velocity Std                0.0526854\n",
      "evaluation/env_infos/torso_velocity Max                1.20751\n",
      "evaluation/env_infos/torso_velocity Min               -1.84799\n",
      "time/data storing (s)                                  0.0154693\n",
      "time/evaluation sampling (s)                          45.7852\n",
      "time/exploration sampling (s)                          2.09638\n",
      "time/logging (s)                                       0.286289\n",
      "time/saving (s)                                        0.0280734\n",
      "time/training (s)                                      4.32673\n",
      "time/epoch (s)                                        52.5382\n",
      "time/total (s)                                      2353.85\n",
      "Epoch                                                 43\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:04:58.071494 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 44 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 46000\n",
      "trainer/QF1 Loss                                       0.25096\n",
      "trainer/QF2 Loss                                       0.252083\n",
      "trainer/Policy Loss                                   -8.54844\n",
      "trainer/Q1 Predictions Mean                           15.6399\n",
      "trainer/Q1 Predictions Std                             1.26408\n",
      "trainer/Q1 Predictions Max                            17.5498\n",
      "trainer/Q1 Predictions Min                             8.59845\n",
      "trainer/Q2 Predictions Mean                           15.7043\n",
      "trainer/Q2 Predictions Std                             1.31108\n",
      "trainer/Q2 Predictions Max                            17.6754\n",
      "trainer/Q2 Predictions Min                             8.27835\n",
      "trainer/Q Targets Mean                                15.6929\n",
      "trainer/Q Targets Std                                  1.32289\n",
      "trainer/Q Targets Max                                 17.881\n",
      "trainer/Q Targets Min                                  7.91812\n",
      "trainer/Log Pis Mean                                   7.33822\n",
      "trainer/Log Pis Std                                    2.69229\n",
      "trainer/Log Pis Max                                   21.1712\n",
      "trainer/Log Pis Min                                   -2.76272\n",
      "trainer/Policy mu Mean                                -0.0237385\n",
      "trainer/Policy mu Std                                  0.197373\n",
      "trainer/Policy mu Max                                  3.79569\n",
      "trainer/Policy mu Min                                 -0.863068\n",
      "trainer/Policy log std Mean                           -2.30769\n",
      "trainer/Policy log std Std                             0.239613\n",
      "trainer/Policy log std Max                            -0.797079\n",
      "trainer/Policy log std Min                            -3.69448\n",
      "trainer/Alpha                                          0.009852\n",
      "trainer/Alpha Loss                                    -3.05715\n",
      "exploration/num steps total                        46000\n",
      "exploration/num paths total                          106\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.90939\n",
      "exploration/Rewards Std                                0.0505736\n",
      "exploration/Rewards Max                                1.20673\n",
      "exploration/Rewards Min                                0.717935\n",
      "exploration/Returns Mean                             909.39\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              909.39\n",
      "exploration/Returns Min                              909.39\n",
      "exploration/Actions Mean                              -0.031636\n",
      "exploration/Actions Std                                0.153702\n",
      "exploration/Actions Max                                0.449057\n",
      "exploration/Actions Min                               -0.548947\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          909.39\n",
      "exploration/env_infos/final/reward_forward Mean        0.0123626\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0123626\n",
      "exploration/env_infos/final/reward_forward Min         0.0123626\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.130686\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.130686\n",
      "exploration/env_infos/initial/reward_forward Min      -0.130686\n",
      "exploration/env_infos/reward_forward Mean              0.00771098\n",
      "exploration/env_infos/reward_forward Std               0.0853124\n",
      "exploration/env_infos/reward_forward Max               0.65505\n",
      "exploration/env_infos/reward_forward Min              -1.11859\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0843293\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0843293\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0843293\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0351624\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0351624\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0351624\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0985006\n",
      "exploration/env_infos/reward_ctrl Std                  0.0402681\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0102715\n",
      "exploration/env_infos/reward_ctrl Min                 -0.282065\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0347453\n",
      "exploration/env_infos/final/torso_velocity Std         0.0402494\n",
      "exploration/env_infos/final/torso_velocity Max         0.0123626\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0859698\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.101874\n",
      "exploration/env_infos/initial/torso_velocity Std       0.199851\n",
      "exploration/env_infos/initial/torso_velocity Max       0.357249\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.130686\n",
      "exploration/env_infos/torso_velocity Mean             -0.00543321\n",
      "exploration/env_infos/torso_velocity Std               0.100489\n",
      "exploration/env_infos/torso_velocity Max               0.65505\n",
      "exploration/env_infos/torso_velocity Min              -1.61994\n",
      "evaluation/num steps total                             1.125e+06\n",
      "evaluation/num paths total                          1125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.921393\n",
      "evaluation/Rewards Std                                 0.0398563\n",
      "evaluation/Rewards Max                                 2.20234\n",
      "evaluation/Rewards Min                                -1.06942\n",
      "evaluation/Returns Mean                              921.393\n",
      "evaluation/Returns Std                                13.1332\n",
      "evaluation/Returns Max                               946.546\n",
      "evaluation/Returns Min                               898.759\n",
      "evaluation/Actions Mean                               -0.0336313\n",
      "evaluation/Actions Std                                 0.136386\n",
      "evaluation/Actions Max                                 0.995351\n",
      "evaluation/Actions Min                                -0.380936\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           921.393\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.92382e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.72521e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.88953e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.77892e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.105989\n",
      "evaluation/env_infos/initial/reward_forward Std        0.15838\n",
      "evaluation/env_infos/initial/reward_forward Max        0.344885\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.275411\n",
      "evaluation/env_infos/reward_forward Mean               0.00259771\n",
      "evaluation/env_infos/reward_forward Std                0.0453854\n",
      "evaluation/env_infos/reward_forward Max                1.46778\n",
      "evaluation/env_infos/reward_forward Min               -1.05607\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0782885\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0136709\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0515799\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.101355\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.952327\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.669631\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0157174\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -2.06942\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0789286\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0375898\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0157174\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.06942\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.45016e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.91711e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.88953e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.77892e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.179904\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.235532\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.696394\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.275411\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00071326\n",
      "evaluation/env_infos/torso_velocity Std                0.0517214\n",
      "evaluation/env_infos/torso_velocity Max                1.46778\n",
      "evaluation/env_infos/torso_velocity Min               -1.72699\n",
      "time/data storing (s)                                  0.015117\n",
      "time/evaluation sampling (s)                          46.1418\n",
      "time/exploration sampling (s)                          2.02578\n",
      "time/logging (s)                                       0.271437\n",
      "time/saving (s)                                        0.0250003\n",
      "time/training (s)                                      4.30069\n",
      "time/epoch (s)                                        52.7798\n",
      "time/total (s)                                      2407.1\n",
      "Epoch                                                 44\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:05:54.752489 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 45 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 47000\n",
      "trainer/QF1 Loss                                       0.244472\n",
      "trainer/QF2 Loss                                       0.226965\n",
      "trainer/Policy Loss                                   -8.06657\n",
      "trainer/Q1 Predictions Mean                           15.8345\n",
      "trainer/Q1 Predictions Std                             1.22118\n",
      "trainer/Q1 Predictions Max                            18.0108\n",
      "trainer/Q1 Predictions Min                             9.94594\n",
      "trainer/Q2 Predictions Mean                           15.863\n",
      "trainer/Q2 Predictions Std                             1.22535\n",
      "trainer/Q2 Predictions Max                            17.851\n",
      "trainer/Q2 Predictions Min                            10.5986\n",
      "trainer/Q Targets Mean                                16.0428\n",
      "trainer/Q Targets Std                                  1.12452\n",
      "trainer/Q Targets Max                                 18.0791\n",
      "trainer/Q Targets Min                                  9.55521\n",
      "trainer/Log Pis Mean                                   8.0386\n",
      "trainer/Log Pis Std                                    2.59844\n",
      "trainer/Log Pis Max                                   18.9324\n",
      "trainer/Log Pis Min                                    1.75933\n",
      "trainer/Policy mu Mean                                 0.0192641\n",
      "trainer/Policy mu Std                                  0.148756\n",
      "trainer/Policy mu Max                                  1.20052\n",
      "trainer/Policy mu Min                                 -0.823209\n",
      "trainer/Policy log std Mean                           -2.39581\n",
      "trainer/Policy log std Std                             0.246073\n",
      "trainer/Policy log std Max                            -1.60451\n",
      "trainer/Policy log std Min                            -3.95278\n",
      "trainer/Alpha                                          0.00959505\n",
      "trainer/Alpha Loss                                     0.179354\n",
      "exploration/num steps total                        47000\n",
      "exploration/num paths total                          107\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.941471\n",
      "exploration/Rewards Std                                0.07552\n",
      "exploration/Rewards Max                                1.37232\n",
      "exploration/Rewards Min                               -0.819931\n",
      "exploration/Returns Mean                             941.471\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              941.471\n",
      "exploration/Returns Min                              941.471\n",
      "exploration/Actions Mean                              -0.0050362\n",
      "exploration/Actions Std                                0.130362\n",
      "exploration/Actions Max                                0.98527\n",
      "exploration/Actions Min                               -0.543555\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          941.471\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0893461\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0893461\n",
      "exploration/env_infos/final/reward_forward Min        -0.0893461\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0984342\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0984342\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0984342\n",
      "exploration/env_infos/reward_forward Mean              0.0267979\n",
      "exploration/env_infos/reward_forward Std               0.155184\n",
      "exploration/env_infos/reward_forward Max               0.87404\n",
      "exploration/env_infos/reward_forward Min              -0.502345\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0375643\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0375643\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0375643\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.81993\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.81993\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.81993\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0680787\n",
      "exploration/env_infos/reward_ctrl Std                  0.0659988\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00617592\n",
      "exploration/env_infos/reward_ctrl Min                 -1.81993\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0253642\n",
      "exploration/env_infos/final/torso_velocity Std         0.049541\n",
      "exploration/env_infos/final/torso_velocity Max         0.0313491\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0893461\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0828631\n",
      "exploration/env_infos/initial/torso_velocity Std       0.236944\n",
      "exploration/env_infos/initial/torso_velocity Max       0.417565\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0984342\n",
      "exploration/env_infos/torso_velocity Mean              0.00751188\n",
      "exploration/env_infos/torso_velocity Std               0.131333\n",
      "exploration/env_infos/torso_velocity Max               0.87404\n",
      "exploration/env_infos/torso_velocity Min              -1.46576\n",
      "evaluation/num steps total                             1.15e+06\n",
      "evaluation/num paths total                          1150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.970762\n",
      "evaluation/Rewards Std                                 0.0334268\n",
      "evaluation/Rewards Max                                 2.4585\n",
      "evaluation/Rewards Min                                -0.816779\n",
      "evaluation/Returns Mean                              970.762\n",
      "evaluation/Returns Std                                 5.43877\n",
      "evaluation/Returns Max                               982.128\n",
      "evaluation/Returns Min                               962.696\n",
      "evaluation/Actions Mean                                0.0211437\n",
      "evaluation/Actions Std                                 0.0840802\n",
      "evaluation/Actions Max                                 0.986221\n",
      "evaluation/Actions Min                                -0.487132\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           970.762\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.15332e-09\n",
      "evaluation/env_infos/final/reward_forward Std          1.85353e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.75892e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.95754e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.070382\n",
      "evaluation/env_infos/initial/reward_forward Std        0.118092\n",
      "evaluation/env_infos/initial/reward_forward Max        0.243664\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.194273\n",
      "evaluation/env_infos/reward_forward Mean               0.000610326\n",
      "evaluation/env_infos/reward_forward Std                0.0477151\n",
      "evaluation/env_infos/reward_forward Max                0.810784\n",
      "evaluation/env_infos/reward_forward Min               -1.37265\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0288646\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00582145\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.016926\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0372411\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.581236\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.605573\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0193783\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.81678\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0300662\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0280011\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00988363\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.81678\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         6.01778e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.26111e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.43247e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -4.95754e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.154845\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.201309\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.574512\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.194273\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00161428\n",
      "evaluation/env_infos/torso_velocity Std                0.0556056\n",
      "evaluation/env_infos/torso_velocity Max                1.13882\n",
      "evaluation/env_infos/torso_velocity Min               -2.14795\n",
      "time/data storing (s)                                  0.0149389\n",
      "time/evaluation sampling (s)                          49.7171\n",
      "time/exploration sampling (s)                          2.07857\n",
      "time/logging (s)                                       0.274477\n",
      "time/saving (s)                                        0.0250012\n",
      "time/training (s)                                      4.14208\n",
      "time/epoch (s)                                        56.2522\n",
      "time/total (s)                                      2463.79\n",
      "Epoch                                                 45\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:06:48.196736 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 46 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 48000\n",
      "trainer/QF1 Loss                                       0.26069\n",
      "trainer/QF2 Loss                                       0.396598\n",
      "trainer/Policy Loss                                   -8.36131\n",
      "trainer/Q1 Predictions Mean                           16.4981\n",
      "trainer/Q1 Predictions Std                             1.06643\n",
      "trainer/Q1 Predictions Max                            18.5143\n",
      "trainer/Q1 Predictions Min                            11.8395\n",
      "trainer/Q2 Predictions Mean                           16.0873\n",
      "trainer/Q2 Predictions Std                             1.04827\n",
      "trainer/Q2 Predictions Max                            18.3833\n",
      "trainer/Q2 Predictions Min                            12.2349\n",
      "trainer/Q Targets Mean                                16.4032\n",
      "trainer/Q Targets Std                                  1.11107\n",
      "trainer/Q Targets Max                                 20.6083\n",
      "trainer/Q Targets Min                                 11.4064\n",
      "trainer/Log Pis Mean                                   8.07755\n",
      "trainer/Log Pis Std                                    2.60111\n",
      "trainer/Log Pis Max                                   21.9796\n",
      "trainer/Log Pis Min                                   -1.65137\n",
      "trainer/Policy mu Mean                                 0.0166117\n",
      "trainer/Policy mu Std                                  0.180138\n",
      "trainer/Policy mu Max                                  0.809159\n",
      "trainer/Policy mu Min                                 -0.809865\n",
      "trainer/Policy log std Mean                           -2.37401\n",
      "trainer/Policy log std Std                             0.221293\n",
      "trainer/Policy log std Max                            -1.57045\n",
      "trainer/Policy log std Min                            -4.24042\n",
      "trainer/Alpha                                          0.00957698\n",
      "trainer/Alpha Loss                                     0.3605\n",
      "exploration/num steps total                        48000\n",
      "exploration/num paths total                          108\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.945135\n",
      "exploration/Rewards Std                                0.134072\n",
      "exploration/Rewards Max                                1.99321\n",
      "exploration/Rewards Min                                0.693558\n",
      "exploration/Returns Mean                             945.135\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              945.135\n",
      "exploration/Returns Min                              945.135\n",
      "exploration/Actions Mean                               0.023326\n",
      "exploration/Actions Std                                0.145394\n",
      "exploration/Actions Max                                0.53584\n",
      "exploration/Actions Min                               -0.432056\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          945.135\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0702084\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0702084\n",
      "exploration/env_infos/final/reward_forward Min        -0.0702084\n",
      "exploration/env_infos/initial/reward_forward Mean      0.000492915\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.000492915\n",
      "exploration/env_infos/initial/reward_forward Min       0.000492915\n",
      "exploration/env_infos/reward_forward Mean              0.0107774\n",
      "exploration/env_infos/reward_forward Std               0.250157\n",
      "exploration/env_infos/reward_forward Max               1.05899\n",
      "exploration/env_infos/reward_forward Min              -0.887849\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0426707\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0426707\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0426707\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.111247\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.111247\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.111247\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0867344\n",
      "exploration/env_infos/reward_ctrl Std                  0.0368293\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0140323\n",
      "exploration/env_infos/reward_ctrl Min                 -0.306442\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00881973\n",
      "exploration/env_infos/final/torso_velocity Std         0.0535303\n",
      "exploration/env_infos/final/torso_velocity Max         0.0602387\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0702084\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.13908\n",
      "exploration/env_infos/initial/torso_velocity Std       0.222771\n",
      "exploration/env_infos/initial/torso_velocity Max       0.453395\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0366469\n",
      "exploration/env_infos/torso_velocity Mean             -0.00142183\n",
      "exploration/env_infos/torso_velocity Std               0.195306\n",
      "exploration/env_infos/torso_velocity Max               1.05899\n",
      "exploration/env_infos/torso_velocity Min              -1.06448\n",
      "evaluation/num steps total                             1.175e+06\n",
      "evaluation/num paths total                          1175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.945237\n",
      "evaluation/Rewards Std                                 0.0441843\n",
      "evaluation/Rewards Max                                 2.68014\n",
      "evaluation/Rewards Min                                -0.126264\n",
      "evaluation/Returns Mean                              945.237\n",
      "evaluation/Returns Std                                19.039\n",
      "evaluation/Returns Max                               983.249\n",
      "evaluation/Returns Min                               911.51\n",
      "evaluation/Actions Mean                                0.0299772\n",
      "evaluation/Actions Std                                 0.11554\n",
      "evaluation/Actions Max                                 0.891019\n",
      "evaluation/Actions Min                                -0.602983\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           945.237\n",
      "evaluation/env_infos/final/reward_forward Mean         4.72676e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.89388e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.16343e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -6.45567e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0630639\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0758711\n",
      "evaluation/env_infos/initial/reward_forward Max        0.274555\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.0900099\n",
      "evaluation/env_infos/reward_forward Mean              -0.00107828\n",
      "evaluation/env_infos/reward_forward Std                0.0693292\n",
      "evaluation/env_infos/reward_forward Max                1.64872\n",
      "evaluation/env_infos/reward_forward Min               -1.56566\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0560324\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.01761\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0228854\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0880218\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.288536\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.322038\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0390391\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.12626\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0569921\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0236252\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0174623\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.12626\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.08098e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.5717e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.16343e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.45567e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.183311\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.227089\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.596554\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.265225\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000945581\n",
      "evaluation/env_infos/torso_velocity Std                0.0720104\n",
      "evaluation/env_infos/torso_velocity Max                1.64872\n",
      "evaluation/env_infos/torso_velocity Min               -1.81344\n",
      "time/data storing (s)                                  0.0153732\n",
      "time/evaluation sampling (s)                          45.9349\n",
      "time/exploration sampling (s)                          1.92952\n",
      "time/logging (s)                                       0.304403\n",
      "time/saving (s)                                        0.0250503\n",
      "time/training (s)                                      4.78624\n",
      "time/epoch (s)                                        52.9955\n",
      "time/total (s)                                      2517.26\n",
      "Epoch                                                 46\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:07:41.222303 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 47 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 49000\n",
      "trainer/QF1 Loss                                       0.877301\n",
      "trainer/QF2 Loss                                       0.618322\n",
      "trainer/Policy Loss                                   -8.28306\n",
      "trainer/Q1 Predictions Mean                           16.6148\n",
      "trainer/Q1 Predictions Std                             1.35912\n",
      "trainer/Q1 Predictions Max                            21.0493\n",
      "trainer/Q1 Predictions Min                             6.88453\n",
      "trainer/Q2 Predictions Mean                           16.6658\n",
      "trainer/Q2 Predictions Std                             1.51217\n",
      "trainer/Q2 Predictions Max                            20.84\n",
      "trainer/Q2 Predictions Min                             4.99986\n",
      "trainer/Q Targets Mean                                16.6816\n",
      "trainer/Q Targets Std                                  1.99013\n",
      "trainer/Q Targets Max                                 26.9213\n",
      "trainer/Q Targets Min                                 -0.324779\n",
      "trainer/Log Pis Mean                                   8.68621\n",
      "trainer/Log Pis Std                                    2.62259\n",
      "trainer/Log Pis Max                                   22.9913\n",
      "trainer/Log Pis Min                                    0.80108\n",
      "trainer/Policy mu Mean                                 0.0220581\n",
      "trainer/Policy mu Std                                  0.286147\n",
      "trainer/Policy mu Max                                  4.63029\n",
      "trainer/Policy mu Min                                 -0.995513\n",
      "trainer/Policy log std Mean                           -2.41641\n",
      "trainer/Policy log std Std                             0.252931\n",
      "trainer/Policy log std Max                            -0.393446\n",
      "trainer/Policy log std Min                            -3.72222\n",
      "trainer/Alpha                                          0.00987878\n",
      "trainer/Alpha Loss                                     3.16858\n",
      "exploration/num steps total                        49000\n",
      "exploration/num paths total                          109\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.882308\n",
      "exploration/Rewards Std                                0.0360684\n",
      "exploration/Rewards Max                                1.03854\n",
      "exploration/Rewards Min                                0.626757\n",
      "exploration/Returns Mean                             882.308\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              882.308\n",
      "exploration/Returns Min                              882.308\n",
      "exploration/Actions Mean                               0.0168692\n",
      "exploration/Actions Std                                0.171395\n",
      "exploration/Actions Max                                0.552682\n",
      "exploration/Actions Min                               -0.480844\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          882.308\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00424933\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00424933\n",
      "exploration/env_infos/final/reward_forward Min        -0.00424933\n",
      "exploration/env_infos/initial/reward_forward Mean      0.189061\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.189061\n",
      "exploration/env_infos/initial/reward_forward Min       0.189061\n",
      "exploration/env_infos/reward_forward Mean             -0.0122155\n",
      "exploration/env_infos/reward_forward Std               0.133545\n",
      "exploration/env_infos/reward_forward Max               0.416275\n",
      "exploration/env_infos/reward_forward Min              -0.976574\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.124885\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.124885\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.124885\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0308108\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0308108\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0308108\n",
      "exploration/env_infos/reward_ctrl Mean                -0.118644\n",
      "exploration/env_infos/reward_ctrl Std                  0.036645\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0308108\n",
      "exploration/env_infos/reward_ctrl Min                 -0.449477\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00195266\n",
      "exploration/env_infos/final/torso_velocity Std         0.00165523\n",
      "exploration/env_infos/final/torso_velocity Max        -0.000412346\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00424933\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.13486\n",
      "exploration/env_infos/initial/torso_velocity Std       0.201679\n",
      "exploration/env_infos/initial/torso_velocity Max       0.350264\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.134745\n",
      "exploration/env_infos/torso_velocity Mean             -0.000783834\n",
      "exploration/env_infos/torso_velocity Std               0.0975979\n",
      "exploration/env_infos/torso_velocity Max               1.29537\n",
      "exploration/env_infos/torso_velocity Min              -0.976574\n",
      "evaluation/num steps total                             1.2e+06\n",
      "evaluation/num paths total                          1200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.89678\n",
      "evaluation/Rewards Std                                 0.0543421\n",
      "evaluation/Rewards Max                                 2.03347\n",
      "evaluation/Rewards Min                                 0.231041\n",
      "evaluation/Returns Mean                              896.78\n",
      "evaluation/Returns Std                                42.4512\n",
      "evaluation/Returns Max                               934.585\n",
      "evaluation/Returns Min                               779.073\n",
      "evaluation/Actions Mean                                0.0174217\n",
      "evaluation/Actions Std                                 0.160987\n",
      "evaluation/Actions Max                                 0.639008\n",
      "evaluation/Actions Min                                -0.508635\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           896.78\n",
      "evaluation/env_infos/final/reward_forward Mean         2.37413e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.94097e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.14993e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.23189e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0181699\n",
      "evaluation/env_infos/initial/reward_forward Std        0.187101\n",
      "evaluation/env_infos/initial/reward_forward Max        0.40654\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.349572\n",
      "evaluation/env_infos/reward_forward Mean              -0.00149659\n",
      "evaluation/env_infos/reward_forward Std                0.0576727\n",
      "evaluation/env_infos/reward_forward Max                1.08163\n",
      "evaluation/env_infos/reward_forward Min               -1.26378\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.10399\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0426337\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.068091\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.223655\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0888107\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.069329\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.020712\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.358337\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.104881\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0455126\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.020712\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.768959\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.09374e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.54936e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.14993e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -4.23189e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.16445\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.261685\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.620689\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.349572\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000937212\n",
      "evaluation/env_infos/torso_velocity Std                0.0598771\n",
      "evaluation/env_infos/torso_velocity Max                1.62201\n",
      "evaluation/env_infos/torso_velocity Min               -1.80214\n",
      "time/data storing (s)                                  0.0149396\n",
      "time/evaluation sampling (s)                          45.8587\n",
      "time/exploration sampling (s)                          2.07768\n",
      "time/logging (s)                                       0.277092\n",
      "time/saving (s)                                        0.0258098\n",
      "time/training (s)                                      4.19946\n",
      "time/epoch (s)                                        52.4537\n",
      "time/total (s)                                      2570.26\n",
      "Epoch                                                 47\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:08:34.330205 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 48 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 50000\n",
      "trainer/QF1 Loss                                       0.68695\n",
      "trainer/QF2 Loss                                       0.334784\n",
      "trainer/Policy Loss                                   -8.98509\n",
      "trainer/Q1 Predictions Mean                           17.1862\n",
      "trainer/Q1 Predictions Std                             1.20534\n",
      "trainer/Q1 Predictions Max                            22.7971\n",
      "trainer/Q1 Predictions Min                             8.27579\n",
      "trainer/Q2 Predictions Mean                           17.1943\n",
      "trainer/Q2 Predictions Std                             1.2929\n",
      "trainer/Q2 Predictions Max                            20.5245\n",
      "trainer/Q2 Predictions Min                             4.41752\n",
      "trainer/Q Targets Mean                                17.1019\n",
      "trainer/Q Targets Std                                  1.55228\n",
      "trainer/Q Targets Max                                 23.2711\n",
      "trainer/Q Targets Min                                 -0.508274\n",
      "trainer/Log Pis Mean                                   8.47044\n",
      "trainer/Log Pis Std                                    2.55468\n",
      "trainer/Log Pis Max                                   17.0898\n",
      "trainer/Log Pis Min                                   -0.939442\n",
      "trainer/Policy mu Mean                                 0.0228478\n",
      "trainer/Policy mu Std                                  0.224709\n",
      "trainer/Policy mu Max                                  1.81752\n",
      "trainer/Policy mu Min                                 -0.913232\n",
      "trainer/Policy log std Mean                           -2.4409\n",
      "trainer/Policy log std Std                             0.236179\n",
      "trainer/Policy log std Max                            -1.10258\n",
      "trainer/Policy log std Min                            -3.53782\n",
      "trainer/Alpha                                          0.00956304\n",
      "trainer/Alpha Loss                                     2.18767\n",
      "exploration/num steps total                        50000\n",
      "exploration/num paths total                          110\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.817469\n",
      "exploration/Rewards Std                                0.0583918\n",
      "exploration/Rewards Max                                0.975571\n",
      "exploration/Rewards Min                                0.619535\n",
      "exploration/Returns Mean                             817.469\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              817.469\n",
      "exploration/Returns Min                              817.469\n",
      "exploration/Actions Mean                              -0.0134352\n",
      "exploration/Actions Std                                0.213465\n",
      "exploration/Actions Max                                0.422652\n",
      "exploration/Actions Min                               -0.738856\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          817.469\n",
      "exploration/env_infos/final/reward_forward Mean       -0.241721\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.241721\n",
      "exploration/env_infos/final/reward_forward Min        -0.241721\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0838181\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0838181\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0838181\n",
      "exploration/env_infos/reward_forward Mean              0.0218235\n",
      "exploration/env_infos/reward_forward Std               0.121079\n",
      "exploration/env_infos/reward_forward Max               0.758295\n",
      "exploration/env_infos/reward_forward Min              -0.48519\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112242\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112242\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112242\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0529753\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0529753\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0529753\n",
      "exploration/env_infos/reward_ctrl Mean                -0.182991\n",
      "exploration/env_infos/reward_ctrl Std                  0.0582502\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0388561\n",
      "exploration/env_infos/reward_ctrl Min                 -0.380465\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0208127\n",
      "exploration/env_infos/final/torso_velocity Std         0.158365\n",
      "exploration/env_infos/final/torso_velocity Max         0.121559\n",
      "exploration/env_infos/final/torso_velocity Min        -0.241721\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0537056\n",
      "exploration/env_infos/initial/torso_velocity Std       0.186964\n",
      "exploration/env_infos/initial/torso_velocity Max       0.318041\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0838181\n",
      "exploration/env_infos/torso_velocity Mean              0.00532589\n",
      "exploration/env_infos/torso_velocity Std               0.110132\n",
      "exploration/env_infos/torso_velocity Max               0.855253\n",
      "exploration/env_infos/torso_velocity Min              -1.32992\n",
      "evaluation/num steps total                             1.225e+06\n",
      "evaluation/num paths total                          1225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.921884\n",
      "evaluation/Rewards Std                                 0.0738938\n",
      "evaluation/Rewards Max                                 2.24252\n",
      "evaluation/Rewards Min                                 0.323083\n",
      "evaluation/Returns Mean                              921.884\n",
      "evaluation/Returns Std                                35.4251\n",
      "evaluation/Returns Max                              1008.16\n",
      "evaluation/Returns Min                               847.333\n",
      "evaluation/Actions Mean                                0.0118951\n",
      "evaluation/Actions Std                                 0.150905\n",
      "evaluation/Actions Max                                 0.711747\n",
      "evaluation/Actions Min                                -0.621659\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           921.884\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00493017\n",
      "evaluation/env_infos/final/reward_forward Std          0.0488471\n",
      "evaluation/env_infos/final/reward_forward Max          0.154065\n",
      "evaluation/env_infos/final/reward_forward Min         -0.113913\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0112095\n",
      "evaluation/env_infos/initial/reward_forward Std        0.121187\n",
      "evaluation/env_infos/initial/reward_forward Max        0.254966\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.277752\n",
      "evaluation/env_infos/reward_forward Mean               0.00236278\n",
      "evaluation/env_infos/reward_forward Std                0.076406\n",
      "evaluation/env_infos/reward_forward Max                1.41569\n",
      "evaluation/env_infos/reward_forward Min               -0.910268\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0901857\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0231158\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0537598\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.152555\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0932661\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.111687\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0298169\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.620315\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0916553\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0259341\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0284596\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.771195\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00248919\n",
      "evaluation/env_infos/final/torso_velocity Std          0.053709\n",
      "evaluation/env_infos/final/torso_velocity Max          0.154065\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.262316\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.142203\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.221441\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.648204\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.277752\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000120921\n",
      "evaluation/env_infos/torso_velocity Std                0.0957934\n",
      "evaluation/env_infos/torso_velocity Max                1.41569\n",
      "evaluation/env_infos/torso_velocity Min               -1.92746\n",
      "time/data storing (s)                                  0.0157297\n",
      "time/evaluation sampling (s)                          46.0425\n",
      "time/exploration sampling (s)                          1.99459\n",
      "time/logging (s)                                       0.280948\n",
      "time/saving (s)                                        0.0277157\n",
      "time/training (s)                                      4.26254\n",
      "time/epoch (s)                                        52.624\n",
      "time/total (s)                                      2623.37\n",
      "Epoch                                                 48\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:09:27.168386 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 49 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 51000\n",
      "trainer/QF1 Loss                                       0.208237\n",
      "trainer/QF2 Loss                                       0.232698\n",
      "trainer/Policy Loss                                  -11.1027\n",
      "trainer/Q1 Predictions Mean                           17.3326\n",
      "trainer/Q1 Predictions Std                             1.20568\n",
      "trainer/Q1 Predictions Max                            21.6419\n",
      "trainer/Q1 Predictions Min                            12.4196\n",
      "trainer/Q2 Predictions Mean                           17.5687\n",
      "trainer/Q2 Predictions Std                             1.24893\n",
      "trainer/Q2 Predictions Max                            23.0054\n",
      "trainer/Q2 Predictions Min                            12.368\n",
      "trainer/Q Targets Mean                                17.429\n",
      "trainer/Q Targets Std                                  1.21843\n",
      "trainer/Q Targets Max                                 22.6437\n",
      "trainer/Q Targets Min                                 12.6184\n",
      "trainer/Log Pis Mean                                   6.48455\n",
      "trainer/Log Pis Std                                    2.51198\n",
      "trainer/Log Pis Max                                   14.1531\n",
      "trainer/Log Pis Min                                   -1.68844\n",
      "trainer/Policy mu Mean                                -0.0207469\n",
      "trainer/Policy mu Std                                  0.144019\n",
      "trainer/Policy mu Max                                  0.867701\n",
      "trainer/Policy mu Min                                 -0.581624\n",
      "trainer/Policy log std Mean                           -2.22352\n",
      "trainer/Policy log std Std                             0.227578\n",
      "trainer/Policy log std Max                            -1.33861\n",
      "trainer/Policy log std Min                            -3.59848\n",
      "trainer/Alpha                                          0.0095015\n",
      "trainer/Alpha Loss                                    -7.05444\n",
      "exploration/num steps total                        51000\n",
      "exploration/num paths total                          111\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.897531\n",
      "exploration/Rewards Std                                0.0881487\n",
      "exploration/Rewards Max                                1.81492\n",
      "exploration/Rewards Min                                0.611674\n",
      "exploration/Returns Mean                             897.531\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              897.531\n",
      "exploration/Returns Min                              897.531\n",
      "exploration/Actions Mean                              -0.010768\n",
      "exploration/Actions Std                                0.168778\n",
      "exploration/Actions Max                                0.652924\n",
      "exploration/Actions Min                               -0.567373\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          897.531\n",
      "exploration/env_infos/final/reward_forward Mean        0.00341738\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00341738\n",
      "exploration/env_infos/final/reward_forward Min         0.00341738\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0642612\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0642612\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0642612\n",
      "exploration/env_infos/reward_forward Mean              0.0307003\n",
      "exploration/env_infos/reward_forward Std               0.168501\n",
      "exploration/env_infos/reward_forward Max               0.988661\n",
      "exploration/env_infos/reward_forward Min              -0.747316\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0983733\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0983733\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0983733\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.182305\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.182305\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.182305\n",
      "exploration/env_infos/reward_ctrl Mean                -0.114407\n",
      "exploration/env_infos/reward_ctrl Std                  0.0586568\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0168186\n",
      "exploration/env_infos/reward_ctrl Min                 -0.388326\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00739281\n",
      "exploration/env_infos/final/torso_velocity Std         0.0207931\n",
      "exploration/env_infos/final/torso_velocity Max         0.0108851\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0364809\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0887312\n",
      "exploration/env_infos/initial/torso_velocity Std       0.246148\n",
      "exploration/env_infos/initial/torso_velocity Max       0.436019\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.105564\n",
      "exploration/env_infos/torso_velocity Mean              0.00660264\n",
      "exploration/env_infos/torso_velocity Std               0.164446\n",
      "exploration/env_infos/torso_velocity Max               0.988661\n",
      "exploration/env_infos/torso_velocity Min              -1.02051\n",
      "evaluation/num steps total                             1.25e+06\n",
      "evaluation/num paths total                          1250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.962668\n",
      "evaluation/Rewards Std                                 0.0463069\n",
      "evaluation/Rewards Max                                 2.28031\n",
      "evaluation/Rewards Min                                 0.564414\n",
      "evaluation/Returns Mean                              962.668\n",
      "evaluation/Returns Std                                22.6938\n",
      "evaluation/Returns Max                               987.396\n",
      "evaluation/Returns Min                               907.869\n",
      "evaluation/Actions Mean                               -0.00583817\n",
      "evaluation/Actions Std                                 0.0997893\n",
      "evaluation/Actions Max                                 0.666086\n",
      "evaluation/Actions Min                                -0.457328\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           962.668\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000630827\n",
      "evaluation/env_infos/final/reward_forward Std          0.00310644\n",
      "evaluation/env_infos/final/reward_forward Max          0.0158491\n",
      "evaluation/env_infos/final/reward_forward Min         -5.88905e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0454466\n",
      "evaluation/env_infos/initial/reward_forward Std        0.129938\n",
      "evaluation/env_infos/initial/reward_forward Max        0.266773\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.245016\n",
      "evaluation/env_infos/reward_forward Mean              -0.00011548\n",
      "evaluation/env_infos/reward_forward Std                0.0813685\n",
      "evaluation/env_infos/reward_forward Max                1.67531\n",
      "evaluation/env_infos/reward_forward Min               -1.01363\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0404804\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0243761\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0166444\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0953851\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0939475\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.118247\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00489233\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.435586\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0399679\n",
      "evaluation/env_infos/reward_ctrl Std                   0.025826\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00349007\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.435586\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000731591\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0062304\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0506072\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.0117919\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.124036\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.24894\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.645642\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.245016\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000664109\n",
      "evaluation/env_infos/torso_velocity Std                0.0791328\n",
      "evaluation/env_infos/torso_velocity Max                1.67531\n",
      "evaluation/env_infos/torso_velocity Min               -2.16475\n",
      "time/data storing (s)                                  0.014831\n",
      "time/evaluation sampling (s)                          45.7238\n",
      "time/exploration sampling (s)                          2.03801\n",
      "time/logging (s)                                       0.283567\n",
      "time/saving (s)                                        0.0302505\n",
      "time/training (s)                                      4.25268\n",
      "time/epoch (s)                                        52.3432\n",
      "time/total (s)                                      2676.21\n",
      "Epoch                                                 49\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:10:20.948529 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 50 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 52000\n",
      "trainer/QF1 Loss                                       0.293024\n",
      "trainer/QF2 Loss                                       0.399887\n",
      "trainer/Policy Loss                                  -10.2404\n",
      "trainer/Q1 Predictions Mean                           17.7121\n",
      "trainer/Q1 Predictions Std                             1.64034\n",
      "trainer/Q1 Predictions Max                            20.3336\n",
      "trainer/Q1 Predictions Min                             0.466837\n",
      "trainer/Q2 Predictions Mean                           17.7732\n",
      "trainer/Q2 Predictions Std                             1.57373\n",
      "trainer/Q2 Predictions Max                            21.2394\n",
      "trainer/Q2 Predictions Min                             3.76451\n",
      "trainer/Q Targets Mean                                17.5778\n",
      "trainer/Q Targets Std                                  1.66124\n",
      "trainer/Q Targets Max                                 22.3642\n",
      "trainer/Q Targets Min                                 -0.0683153\n",
      "trainer/Log Pis Mean                                   7.7778\n",
      "trainer/Log Pis Std                                    2.52596\n",
      "trainer/Log Pis Max                                   15.9223\n",
      "trainer/Log Pis Min                                   -2.30921\n",
      "trainer/Policy mu Mean                                -0.0739183\n",
      "trainer/Policy mu Std                                  0.224506\n",
      "trainer/Policy mu Max                                  1.25345\n",
      "trainer/Policy mu Min                                 -1.14946\n",
      "trainer/Policy log std Mean                           -2.32652\n",
      "trainer/Policy log std Std                             0.247873\n",
      "trainer/Policy log std Max                            -1.16432\n",
      "trainer/Policy log std Min                            -3.47952\n",
      "trainer/Alpha                                          0.00974095\n",
      "trainer/Alpha Loss                                    -1.0294\n",
      "exploration/num steps total                        52000\n",
      "exploration/num paths total                          112\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.894194\n",
      "exploration/Rewards Std                                0.0786866\n",
      "exploration/Rewards Max                                1.44704\n",
      "exploration/Rewards Min                                0.681374\n",
      "exploration/Returns Mean                             894.194\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              894.194\n",
      "exploration/Returns Min                              894.194\n",
      "exploration/Actions Mean                              -0.0601788\n",
      "exploration/Actions Std                                0.163612\n",
      "exploration/Actions Max                                0.394911\n",
      "exploration/Actions Min                               -0.713957\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          894.194\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0203287\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0203287\n",
      "exploration/env_infos/final/reward_forward Min        -0.0203287\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.173287\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.173287\n",
      "exploration/env_infos/initial/reward_forward Min      -0.173287\n",
      "exploration/env_infos/reward_forward Mean              0.00703227\n",
      "exploration/env_infos/reward_forward Std               0.177345\n",
      "exploration/env_infos/reward_forward Max               0.877949\n",
      "exploration/env_infos/reward_forward Min              -0.666308\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.130039\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.130039\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.130039\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.05295\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.05295\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.05295\n",
      "exploration/env_infos/reward_ctrl Mean                -0.121562\n",
      "exploration/env_infos/reward_ctrl Std                  0.051033\n",
      "exploration/env_infos/reward_ctrl Max                 -0.018317\n",
      "exploration/env_infos/reward_ctrl Min                 -0.318626\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0220443\n",
      "exploration/env_infos/final/torso_velocity Std         0.0396287\n",
      "exploration/env_infos/final/torso_velocity Max         0.0749964\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0203287\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0680483\n",
      "exploration/env_infos/initial/torso_velocity Std       0.243254\n",
      "exploration/env_infos/initial/torso_velocity Max       0.401028\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.173287\n",
      "exploration/env_infos/torso_velocity Mean              0.00682342\n",
      "exploration/env_infos/torso_velocity Std               0.163278\n",
      "exploration/env_infos/torso_velocity Max               1.09967\n",
      "exploration/env_infos/torso_velocity Min              -1.57512\n",
      "evaluation/num steps total                             1.275e+06\n",
      "evaluation/num paths total                          1275\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.888104\n",
      "evaluation/Rewards Std                                 0.0595595\n",
      "evaluation/Rewards Max                                 2.51009\n",
      "evaluation/Rewards Min                                 0.2641\n",
      "evaluation/Returns Mean                              888.104\n",
      "evaluation/Returns Std                                50.1196\n",
      "evaluation/Returns Max                               947.636\n",
      "evaluation/Returns Min                               747.576\n",
      "evaluation/Actions Mean                               -0.0826793\n",
      "evaluation/Actions Std                                 0.147018\n",
      "evaluation/Actions Max                                 0.777934\n",
      "evaluation/Actions Min                                -0.730578\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           888.104\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.47173e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.96864e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.31372e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.613e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0668752\n",
      "evaluation/env_infos/initial/reward_forward Std        0.121555\n",
      "evaluation/env_infos/initial/reward_forward Max        0.12493\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.410766\n",
      "evaluation/env_infos/reward_forward Mean              -0.000674608\n",
      "evaluation/env_infos/reward_forward Std                0.0581002\n",
      "evaluation/env_infos/reward_forward Max                1.27802\n",
      "evaluation/env_infos/reward_forward Min               -1.34209\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.114423\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0497822\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0534794\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.255166\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.17113\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.142297\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0408681\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.7359\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.1138\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0505422\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0187846\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.7359\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.21159e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          2.97827e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          5.31372e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.16432e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0974589\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.251271\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.658499\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.410766\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00205081\n",
      "evaluation/env_infos/torso_velocity Std                0.0609253\n",
      "evaluation/env_infos/torso_velocity Max                1.27802\n",
      "evaluation/env_infos/torso_velocity Min               -1.89929\n",
      "time/data storing (s)                                  0.0152958\n",
      "time/evaluation sampling (s)                          46.3982\n",
      "time/exploration sampling (s)                          2.12117\n",
      "time/logging (s)                                       0.282472\n",
      "time/saving (s)                                        0.0542988\n",
      "time/training (s)                                      4.38022\n",
      "time/epoch (s)                                        53.2517\n",
      "time/total (s)                                      2729.98\n",
      "Epoch                                                 50\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:11:16.189336 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 51 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 53000\n",
      "trainer/QF1 Loss                                       0.271532\n",
      "trainer/QF2 Loss                                       0.234731\n",
      "trainer/Policy Loss                                  -10.947\n",
      "trainer/Q1 Predictions Mean                           18.1634\n",
      "trainer/Q1 Predictions Std                             1.30718\n",
      "trainer/Q1 Predictions Max                            20.6321\n",
      "trainer/Q1 Predictions Min                            13.4472\n",
      "trainer/Q2 Predictions Mean                           18.0291\n",
      "trainer/Q2 Predictions Std                             1.27924\n",
      "trainer/Q2 Predictions Max                            20.5353\n",
      "trainer/Q2 Predictions Min                            13.0665\n",
      "trainer/Q Targets Mean                                17.983\n",
      "trainer/Q Targets Std                                  1.22604\n",
      "trainer/Q Targets Max                                 20.1846\n",
      "trainer/Q Targets Min                                 13.977\n",
      "trainer/Log Pis Mean                                   7.38323\n",
      "trainer/Log Pis Std                                    2.6501\n",
      "trainer/Log Pis Max                                   15.7071\n",
      "trainer/Log Pis Min                                    0.25939\n",
      "trainer/Policy mu Mean                                -0.019235\n",
      "trainer/Policy mu Std                                  0.207376\n",
      "trainer/Policy mu Max                                  2.27677\n",
      "trainer/Policy mu Min                                 -1.21554\n",
      "trainer/Policy log std Mean                           -2.27898\n",
      "trainer/Policy log std Std                             0.227731\n",
      "trainer/Policy log std Max                            -0.854142\n",
      "trainer/Policy log std Min                            -3.46364\n",
      "trainer/Alpha                                          0.00941383\n",
      "trainer/Alpha Loss                                    -2.87688\n",
      "exploration/num steps total                        53000\n",
      "exploration/num paths total                          113\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.89544\n",
      "exploration/Rewards Std                                0.0986078\n",
      "exploration/Rewards Max                                1.54339\n",
      "exploration/Rewards Min                                0.685495\n",
      "exploration/Returns Mean                             895.44\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              895.44\n",
      "exploration/Returns Min                              895.44\n",
      "exploration/Actions Mean                              -0.0163161\n",
      "exploration/Actions Std                                0.180459\n",
      "exploration/Actions Max                                0.724943\n",
      "exploration/Actions Min                               -0.590461\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          895.44\n",
      "exploration/env_infos/final/reward_forward Mean       -0.026889\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.026889\n",
      "exploration/env_infos/final/reward_forward Min        -0.026889\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00519022\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00519022\n",
      "exploration/env_infos/initial/reward_forward Min       0.00519022\n",
      "exploration/env_infos/reward_forward Mean              0.0227983\n",
      "exploration/env_infos/reward_forward Std               0.187253\n",
      "exploration/env_infos/reward_forward Max               0.905604\n",
      "exploration/env_infos/reward_forward Min              -0.439708\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.127584\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.127584\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.127584\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.11068\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.11068\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.11068\n",
      "exploration/env_infos/reward_ctrl Mean                -0.131327\n",
      "exploration/env_infos/reward_ctrl Std                  0.0489018\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0169592\n",
      "exploration/env_infos/reward_ctrl Min                 -0.432485\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0266334\n",
      "exploration/env_infos/final/torso_velocity Std         0.0310348\n",
      "exploration/env_infos/final/torso_velocity Max         0.0115034\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0645146\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.138176\n",
      "exploration/env_infos/initial/torso_velocity Std       0.155714\n",
      "exploration/env_infos/initial/torso_velocity Max       0.356676\n",
      "exploration/env_infos/initial/torso_velocity Min       0.00519022\n",
      "exploration/env_infos/torso_velocity Mean              0.0134325\n",
      "exploration/env_infos/torso_velocity Std               0.143685\n",
      "exploration/env_infos/torso_velocity Max               1.17783\n",
      "exploration/env_infos/torso_velocity Min              -1.66541\n",
      "evaluation/num steps total                             1.3e+06\n",
      "evaluation/num paths total                          1300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.928051\n",
      "evaluation/Rewards Std                                 0.0359244\n",
      "evaluation/Rewards Max                                 2.18752\n",
      "evaluation/Rewards Min                                -1.56858\n",
      "evaluation/Returns Mean                              928.051\n",
      "evaluation/Returns Std                                18.6305\n",
      "evaluation/Returns Max                               950.609\n",
      "evaluation/Returns Min                               875.739\n",
      "evaluation/Actions Mean                               -0.00775562\n",
      "evaluation/Actions Std                                 0.135062\n",
      "evaluation/Actions Max                                 0.9914\n",
      "evaluation/Actions Min                                -0.900972\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           928.051\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.63253e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.48961e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.78955e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.15408e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0174271\n",
      "evaluation/env_infos/initial/reward_forward Std        0.129711\n",
      "evaluation/env_infos/initial/reward_forward Max        0.190458\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.382658\n",
      "evaluation/env_infos/reward_forward Mean               0.00198143\n",
      "evaluation/env_infos/reward_forward Std                0.0536536\n",
      "evaluation/env_infos/reward_forward Max                1.53049\n",
      "evaluation/env_infos/reward_forward Min               -1.16457\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0723879\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0189472\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0481122\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.123854\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0635303\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.077167\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.020159\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.415893\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0732076\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0284111\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0147749\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.56858\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.87143e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.04986e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          4.83108e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.86353e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.154062\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.226479\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.616723\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.382658\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000241525\n",
      "evaluation/env_infos/torso_velocity Std                0.052514\n",
      "evaluation/env_infos/torso_velocity Max                1.53049\n",
      "evaluation/env_infos/torso_velocity Min               -1.71429\n",
      "time/data storing (s)                                  0.0208632\n",
      "time/evaluation sampling (s)                          47.4362\n",
      "time/exploration sampling (s)                          2.50986\n",
      "time/logging (s)                                       0.272715\n",
      "time/saving (s)                                        0.031651\n",
      "time/training (s)                                      4.39189\n",
      "time/epoch (s)                                        54.6632\n",
      "time/total (s)                                      2785.21\n",
      "Epoch                                                 51\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:12:09.415901 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 52 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 54000\n",
      "trainer/QF1 Loss                                       0.411256\n",
      "trainer/QF2 Loss                                       0.4295\n",
      "trainer/Policy Loss                                  -10.0532\n",
      "trainer/Q1 Predictions Mean                           18.0708\n",
      "trainer/Q1 Predictions Std                             1.41317\n",
      "trainer/Q1 Predictions Max                            20.6039\n",
      "trainer/Q1 Predictions Min                            12.1441\n",
      "trainer/Q2 Predictions Mean                           18.064\n",
      "trainer/Q2 Predictions Std                             1.4998\n",
      "trainer/Q2 Predictions Max                            20.5114\n",
      "trainer/Q2 Predictions Min                            10.9527\n",
      "trainer/Q Targets Mean                                18.2116\n",
      "trainer/Q Targets Std                                  1.36193\n",
      "trainer/Q Targets Max                                 21.0542\n",
      "trainer/Q Targets Min                                 11.4137\n",
      "trainer/Log Pis Mean                                   8.26345\n",
      "trainer/Log Pis Std                                    3.10065\n",
      "trainer/Log Pis Max                                   22.0747\n",
      "trainer/Log Pis Min                                   -4.46762\n",
      "trainer/Policy mu Mean                                 0.0117149\n",
      "trainer/Policy mu Std                                  0.226763\n",
      "trainer/Policy mu Max                                  1.15895\n",
      "trainer/Policy mu Min                                 -1.04708\n",
      "trainer/Policy log std Mean                           -2.41702\n",
      "trainer/Policy log std Std                             0.25697\n",
      "trainer/Policy log std Max                            -1.49496\n",
      "trainer/Policy log std Min                            -4.08579\n",
      "trainer/Alpha                                          0.0087328\n",
      "trainer/Alpha Loss                                     1.24906\n",
      "exploration/num steps total                        54000\n",
      "exploration/num paths total                          114\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.722054\n",
      "exploration/Rewards Std                                0.0839564\n",
      "exploration/Rewards Max                                1.60166\n",
      "exploration/Rewards Min                                0.403662\n",
      "exploration/Returns Mean                             722.054\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              722.054\n",
      "exploration/Returns Min                              722.054\n",
      "exploration/Actions Mean                               0.016899\n",
      "exploration/Actions Std                                0.266744\n",
      "exploration/Actions Max                                0.807685\n",
      "exploration/Actions Min                               -0.649179\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          722.054\n",
      "exploration/env_infos/final/reward_forward Mean        0.122698\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.122698\n",
      "exploration/env_infos/final/reward_forward Min         0.122698\n",
      "exploration/env_infos/initial/reward_forward Mean      0.171701\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.171701\n",
      "exploration/env_infos/initial/reward_forward Min       0.171701\n",
      "exploration/env_infos/reward_forward Mean              0.00810033\n",
      "exploration/env_infos/reward_forward Std               0.0769775\n",
      "exploration/env_infos/reward_forward Max               0.304633\n",
      "exploration/env_infos/reward_forward Min              -0.561667\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.205945\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.205945\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.205945\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.215063\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.215063\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.215063\n",
      "exploration/env_infos/reward_ctrl Mean                -0.285751\n",
      "exploration/env_infos/reward_ctrl Std                  0.0575809\n",
      "exploration/env_infos/reward_ctrl Max                 -0.140223\n",
      "exploration/env_infos/reward_ctrl Min                 -0.596338\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0328918\n",
      "exploration/env_infos/final/torso_velocity Std         0.0636416\n",
      "exploration/env_infos/final/torso_velocity Max         0.122698\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0171568\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.22386\n",
      "exploration/env_infos/initial/torso_velocity Std       0.192726\n",
      "exploration/env_infos/initial/torso_velocity Max       0.481616\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0182616\n",
      "exploration/env_infos/torso_velocity Mean              4.10843e-05\n",
      "exploration/env_infos/torso_velocity Std               0.0784154\n",
      "exploration/env_infos/torso_velocity Max               0.881123\n",
      "exploration/env_infos/torso_velocity Min              -0.841397\n",
      "evaluation/num steps total                             1.325e+06\n",
      "evaluation/num paths total                          1325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.822773\n",
      "evaluation/Rewards Std                                 0.0853484\n",
      "evaluation/Rewards Max                                 2.01038\n",
      "evaluation/Rewards Min                                 0.0979388\n",
      "evaluation/Returns Mean                              822.773\n",
      "evaluation/Returns Std                                76.3214\n",
      "evaluation/Returns Max                               954.36\n",
      "evaluation/Returns Min                               727.948\n",
      "evaluation/Actions Mean                                0.0375083\n",
      "evaluation/Actions Std                                 0.208257\n",
      "evaluation/Actions Max                                 0.811352\n",
      "evaluation/Actions Min                                -0.771633\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           822.773\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.00144996\n",
      "evaluation/env_infos/final/reward_forward Std          0.00710417\n",
      "evaluation/env_infos/final/reward_forward Max          9.3921e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0362532\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0287337\n",
      "evaluation/env_infos/initial/reward_forward Std        0.137549\n",
      "evaluation/env_infos/initial/reward_forward Max        0.220398\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.277305\n",
      "evaluation/env_infos/reward_forward Mean              -0.00256056\n",
      "evaluation/env_infos/reward_forward Std                0.0614509\n",
      "evaluation/env_infos/reward_forward Max                1.24028\n",
      "evaluation/env_infos/reward_forward Min               -1.16497\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.177834\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0779341\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0442039\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.276964\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.131447\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0716955\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0388618\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.31796\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.179112\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0799833\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0220169\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.902061\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00117304\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0119516\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0919945\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.0362532\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.112554\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237499\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.618242\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.277305\n",
      "evaluation/env_infos/torso_velocity Mean              -6.64981e-06\n",
      "evaluation/env_infos/torso_velocity Std                0.0731224\n",
      "evaluation/env_infos/torso_velocity Max                1.24028\n",
      "evaluation/env_infos/torso_velocity Min               -1.6937\n",
      "time/data storing (s)                                  0.0148326\n",
      "time/evaluation sampling (s)                          46.0231\n",
      "time/exploration sampling (s)                          2.0672\n",
      "time/logging (s)                                       0.288844\n",
      "time/saving (s)                                        0.0268396\n",
      "time/training (s)                                      4.30438\n",
      "time/epoch (s)                                        52.7252\n",
      "time/total (s)                                      2838.46\n",
      "Epoch                                                 52\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:13:03.111264 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 53 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 55000\n",
      "trainer/QF1 Loss                                       0.289075\n",
      "trainer/QF2 Loss                                       0.287067\n",
      "trainer/Policy Loss                                  -11.1346\n",
      "trainer/Q1 Predictions Mean                           18.5209\n",
      "trainer/Q1 Predictions Std                             1.123\n",
      "trainer/Q1 Predictions Max                            22.4081\n",
      "trainer/Q1 Predictions Min                            12.5968\n",
      "trainer/Q2 Predictions Mean                           18.7029\n",
      "trainer/Q2 Predictions Std                             1.12317\n",
      "trainer/Q2 Predictions Max                            21.9571\n",
      "trainer/Q2 Predictions Min                            13.4467\n",
      "trainer/Q Targets Mean                                18.7556\n",
      "trainer/Q Targets Std                                  1.06229\n",
      "trainer/Q Targets Max                                 22.6581\n",
      "trainer/Q Targets Min                                 13.8481\n",
      "trainer/Log Pis Mean                                   7.65517\n",
      "trainer/Log Pis Std                                    2.78626\n",
      "trainer/Log Pis Max                                   25.9324\n",
      "trainer/Log Pis Min                                   -0.96148\n",
      "trainer/Policy mu Mean                                -0.0073427\n",
      "trainer/Policy mu Std                                  0.240684\n",
      "trainer/Policy mu Max                                  4.56094\n",
      "trainer/Policy mu Min                                 -1.51809\n",
      "trainer/Policy log std Mean                           -2.33591\n",
      "trainer/Policy log std Std                             0.26438\n",
      "trainer/Policy log std Max                             0.647098\n",
      "trainer/Policy log std Min                            -3.28238\n",
      "trainer/Alpha                                          0.00800511\n",
      "trainer/Alpha Loss                                    -1.66477\n",
      "exploration/num steps total                        55000\n",
      "exploration/num paths total                          115\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.868948\n",
      "exploration/Rewards Std                                0.05879\n",
      "exploration/Rewards Max                                1.05191\n",
      "exploration/Rewards Min                                0.430191\n",
      "exploration/Returns Mean                             868.948\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              868.948\n",
      "exploration/Returns Min                              868.948\n",
      "exploration/Actions Mean                               0.0136658\n",
      "exploration/Actions Std                                0.18223\n",
      "exploration/Actions Max                                0.624449\n",
      "exploration/Actions Min                               -0.498382\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          868.948\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0113681\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0113681\n",
      "exploration/env_infos/final/reward_forward Min        -0.0113681\n",
      "exploration/env_infos/initial/reward_forward Mean      0.065241\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.065241\n",
      "exploration/env_infos/initial/reward_forward Min       0.065241\n",
      "exploration/env_infos/reward_forward Mean              0.000323994\n",
      "exploration/env_infos/reward_forward Std               0.0597402\n",
      "exploration/env_infos/reward_forward Max               0.244571\n",
      "exploration/env_infos/reward_forward Min              -0.471643\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.159769\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.159769\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.159769\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0372142\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0372142\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0372142\n",
      "exploration/env_infos/reward_ctrl Mean                -0.133578\n",
      "exploration/env_infos/reward_ctrl Std                  0.0566094\n",
      "exploration/env_infos/reward_ctrl Max                 -0.015335\n",
      "exploration/env_infos/reward_ctrl Min                 -0.569809\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0549369\n",
      "exploration/env_infos/final/torso_velocity Std         0.117547\n",
      "exploration/env_infos/final/torso_velocity Max         0.0622116\n",
      "exploration/env_infos/final/torso_velocity Min        -0.215654\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.131947\n",
      "exploration/env_infos/initial/torso_velocity Std       0.112844\n",
      "exploration/env_infos/initial/torso_velocity Max       0.290854\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0397479\n",
      "exploration/env_infos/torso_velocity Mean              0.000889712\n",
      "exploration/env_infos/torso_velocity Std               0.110822\n",
      "exploration/env_infos/torso_velocity Max               0.612309\n",
      "exploration/env_infos/torso_velocity Min              -0.90398\n",
      "evaluation/num steps total                             1.35e+06\n",
      "evaluation/num paths total                          1350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.872788\n",
      "evaluation/Rewards Std                                 0.0497028\n",
      "evaluation/Rewards Max                                 1.75858\n",
      "evaluation/Rewards Min                                -2.05221\n",
      "evaluation/Returns Mean                              872.788\n",
      "evaluation/Returns Std                                36.5152\n",
      "evaluation/Returns Max                               964.313\n",
      "evaluation/Returns Min                               814.251\n",
      "evaluation/Actions Mean                                0.00822655\n",
      "evaluation/Actions Std                                 0.178544\n",
      "evaluation/Actions Max                                 0.999994\n",
      "evaluation/Actions Min                                -0.972477\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           872.788\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.17016e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.78254e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.51375e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.47176e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00760059\n",
      "evaluation/env_infos/initial/reward_forward Std        0.110331\n",
      "evaluation/env_infos/initial/reward_forward Max        0.177632\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.215171\n",
      "evaluation/env_infos/reward_forward Mean              -0.00253806\n",
      "evaluation/env_infos/reward_forward Std                0.0499767\n",
      "evaluation/env_infos/reward_forward Max                1.12559\n",
      "evaluation/env_infos/reward_forward Min               -1.50134\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.127065\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0369734\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0356511\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.186549\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0732902\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.080896\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0154466\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.356855\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.127783\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0473646\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0154466\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.05221\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -3.80931e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.55372e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.79277e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.84069e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.1499\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.225241\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.709502\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.215171\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0028554\n",
      "evaluation/env_infos/torso_velocity Std                0.066903\n",
      "evaluation/env_infos/torso_velocity Max                1.12559\n",
      "evaluation/env_infos/torso_velocity Min               -1.84568\n",
      "time/data storing (s)                                  0.0155694\n",
      "time/evaluation sampling (s)                          46.526\n",
      "time/exploration sampling (s)                          2.06731\n",
      "time/logging (s)                                       0.277397\n",
      "time/saving (s)                                        0.0305009\n",
      "time/training (s)                                      4.24405\n",
      "time/epoch (s)                                        53.1608\n",
      "time/total (s)                                      2892.14\n",
      "Epoch                                                 53\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:13:57.337759 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 54 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 56000\n",
      "trainer/QF1 Loss                                       0.283367\n",
      "trainer/QF2 Loss                                       0.378746\n",
      "trainer/Policy Loss                                  -11.7902\n",
      "trainer/Q1 Predictions Mean                           19.1171\n",
      "trainer/Q1 Predictions Std                             1.3644\n",
      "trainer/Q1 Predictions Max                            24.6508\n",
      "trainer/Q1 Predictions Min                            13.8927\n",
      "trainer/Q2 Predictions Mean                           18.9021\n",
      "trainer/Q2 Predictions Std                             1.31006\n",
      "trainer/Q2 Predictions Max                            21.8916\n",
      "trainer/Q2 Predictions Min                            12.1716\n",
      "trainer/Q Targets Mean                                19.0475\n",
      "trainer/Q Targets Std                                  1.3049\n",
      "trainer/Q Targets Max                                 27.2682\n",
      "trainer/Q Targets Min                                 14.4653\n",
      "trainer/Log Pis Mean                                   7.3846\n",
      "trainer/Log Pis Std                                    3.20329\n",
      "trainer/Log Pis Max                                   38.9983\n",
      "trainer/Log Pis Min                                   -0.304516\n",
      "trainer/Policy mu Mean                                -0.00183205\n",
      "trainer/Policy mu Std                                  0.246005\n",
      "trainer/Policy mu Max                                  5.3112\n",
      "trainer/Policy mu Min                                 -2.7155\n",
      "trainer/Policy log std Mean                           -2.28223\n",
      "trainer/Policy log std Std                             0.228988\n",
      "trainer/Policy log std Max                            -0.944158\n",
      "trainer/Policy log std Min                            -3.47409\n",
      "trainer/Alpha                                          0.00876292\n",
      "trainer/Alpha Loss                                    -2.91523\n",
      "exploration/num steps total                        56000\n",
      "exploration/num paths total                          116\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.921287\n",
      "exploration/Rewards Std                                0.18634\n",
      "exploration/Rewards Max                                1.99311\n",
      "exploration/Rewards Min                                0.610969\n",
      "exploration/Returns Mean                             921.287\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              921.287\n",
      "exploration/Returns Min                              921.287\n",
      "exploration/Actions Mean                              -0.0214098\n",
      "exploration/Actions Std                                0.181493\n",
      "exploration/Actions Max                                0.571712\n",
      "exploration/Actions Min                               -0.540214\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          921.287\n",
      "exploration/env_infos/final/reward_forward Mean       -0.121256\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.121256\n",
      "exploration/env_infos/final/reward_forward Min        -0.121256\n",
      "exploration/env_infos/initial/reward_forward Mean      0.065255\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.065255\n",
      "exploration/env_infos/initial/reward_forward Min       0.065255\n",
      "exploration/env_infos/reward_forward Mean              0.0674221\n",
      "exploration/env_infos/reward_forward Std               0.27036\n",
      "exploration/env_infos/reward_forward Max               1.10485\n",
      "exploration/env_infos/reward_forward Min              -0.623582\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.123274\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.123274\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.123274\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.112924\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.112924\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.112924\n",
      "exploration/env_infos/reward_ctrl Mean                -0.133592\n",
      "exploration/env_infos/reward_ctrl Std                  0.0598368\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0223319\n",
      "exploration/env_infos/reward_ctrl Min                 -0.389031\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0177431\n",
      "exploration/env_infos/final/torso_velocity Std         0.0734146\n",
      "exploration/env_infos/final/torso_velocity Max         0.0409712\n",
      "exploration/env_infos/final/torso_velocity Min        -0.121256\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.115548\n",
      "exploration/env_infos/initial/torso_velocity Std       0.160077\n",
      "exploration/env_infos/initial/torso_velocity Max       0.331848\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0504604\n",
      "exploration/env_infos/torso_velocity Mean              0.0254975\n",
      "exploration/env_infos/torso_velocity Std               0.204124\n",
      "exploration/env_infos/torso_velocity Max               1.10485\n",
      "exploration/env_infos/torso_velocity Min              -1.11594\n",
      "evaluation/num steps total                             1.375e+06\n",
      "evaluation/num paths total                          1375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.940133\n",
      "evaluation/Rewards Std                                 0.0674483\n",
      "evaluation/Rewards Max                                 2.48742\n",
      "evaluation/Rewards Min                                -2.01543\n",
      "evaluation/Returns Mean                              940.133\n",
      "evaluation/Returns Std                                19.7506\n",
      "evaluation/Returns Max                               967.064\n",
      "evaluation/Returns Min                               900.643\n",
      "evaluation/Actions Mean                               -0.0141288\n",
      "evaluation/Actions Std                                 0.129118\n",
      "evaluation/Actions Max                                 0.999953\n",
      "evaluation/Actions Min                                -0.984093\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           940.133\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00460098\n",
      "evaluation/env_infos/final/reward_forward Std          0.02254\n",
      "evaluation/env_infos/final/reward_forward Max          0.115024\n",
      "evaluation/env_infos/final/reward_forward Min         -9.69998e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0244657\n",
      "evaluation/env_infos/initial/reward_forward Std        0.138078\n",
      "evaluation/env_infos/initial/reward_forward Max        0.217342\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.302227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/reward_forward Mean               0.00605927\n",
      "evaluation/env_infos/reward_forward Std                0.0787809\n",
      "evaluation/env_infos/reward_forward Max                1.32388\n",
      "evaluation/env_infos/reward_forward Min               -0.853801\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0682299\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0231142\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0336681\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.115771\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0899705\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0739977\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0233473\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.316072\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0674838\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0365576\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.011265\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.01543\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000339773\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0178857\n",
      "evaluation/env_infos/final/torso_velocity Max          0.115024\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.102916\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.129637\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.235719\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.613994\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.302227\n",
      "evaluation/env_infos/torso_velocity Mean               0.00022433\n",
      "evaluation/env_infos/torso_velocity Std                0.0796968\n",
      "evaluation/env_infos/torso_velocity Max                1.32388\n",
      "evaluation/env_infos/torso_velocity Min               -1.6832\n",
      "time/data storing (s)                                  0.0152201\n",
      "time/evaluation sampling (s)                          47.0108\n",
      "time/exploration sampling (s)                          1.9728\n",
      "time/logging (s)                                       0.267187\n",
      "time/saving (s)                                        0.0255926\n",
      "time/training (s)                                      4.31237\n",
      "time/epoch (s)                                        53.604\n",
      "time/total (s)                                      2946.35\n",
      "Epoch                                                 54\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 11:14:50.115910 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 55 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 57000\n",
      "trainer/QF1 Loss                                       0.25016\n",
      "trainer/QF2 Loss                                       0.367749\n",
      "trainer/Policy Loss                                  -11.3982\n",
      "trainer/Q1 Predictions Mean                           19.3264\n",
      "trainer/Q1 Predictions Std                             3.49708\n",
      "trainer/Q1 Predictions Max                            67.3792\n",
      "trainer/Q1 Predictions Min                            -1.71356\n",
      "trainer/Q2 Predictions Mean                           19.4636\n",
      "trainer/Q2 Predictions Std                             3.64688\n",
      "trainer/Q2 Predictions Max                            71.9626\n",
      "trainer/Q2 Predictions Min                             3.11452\n",
      "trainer/Q Targets Mean                                19.4103\n",
      "trainer/Q Targets Std                                  3.50786\n",
      "trainer/Q Targets Max                                 68.3146\n",
      "trainer/Q Targets Min                                  0.00711232\n",
      "trainer/Log Pis Mean                                   8.26017\n",
      "trainer/Log Pis Std                                    5.47002\n",
      "trainer/Log Pis Max                                   83.3321\n",
      "trainer/Log Pis Min                                   -1.60837\n",
      "trainer/Policy mu Mean                                 0.00490674\n",
      "trainer/Policy mu Std                                  1.09545\n",
      "trainer/Policy mu Max                                 28.1594\n",
      "trainer/Policy mu Min                                -12.0996\n",
      "trainer/Policy log std Mean                           -2.33294\n",
      "trainer/Policy log std Std                             0.34039\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -6.70748\n",
      "trainer/Alpha                                          0.00875029\n",
      "trainer/Alpha Loss                                     1.23285\n",
      "exploration/num steps total                        57000\n",
      "exploration/num paths total                          117\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.752012\n",
      "exploration/Rewards Std                                0.0673167\n",
      "exploration/Rewards Max                                1.0698\n",
      "exploration/Rewards Min                                0.528821\n",
      "exploration/Returns Mean                             752.012\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              752.012\n",
      "exploration/Returns Min                              752.012\n",
      "exploration/Actions Mean                              -0.0416221\n",
      "exploration/Actions Std                                0.246155\n",
      "exploration/Actions Max                                0.578274\n",
      "exploration/Actions Min                               -0.710501\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          752.012\n",
      "exploration/env_infos/final/reward_forward Mean        0.00175315\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00175315\n",
      "exploration/env_infos/final/reward_forward Min         0.00175315\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.246556\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.246556\n",
      "exploration/env_infos/initial/reward_forward Min      -0.246556\n",
      "exploration/env_infos/reward_forward Mean              0.00109162\n",
      "exploration/env_infos/reward_forward Std               0.0603859\n",
      "exploration/env_infos/reward_forward Max               0.529829\n",
      "exploration/env_infos/reward_forward Min              -0.597666\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.306739\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.306739\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.306739\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.1942\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.1942\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.1942\n",
      "exploration/env_infos/reward_ctrl Mean                -0.249298\n",
      "exploration/env_infos/reward_ctrl Std                  0.0662424\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0453263\n",
      "exploration/env_infos/reward_ctrl Min                 -0.471179\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.000202303\n",
      "exploration/env_infos/final/torso_velocity Std         0.00736848\n",
      "exploration/env_infos/final/torso_velocity Max         0.00835088\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00949712\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.163029\n",
      "exploration/env_infos/initial/torso_velocity Std       0.365692\n",
      "exploration/env_infos/initial/torso_velocity Max       0.641274\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.246556\n",
      "exploration/env_infos/torso_velocity Mean              0.00439024\n",
      "exploration/env_infos/torso_velocity Std               0.0729448\n",
      "exploration/env_infos/torso_velocity Max               1.19054\n",
      "exploration/env_infos/torso_velocity Min              -1.03142\n",
      "evaluation/num steps total                             1.4e+06\n",
      "evaluation/num paths total                          1400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.841255\n",
      "evaluation/Rewards Std                                 0.071858\n",
      "evaluation/Rewards Max                                 2.13263\n",
      "evaluation/Rewards Min                                 0.649623\n",
      "evaluation/Returns Mean                              841.255\n",
      "evaluation/Returns Std                                67.6046\n",
      "evaluation/Returns Max                               973.298\n",
      "evaluation/Returns Min                               755.43\n",
      "evaluation/Actions Mean                               -0.0301968\n",
      "evaluation/Actions Std                                 0.197461\n",
      "evaluation/Actions Max                                 0.607372\n",
      "evaluation/Actions Min                                -0.497846\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           841.255\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0119097\n",
      "evaluation/env_infos/final/reward_forward Std          0.03999\n",
      "evaluation/env_infos/final/reward_forward Max          0.185443\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00759368\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.000669201\n",
      "evaluation/env_infos/initial/reward_forward Std        0.123108\n",
      "evaluation/env_infos/initial/reward_forward Max        0.215045\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.208474\n",
      "evaluation/env_infos/reward_forward Mean               0.0027868\n",
      "evaluation/env_infos/reward_forward Std                0.0574979\n",
      "evaluation/env_infos/reward_forward Max                1.60075\n",
      "evaluation/env_infos/reward_forward Min               -1.11989\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.158711\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0708971\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0196427\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.246816\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0836597\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0819555\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0222939\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.350377\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.15961\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0687982\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00946157\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.350377\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00414994\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0636814\n",
      "evaluation/env_infos/final/torso_velocity Max          0.185443\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.468545\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.146864\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.214777\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.599449\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.244775\n",
      "evaluation/env_infos/torso_velocity Mean               0.000348816\n",
      "evaluation/env_infos/torso_velocity Std                0.0645473\n",
      "evaluation/env_infos/torso_velocity Max                1.60075\n",
      "evaluation/env_infos/torso_velocity Min               -1.75648\n",
      "time/data storing (s)                                  0.0199334\n",
      "time/evaluation sampling (s)                          45.154\n",
      "time/exploration sampling (s)                          2.02781\n",
      "time/logging (s)                                       0.281022\n",
      "time/saving (s)                                        0.0331252\n",
      "time/training (s)                                      4.71414\n",
      "time/epoch (s)                                        52.23\n",
      "time/total (s)                                      2999.14\n",
      "Epoch                                                 55\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:15:46.907277 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 56 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 58000\n",
      "trainer/QF1 Loss                                       0.508148\n",
      "trainer/QF2 Loss                                       0.441923\n",
      "trainer/Policy Loss                                  -12.3403\n",
      "trainer/Q1 Predictions Mean                           19.7538\n",
      "trainer/Q1 Predictions Std                             1.37144\n",
      "trainer/Q1 Predictions Max                            23.7616\n",
      "trainer/Q1 Predictions Min                            11.6339\n",
      "trainer/Q2 Predictions Mean                           19.6402\n",
      "trainer/Q2 Predictions Std                             1.2957\n",
      "trainer/Q2 Predictions Max                            22.255\n",
      "trainer/Q2 Predictions Min                            14.1335\n",
      "trainer/Q Targets Mean                                19.4876\n",
      "trainer/Q Targets Std                                  1.5069\n",
      "trainer/Q Targets Max                                 22.4606\n",
      "trainer/Q Targets Min                                  8.00247\n",
      "trainer/Log Pis Mean                                   7.54856\n",
      "trainer/Log Pis Std                                    2.81009\n",
      "trainer/Log Pis Max                                   20.3007\n",
      "trainer/Log Pis Min                                   -1.39261\n",
      "trainer/Policy mu Mean                                 0.0201886\n",
      "trainer/Policy mu Std                                  0.228717\n",
      "trainer/Policy mu Max                                  3.21345\n",
      "trainer/Policy mu Min                                 -2.37305\n",
      "trainer/Policy log std Mean                           -2.33864\n",
      "trainer/Policy log std Std                             0.255806\n",
      "trainer/Policy log std Max                            -0.449177\n",
      "trainer/Policy log std Min                            -4.02959\n",
      "trainer/Alpha                                          0.00909529\n",
      "trainer/Alpha Loss                                    -2.12214\n",
      "exploration/num steps total                        58000\n",
      "exploration/num paths total                          118\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.868312\n",
      "exploration/Rewards Std                                0.13026\n",
      "exploration/Rewards Max                                1.81099\n",
      "exploration/Rewards Min                                0.468322\n",
      "exploration/Returns Mean                             868.312\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              868.312\n",
      "exploration/Returns Min                              868.312\n",
      "exploration/Actions Mean                               0.0510694\n",
      "exploration/Actions Std                                0.199121\n",
      "exploration/Actions Max                                0.723524\n",
      "exploration/Actions Min                               -0.486287\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          868.312\n",
      "exploration/env_infos/final/reward_forward Mean        0.0191238\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0191238\n",
      "exploration/env_infos/final/reward_forward Min         0.0191238\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0682939\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0682939\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0682939\n",
      "exploration/env_infos/reward_forward Mean              0.00770028\n",
      "exploration/env_infos/reward_forward Std               0.13723\n",
      "exploration/env_infos/reward_forward Max               0.545351\n",
      "exploration/env_infos/reward_forward Min              -0.533815\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.24776\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.24776\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.24776\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.463517\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.463517\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.463517\n",
      "exploration/env_infos/reward_ctrl Mean                -0.169029\n",
      "exploration/env_infos/reward_ctrl Std                  0.0589251\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0333544\n",
      "exploration/env_infos/reward_ctrl Min                 -0.531678\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.10409\n",
      "exploration/env_infos/final/torso_velocity Std         0.114949\n",
      "exploration/env_infos/final/torso_velocity Max         0.266595\n",
      "exploration/env_infos/final/torso_velocity Min         0.0191238\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0647849\n",
      "exploration/env_infos/initial/torso_velocity Std       0.247433\n",
      "exploration/env_infos/initial/torso_velocity Max       0.411596\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.148948\n",
      "exploration/env_infos/torso_velocity Mean              0.0124955\n",
      "exploration/env_infos/torso_velocity Std               0.139709\n",
      "exploration/env_infos/torso_velocity Max               0.908103\n",
      "exploration/env_infos/torso_velocity Min              -0.854794\n",
      "evaluation/num steps total                             1.425e+06\n",
      "evaluation/num paths total                          1425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.894219\n",
      "evaluation/Rewards Std                                 0.0422453\n",
      "evaluation/Rewards Max                                 1.94791\n",
      "evaluation/Rewards Min                                -0.103437\n",
      "evaluation/Returns Mean                              894.219\n",
      "evaluation/Returns Std                                28.6888\n",
      "evaluation/Returns Max                               962.529\n",
      "evaluation/Returns Min                               857.504\n",
      "evaluation/Actions Mean                                0.0426282\n",
      "evaluation/Actions Std                                 0.158111\n",
      "evaluation/Actions Max                                 0.821262\n",
      "evaluation/Actions Min                                -0.589397\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           894.219\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00860652\n",
      "evaluation/env_infos/final/reward_forward Std          0.0401039\n",
      "evaluation/env_infos/final/reward_forward Max          0.204784\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00070268\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.000701759\n",
      "evaluation/env_infos/initial/reward_forward Std        0.112477\n",
      "evaluation/env_infos/initial/reward_forward Max        0.259078\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.203563\n",
      "evaluation/env_infos/reward_forward Mean              -0.0023206\n",
      "evaluation/env_infos/reward_forward Std                0.0565715\n",
      "evaluation/env_infos/reward_forward Max                1.1505\n",
      "evaluation/env_infos/reward_forward Min               -1.50659\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.10696\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0291665\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0383448\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.141921\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.172059\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.23815\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0247427\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.10344\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.107265\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0328827\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0247427\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.10344\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00182057\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0387488\n",
      "evaluation/env_infos/final/torso_velocity Max          0.204784\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.247252\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.130866\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.218558\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.609967\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.239388\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00124218\n",
      "evaluation/env_infos/torso_velocity Std                0.0641531\n",
      "evaluation/env_infos/torso_velocity Max                1.1505\n",
      "evaluation/env_infos/torso_velocity Min               -1.95487\n",
      "time/data storing (s)                                  0.0146452\n",
      "time/evaluation sampling (s)                          48.7929\n",
      "time/exploration sampling (s)                          2.16984\n",
      "time/logging (s)                                       0.29613\n",
      "time/saving (s)                                        0.026406\n",
      "time/training (s)                                      4.93769\n",
      "time/epoch (s)                                        56.2376\n",
      "time/total (s)                                      3055.95\n",
      "Epoch                                                 56\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:16:44.086803 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 57 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 59000\n",
      "trainer/QF1 Loss                                       0.302068\n",
      "trainer/QF2 Loss                                       0.431455\n",
      "trainer/Policy Loss                                  -11.4625\n",
      "trainer/Q1 Predictions Mean                           19.5586\n",
      "trainer/Q1 Predictions Std                             1.67494\n",
      "trainer/Q1 Predictions Max                            21.8425\n",
      "trainer/Q1 Predictions Min                             0.797774\n",
      "trainer/Q2 Predictions Mean                           19.6159\n",
      "trainer/Q2 Predictions Std                             1.93363\n",
      "trainer/Q2 Predictions Max                            21.7259\n",
      "trainer/Q2 Predictions Min                            -3.14119\n",
      "trainer/Q Targets Mean                                19.7447\n",
      "trainer/Q Targets Std                                  1.72379\n",
      "trainer/Q Targets Max                                 22.7501\n",
      "trainer/Q Targets Min                                 -0.0396092\n",
      "trainer/Log Pis Mean                                   8.42498\n",
      "trainer/Log Pis Std                                    2.58803\n",
      "trainer/Log Pis Max                                   20.6485\n",
      "trainer/Log Pis Min                                   -0.0644589\n",
      "trainer/Policy mu Mean                                -0.0102825\n",
      "trainer/Policy mu Std                                  0.195846\n",
      "trainer/Policy mu Max                                  1.53417\n",
      "trainer/Policy mu Min                                 -1.15414\n",
      "trainer/Policy log std Mean                           -2.41383\n",
      "trainer/Policy log std Std                             0.264246\n",
      "trainer/Policy log std Max                            -1.51395\n",
      "trainer/Policy log std Min                            -4.47884\n",
      "trainer/Alpha                                          0.00888878\n",
      "trainer/Alpha Loss                                     2.0076\n",
      "exploration/num steps total                        59000\n",
      "exploration/num paths total                          119\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.894887\n",
      "exploration/Rewards Std                                0.0688304\n",
      "exploration/Rewards Max                                1.45629\n",
      "exploration/Rewards Min                                0.687133\n",
      "exploration/Returns Mean                             894.887\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              894.887\n",
      "exploration/Returns Min                              894.887\n",
      "exploration/Actions Mean                               0.0097076\n",
      "exploration/Actions Std                                0.170748\n",
      "exploration/Actions Max                                0.578815\n",
      "exploration/Actions Min                               -0.547252\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          894.887\n",
      "exploration/env_infos/final/reward_forward Mean        0.0308295\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0308295\n",
      "exploration/env_infos/final/reward_forward Min         0.0308295\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.032705\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.032705\n",
      "exploration/env_infos/initial/reward_forward Min      -0.032705\n",
      "exploration/env_infos/reward_forward Mean              0.014156\n",
      "exploration/env_infos/reward_forward Std               0.108284\n",
      "exploration/env_infos/reward_forward Max               0.596019\n",
      "exploration/env_infos/reward_forward Min              -0.513401\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.101052\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.101052\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.101052\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0639897\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0639897\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0639897\n",
      "exploration/env_infos/reward_ctrl Mean                -0.116996\n",
      "exploration/env_infos/reward_ctrl Std                  0.0441708\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0170351\n",
      "exploration/env_infos/reward_ctrl Min                 -0.312867\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0900416\n",
      "exploration/env_infos/final/torso_velocity Std         0.09102\n",
      "exploration/env_infos/final/torso_velocity Max         0.0308295\n",
      "exploration/env_infos/final/torso_velocity Min        -0.188812\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.135749\n",
      "exploration/env_infos/initial/torso_velocity Std       0.189031\n",
      "exploration/env_infos/initial/torso_velocity Max       0.399745\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.032705\n",
      "exploration/env_infos/torso_velocity Mean              0.0037205\n",
      "exploration/env_infos/torso_velocity Std               0.139256\n",
      "exploration/env_infos/torso_velocity Max               0.596019\n",
      "exploration/env_infos/torso_velocity Min              -1.39301\n",
      "evaluation/num steps total                             1.45e+06\n",
      "evaluation/num paths total                          1450\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.925676\n",
      "evaluation/Rewards Std                                 0.0271289\n",
      "evaluation/Rewards Max                                 2.15409\n",
      "evaluation/Rewards Min                                 0.218433\n",
      "evaluation/Returns Mean                              925.676\n",
      "evaluation/Returns Std                                14.3874\n",
      "evaluation/Returns Max                               970.21\n",
      "evaluation/Returns Min                               909.401\n",
      "evaluation/Actions Mean                               -0.00393347\n",
      "evaluation/Actions Std                                 0.136828\n",
      "evaluation/Actions Max                                 0.653349\n",
      "evaluation/Actions Min                                -0.51351\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           925.676\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.05784e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.45436e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.45764e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.43274e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0246458\n",
      "evaluation/env_infos/initial/reward_forward Std        0.111367\n",
      "evaluation/env_infos/initial/reward_forward Max        0.219982\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.163616\n",
      "evaluation/env_infos/reward_forward Mean               0.00212098\n",
      "evaluation/env_infos/reward_forward Std                0.0497132\n",
      "evaluation/env_infos/reward_forward Max                1.81812\n",
      "evaluation/env_infos/reward_forward Min               -0.816554\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0747211\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0145815\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0281004\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0907786\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.103671\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.139942\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0514635\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.781567\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0749497\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0164025\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0243581\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.781567\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.17878e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.00174e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.45764e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.43274e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123329\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.254705\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.610965\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.379413\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00173873\n",
      "evaluation/env_infos/torso_velocity Std                0.0555266\n",
      "evaluation/env_infos/torso_velocity Max                1.81812\n",
      "evaluation/env_infos/torso_velocity Min               -1.83547\n",
      "time/data storing (s)                                  0.0242297\n",
      "time/evaluation sampling (s)                          49.1127\n",
      "time/exploration sampling (s)                          2.20399\n",
      "time/logging (s)                                       0.326627\n",
      "time/saving (s)                                        0.0266717\n",
      "time/training (s)                                      4.83571\n",
      "time/epoch (s)                                        56.53\n",
      "time/total (s)                                      3113.16\n",
      "Epoch                                                 57\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:17:43.706733 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 58 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 60000\n",
      "trainer/QF1 Loss                                       1.76333\n",
      "trainer/QF2 Loss                                       1.72184\n",
      "trainer/Policy Loss                                  -12.2941\n",
      "trainer/Q1 Predictions Mean                           19.8491\n",
      "trainer/Q1 Predictions Std                             1.34918\n",
      "trainer/Q1 Predictions Max                            21.9076\n",
      "trainer/Q1 Predictions Min                            13.7846\n",
      "trainer/Q2 Predictions Mean                           19.8621\n",
      "trainer/Q2 Predictions Std                             1.5169\n",
      "trainer/Q2 Predictions Max                            22.0521\n",
      "trainer/Q2 Predictions Min                            12.8644\n",
      "trainer/Q Targets Mean                                20.2487\n",
      "trainer/Q Targets Std                                  1.71279\n",
      "trainer/Q Targets Max                                 35.3517\n",
      "trainer/Q Targets Min                                 13.9934\n",
      "trainer/Log Pis Mean                                   7.69438\n",
      "trainer/Log Pis Std                                    2.48035\n",
      "trainer/Log Pis Max                                   26.6007\n",
      "trainer/Log Pis Min                                   -0.373184\n",
      "trainer/Policy mu Mean                                 0.00217761\n",
      "trainer/Policy mu Std                                  0.235449\n",
      "trainer/Policy mu Max                                  3.64954\n",
      "trainer/Policy mu Min                                 -2.57159\n",
      "trainer/Policy log std Mean                           -2.33121\n",
      "trainer/Policy log std Std                             0.22688\n",
      "trainer/Policy log std Max                            -1.10235\n",
      "trainer/Policy log std Min                            -4.49555\n",
      "trainer/Alpha                                          0.0093489\n",
      "trainer/Alpha Loss                                    -1.42814\n",
      "exploration/num steps total                        60000\n",
      "exploration/num paths total                          120\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.909444\n",
      "exploration/Rewards Std                                0.107861\n",
      "exploration/Rewards Max                                1.78806\n",
      "exploration/Rewards Min                                0.447165\n",
      "exploration/Returns Mean                             909.444\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              909.444\n",
      "exploration/Returns Min                              909.444\n",
      "exploration/Actions Mean                               0.0229467\n",
      "exploration/Actions Std                                0.171801\n",
      "exploration/Actions Max                                0.815479\n",
      "exploration/Actions Min                               -0.553736\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          909.444\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0192545\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0192545\n",
      "exploration/env_infos/final/reward_forward Min        -0.0192545\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0656825\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0656825\n",
      "exploration/env_infos/initial/reward_forward Min       0.0656825\n",
      "exploration/env_infos/reward_forward Mean             -0.00872744\n",
      "exploration/env_infos/reward_forward Std               0.181062\n",
      "exploration/env_infos/reward_forward Max               0.767977\n",
      "exploration/env_infos/reward_forward Min              -0.566112\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0732792\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0732792\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0732792\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0959132\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0959132\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0959132\n",
      "exploration/env_infos/reward_ctrl Mean                -0.120168\n",
      "exploration/env_infos/reward_ctrl Std                  0.0477945\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0172252\n",
      "exploration/env_infos/reward_ctrl Min                 -0.552835\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00900639\n",
      "exploration/env_infos/final/torso_velocity Std         0.0113979\n",
      "exploration/env_infos/final/torso_velocity Max         0.00689269\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0192545\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.229546\n",
      "exploration/env_infos/initial/torso_velocity Std       0.16779\n",
      "exploration/env_infos/initial/torso_velocity Max       0.460112\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0656825\n",
      "exploration/env_infos/torso_velocity Mean              0.0164248\n",
      "exploration/env_infos/torso_velocity Std               0.21737\n",
      "exploration/env_infos/torso_velocity Max               1.25223\n",
      "exploration/env_infos/torso_velocity Min              -1.8811\n",
      "evaluation/num steps total                             1.475e+06\n",
      "evaluation/num paths total                          1475\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.930808\n",
      "evaluation/Rewards Std                                 0.0457036\n",
      "evaluation/Rewards Max                                 2.60027\n",
      "evaluation/Rewards Min                                -2.13911\n",
      "evaluation/Returns Mean                              930.808\n",
      "evaluation/Returns Std                                 8.32009\n",
      "evaluation/Returns Max                               953.278\n",
      "evaluation/Returns Min                               917.944\n",
      "evaluation/Actions Mean                                0.0207904\n",
      "evaluation/Actions Std                                 0.131196\n",
      "evaluation/Actions Max                                 0.999875\n",
      "evaluation/Actions Min                                -0.996194\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           930.808\n",
      "evaluation/env_infos/final/reward_forward Mean         5.53272e-06\n",
      "evaluation/env_infos/final/reward_forward Std          2.34754e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000119997\n",
      "evaluation/env_infos/final/reward_forward Min         -7.97782e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00253703\n",
      "evaluation/env_infos/initial/reward_forward Std        0.160992\n",
      "evaluation/env_infos/initial/reward_forward Max        0.357736\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.305085\n",
      "evaluation/env_infos/reward_forward Mean              -0.000973144\n",
      "evaluation/env_infos/reward_forward Std                0.0586745\n",
      "evaluation/env_infos/reward_forward Max                1.37258\n",
      "evaluation/env_infos/reward_forward Min               -1.58127\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0688238\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00812783\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0498166\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0799351\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.131044\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.189062\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0339274\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.874968\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.070578\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0374304\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0330613\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.13911\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.96172e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          1.401e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000119997\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.28895e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.136754\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.240307\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.609485\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.360694\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00139569\n",
      "evaluation/env_infos/torso_velocity Std                0.0603307\n",
      "evaluation/env_infos/torso_velocity Max                1.37365\n",
      "evaluation/env_infos/torso_velocity Min               -1.77\n",
      "time/data storing (s)                                  0.0160395\n",
      "time/evaluation sampling (s)                          51.4061\n",
      "time/exploration sampling (s)                          2.31833\n",
      "time/logging (s)                                       0.278716\n",
      "time/saving (s)                                        0.0293839\n",
      "time/training (s)                                      4.79909\n",
      "time/epoch (s)                                        58.8477\n",
      "time/total (s)                                      3172.73\n",
      "Epoch                                                 58\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:18:41.341858 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 59 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 61000\n",
      "trainer/QF1 Loss                                       0.413592\n",
      "trainer/QF2 Loss                                       0.481166\n",
      "trainer/Policy Loss                                  -13.128\n",
      "trainer/Q1 Predictions Mean                           20.6284\n",
      "trainer/Q1 Predictions Std                             1.19584\n",
      "trainer/Q1 Predictions Max                            24.8572\n",
      "trainer/Q1 Predictions Min                            13.9063\n",
      "trainer/Q2 Predictions Mean                           20.7218\n",
      "trainer/Q2 Predictions Std                             1.24011\n",
      "trainer/Q2 Predictions Max                            25.2555\n",
      "trainer/Q2 Predictions Min                            11.7574\n",
      "trainer/Q Targets Mean                                20.6137\n",
      "trainer/Q Targets Std                                  1.16621\n",
      "trainer/Q Targets Max                                 24.9781\n",
      "trainer/Q Targets Min                                 15.1145\n",
      "trainer/Log Pis Mean                                   7.70845\n",
      "trainer/Log Pis Std                                    2.51048\n",
      "trainer/Log Pis Max                                   25.3164\n",
      "trainer/Log Pis Min                                    1.89413\n",
      "trainer/Policy mu Mean                                 0.0681178\n",
      "trainer/Policy mu Std                                  0.285887\n",
      "trainer/Policy mu Max                                  3.55593\n",
      "trainer/Policy mu Min                                 -2.3003\n",
      "trainer/Policy log std Mean                           -2.30302\n",
      "trainer/Policy log std Std                             0.240107\n",
      "trainer/Policy log std Max                            -0.231848\n",
      "trainer/Policy log std Min                            -3.936\n",
      "trainer/Alpha                                          0.00864443\n",
      "trainer/Alpha Loss                                    -1.38488\n",
      "exploration/num steps total                        61000\n",
      "exploration/num paths total                          121\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.835118\n",
      "exploration/Rewards Std                                0.127482\n",
      "exploration/Rewards Max                                1.52555\n",
      "exploration/Rewards Min                                0.531304\n",
      "exploration/Returns Mean                             835.118\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              835.118\n",
      "exploration/Returns Min                              835.118\n",
      "exploration/Actions Mean                               0.131089\n",
      "exploration/Actions Std                                0.183449\n",
      "exploration/Actions Max                                0.747674\n",
      "exploration/Actions Min                               -0.471747\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          835.118\n",
      "exploration/env_infos/final/reward_forward Mean       -0.024078\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.024078\n",
      "exploration/env_infos/final/reward_forward Min        -0.024078\n",
      "exploration/env_infos/initial/reward_forward Mean      0.262515\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.262515\n",
      "exploration/env_infos/initial/reward_forward Min       0.262515\n",
      "exploration/env_infos/reward_forward Mean             -0.0442586\n",
      "exploration/env_infos/reward_forward Std               0.151587\n",
      "exploration/env_infos/reward_forward Max               0.779228\n",
      "exploration/env_infos/reward_forward Min              -0.548668\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.191754\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.191754\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.191754\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0893147\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0893147\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0893147\n",
      "exploration/env_infos/reward_ctrl Mean                -0.203351\n",
      "exploration/env_infos/reward_ctrl Std                  0.0623319\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0218146\n",
      "exploration/env_infos/reward_ctrl Min                 -0.468696\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0356986\n",
      "exploration/env_infos/final/torso_velocity Std         0.0721624\n",
      "exploration/env_infos/final/torso_velocity Max         0.0462969\n",
      "exploration/env_infos/final/torso_velocity Min        -0.129315\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.295297\n",
      "exploration/env_infos/initial/torso_velocity Std       0.263192\n",
      "exploration/env_infos/initial/torso_velocity Max       0.632779\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.00940173\n",
      "exploration/env_infos/torso_velocity Mean             -0.0057297\n",
      "exploration/env_infos/torso_velocity Std               0.170576\n",
      "exploration/env_infos/torso_velocity Max               0.779228\n",
      "exploration/env_infos/torso_velocity Min              -0.908846\n",
      "evaluation/num steps total                             1.5e+06\n",
      "evaluation/num paths total                          1500\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.845095\n",
      "evaluation/Rewards Std                                 0.03953\n",
      "evaluation/Rewards Max                                 2.21457\n",
      "evaluation/Rewards Min                                -2.11711\n",
      "evaluation/Returns Mean                              845.095\n",
      "evaluation/Returns Std                                20.9701\n",
      "evaluation/Returns Max                               902.748\n",
      "evaluation/Returns Min                               818.862\n",
      "evaluation/Actions Mean                                0.0936063\n",
      "evaluation/Actions Std                                 0.173664\n",
      "evaluation/Actions Max                                 0.999808\n",
      "evaluation/Actions Min                                -0.998327\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           845.095\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.68047e-08\n",
      "evaluation/env_infos/final/reward_forward Std          3.15286e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.29518e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.97868e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0371118\n",
      "evaluation/env_infos/initial/reward_forward Std        0.123787\n",
      "evaluation/env_infos/initial/reward_forward Max        0.233654\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.261029\n",
      "evaluation/env_infos/reward_forward Mean              -0.000343229\n",
      "evaluation/env_infos/reward_forward Std                0.0546955\n",
      "evaluation/env_infos/reward_forward Max                1.45901\n",
      "evaluation/env_infos/reward_forward Min               -1.40112\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.155395\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0213802\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0964483\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.181439\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.108142\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0604638\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0392979\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.351321\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.155686\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0327998\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0392979\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.11711\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.22776e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.34741e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.62543e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.84969e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.163988\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.231761\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.71849\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.261029\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00046649\n",
      "evaluation/env_infos/torso_velocity Std                0.0575894\n",
      "evaluation/env_infos/torso_velocity Max                2.72799\n",
      "evaluation/env_infos/torso_velocity Min               -1.77194\n",
      "time/data storing (s)                                  0.0227085\n",
      "time/evaluation sampling (s)                          49.6722\n",
      "time/exploration sampling (s)                          2.202\n",
      "time/logging (s)                                       0.297876\n",
      "time/saving (s)                                        0.0264565\n",
      "time/training (s)                                      4.79559\n",
      "time/epoch (s)                                        57.0168\n",
      "time/total (s)                                      3230.38\n",
      "Epoch                                                 59\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:19:38.820161 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 60 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 62000\n",
      "trainer/QF1 Loss                                       0.637748\n",
      "trainer/QF2 Loss                                       0.417092\n",
      "trainer/Policy Loss                                  -13.2469\n",
      "trainer/Q1 Predictions Mean                           20.2727\n",
      "trainer/Q1 Predictions Std                             1.58257\n",
      "trainer/Q1 Predictions Max                            24.342\n",
      "trainer/Q1 Predictions Min                             7.76703\n",
      "trainer/Q2 Predictions Mean                           20.4534\n",
      "trainer/Q2 Predictions Std                             1.6313\n",
      "trainer/Q2 Predictions Max                            22.6022\n",
      "trainer/Q2 Predictions Min                             6.80511\n",
      "trainer/Q Targets Mean                                20.6877\n",
      "trainer/Q Targets Std                                  1.55398\n",
      "trainer/Q Targets Max                                 23.1107\n",
      "trainer/Q Targets Min                                  7.6508\n",
      "trainer/Log Pis Mean                                   7.32315\n",
      "trainer/Log Pis Std                                    5.10527\n",
      "trainer/Log Pis Max                                   63.4503\n",
      "trainer/Log Pis Min                                   -0.11605\n",
      "trainer/Policy mu Mean                                -0.0100886\n",
      "trainer/Policy mu Std                                  0.578406\n",
      "trainer/Policy mu Max                                  8.77669\n",
      "trainer/Policy mu Min                                 -6.78075\n",
      "trainer/Policy log std Mean                           -2.18117\n",
      "trainer/Policy log std Std                             0.349469\n",
      "trainer/Policy log std Max                             1.33584\n",
      "trainer/Policy log std Min                            -4.30652\n",
      "trainer/Alpha                                          0.00922506\n",
      "trainer/Alpha Loss                                    -3.17194\n",
      "exploration/num steps total                        62000\n",
      "exploration/num paths total                          122\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.86849\n",
      "exploration/Rewards Std                                0.161119\n",
      "exploration/Rewards Max                                1.69263\n",
      "exploration/Rewards Min                               -1.32408\n",
      "exploration/Returns Mean                             868.49\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              868.49\n",
      "exploration/Returns Min                              868.49\n",
      "exploration/Actions Mean                              -0.00583473\n",
      "exploration/Actions Std                                0.199324\n",
      "exploration/Actions Max                                0.979928\n",
      "exploration/Actions Min                               -0.923709\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          868.49\n",
      "exploration/env_infos/final/reward_forward Mean        0.548196\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.548196\n",
      "exploration/env_infos/final/reward_forward Min         0.548196\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0888596\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0888596\n",
      "exploration/env_infos/initial/reward_forward Min       0.0888596\n",
      "exploration/env_infos/reward_forward Mean             -0.0555493\n",
      "exploration/env_infos/reward_forward Std               0.316236\n",
      "exploration/env_infos/reward_forward Max               0.884349\n",
      "exploration/env_infos/reward_forward Min              -1.23419\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.18311\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.18311\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.18311\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.177331\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.177331\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.177331\n",
      "exploration/env_infos/reward_ctrl Mean                -0.159057\n",
      "exploration/env_infos/reward_ctrl Std                  0.110543\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0177358\n",
      "exploration/env_infos/reward_ctrl Min                 -2.32408\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.306777\n",
      "exploration/env_infos/final/torso_velocity Std         0.310077\n",
      "exploration/env_infos/final/torso_velocity Max         0.548196\n",
      "exploration/env_infos/final/torso_velocity Min        -0.130965\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.143882\n",
      "exploration/env_infos/initial/torso_velocity Std       0.141404\n",
      "exploration/env_infos/initial/torso_velocity Max       0.337893\n",
      "exploration/env_infos/initial/torso_velocity Min       0.00489426\n",
      "exploration/env_infos/torso_velocity Mean             -0.0186955\n",
      "exploration/env_infos/torso_velocity Std               0.325888\n",
      "exploration/env_infos/torso_velocity Max               1.53006\n",
      "exploration/env_infos/torso_velocity Min              -1.41028\n",
      "evaluation/num steps total                             1.525e+06\n",
      "evaluation/num paths total                          1525\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.840784\n",
      "evaluation/Rewards Std                                 0.112218\n",
      "evaluation/Rewards Max                                 2.25928\n",
      "evaluation/Rewards Min                                -1.65404\n",
      "evaluation/Returns Mean                              840.784\n",
      "evaluation/Returns Std                                90.9337\n",
      "evaluation/Returns Max                               954.438\n",
      "evaluation/Returns Min                               696.715\n",
      "evaluation/Actions Mean                               -0.0600475\n",
      "evaluation/Actions Std                                 0.191255\n",
      "evaluation/Actions Max                                 0.98842\n",
      "evaluation/Actions Min                                -0.967623\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           840.784\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00404512\n",
      "evaluation/env_infos/final/reward_forward Std          0.0774527\n",
      "evaluation/env_infos/final/reward_forward Max          0.320306\n",
      "evaluation/env_infos/final/reward_forward Min         -0.218596\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0455075\n",
      "evaluation/env_infos/initial/reward_forward Std        0.131856\n",
      "evaluation/env_infos/initial/reward_forward Max        0.280545\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.299842\n",
      "evaluation/env_infos/reward_forward Mean              -0.00200387\n",
      "evaluation/env_infos/reward_forward Std                0.106395\n",
      "evaluation/env_infos/reward_forward Max                1.4598\n",
      "evaluation/env_infos/reward_forward Min               -1.23527\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.151971\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0963046\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0430661\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.302271\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.610012\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.645565\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0779102\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.93355\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.160737\n",
      "evaluation/env_infos/reward_ctrl Std                   0.1093\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0363783\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.65404\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00429347\n",
      "evaluation/env_infos/final/torso_velocity Std          0.059711\n",
      "evaluation/env_infos/final/torso_velocity Max          0.329932\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.218596\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.112994\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.268794\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.609602\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.538706\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00303377\n",
      "evaluation/env_infos/torso_velocity Std                0.110681\n",
      "evaluation/env_infos/torso_velocity Max                1.99591\n",
      "evaluation/env_infos/torso_velocity Min               -2.03033\n",
      "time/data storing (s)                                  0.0172315\n",
      "time/evaluation sampling (s)                          49.6005\n",
      "time/exploration sampling (s)                          2.05447\n",
      "time/logging (s)                                       0.302056\n",
      "time/saving (s)                                        0.0332375\n",
      "time/training (s)                                      4.8284\n",
      "time/epoch (s)                                        56.8359\n",
      "time/total (s)                                      3287.86\n",
      "Epoch                                                 60\n",
      "-------------------------------------------------  --------------\n",
      "2021-05-25 11:20:34.230751 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 61 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 63000\n",
      "trainer/QF1 Loss                                       0.396862\n",
      "trainer/QF2 Loss                                       0.653778\n",
      "trainer/Policy Loss                                  -14.1903\n",
      "trainer/Q1 Predictions Mean                           20.8411\n",
      "trainer/Q1 Predictions Std                             1.37612\n",
      "trainer/Q1 Predictions Max                            23.3537\n",
      "trainer/Q1 Predictions Min                            14.2892\n",
      "trainer/Q2 Predictions Mean                           21.2511\n",
      "trainer/Q2 Predictions Std                             1.28871\n",
      "trainer/Q2 Predictions Max                            24.6846\n",
      "trainer/Q2 Predictions Min                            15.3775\n",
      "trainer/Q Targets Mean                                21.0609\n",
      "trainer/Q Targets Std                                  1.37005\n",
      "trainer/Q Targets Max                                 27.8074\n",
      "trainer/Q Targets Min                                 15.8665\n",
      "trainer/Log Pis Mean                                   6.89335\n",
      "trainer/Log Pis Std                                    3.54798\n",
      "trainer/Log Pis Max                                   32.9711\n",
      "trainer/Log Pis Min                                   -1.74761\n",
      "trainer/Policy mu Mean                                 0.077152\n",
      "trainer/Policy mu Std                                  0.362579\n",
      "trainer/Policy mu Max                                  4.95907\n",
      "trainer/Policy mu Min                                 -3.14655\n",
      "trainer/Policy log std Mean                           -2.17836\n",
      "trainer/Policy log std Std                             0.279876\n",
      "trainer/Policy log std Max                            -0.286633\n",
      "trainer/Policy log std Min                            -3.57083\n",
      "trainer/Alpha                                          0.00875907\n",
      "trainer/Alpha Loss                                    -5.24016\n",
      "exploration/num steps total                        63000\n",
      "exploration/num paths total                          123\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.793104\n",
      "exploration/Rewards Std                                0.113573\n",
      "exploration/Rewards Max                                1.94077\n",
      "exploration/Rewards Min                               -0.314702\n",
      "exploration/Returns Mean                             793.104\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              793.104\n",
      "exploration/Returns Min                              793.104\n",
      "exploration/Actions Mean                               0.057698\n",
      "exploration/Actions Std                                0.229483\n",
      "exploration/Actions Max                                0.99861\n",
      "exploration/Actions Min                               -0.987369\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          793.104\n",
      "exploration/env_infos/final/reward_forward Mean       -0.34847\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.34847\n",
      "exploration/env_infos/final/reward_forward Min        -0.34847\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.180509\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.180509\n",
      "exploration/env_infos/initial/reward_forward Min      -0.180509\n",
      "exploration/env_infos/reward_forward Mean             -0.00910524\n",
      "exploration/env_infos/reward_forward Std               0.13546\n",
      "exploration/env_infos/reward_forward Max               1.18926\n",
      "exploration/env_infos/reward_forward Min              -0.393106\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.128695\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.128695\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.128695\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.457007\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.457007\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.457007\n",
      "exploration/env_infos/reward_ctrl Mean                -0.223965\n",
      "exploration/env_infos/reward_ctrl Std                  0.105849\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0654078\n",
      "exploration/env_infos/reward_ctrl Min                 -2.46035\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.019596\n",
      "exploration/env_infos/final/torso_velocity Std         0.272616\n",
      "exploration/env_infos/final/torso_velocity Max         0.319084\n",
      "exploration/env_infos/final/torso_velocity Min        -0.34847\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0505816\n",
      "exploration/env_infos/initial/torso_velocity Std       0.226699\n",
      "exploration/env_infos/initial/torso_velocity Max       0.358575\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.180509\n",
      "exploration/env_infos/torso_velocity Mean             -0.00849145\n",
      "exploration/env_infos/torso_velocity Std               0.15233\n",
      "exploration/env_infos/torso_velocity Max               1.18926\n",
      "exploration/env_infos/torso_velocity Min              -1.25619\n",
      "evaluation/num steps total                             1.55e+06\n",
      "evaluation/num paths total                          1550\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.781483\n",
      "evaluation/Rewards Std                                 0.0783956\n",
      "evaluation/Rewards Max                                 2.15512\n",
      "evaluation/Rewards Min                                -1.72705\n",
      "evaluation/Returns Mean                              781.483\n",
      "evaluation/Returns Std                                49.1102\n",
      "evaluation/Returns Max                               855.202\n",
      "evaluation/Returns Min                               676.626\n",
      "evaluation/Actions Mean                                0.076419\n",
      "evaluation/Actions Std                                 0.221168\n",
      "evaluation/Actions Max                                 0.999972\n",
      "evaluation/Actions Min                                -0.999902\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           781.483\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.0011e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.87381e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.20384e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.04184e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0725779\n",
      "evaluation/env_infos/initial/reward_forward Std        0.16691\n",
      "evaluation/env_infos/initial/reward_forward Max        0.422918\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.243466\n",
      "evaluation/env_infos/reward_forward Mean              -0.000780553\n",
      "evaluation/env_infos/reward_forward Std                0.0446257\n",
      "evaluation/env_infos/reward_forward Max                1.0542\n",
      "evaluation/env_infos/reward_forward Min               -1.3063\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.215927\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0489905\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.140229\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.317949\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.727197\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.659622\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.14151\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.93246\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.219021\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0775957\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0763568\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.72705\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.88291e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.87411e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.77596e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.04184e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.157335\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237564\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.575907\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.400591\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0017056\n",
      "evaluation/env_infos/torso_velocity Std                0.0658402\n",
      "evaluation/env_infos/torso_velocity Max                1.89142\n",
      "evaluation/env_infos/torso_velocity Min               -2.34165\n",
      "time/data storing (s)                                  0.0240223\n",
      "time/evaluation sampling (s)                          48.0752\n",
      "time/exploration sampling (s)                          2.09055\n",
      "time/logging (s)                                       0.286904\n",
      "time/saving (s)                                        0.0280079\n",
      "time/training (s)                                      4.18811\n",
      "time/epoch (s)                                        54.6928\n",
      "time/total (s)                                      3343.25\n",
      "Epoch                                                 61\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:21:28.539402 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 62 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 64000\n",
      "trainer/QF1 Loss                                       1.20705\n",
      "trainer/QF2 Loss                                       0.846779\n",
      "trainer/Policy Loss                                  -13.0831\n",
      "trainer/Q1 Predictions Mean                           20.9253\n",
      "trainer/Q1 Predictions Std                             1.47558\n",
      "trainer/Q1 Predictions Max                            24.816\n",
      "trainer/Q1 Predictions Min                            11.7785\n",
      "trainer/Q2 Predictions Mean                           21.3705\n",
      "trainer/Q2 Predictions Std                             1.46081\n",
      "trainer/Q2 Predictions Max                            26.1148\n",
      "trainer/Q2 Predictions Min                            12.6319\n",
      "trainer/Q Targets Mean                                21.4473\n",
      "trainer/Q Targets Std                                  1.52685\n",
      "trainer/Q Targets Max                                 30.1792\n",
      "trainer/Q Targets Min                                 15.0741\n",
      "trainer/Log Pis Mean                                   8.16562\n",
      "trainer/Log Pis Std                                    2.67991\n",
      "trainer/Log Pis Max                                   25.3623\n",
      "trainer/Log Pis Min                                   -1.77856\n",
      "trainer/Policy mu Mean                                 0.0270758\n",
      "trainer/Policy mu Std                                  0.258256\n",
      "trainer/Policy mu Max                                  3.53317\n",
      "trainer/Policy mu Min                                 -2.23142\n",
      "trainer/Policy log std Mean                           -2.36396\n",
      "trainer/Policy log std Std                             0.230719\n",
      "trainer/Policy log std Max                            -0.916845\n",
      "trainer/Policy log std Min                            -3.89347\n",
      "trainer/Alpha                                          0.00825995\n",
      "trainer/Alpha Loss                                     0.794606\n",
      "exploration/num steps total                        64000\n",
      "exploration/num paths total                          124\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.883736\n",
      "exploration/Rewards Std                                0.053215\n",
      "exploration/Rewards Max                                1.23369\n",
      "exploration/Rewards Min                                0.691152\n",
      "exploration/Returns Mean                             883.736\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              883.736\n",
      "exploration/Returns Min                              883.736\n",
      "exploration/Actions Mean                              -0.00349251\n",
      "exploration/Actions Std                                0.173805\n",
      "exploration/Actions Max                                0.580954\n",
      "exploration/Actions Min                               -0.528909\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          883.736\n",
      "exploration/env_infos/final/reward_forward Mean        0.0302285\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0302285\n",
      "exploration/env_infos/final/reward_forward Min         0.0302285\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0867414\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0867414\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0867414\n",
      "exploration/env_infos/reward_forward Mean             -0.00126222\n",
      "exploration/env_infos/reward_forward Std               0.104008\n",
      "exploration/env_infos/reward_forward Max               0.526051\n",
      "exploration/env_infos/reward_forward Min              -1.0925\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.171492\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.171492\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.171492\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0606599\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0606599\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0606599\n",
      "exploration/env_infos/reward_ctrl Mean                -0.120881\n",
      "exploration/env_infos/reward_ctrl Std                  0.048132\n",
      "exploration/env_infos/reward_ctrl Max                 -0.013097\n",
      "exploration/env_infos/reward_ctrl Min                 -0.308848\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0208584\n",
      "exploration/env_infos/final/torso_velocity Std         0.0950396\n",
      "exploration/env_infos/final/torso_velocity Max         0.132289\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0999427\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0213235\n",
      "exploration/env_infos/initial/torso_velocity Std       0.227376\n",
      "exploration/env_infos/initial/torso_velocity Max       0.337637\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.186925\n",
      "exploration/env_infos/torso_velocity Mean             -0.0055491\n",
      "exploration/env_infos/torso_velocity Std               0.146576\n",
      "exploration/env_infos/torso_velocity Max               0.601327\n",
      "exploration/env_infos/torso_velocity Min              -1.0925\n",
      "evaluation/num steps total                             1.575e+06\n",
      "evaluation/num paths total                          1575\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.884975\n",
      "evaluation/Rewards Std                                 0.0756778\n",
      "evaluation/Rewards Max                                 2.25807\n",
      "evaluation/Rewards Min                                -2.12098\n",
      "evaluation/Returns Mean                              884.975\n",
      "evaluation/Returns Std                                43.3762\n",
      "evaluation/Returns Max                               955.749\n",
      "evaluation/Returns Min                               805.994\n",
      "evaluation/Actions Mean                                0.0431475\n",
      "evaluation/Actions Std                                 0.165104\n",
      "evaluation/Actions Max                                 0.999983\n",
      "evaluation/Actions Min                                -0.995473\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           884.975\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.17879e-05\n",
      "evaluation/env_infos/final/reward_forward Std          5.80507e-05\n",
      "evaluation/env_infos/final/reward_forward Max          4.47514e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000296175\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0498694\n",
      "evaluation/env_infos/initial/reward_forward Std        0.159134\n",
      "evaluation/env_infos/initial/reward_forward Max        0.340434\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.278008\n",
      "evaluation/env_infos/reward_forward Mean               0.00175452\n",
      "evaluation/env_infos/reward_forward Std                0.0639663\n",
      "evaluation/env_infos/reward_forward Max                1.3642\n",
      "evaluation/env_infos/reward_forward Min               -1.77017\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.114566\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0453237\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0418458\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.194149\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.429671\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.43427\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.03191\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.30296\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.116484\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0695352\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0274477\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.12098\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.26737e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          7.05853e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          7.68574e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000531967\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.12047\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.262107\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.588596\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.345278\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0026209\n",
      "evaluation/env_infos/torso_velocity Std                0.0716897\n",
      "evaluation/env_infos/torso_velocity Max                1.40482\n",
      "evaluation/env_infos/torso_velocity Min               -1.8367\n",
      "time/data storing (s)                                  0.0162045\n",
      "time/evaluation sampling (s)                          47.2786\n",
      "time/exploration sampling (s)                          1.96706\n",
      "time/logging (s)                                       0.284899\n",
      "time/saving (s)                                        0.0255806\n",
      "time/training (s)                                      4.10561\n",
      "time/epoch (s)                                        53.678\n",
      "time/total (s)                                      3397.56\n",
      "Epoch                                                 62\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:22:20.987720 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 63 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 65000\n",
      "trainer/QF1 Loss                                       1.4535\n",
      "trainer/QF2 Loss                                       1.26436\n",
      "trainer/Policy Loss                                  -14.7074\n",
      "trainer/Q1 Predictions Mean                           21.6746\n",
      "trainer/Q1 Predictions Std                             2.7418\n",
      "trainer/Q1 Predictions Max                            56.8171\n",
      "trainer/Q1 Predictions Min                            12.7277\n",
      "trainer/Q2 Predictions Mean                           21.4959\n",
      "trainer/Q2 Predictions Std                             2.79798\n",
      "trainer/Q2 Predictions Max                            58.5117\n",
      "trainer/Q2 Predictions Min                            14.131\n",
      "trainer/Q Targets Mean                                21.5487\n",
      "trainer/Q Targets Std                                  3.09032\n",
      "trainer/Q Targets Max                                 58.831\n",
      "trainer/Q Targets Min                                 -0.301239\n",
      "trainer/Log Pis Mean                                   7.05613\n",
      "trainer/Log Pis Std                                    4.68881\n",
      "trainer/Log Pis Max                                   62.5437\n",
      "trainer/Log Pis Min                                   -3.4795\n",
      "trainer/Policy mu Mean                                 0.0279056\n",
      "trainer/Policy mu Std                                  0.789066\n",
      "trainer/Policy mu Max                                 17.807\n",
      "trainer/Policy mu Min                                -11.2188\n",
      "trainer/Policy log std Mean                           -2.15685\n",
      "trainer/Policy log std Std                             0.3706\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -4.2882\n",
      "trainer/Alpha                                          0.0085071\n",
      "trainer/Alpha Loss                                    -4.49821\n",
      "exploration/num steps total                        65000\n",
      "exploration/num paths total                          125\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.914299\n",
      "exploration/Rewards Std                                0.231314\n",
      "exploration/Rewards Max                                1.68224\n",
      "exploration/Rewards Min                               -1.99261\n",
      "exploration/Returns Mean                             914.299\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              914.299\n",
      "exploration/Returns Min                              914.299\n",
      "exploration/Actions Mean                               0.00308251\n",
      "exploration/Actions Std                                0.187513\n",
      "exploration/Actions Max                                1\n",
      "exploration/Actions Min                               -1\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          914.299\n",
      "exploration/env_infos/final/reward_forward Mean       -0.537677\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.537677\n",
      "exploration/env_infos/final/reward_forward Min        -0.537677\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00249946\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00249946\n",
      "exploration/env_infos/initial/reward_forward Min       0.00249946\n",
      "exploration/env_infos/reward_forward Mean             -0.00413968\n",
      "exploration/env_infos/reward_forward Std               0.356963\n",
      "exploration/env_infos/reward_forward Max               0.979137\n",
      "exploration/env_infos/reward_forward Min              -1.25673\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.192804\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.192804\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.192804\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.195964\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.195964\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.195964\n",
      "exploration/env_infos/reward_ctrl Mean                -0.140682\n",
      "exploration/env_infos/reward_ctrl Std                  0.177334\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0136412\n",
      "exploration/env_infos/reward_ctrl Min                 -2.99261\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.251247\n",
      "exploration/env_infos/final/torso_velocity Std         0.27434\n",
      "exploration/env_infos/final/torso_velocity Max         0.118599\n",
      "exploration/env_infos/final/torso_velocity Min        -0.537677\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.134221\n",
      "exploration/env_infos/initial/torso_velocity Std       0.182769\n",
      "exploration/env_infos/initial/torso_velocity Max       0.39268\n",
      "exploration/env_infos/initial/torso_velocity Min       0.00249946\n",
      "exploration/env_infos/torso_velocity Mean             -0.00413336\n",
      "exploration/env_infos/torso_velocity Std               0.293662\n",
      "exploration/env_infos/torso_velocity Max               1.21469\n",
      "exploration/env_infos/torso_velocity Min              -1.33279\n",
      "evaluation/num steps total                             1.6e+06\n",
      "evaluation/num paths total                          1600\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.90384\n",
      "evaluation/Rewards Std                                 0.0872961\n",
      "evaluation/Rewards Max                                 2.34808\n",
      "evaluation/Rewards Min                                -1.79035\n",
      "evaluation/Returns Mean                              903.84\n",
      "evaluation/Returns Std                                45.4146\n",
      "evaluation/Returns Max                               949.887\n",
      "evaluation/Returns Min                               780.747\n",
      "evaluation/Actions Mean                               -0.00262957\n",
      "evaluation/Actions Std                                 0.158355\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999727\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           903.84\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.000391643\n",
      "evaluation/env_infos/final/reward_forward Std          0.00183243\n",
      "evaluation/env_infos/final/reward_forward Max          1.06762e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00935913\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0596235\n",
      "evaluation/env_infos/initial/reward_forward Std        0.118385\n",
      "evaluation/env_infos/initial/reward_forward Max        0.265458\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.208793\n",
      "evaluation/env_infos/reward_forward Mean              -0.00359818\n",
      "evaluation/env_infos/reward_forward Std                0.129582\n",
      "evaluation/env_infos/reward_forward Max                1.39839\n",
      "evaluation/env_infos/reward_forward Min               -2.09893\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0960979\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0446794\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0511704\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.219956\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.406987\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.427033\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0862665\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.46174\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.100333\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0711422\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0192055\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.79035\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.22303e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00170229\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0113802\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00935913\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.17433\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.244941\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.598749\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.262696\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00237988\n",
      "evaluation/env_infos/torso_velocity Std                0.114373\n",
      "evaluation/env_infos/torso_velocity Max                1.39839\n",
      "evaluation/env_infos/torso_velocity Min               -2.09893\n",
      "time/data storing (s)                                  0.0165722\n",
      "time/evaluation sampling (s)                          45.4474\n",
      "time/exploration sampling (s)                          1.93301\n",
      "time/logging (s)                                       0.280595\n",
      "time/saving (s)                                        0.026243\n",
      "time/training (s)                                      4.14306\n",
      "time/epoch (s)                                        51.8469\n",
      "time/total (s)                                      3450\n",
      "Epoch                                                 63\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:23:13.244412 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 64 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 66000\n",
      "trainer/QF1 Loss                                       0.50814\n",
      "trainer/QF2 Loss                                       0.298286\n",
      "trainer/Policy Loss                                  -14.1566\n",
      "trainer/Q1 Predictions Mean                           21.4627\n",
      "trainer/Q1 Predictions Std                             1.4901\n",
      "trainer/Q1 Predictions Max                            26.9375\n",
      "trainer/Q1 Predictions Min                            15.4388\n",
      "trainer/Q2 Predictions Mean                           21.5621\n",
      "trainer/Q2 Predictions Std                             1.52077\n",
      "trainer/Q2 Predictions Max                            30.0801\n",
      "trainer/Q2 Predictions Min                            16.1044\n",
      "trainer/Q Targets Mean                                21.7842\n",
      "trainer/Q Targets Std                                  1.53497\n",
      "trainer/Q Targets Max                                 29.9481\n",
      "trainer/Q Targets Min                                 16.0792\n",
      "trainer/Log Pis Mean                                   7.70219\n",
      "trainer/Log Pis Std                                    4.62484\n",
      "trainer/Log Pis Max                                   58.5527\n",
      "trainer/Log Pis Min                                   -1.0771\n",
      "trainer/Policy mu Mean                                 0.177539\n",
      "trainer/Policy mu Std                                  0.468725\n",
      "trainer/Policy mu Max                                  7.93273\n",
      "trainer/Policy mu Min                                 -5.49854\n",
      "trainer/Policy log std Mean                           -2.22677\n",
      "trainer/Policy log std Std                             0.31729\n",
      "trainer/Policy log std Max                            -0.245727\n",
      "trainer/Policy log std Min                            -4.72901\n",
      "trainer/Alpha                                          0.00967749\n",
      "trainer/Alpha Loss                                    -1.38067\n",
      "exploration/num steps total                        66000\n",
      "exploration/num paths total                          126\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.791052\n",
      "exploration/Rewards Std                                0.202391\n",
      "exploration/Rewards Max                                2.08035\n",
      "exploration/Rewards Min                               -1.50537\n",
      "exploration/Returns Mean                             791.052\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              791.052\n",
      "exploration/Returns Min                              791.052\n",
      "exploration/Actions Mean                               0.11716\n",
      "exploration/Actions Std                                0.20464\n",
      "exploration/Actions Max                                0.999987\n",
      "exploration/Actions Min                               -0.99877\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          791.052\n",
      "exploration/env_infos/final/reward_forward Mean        0.50341\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.50341\n",
      "exploration/env_infos/final/reward_forward Min         0.50341\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0190771\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0190771\n",
      "exploration/env_infos/initial/reward_forward Min       0.0190771\n",
      "exploration/env_infos/reward_forward Mean              0.0353385\n",
      "exploration/env_infos/reward_forward Std               0.320984\n",
      "exploration/env_infos/reward_forward Max               1.42249\n",
      "exploration/env_infos/reward_forward Min              -1.05027\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.162289\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.162289\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.162289\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.804799\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.804799\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.804799\n",
      "exploration/env_infos/reward_ctrl Mean                -0.222416\n",
      "exploration/env_infos/reward_ctrl Std                  0.183613\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0272384\n",
      "exploration/env_infos/reward_ctrl Min                 -2.50537\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.201536\n",
      "exploration/env_infos/final/torso_velocity Std         0.241459\n",
      "exploration/env_infos/final/torso_velocity Max         0.50341\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0876332\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0285698\n",
      "exploration/env_infos/initial/torso_velocity Std       0.230028\n",
      "exploration/env_infos/initial/torso_velocity Max       0.314922\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.24829\n",
      "exploration/env_infos/torso_velocity Mean              0.000634299\n",
      "exploration/env_infos/torso_velocity Std               0.31338\n",
      "exploration/env_infos/torso_velocity Max               1.42249\n",
      "exploration/env_infos/torso_velocity Min              -1.95834\n",
      "evaluation/num steps total                             1.625e+06\n",
      "evaluation/num paths total                          1625\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.8869\n",
      "evaluation/Rewards Std                                 0.0783125\n",
      "evaluation/Rewards Max                                 1.96751\n",
      "evaluation/Rewards Min                                -0.727624\n",
      "evaluation/Returns Mean                              886.9\n",
      "evaluation/Returns Std                                70.0373\n",
      "evaluation/Returns Max                               957.179\n",
      "evaluation/Returns Min                               590.731\n",
      "evaluation/Actions Mean                                0.0890095\n",
      "evaluation/Actions Std                                 0.143253\n",
      "evaluation/Actions Max                                 0.976562\n",
      "evaluation/Actions Min                                -0.655021\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           886.9\n",
      "evaluation/env_infos/final/reward_forward Mean         8.75233e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.05795e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.8771e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.57685e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0603721\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132102\n",
      "evaluation/env_infos/initial/reward_forward Max        0.325223\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.170298\n",
      "evaluation/env_infos/reward_forward Mean              -0.000994528\n",
      "evaluation/env_infos/reward_forward Std                0.0582347\n",
      "evaluation/env_infos/reward_forward Max                1.11506\n",
      "evaluation/env_infos/reward_forward Min               -1.37676\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.112301\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0718239\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0414785\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.41802\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.626211\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.404629\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.137856\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.72762\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.113776\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0768255\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0228429\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.72762\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         8.86598e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          3.34054e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.8771e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.05407e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.135465\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.247703\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.689429\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.572657\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000810517\n",
      "evaluation/env_infos/torso_velocity Std                0.0623489\n",
      "evaluation/env_infos/torso_velocity Max                1.11506\n",
      "evaluation/env_infos/torso_velocity Min               -2.08829\n",
      "time/data storing (s)                                  0.0144323\n",
      "time/evaluation sampling (s)                          44.4765\n",
      "time/exploration sampling (s)                          1.94341\n",
      "time/logging (s)                                       0.272783\n",
      "time/saving (s)                                        0.0256683\n",
      "time/training (s)                                      4.89892\n",
      "time/epoch (s)                                        51.6318\n",
      "time/total (s)                                      3502.25\n",
      "Epoch                                                 64\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:05.188837 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 65 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 67000\n",
      "trainer/QF1 Loss                                       0.378769\n",
      "trainer/QF2 Loss                                       0.275549\n",
      "trainer/Policy Loss                                  -14.8979\n",
      "trainer/Q1 Predictions Mean                           21.6619\n",
      "trainer/Q1 Predictions Std                             1.49186\n",
      "trainer/Q1 Predictions Max                            25.3558\n",
      "trainer/Q1 Predictions Min                            15.6871\n",
      "trainer/Q2 Predictions Mean                           22.0169\n",
      "trainer/Q2 Predictions Std                             1.50937\n",
      "trainer/Q2 Predictions Max                            25.3815\n",
      "trainer/Q2 Predictions Min                            16.3754\n",
      "trainer/Q Targets Mean                                21.968\n",
      "trainer/Q Targets Std                                  1.5825\n",
      "trainer/Q Targets Max                                 26.4316\n",
      "trainer/Q Targets Min                                 15.8476\n",
      "trainer/Log Pis Mean                                   7.09868\n",
      "trainer/Log Pis Std                                    3.18003\n",
      "trainer/Log Pis Max                                   25.3728\n",
      "trainer/Log Pis Min                                   -5.69695\n",
      "trainer/Policy mu Mean                                 0.00799549\n",
      "trainer/Policy mu Std                                  0.309217\n",
      "trainer/Policy mu Max                                  3.98448\n",
      "trainer/Policy mu Min                                 -2.83851\n",
      "trainer/Policy log std Mean                           -2.2355\n",
      "trainer/Policy log std Std                             0.288066\n",
      "trainer/Policy log std Max                            -0.28627\n",
      "trainer/Policy log std Min                            -4.56711\n",
      "trainer/Alpha                                          0.00850909\n",
      "trainer/Alpha Loss                                    -4.29399\n",
      "exploration/num steps total                        67000\n",
      "exploration/num paths total                          127\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.815249\n",
      "exploration/Rewards Std                                0.258563\n",
      "exploration/Rewards Max                                1.54894\n",
      "exploration/Rewards Min                               -1.96614\n",
      "exploration/Returns Mean                             815.249\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              815.249\n",
      "exploration/Returns Min                              815.249\n",
      "exploration/Actions Mean                               0.0457631\n",
      "exploration/Actions Std                                0.217476\n",
      "exploration/Actions Max                                0.999999\n",
      "exploration/Actions Min                               -0.99994\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          815.249\n",
      "exploration/env_infos/final/reward_forward Mean        0.174793\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.174793\n",
      "exploration/env_infos/final/reward_forward Min         0.174793\n",
      "exploration/env_infos/initial/reward_forward Mean      0.176323\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.176323\n",
      "exploration/env_infos/initial/reward_forward Min       0.176323\n",
      "exploration/env_infos/reward_forward Mean             -0.0102736\n",
      "exploration/env_infos/reward_forward Std               0.290001\n",
      "exploration/env_infos/reward_forward Max               1.0521\n",
      "exploration/env_infos/reward_forward Min              -0.920606\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.111209\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.111209\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.111209\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.414128\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.414128\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.414128\n",
      "exploration/env_infos/reward_ctrl Mean                -0.19756\n",
      "exploration/env_infos/reward_ctrl Std                  0.248286\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0250277\n",
      "exploration/env_infos/reward_ctrl Min                 -2.96614\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.162217\n",
      "exploration/env_infos/final/torso_velocity Std         0.161278\n",
      "exploration/env_infos/final/torso_velocity Max         0.353153\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0412938\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0537528\n",
      "exploration/env_infos/initial/torso_velocity Std       0.239269\n",
      "exploration/env_infos/initial/torso_velocity Max       0.26561\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.280674\n",
      "exploration/env_infos/torso_velocity Mean              0.0346359\n",
      "exploration/env_infos/torso_velocity Std               0.415547\n",
      "exploration/env_infos/torso_velocity Max               1.92426\n",
      "exploration/env_infos/torso_velocity Min              -1.61568\n",
      "evaluation/num steps total                             1.65e+06\n",
      "evaluation/num paths total                          1650\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.933054\n",
      "evaluation/Rewards Std                                 0.132753\n",
      "evaluation/Rewards Max                                 2.63523\n",
      "evaluation/Rewards Min                                -2.07732\n",
      "evaluation/Returns Mean                              933.054\n",
      "evaluation/Returns Std                                31.9572\n",
      "evaluation/Returns Max                              1052.49\n",
      "evaluation/Returns Min                               882.43\n",
      "evaluation/Actions Mean                                0.00469189\n",
      "evaluation/Actions Std                                 0.148553\n",
      "evaluation/Actions Max                                 0.999998\n",
      "evaluation/Actions Min                                -0.999985\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           933.054\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.00482984\n",
      "evaluation/env_infos/final/reward_forward Std          0.16023\n",
      "evaluation/env_infos/final/reward_forward Max          0.485518\n",
      "evaluation/env_infos/final/reward_forward Min         -0.4665\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0741854\n",
      "evaluation/env_infos/initial/reward_forward Std        0.163482\n",
      "evaluation/env_infos/initial/reward_forward Max        0.407067\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.177865\n",
      "evaluation/env_infos/reward_forward Mean              -0.00329217\n",
      "evaluation/env_infos/reward_forward Std                0.268804\n",
      "evaluation/env_infos/reward_forward Max                1.67815\n",
      "evaluation/env_infos/reward_forward Min               -2.13777\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0876679\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0250193\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0546966\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.14843\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.357836\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.353088\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.044394\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.19441\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0883598\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0567329\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0120618\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.07732\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000908362\n",
      "evaluation/env_infos/final/torso_velocity Std          0.134174\n",
      "evaluation/env_infos/final/torso_velocity Max          0.662356\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.4665\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.146888\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.241697\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.570321\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.288435\n",
      "evaluation/env_infos/torso_velocity Mean               0.00175764\n",
      "evaluation/env_infos/torso_velocity Std                0.206378\n",
      "evaluation/env_infos/torso_velocity Max                1.95052\n",
      "evaluation/env_infos/torso_velocity Min               -2.13777\n",
      "time/data storing (s)                                  0.015554\n",
      "time/evaluation sampling (s)                          44.8194\n",
      "time/exploration sampling (s)                          1.93316\n",
      "time/logging (s)                                       0.281855\n",
      "time/saving (s)                                        0.0267454\n",
      "time/training (s)                                      4.15554\n",
      "time/epoch (s)                                        51.2323\n",
      "time/total (s)                                      3554.2\n",
      "Epoch                                                 65\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:24:56.907140 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 66 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 68000\n",
      "trainer/QF1 Loss                                       2.25438\n",
      "trainer/QF2 Loss                                       3.02226\n",
      "trainer/Policy Loss                                  -14.8338\n",
      "trainer/Q1 Predictions Mean                           22.8654\n",
      "trainer/Q1 Predictions Std                             6.50869\n",
      "trainer/Q1 Predictions Max                           122.265\n",
      "trainer/Q1 Predictions Min                             7.77564\n",
      "trainer/Q2 Predictions Mean                           22.5865\n",
      "trainer/Q2 Predictions Std                             5.1782\n",
      "trainer/Q2 Predictions Max                           100.683\n",
      "trainer/Q2 Predictions Min                             8.51925\n",
      "trainer/Q Targets Mean                                22.7916\n",
      "trainer/Q Targets Std                                  6.50056\n",
      "trainer/Q Targets Max                                121.188\n",
      "trainer/Q Targets Min                                  1.18822\n",
      "trainer/Log Pis Mean                                   8.02563\n",
      "trainer/Log Pis Std                                    7.37945\n",
      "trainer/Log Pis Max                                   85.8307\n",
      "trainer/Log Pis Min                                   -1.06276\n",
      "trainer/Policy mu Mean                                 0.0812509\n",
      "trainer/Policy mu Std                                  1.40085\n",
      "trainer/Policy mu Max                                 34.927\n",
      "trainer/Policy mu Min                                -22.0362\n",
      "trainer/Policy log std Mean                           -2.24899\n",
      "trainer/Policy log std Std                             0.307649\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -4.64907\n",
      "trainer/Alpha                                          0.00854237\n",
      "trainer/Alpha Loss                                     0.122049\n",
      "exploration/num steps total                        68000\n",
      "exploration/num paths total                          128\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.831469\n",
      "exploration/Rewards Std                                0.25244\n",
      "exploration/Rewards Max                                1.60695\n",
      "exploration/Rewards Min                               -2.12991\n",
      "exploration/Returns Mean                             831.469\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              831.469\n",
      "exploration/Returns Min                              831.469\n",
      "exploration/Actions Mean                               0.0425495\n",
      "exploration/Actions Std                                0.215579\n",
      "exploration/Actions Max                                0.999991\n",
      "exploration/Actions Min                               -0.999768\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          831.469\n",
      "exploration/env_infos/final/reward_forward Mean        0.381477\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.381477\n",
      "exploration/env_infos/final/reward_forward Min         0.381477\n",
      "exploration/env_infos/initial/reward_forward Mean      0.256375\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.256375\n",
      "exploration/env_infos/initial/reward_forward Min       0.256375\n",
      "exploration/env_infos/reward_forward Mean              0.0140476\n",
      "exploration/env_infos/reward_forward Std               0.337244\n",
      "exploration/env_infos/reward_forward Max               1.27418\n",
      "exploration/env_infos/reward_forward Min              -0.93871\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.306526\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.306526\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.306526\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.77119\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.77119\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.77119\n",
      "exploration/env_infos/reward_ctrl Mean                -0.193138\n",
      "exploration/env_infos/reward_ctrl Std                  0.227747\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0233365\n",
      "exploration/env_infos/reward_ctrl Min                 -3.12991\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0129967\n",
      "exploration/env_infos/final/torso_velocity Std         0.308241\n",
      "exploration/env_infos/final/torso_velocity Max         0.381477\n",
      "exploration/env_infos/final/torso_velocity Min        -0.370895\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.123464\n",
      "exploration/env_infos/initial/torso_velocity Std       0.252035\n",
      "exploration/env_infos/initial/torso_velocity Max       0.343424\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.229406\n",
      "exploration/env_infos/torso_velocity Mean              0.0187477\n",
      "exploration/env_infos/torso_velocity Std               0.32163\n",
      "exploration/env_infos/torso_velocity Max               1.48179\n",
      "exploration/env_infos/torso_velocity Min              -1.49585\n",
      "evaluation/num steps total                             1.675e+06\n",
      "evaluation/num paths total                          1675\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.851235\n",
      "evaluation/Rewards Std                                 0.186319\n",
      "evaluation/Rewards Max                                 2.31729\n",
      "evaluation/Rewards Min                                -2.68336\n",
      "evaluation/Returns Mean                              851.235\n",
      "evaluation/Returns Std                                68.3253\n",
      "evaluation/Returns Max                              1010.14\n",
      "evaluation/Returns Min                               725.282\n",
      "evaluation/Actions Mean                                0.0126737\n",
      "evaluation/Actions Std                                 0.208764\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           851.235\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0297178\n",
      "evaluation/env_infos/final/reward_forward Std          0.122295\n",
      "evaluation/env_infos/final/reward_forward Max          0.191023\n",
      "evaluation/env_infos/final/reward_forward Min         -0.454966\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.072738\n",
      "evaluation/env_infos/initial/reward_forward Std        0.160789\n",
      "evaluation/env_infos/initial/reward_forward Max        0.328253\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.27155\n",
      "evaluation/env_infos/reward_forward Mean               0.0304341\n",
      "evaluation/env_infos/reward_forward Std                0.188786\n",
      "evaluation/env_infos/reward_forward Max                1.94175\n",
      "evaluation/env_infos/reward_forward Min               -1.46435\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.168408\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0518968\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0733817\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.275428\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.650548\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.566013\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.104179\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.9555\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.174972\n",
      "evaluation/env_infos/reward_ctrl Std                   0.125523\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.028935\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.68336\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0244235\n",
      "evaluation/env_infos/final/torso_velocity Std          0.121865\n",
      "evaluation/env_infos/final/torso_velocity Max          0.643388\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.454966\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.109241\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.256613\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.545929\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.358028\n",
      "evaluation/env_infos/torso_velocity Mean               0.0158836\n",
      "evaluation/env_infos/torso_velocity Std                0.213955\n",
      "evaluation/env_infos/torso_velocity Max                2.14066\n",
      "evaluation/env_infos/torso_velocity Min               -1.96207\n",
      "time/data storing (s)                                  0.0150848\n",
      "time/evaluation sampling (s)                          44.6612\n",
      "time/exploration sampling (s)                          1.93122\n",
      "time/logging (s)                                       0.263633\n",
      "time/saving (s)                                        0.0258535\n",
      "time/training (s)                                      4.14142\n",
      "time/epoch (s)                                        51.0384\n",
      "time/total (s)                                      3605.9\n",
      "Epoch                                                 66\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:25:49.893797 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 67 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 69000\n",
      "trainer/QF1 Loss                                       0.889521\n",
      "trainer/QF2 Loss                                       0.693878\n",
      "trainer/Policy Loss                                  -15.9046\n",
      "trainer/Q1 Predictions Mean                           23.5282\n",
      "trainer/Q1 Predictions Std                             3.47973\n",
      "trainer/Q1 Predictions Max                            73.3115\n",
      "trainer/Q1 Predictions Min                            17.0727\n",
      "trainer/Q2 Predictions Mean                           23.383\n",
      "trainer/Q2 Predictions Std                             2.84992\n",
      "trainer/Q2 Predictions Max                            61.9417\n",
      "trainer/Q2 Predictions Min                            16.8\n",
      "trainer/Q Targets Mean                                23.0202\n",
      "trainer/Q Targets Std                                  3.21454\n",
      "trainer/Q Targets Max                                 67.9767\n",
      "trainer/Q Targets Min                                 10.6957\n",
      "trainer/Log Pis Mean                                   7.74529\n",
      "trainer/Log Pis Std                                    3.27304\n",
      "trainer/Log Pis Max                                   39.9431\n",
      "trainer/Log Pis Min                                   -2.49474\n",
      "trainer/Policy mu Mean                                 0.00341699\n",
      "trainer/Policy mu Std                                  0.672238\n",
      "trainer/Policy mu Max                                 18.35\n",
      "trainer/Policy mu Min                                 -8.11257\n",
      "trainer/Policy log std Mean                           -2.2979\n",
      "trainer/Policy log std Std                             0.300765\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -4.05527\n",
      "trainer/Alpha                                          0.00806186\n",
      "trainer/Alpha Loss                                    -1.2276\n",
      "exploration/num steps total                        69000\n",
      "exploration/num paths total                          129\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.878017\n",
      "exploration/Rewards Std                                0.0791444\n",
      "exploration/Rewards Max                                1.26655\n",
      "exploration/Rewards Min                                0.29363\n",
      "exploration/Returns Mean                             878.017\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              878.017\n",
      "exploration/Returns Min                              878.017\n",
      "exploration/Actions Mean                               0.0274877\n",
      "exploration/Actions Std                                0.184994\n",
      "exploration/Actions Max                                0.849388\n",
      "exploration/Actions Min                               -0.678684\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          878.017\n",
      "exploration/env_infos/final/reward_forward Mean        0.0569901\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0569901\n",
      "exploration/env_infos/final/reward_forward Min         0.0569901\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0986436\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0986436\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0986436\n",
      "exploration/env_infos/reward_forward Mean             -0.0173073\n",
      "exploration/env_infos/reward_forward Std               0.138412\n",
      "exploration/env_infos/reward_forward Max               0.366665\n",
      "exploration/env_infos/reward_forward Min              -1.08151\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.122661\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.122661\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.122661\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.185425\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.185425\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.185425\n",
      "exploration/env_infos/reward_ctrl Mean                -0.139914\n",
      "exploration/env_infos/reward_ctrl Std                  0.0587421\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0189869\n",
      "exploration/env_infos/reward_ctrl Min                 -0.70637\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0703017\n",
      "exploration/env_infos/final/torso_velocity Std         0.0411503\n",
      "exploration/env_infos/final/torso_velocity Max         0.12602\n",
      "exploration/env_infos/final/torso_velocity Min         0.027895\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.153271\n",
      "exploration/env_infos/initial/torso_velocity Std       0.181853\n",
      "exploration/env_infos/initial/torso_velocity Max       0.324064\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0986436\n",
      "exploration/env_infos/torso_velocity Mean              0.00334453\n",
      "exploration/env_infos/torso_velocity Std               0.132541\n",
      "exploration/env_infos/torso_velocity Max               0.887301\n",
      "exploration/env_infos/torso_velocity Min              -1.08151\n",
      "evaluation/num steps total                             1.7e+06\n",
      "evaluation/num paths total                          1700\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.904536\n",
      "evaluation/Rewards Std                                 0.0416556\n",
      "evaluation/Rewards Max                                 2.26412\n",
      "evaluation/Rewards Min                                 0.122053\n",
      "evaluation/Returns Mean                              904.536\n",
      "evaluation/Returns Std                                23.9888\n",
      "evaluation/Returns Max                               941.663\n",
      "evaluation/Returns Min                               839.802\n",
      "evaluation/Actions Mean                                0.0178998\n",
      "evaluation/Actions Std                                 0.154388\n",
      "evaluation/Actions Max                                 0.753723\n",
      "evaluation/Actions Min                                -0.732362\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           904.536\n",
      "evaluation/env_infos/final/reward_forward Mean         1.87971e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.79353e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.81958e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.56517e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00997065\n",
      "evaluation/env_infos/initial/reward_forward Std        0.135382\n",
      "evaluation/env_infos/initial/reward_forward Max        0.22376\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.302538\n",
      "evaluation/env_infos/reward_forward Mean              -0.0019523\n",
      "evaluation/env_infos/reward_forward Std                0.0554786\n",
      "evaluation/env_infos/reward_forward Max                1.40388\n",
      "evaluation/env_infos/reward_forward Min               -1.67396\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0947665\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0246832\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0563173\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.160629\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.218488\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.209518\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0389682\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.877947\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0966239\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0319691\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0389682\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.877947\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         5.00355e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.75322e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.03819e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.11559e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.125902\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.243516\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.587697\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.317036\n",
      "evaluation/env_infos/torso_velocity Mean              -6.85434e-05\n",
      "evaluation/env_infos/torso_velocity Std                0.0739443\n",
      "evaluation/env_infos/torso_velocity Max                1.40388\n",
      "evaluation/env_infos/torso_velocity Min               -2.08679\n",
      "time/data storing (s)                                  0.0216082\n",
      "time/evaluation sampling (s)                          44.9656\n",
      "time/exploration sampling (s)                          2.14478\n",
      "time/logging (s)                                       0.271646\n",
      "time/saving (s)                                        0.0263979\n",
      "time/training (s)                                      4.9568\n",
      "time/epoch (s)                                        52.3868\n",
      "time/total (s)                                      3658.89\n",
      "Epoch                                                 67\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:26:44.119664 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 68 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 70000\n",
      "trainer/QF1 Loss                                       2.04021\n",
      "trainer/QF2 Loss                                       1.39827\n",
      "trainer/Policy Loss                                  -15.2122\n",
      "trainer/Q1 Predictions Mean                           22.8678\n",
      "trainer/Q1 Predictions Std                             2.01363\n",
      "trainer/Q1 Predictions Max                            39.1625\n",
      "trainer/Q1 Predictions Min                            12.5527\n",
      "trainer/Q2 Predictions Mean                           23.2037\n",
      "trainer/Q2 Predictions Std                             1.88499\n",
      "trainer/Q2 Predictions Max                            33.6678\n",
      "trainer/Q2 Predictions Min                            15.6105\n",
      "trainer/Q Targets Mean                                23.0614\n",
      "trainer/Q Targets Std                                  1.63712\n",
      "trainer/Q Targets Max                                 31.6594\n",
      "trainer/Q Targets Min                                 14.9219\n",
      "trainer/Log Pis Mean                                   8.01363\n",
      "trainer/Log Pis Std                                    2.41689\n",
      "trainer/Log Pis Max                                   17.7496\n",
      "trainer/Log Pis Min                                    1.81341\n",
      "trainer/Policy mu Mean                                -0.0443425\n",
      "trainer/Policy mu Std                                  0.252909\n",
      "trainer/Policy mu Max                                  2.17593\n",
      "trainer/Policy mu Min                                 -1.13776\n",
      "trainer/Policy log std Mean                           -2.3445\n",
      "trainer/Policy log std Std                             0.270577\n",
      "trainer/Policy log std Max                            -1.42691\n",
      "trainer/Policy log std Min                            -4.46528\n",
      "trainer/Alpha                                          0.00810936\n",
      "trainer/Alpha Loss                                     0.0656022\n",
      "exploration/num steps total                        70000\n",
      "exploration/num paths total                          130\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.874044\n",
      "exploration/Rewards Std                                0.0536444\n",
      "exploration/Rewards Max                                1.05793\n",
      "exploration/Rewards Min                                0.629571\n",
      "exploration/Returns Mean                             874.044\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              874.044\n",
      "exploration/Returns Min                              874.044\n",
      "exploration/Actions Mean                              -0.0480096\n",
      "exploration/Actions Std                                0.171811\n",
      "exploration/Actions Max                                0.554676\n",
      "exploration/Actions Min                               -0.589786\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          874.044\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0752089\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0752089\n",
      "exploration/env_infos/final/reward_forward Min        -0.0752089\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00244516\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00244516\n",
      "exploration/env_infos/initial/reward_forward Min       0.00244516\n",
      "exploration/env_infos/reward_forward Mean              0.0234501\n",
      "exploration/env_infos/reward_forward Std               0.2073\n",
      "exploration/env_infos/reward_forward Max               0.855964\n",
      "exploration/env_infos/reward_forward Min              -1.3915\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.110008\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.110008\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.110008\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.296108\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.296108\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.296108\n",
      "exploration/env_infos/reward_ctrl Mean                -0.127296\n",
      "exploration/env_infos/reward_ctrl Std                  0.0520858\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00983179\n",
      "exploration/env_infos/reward_ctrl Min                 -0.370429\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.031987\n",
      "exploration/env_infos/final/torso_velocity Std         0.0469777\n",
      "exploration/env_infos/final/torso_velocity Max         0.0333191\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0752089\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0620675\n",
      "exploration/env_infos/initial/torso_velocity Std       0.150817\n",
      "exploration/env_infos/initial/torso_velocity Max       0.269227\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0854702\n",
      "exploration/env_infos/torso_velocity Mean              0.00105224\n",
      "exploration/env_infos/torso_velocity Std               0.153219\n",
      "exploration/env_infos/torso_velocity Max               0.855964\n",
      "exploration/env_infos/torso_velocity Min              -1.3915\n",
      "evaluation/num steps total                             1.725e+06\n",
      "evaluation/num paths total                          1725\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.939702\n",
      "evaluation/Rewards Std                                 0.0387654\n",
      "evaluation/Rewards Max                                 1.77103\n",
      "evaluation/Rewards Min                                 0.0800136\n",
      "evaluation/Returns Mean                              939.702\n",
      "evaluation/Returns Std                                20.2188\n",
      "evaluation/Returns Max                               966.088\n",
      "evaluation/Returns Min                               867.403\n",
      "evaluation/Actions Mean                               -0.0429098\n",
      "evaluation/Actions Std                                 0.116812\n",
      "evaluation/Actions Max                                 0.712141\n",
      "evaluation/Actions Min                                -0.650419\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           939.702\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00262646\n",
      "evaluation/env_infos/final/reward_forward Std          0.0128668\n",
      "evaluation/env_infos/final/reward_forward Max          0.0656608\n",
      "evaluation/env_infos/final/reward_forward Min         -7.45015e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0223825\n",
      "evaluation/env_infos/initial/reward_forward Std        0.130604\n",
      "evaluation/env_infos/initial/reward_forward Max        0.259327\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.282156\n",
      "evaluation/env_infos/reward_forward Mean               0.00133028\n",
      "evaluation/env_infos/reward_forward Std                0.0600852\n",
      "evaluation/env_infos/reward_forward Max                1.63923\n",
      "evaluation/env_infos/reward_forward Min               -1.0446\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0602339\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0219601\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.033653\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.141311\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.157869\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0914406\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0361848\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.508514\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0619454\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0301523\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00565843\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.919986\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00272107\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0180903\n",
      "evaluation/env_infos/final/torso_velocity Max          0.144071\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00565077\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.109925\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.243868\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.569385\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.486364\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00224296\n",
      "evaluation/env_infos/torso_velocity Std                0.0654862\n",
      "evaluation/env_infos/torso_velocity Max                1.63923\n",
      "evaluation/env_infos/torso_velocity Min               -1.85763\n",
      "time/data storing (s)                                  0.0143949\n",
      "time/evaluation sampling (s)                          46.6186\n",
      "time/exploration sampling (s)                          2.08434\n",
      "time/logging (s)                                       0.290851\n",
      "time/saving (s)                                        0.0332537\n",
      "time/training (s)                                      4.5235\n",
      "time/epoch (s)                                        53.5649\n",
      "time/total (s)                                      3713.13\n",
      "Epoch                                                 68\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:27:41.718173 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 69 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 71000\n",
      "trainer/QF1 Loss                                       1.17451\n",
      "trainer/QF2 Loss                                       0.964654\n",
      "trainer/Policy Loss                                  -15.2432\n",
      "trainer/Q1 Predictions Mean                           23.4255\n",
      "trainer/Q1 Predictions Std                             1.66131\n",
      "trainer/Q1 Predictions Max                            28.5065\n",
      "trainer/Q1 Predictions Min                            15.7148\n",
      "trainer/Q2 Predictions Mean                           23.4501\n",
      "trainer/Q2 Predictions Std                             1.61474\n",
      "trainer/Q2 Predictions Max                            30.051\n",
      "trainer/Q2 Predictions Min                            16.3868\n",
      "trainer/Q Targets Mean                                23.4274\n",
      "trainer/Q Targets Std                                  1.87257\n",
      "trainer/Q Targets Max                                 42.5498\n",
      "trainer/Q Targets Min                                 15.5961\n",
      "trainer/Log Pis Mean                                   8.35647\n",
      "trainer/Log Pis Std                                    2.92385\n",
      "trainer/Log Pis Max                                   22.4759\n",
      "trainer/Log Pis Min                                   -3.5989\n",
      "trainer/Policy mu Mean                                -0.0277056\n",
      "trainer/Policy mu Std                                  0.362133\n",
      "trainer/Policy mu Max                                  3.81334\n",
      "trainer/Policy mu Min                                 -1.53564\n",
      "trainer/Policy log std Mean                           -2.3391\n",
      "trainer/Policy log std Std                             0.270227\n",
      "trainer/Policy log std Max                             0.0310434\n",
      "trainer/Policy log std Min                            -4.27039\n",
      "trainer/Alpha                                          0.00878882\n",
      "trainer/Alpha Loss                                     1.68822\n",
      "exploration/num steps total                        71000\n",
      "exploration/num paths total                          131\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.890093\n",
      "exploration/Rewards Std                                0.169023\n",
      "exploration/Rewards Max                                2.20265\n",
      "exploration/Rewards Min                               -0.271523\n",
      "exploration/Returns Mean                             890.093\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              890.093\n",
      "exploration/Returns Min                              890.093\n",
      "exploration/Actions Mean                              -0.0039955\n",
      "exploration/Actions Std                                0.186175\n",
      "exploration/Actions Max                                0.945229\n",
      "exploration/Actions Min                               -0.627064\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          890.093\n",
      "exploration/env_infos/final/reward_forward Mean        0.11232\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.11232\n",
      "exploration/env_infos/final/reward_forward Min         0.11232\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0281829\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0281829\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0281829\n",
      "exploration/env_infos/reward_forward Mean              0.0125728\n",
      "exploration/env_infos/reward_forward Std               0.271745\n",
      "exploration/env_infos/reward_forward Max               2.26235\n",
      "exploration/env_infos/reward_forward Min              -1.17221\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.154145\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.154145\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.154145\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0484929\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0484929\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0484929\n",
      "exploration/env_infos/reward_ctrl Mean                -0.138708\n",
      "exploration/env_infos/reward_ctrl Std                  0.11026\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0213869\n",
      "exploration/env_infos/reward_ctrl Min                 -1.27152\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0690579\n",
      "exploration/env_infos/final/torso_velocity Std         0.145127\n",
      "exploration/env_infos/final/torso_velocity Max         0.11232\n",
      "exploration/env_infos/final/torso_velocity Min        -0.242928\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0798734\n",
      "exploration/env_infos/initial/torso_velocity Std       0.236401\n",
      "exploration/env_infos/initial/torso_velocity Max       0.407893\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.14009\n",
      "exploration/env_infos/torso_velocity Mean              0.000241273\n",
      "exploration/env_infos/torso_velocity Std               0.256739\n",
      "exploration/env_infos/torso_velocity Max               2.26235\n",
      "exploration/env_infos/torso_velocity Min              -1.60091\n",
      "evaluation/num steps total                             1.75e+06\n",
      "evaluation/num paths total                          1750\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.872683\n",
      "evaluation/Rewards Std                                 0.2056\n",
      "evaluation/Rewards Max                                 2.21915\n",
      "evaluation/Rewards Min                                -2.53765\n",
      "evaluation/Returns Mean                              872.683\n",
      "evaluation/Returns Std                                71.1142\n",
      "evaluation/Returns Max                               955.351\n",
      "evaluation/Returns Min                               686.784\n",
      "evaluation/Actions Mean                               -0.0324906\n",
      "evaluation/Actions Std                                 0.18123\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           872.683\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.0223029\n",
      "evaluation/env_infos/final/reward_forward Std          0.165154\n",
      "evaluation/env_infos/final/reward_forward Max          0.519042\n",
      "evaluation/env_infos/final/reward_forward Min         -0.539024\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0433464\n",
      "evaluation/env_infos/initial/reward_forward Std        0.114909\n",
      "evaluation/env_infos/initial/reward_forward Max        0.227245\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.224106\n",
      "evaluation/env_infos/reward_forward Mean               0.0044889\n",
      "evaluation/env_infos/reward_forward Std                0.212929\n",
      "evaluation/env_infos/reward_forward Max                1.84441\n",
      "evaluation/env_infos/reward_forward Min               -1.91526\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0971077\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0769985\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0328889\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.31451\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.250751\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.271315\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0339521\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.15335\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.135599\n",
      "evaluation/env_infos/reward_ctrl Std                   0.20174\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00579958\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.53765\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00535726\n",
      "evaluation/env_infos/final/torso_velocity Std          0.137618\n",
      "evaluation/env_infos/final/torso_velocity Max          0.734287\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.539024\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.136368\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.251495\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.618411\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.44003\n",
      "evaluation/env_infos/torso_velocity Mean               0.000375247\n",
      "evaluation/env_infos/torso_velocity Std                0.198493\n",
      "evaluation/env_infos/torso_velocity Max                1.94396\n",
      "evaluation/env_infos/torso_velocity Min               -1.91526\n",
      "time/data storing (s)                                  0.0151129\n",
      "time/evaluation sampling (s)                          49.8896\n",
      "time/exploration sampling (s)                          2.02889\n",
      "time/logging (s)                                       0.291636\n",
      "time/saving (s)                                        0.0338861\n",
      "time/training (s)                                      4.64495\n",
      "time/epoch (s)                                        56.9041\n",
      "time/total (s)                                      3770.73\n",
      "Epoch                                                 69\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:28:37.121731 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 70 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 72000\n",
      "trainer/QF1 Loss                                       0.670776\n",
      "trainer/QF2 Loss                                       0.744898\n",
      "trainer/Policy Loss                                  -17.0293\n",
      "trainer/Q1 Predictions Mean                           23.4483\n",
      "trainer/Q1 Predictions Std                             1.55707\n",
      "trainer/Q1 Predictions Max                            28.4765\n",
      "trainer/Q1 Predictions Min                            15.3018\n",
      "trainer/Q2 Predictions Mean                           23.5377\n",
      "trainer/Q2 Predictions Std                             1.69742\n",
      "trainer/Q2 Predictions Max                            32.0338\n",
      "trainer/Q2 Predictions Min                            16.2247\n",
      "trainer/Q Targets Mean                                23.6764\n",
      "trainer/Q Targets Std                                  1.79052\n",
      "trainer/Q Targets Max                                 30.9627\n",
      "trainer/Q Targets Min                                 12.3593\n",
      "trainer/Log Pis Mean                                   6.63298\n",
      "trainer/Log Pis Std                                    2.42868\n",
      "trainer/Log Pis Max                                   16.5464\n",
      "trainer/Log Pis Min                                   -0.54723\n",
      "trainer/Policy mu Mean                                -0.102225\n",
      "trainer/Policy mu Std                                  0.277283\n",
      "trainer/Policy mu Max                                  2.38353\n",
      "trainer/Policy mu Min                                 -2.62531\n",
      "trainer/Policy log std Mean                           -2.14975\n",
      "trainer/Policy log std Std                             0.300778\n",
      "trainer/Policy log std Max                            -0.818023\n",
      "trainer/Policy log std Min                            -3.83844\n",
      "trainer/Alpha                                          0.0106804\n",
      "trainer/Alpha Loss                                    -6.20349\n",
      "exploration/num steps total                        72000\n",
      "exploration/num paths total                          132\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.875376\n",
      "exploration/Rewards Std                                0.0960581\n",
      "exploration/Rewards Max                                1.19918\n",
      "exploration/Rewards Min                               -1.42552\n",
      "exploration/Returns Mean                             875.376\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              875.376\n",
      "exploration/Returns Min                              875.376\n",
      "exploration/Actions Mean                              -0.0760346\n",
      "exploration/Actions Std                                0.160592\n",
      "exploration/Actions Max                                0.993263\n",
      "exploration/Actions Min                               -0.93646\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          875.376\n",
      "exploration/env_infos/final/reward_forward Mean       -0.285949\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.285949\n",
      "exploration/env_infos/final/reward_forward Min        -0.285949\n",
      "exploration/env_infos/initial/reward_forward Mean      0.27184\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.27184\n",
      "exploration/env_infos/initial/reward_forward Min       0.27184\n",
      "exploration/env_infos/reward_forward Mean              0.00360389\n",
      "exploration/env_infos/reward_forward Std               0.27167\n",
      "exploration/env_infos/reward_forward Max               0.83899\n",
      "exploration/env_infos/reward_forward Min              -1.0842\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0958729\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0958729\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0958729\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0638929\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0638929\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0638929\n",
      "exploration/env_infos/reward_ctrl Mean                -0.126285\n",
      "exploration/env_infos/reward_ctrl Std                  0.0948077\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0140536\n",
      "exploration/env_infos/reward_ctrl Min                 -2.42552\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0799889\n",
      "exploration/env_infos/final/torso_velocity Std         0.146891\n",
      "exploration/env_infos/final/torso_velocity Max         0.0464611\n",
      "exploration/env_infos/final/torso_velocity Min        -0.285949\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.258143\n",
      "exploration/env_infos/initial/torso_velocity Std       0.122092\n",
      "exploration/env_infos/initial/torso_velocity Max       0.400355\n",
      "exploration/env_infos/initial/torso_velocity Min       0.102234\n",
      "exploration/env_infos/torso_velocity Mean             -0.0132906\n",
      "exploration/env_infos/torso_velocity Std               0.193018\n",
      "exploration/env_infos/torso_velocity Max               0.83899\n",
      "exploration/env_infos/torso_velocity Min              -1.31025\n",
      "evaluation/num steps total                             1.775e+06\n",
      "evaluation/num paths total                          1775\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.906807\n",
      "evaluation/Rewards Std                                 0.0988364\n",
      "evaluation/Rewards Max                                 1.88652\n",
      "evaluation/Rewards Min                                -2.41738\n",
      "evaluation/Returns Mean                              906.807\n",
      "evaluation/Returns Std                                68.6736\n",
      "evaluation/Returns Max                               967.55\n",
      "evaluation/Returns Min                               674.035\n",
      "evaluation/Actions Mean                               -0.0779842\n",
      "evaluation/Actions Std                                 0.132675\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999773\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           906.807\n",
      "evaluation/env_infos/final/reward_forward Mean         5.30412e-08\n",
      "evaluation/env_infos/final/reward_forward Std          5.3383e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.0867e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -1.09065e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.051077\n",
      "evaluation/env_infos/initial/reward_forward Std        0.12501\n",
      "evaluation/env_infos/initial/reward_forward Max        0.20197\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.260612\n",
      "evaluation/env_infos/reward_forward Mean               0.00241728\n",
      "evaluation/env_infos/reward_forward Std                0.0678704\n",
      "evaluation/env_infos/reward_forward Max                1.35449\n",
      "evaluation/env_infos/reward_forward Min               -1.67372\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0911884\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0664043\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.034226\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.307288\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.115092\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0509888\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0512597\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.237915\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0947369\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0963149\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0165498\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.41738\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.83398e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.80154e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.0867e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.09065e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.118678\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.24913\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.596325\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.260612\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00227048\n",
      "evaluation/env_infos/torso_velocity Std                0.0707211\n",
      "evaluation/env_infos/torso_velocity Max                1.35449\n",
      "evaluation/env_infos/torso_velocity Min               -1.86475\n",
      "time/data storing (s)                                  0.0172725\n",
      "time/evaluation sampling (s)                          47.6251\n",
      "time/exploration sampling (s)                          2.01584\n",
      "time/logging (s)                                       0.305254\n",
      "time/saving (s)                                        0.0357233\n",
      "time/training (s)                                      4.73154\n",
      "time/epoch (s)                                        54.7307\n",
      "time/total (s)                                      3826.15\n",
      "Epoch                                                 70\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:29:30.542635 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 71 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 73000\n",
      "trainer/QF1 Loss                                       3.56734\n",
      "trainer/QF2 Loss                                       0.56284\n",
      "trainer/Policy Loss                                  -16.0807\n",
      "trainer/Q1 Predictions Mean                           24.0052\n",
      "trainer/Q1 Predictions Std                             5.34835\n",
      "trainer/Q1 Predictions Max                           105.327\n",
      "trainer/Q1 Predictions Min                            17.2094\n",
      "trainer/Q2 Predictions Mean                           24.1817\n",
      "trainer/Q2 Predictions Std                             6.84561\n",
      "trainer/Q2 Predictions Max                           130.442\n",
      "trainer/Q2 Predictions Min                            16.8218\n",
      "trainer/Q Targets Mean                                24.2981\n",
      "trainer/Q Targets Std                                  7.01027\n",
      "trainer/Q Targets Max                                133.454\n",
      "trainer/Q Targets Min                                 16.2535\n",
      "trainer/Log Pis Mean                                   8.26361\n",
      "trainer/Log Pis Std                                    5.10596\n",
      "trainer/Log Pis Max                                   70.8154\n",
      "trainer/Log Pis Min                                   -1.24353\n",
      "trainer/Policy mu Mean                                -0.0410588\n",
      "trainer/Policy mu Std                                  0.785996\n",
      "trainer/Policy mu Max                                 24.3843\n",
      "trainer/Policy mu Min                                 -5.9641\n",
      "trainer/Policy log std Mean                           -2.22056\n",
      "trainer/Policy log std Std                             0.381563\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -4.07501\n",
      "trainer/Alpha                                          0.00929692\n",
      "trainer/Alpha Loss                                     1.23311\n",
      "exploration/num steps total                        73000\n",
      "exploration/num paths total                          133\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.68393\n",
      "exploration/Rewards Std                                0.112269\n",
      "exploration/Rewards Max                                1.23053\n",
      "exploration/Rewards Min                               -0.352137\n",
      "exploration/Returns Mean                             683.93\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              683.93\n",
      "exploration/Returns Min                              683.93\n",
      "exploration/Actions Mean                              -0.0964108\n",
      "exploration/Actions Std                                0.26676\n",
      "exploration/Actions Max                                0.84735\n",
      "exploration/Actions Min                               -0.806781\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          683.93\n",
      "exploration/env_infos/final/reward_forward Mean        0.0106064\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0106064\n",
      "exploration/env_infos/final/reward_forward Min         0.0106064\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.198329\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.198329\n",
      "exploration/env_infos/initial/reward_forward Min      -0.198329\n",
      "exploration/env_infos/reward_forward Mean              0.010404\n",
      "exploration/env_infos/reward_forward Std               0.0928009\n",
      "exploration/env_infos/reward_forward Max               1.06856\n",
      "exploration/env_infos/reward_forward Min              -0.427909\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.311567\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.311567\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.311567\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.293911\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.293911\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.293911\n",
      "exploration/env_infos/reward_ctrl Mean                -0.321824\n",
      "exploration/env_infos/reward_ctrl Std                  0.107365\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0570884\n",
      "exploration/env_infos/reward_ctrl Min                 -1.35214\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0123473\n",
      "exploration/env_infos/final/torso_velocity Std         0.0078568\n",
      "exploration/env_infos/final/torso_velocity Max         0.0227214\n",
      "exploration/env_infos/final/torso_velocity Min         0.00371393\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.125049\n",
      "exploration/env_infos/initial/torso_velocity Std       0.254911\n",
      "exploration/env_infos/initial/torso_velocity Max       0.424721\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.198329\n",
      "exploration/env_infos/torso_velocity Mean              0.00427228\n",
      "exploration/env_infos/torso_velocity Std               0.0898836\n",
      "exploration/env_infos/torso_velocity Max               1.06856\n",
      "exploration/env_infos/torso_velocity Min              -1.45663\n",
      "evaluation/num steps total                             1.8e+06\n",
      "evaluation/num paths total                          1800\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.844266\n",
      "evaluation/Rewards Std                                 0.0983582\n",
      "evaluation/Rewards Max                                 2.26061\n",
      "evaluation/Rewards Min                                -1.67196\n",
      "evaluation/Returns Mean                              844.266\n",
      "evaluation/Returns Std                                78.0121\n",
      "evaluation/Returns Max                               943.737\n",
      "evaluation/Returns Min                               611.012\n",
      "evaluation/Actions Mean                               -0.0630363\n",
      "evaluation/Actions Std                                 0.188041\n",
      "evaluation/Actions Max                                 0.999736\n",
      "evaluation/Actions Min                                -0.940402\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           844.266\n",
      "evaluation/env_infos/final/reward_forward Mean         1.72312e-07\n",
      "evaluation/env_infos/final/reward_forward Std          6.6368e-07\n",
      "evaluation/env_infos/final/reward_forward Max          2.36276e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -7.03528e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.109782\n",
      "evaluation/env_infos/initial/reward_forward Std        0.129602\n",
      "evaluation/env_infos/initial/reward_forward Max        0.093305\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.463049\n",
      "evaluation/env_infos/reward_forward Mean              -5.48374e-05\n",
      "evaluation/env_infos/reward_forward Std                0.0666925\n",
      "evaluation/env_infos/reward_forward Max                1.52272\n",
      "evaluation/env_infos/reward_forward Min               -1.39606\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.15332\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0785987\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0539339\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.389074\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.218695\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.187779\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.034504\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.63562\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.157332\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0956362\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0326823\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.67196\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.03025e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          6.18298e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          3.01149e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.56541e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0906467\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.265906\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.687426\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.463049\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00171505\n",
      "evaluation/env_infos/torso_velocity Std                0.0715185\n",
      "evaluation/env_infos/torso_velocity Max                1.56529\n",
      "evaluation/env_infos/torso_velocity Min               -2.13436\n",
      "time/data storing (s)                                  0.0151999\n",
      "time/evaluation sampling (s)                          46.0559\n",
      "time/exploration sampling (s)                          2.02563\n",
      "time/logging (s)                                       0.289827\n",
      "time/saving (s)                                        0.031647\n",
      "time/training (s)                                      4.29148\n",
      "time/epoch (s)                                        52.7097\n",
      "time/total (s)                                      3879.55\n",
      "Epoch                                                 71\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:30:25.562545 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 72 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 74000\n",
      "trainer/QF1 Loss                                       2.35323\n",
      "trainer/QF2 Loss                                       3.80906\n",
      "trainer/Policy Loss                                  -16.7608\n",
      "trainer/Q1 Predictions Mean                           24.1365\n",
      "trainer/Q1 Predictions Std                             4.11256\n",
      "trainer/Q1 Predictions Max                            78.8579\n",
      "trainer/Q1 Predictions Min                            15.6439\n",
      "trainer/Q2 Predictions Mean                           23.9289\n",
      "trainer/Q2 Predictions Std                             3.11904\n",
      "trainer/Q2 Predictions Max                            60.6375\n",
      "trainer/Q2 Predictions Min                            15.065\n",
      "trainer/Q Targets Mean                                24.154\n",
      "trainer/Q Targets Std                                  4.49499\n",
      "trainer/Q Targets Max                                 83.6911\n",
      "trainer/Q Targets Min                                 17.0519\n",
      "trainer/Log Pis Mean                                   7.46613\n",
      "trainer/Log Pis Std                                    5.81437\n",
      "trainer/Log Pis Max                                   61.606\n",
      "trainer/Log Pis Min                                   -0.193582\n",
      "trainer/Policy mu Mean                                -0.0172848\n",
      "trainer/Policy mu Std                                  0.652515\n",
      "trainer/Policy mu Max                                 12.8341\n",
      "trainer/Policy mu Min                                 -7.70829\n",
      "trainer/Policy log std Mean                           -2.14644\n",
      "trainer/Policy log std Std                             0.382551\n",
      "trainer/Policy log std Max                             1.71874\n",
      "trainer/Policy log std Min                            -6.02318\n",
      "trainer/Alpha                                          0.00872936\n",
      "trainer/Alpha Loss                                    -2.52978\n",
      "exploration/num steps total                        74000\n",
      "exploration/num paths total                          134\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.833301\n",
      "exploration/Rewards Std                                0.0821062\n",
      "exploration/Rewards Max                                1.33554\n",
      "exploration/Rewards Min                               -0.0731925\n",
      "exploration/Returns Mean                             833.301\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              833.301\n",
      "exploration/Returns Min                              833.301\n",
      "exploration/Actions Mean                              -0.0224617\n",
      "exploration/Actions Std                                0.206615\n",
      "exploration/Actions Max                                0.740853\n",
      "exploration/Actions Min                               -0.616101\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          833.301\n",
      "exploration/env_infos/final/reward_forward Mean       -0.244525\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.244525\n",
      "exploration/env_infos/final/reward_forward Min        -0.244525\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0561384\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0561384\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0561384\n",
      "exploration/env_infos/reward_forward Mean             -0.0148546\n",
      "exploration/env_infos/reward_forward Std               0.246782\n",
      "exploration/env_infos/reward_forward Max               0.733048\n",
      "exploration/env_infos/reward_forward Min              -0.950107\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.15385\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.15385\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.15385\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.237315\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.237315\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.237315\n",
      "exploration/env_infos/reward_ctrl Mean                -0.172777\n",
      "exploration/env_infos/reward_ctrl Std                  0.0740697\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0229563\n",
      "exploration/env_infos/reward_ctrl Min                 -1.07319\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.127158\n",
      "exploration/env_infos/final/torso_velocity Std         0.0835518\n",
      "exploration/env_infos/final/torso_velocity Max        -0.0566372\n",
      "exploration/env_infos/final/torso_velocity Min        -0.244525\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.184144\n",
      "exploration/env_infos/initial/torso_velocity Std       0.267927\n",
      "exploration/env_infos/initial/torso_velocity Max       0.558009\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0561384\n",
      "exploration/env_infos/torso_velocity Mean              0.0143145\n",
      "exploration/env_infos/torso_velocity Std               0.217614\n",
      "exploration/env_infos/torso_velocity Max               1.18483\n",
      "exploration/env_infos/torso_velocity Min              -1.03465\n",
      "evaluation/num steps total                             1.825e+06\n",
      "evaluation/num paths total                          1825\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.909501\n",
      "evaluation/Rewards Std                                 0.0708784\n",
      "evaluation/Rewards Max                                 2.09809\n",
      "evaluation/Rewards Min                                -1.89404\n",
      "evaluation/Returns Mean                              909.501\n",
      "evaluation/Returns Std                                25.2329\n",
      "evaluation/Returns Max                               950.649\n",
      "evaluation/Returns Min                               836.901\n",
      "evaluation/Actions Mean                               -0.0364825\n",
      "evaluation/Actions Std                                 0.149388\n",
      "evaluation/Actions Max                                 0.998452\n",
      "evaluation/Actions Min                                -0.998404\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           909.501\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00178735\n",
      "evaluation/env_infos/final/reward_forward Std          0.0062829\n",
      "evaluation/env_infos/final/reward_forward Max          0.0282894\n",
      "evaluation/env_infos/final/reward_forward Min         -9.81617e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0115239\n",
      "evaluation/env_infos/initial/reward_forward Std        0.123254\n",
      "evaluation/env_infos/initial/reward_forward Max        0.220516\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.250557\n",
      "evaluation/env_infos/reward_forward Mean              -0.0025112\n",
      "evaluation/env_infos/reward_forward Std                0.104223\n",
      "evaluation/env_infos/reward_forward Max                0.985703\n",
      "evaluation/env_infos/reward_forward Min               -1.33055\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0991113\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0441545\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0400647\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.278947\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.116781\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.044024\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0616368\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.20678\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0945916\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0559805\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0240282\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.89404\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.0179895\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0976277\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0282894\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.780685\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.12128\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.251624\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.613384\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.250557\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00227296\n",
      "evaluation/env_infos/torso_velocity Std                0.125915\n",
      "evaluation/env_infos/torso_velocity Max                1.41611\n",
      "evaluation/env_infos/torso_velocity Min               -2.07051\n",
      "time/data storing (s)                                  0.0145406\n",
      "time/evaluation sampling (s)                          47.7079\n",
      "time/exploration sampling (s)                          2.02981\n",
      "time/logging (s)                                       0.286774\n",
      "time/saving (s)                                        0.0274434\n",
      "time/training (s)                                      4.28199\n",
      "time/epoch (s)                                        54.3485\n",
      "time/total (s)                                      3934.56\n",
      "Epoch                                                 72\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:31:20.463433 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 73 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 75000\n",
      "trainer/QF1 Loss                                       1.51858\n",
      "trainer/QF2 Loss                                       1.23063\n",
      "trainer/Policy Loss                                  -16.4377\n",
      "trainer/Q1 Predictions Mean                           24.4935\n",
      "trainer/Q1 Predictions Std                             2.13774\n",
      "trainer/Q1 Predictions Max                            32.9573\n",
      "trainer/Q1 Predictions Min                             8.42347\n",
      "trainer/Q2 Predictions Mean                           24.5534\n",
      "trainer/Q2 Predictions Std                             2.2092\n",
      "trainer/Q2 Predictions Max                            32.5777\n",
      "trainer/Q2 Predictions Min                             4.10715\n",
      "trainer/Q Targets Mean                                24.3621\n",
      "trainer/Q Targets Std                                  2.38934\n",
      "trainer/Q Targets Max                                 45.3501\n",
      "trainer/Q Targets Min                                  7.45532\n",
      "trainer/Log Pis Mean                                   8.36624\n",
      "trainer/Log Pis Std                                    3.17888\n",
      "trainer/Log Pis Max                                   28.2333\n",
      "trainer/Log Pis Min                                   -0.819927\n",
      "trainer/Policy mu Mean                                -0.0779327\n",
      "trainer/Policy mu Std                                  0.289943\n",
      "trainer/Policy mu Max                                  3.62787\n",
      "trainer/Policy mu Min                                 -1.73048\n",
      "trainer/Policy log std Mean                           -2.3636\n",
      "trainer/Policy log std Std                             0.370965\n",
      "trainer/Policy log std Max                            -0.39562\n",
      "trainer/Policy log std Min                            -5.31228\n",
      "trainer/Alpha                                          0.00744624\n",
      "trainer/Alpha Loss                                     1.79518\n",
      "exploration/num steps total                        75000\n",
      "exploration/num paths total                          135\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.405685\n",
      "exploration/Rewards Std                                0.17688\n",
      "exploration/Rewards Max                                1.07829\n",
      "exploration/Rewards Min                               -0.317059\n",
      "exploration/Returns Mean                             405.685\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              405.685\n",
      "exploration/Returns Min                              405.685\n",
      "exploration/Actions Mean                              -0.113421\n",
      "exploration/Actions Std                                0.371216\n",
      "exploration/Actions Max                                0.92401\n",
      "exploration/Actions Min                               -0.907958\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          405.685\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0571085\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0571085\n",
      "exploration/env_infos/final/reward_forward Min        -0.0571085\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0194719\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0194719\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0194719\n",
      "exploration/env_infos/reward_forward Mean              0.0296089\n",
      "exploration/env_infos/reward_forward Std               0.194065\n",
      "exploration/env_infos/reward_forward Max               0.713254\n",
      "exploration/env_infos/reward_forward Min              -1.51941\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.681916\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.681916\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.681916\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.32131\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.32131\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.32131\n",
      "exploration/env_infos/reward_ctrl Mean                -0.602663\n",
      "exploration/env_infos/reward_ctrl Std                  0.171173\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0178448\n",
      "exploration/env_infos/reward_ctrl Min                 -1.31706\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0471208\n",
      "exploration/env_infos/final/torso_velocity Std         0.0917137\n",
      "exploration/env_infos/final/torso_velocity Max         0.166088\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0571085\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.121238\n",
      "exploration/env_infos/initial/torso_velocity Std       0.289631\n",
      "exploration/env_infos/initial/torso_velocity Max       0.52473\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.141543\n",
      "exploration/env_infos/torso_velocity Mean              0.00991494\n",
      "exploration/env_infos/torso_velocity Std               0.150935\n",
      "exploration/env_infos/torso_velocity Max               0.963199\n",
      "exploration/env_infos/torso_velocity Min              -1.51941\n",
      "evaluation/num steps total                             1.85e+06\n",
      "evaluation/num paths total                          1850\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.775134\n",
      "evaluation/Rewards Std                                 0.215337\n",
      "evaluation/Rewards Max                                 2.52906\n",
      "evaluation/Rewards Min                                -2.11918\n",
      "evaluation/Returns Mean                              775.134\n",
      "evaluation/Returns Std                               194.91\n",
      "evaluation/Returns Max                               965.994\n",
      "evaluation/Returns Min                               396.479\n",
      "evaluation/Actions Mean                               -0.077078\n",
      "evaluation/Actions Std                                 0.228183\n",
      "evaluation/Actions Max                                 0.999977\n",
      "evaluation/Actions Min                                -0.996267\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           775.134\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.0113148\n",
      "evaluation/env_infos/final/reward_forward Std          0.0469756\n",
      "evaluation/env_infos/final/reward_forward Max          1.59597e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -0.237248\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0198866\n",
      "evaluation/env_infos/initial/reward_forward Std        0.146471\n",
      "evaluation/env_infos/initial/reward_forward Max        0.248493\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.322115\n",
      "evaluation/env_infos/reward_forward Mean              -0.0107349\n",
      "evaluation/env_infos/reward_forward Std                0.155976\n",
      "evaluation/env_infos/reward_forward Max                1.84256\n",
      "evaluation/env_infos/reward_forward Min               -2.04347\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.232051\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.193667\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0337147\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.608568\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.152202\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0709666\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0532372\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.279374\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.232034\n",
      "evaluation/env_infos/reward_ctrl Std                   0.201593\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0140546\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.11918\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.000123861\n",
      "evaluation/env_infos/final/torso_velocity Std          0.116106\n",
      "evaluation/env_infos/final/torso_velocity Max          0.76393\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.413773\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123043\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.238588\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.566966\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.322115\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00677734\n",
      "evaluation/env_infos/torso_velocity Std                0.152596\n",
      "evaluation/env_infos/torso_velocity Max                1.84256\n",
      "evaluation/env_infos/torso_velocity Min               -2.04347\n",
      "time/data storing (s)                                  0.015497\n",
      "time/evaluation sampling (s)                          47.1261\n",
      "time/exploration sampling (s)                          2.11224\n",
      "time/logging (s)                                       0.300343\n",
      "time/saving (s)                                        0.0330615\n",
      "time/training (s)                                      4.65333\n",
      "time/epoch (s)                                        54.2406\n",
      "time/total (s)                                      3989.48\n",
      "Epoch                                                 73\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:32:13.413532 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 74 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 76000\n",
      "trainer/QF1 Loss                                       0.470421\n",
      "trainer/QF2 Loss                                       0.686383\n",
      "trainer/Policy Loss                                  -17.316\n",
      "trainer/Q1 Predictions Mean                           24.7431\n",
      "trainer/Q1 Predictions Std                             2.09508\n",
      "trainer/Q1 Predictions Max                            27.1847\n",
      "trainer/Q1 Predictions Min                             2.77907\n",
      "trainer/Q2 Predictions Mean                           24.9955\n",
      "trainer/Q2 Predictions Std                             1.93629\n",
      "trainer/Q2 Predictions Max                            27.7202\n",
      "trainer/Q2 Predictions Min                             6.28062\n",
      "trainer/Q Targets Mean                                24.6173\n",
      "trainer/Q Targets Std                                  2.09173\n",
      "trainer/Q Targets Max                                 28.6951\n",
      "trainer/Q Targets Min                                  2.34865\n",
      "trainer/Log Pis Mean                                   7.7212\n",
      "trainer/Log Pis Std                                    3.32314\n",
      "trainer/Log Pis Max                                   26.1392\n",
      "trainer/Log Pis Min                                   -2.318\n",
      "trainer/Policy mu Mean                                -0.095245\n",
      "trainer/Policy mu Std                                  0.321456\n",
      "trainer/Policy mu Max                                  2.42115\n",
      "trainer/Policy mu Min                                 -2.2589\n",
      "trainer/Policy log std Mean                           -2.28466\n",
      "trainer/Policy log std Std                             0.305384\n",
      "trainer/Policy log std Max                            -0.635289\n",
      "trainer/Policy log std Min                            -4.90978\n",
      "trainer/Alpha                                          0.00790932\n",
      "trainer/Alpha Loss                                    -1.34948\n",
      "exploration/num steps total                        76000\n",
      "exploration/num paths total                          136\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.83765\n",
      "exploration/Rewards Std                                0.0628436\n",
      "exploration/Rewards Max                                1.09507\n",
      "exploration/Rewards Min                                0.339628\n",
      "exploration/Returns Mean                             837.65\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              837.65\n",
      "exploration/Returns Min                              837.65\n",
      "exploration/Actions Mean                              -0.0435073\n",
      "exploration/Actions Std                                0.197573\n",
      "exploration/Actions Max                                0.715442\n",
      "exploration/Actions Min                               -0.708637\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          837.65\n",
      "exploration/env_infos/final/reward_forward Mean        0.0139705\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0139705\n",
      "exploration/env_infos/final/reward_forward Min         0.0139705\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.197275\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.197275\n",
      "exploration/env_infos/initial/reward_forward Min      -0.197275\n",
      "exploration/env_infos/reward_forward Mean             -0.00416467\n",
      "exploration/env_infos/reward_forward Std               0.0575186\n",
      "exploration/env_infos/reward_forward Max               0.563221\n",
      "exploration/env_infos/reward_forward Min              -0.39654\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.135848\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.135848\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.135848\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0904379\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0904379\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0904379\n",
      "exploration/env_infos/reward_ctrl Mean                -0.163712\n",
      "exploration/env_infos/reward_ctrl Std                  0.061452\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0341055\n",
      "exploration/env_infos/reward_ctrl Min                 -0.660372\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0442615\n",
      "exploration/env_infos/final/torso_velocity Std         0.0564083\n",
      "exploration/env_infos/final/torso_velocity Max         0.0139705\n",
      "exploration/env_infos/final/torso_velocity Min        -0.120596\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0677521\n",
      "exploration/env_infos/initial/torso_velocity Std       0.221818\n",
      "exploration/env_infos/initial/torso_velocity Max       0.34561\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.197275\n",
      "exploration/env_infos/torso_velocity Mean             -0.00581286\n",
      "exploration/env_infos/torso_velocity Std               0.0851286\n",
      "exploration/env_infos/torso_velocity Max               0.596526\n",
      "exploration/env_infos/torso_velocity Min              -1.54611\n",
      "evaluation/num steps total                             1.875e+06\n",
      "evaluation/num paths total                          1875\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.872415\n",
      "evaluation/Rewards Std                                 0.0780483\n",
      "evaluation/Rewards Max                                 1.86153\n",
      "evaluation/Rewards Min                                -2.26443\n",
      "evaluation/Returns Mean                              872.415\n",
      "evaluation/Returns Std                                23.5578\n",
      "evaluation/Returns Max                               908.402\n",
      "evaluation/Returns Min                               800.656\n",
      "evaluation/Actions Mean                               -0.0415875\n",
      "evaluation/Actions Std                                 0.174396\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -0.99999\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           872.415\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.29871e-05\n",
      "evaluation/env_infos/final/reward_forward Std          6.31023e-05\n",
      "evaluation/env_infos/final/reward_forward Max          4.66183e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00032212\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0365975\n",
      "evaluation/env_infos/initial/reward_forward Std        0.14909\n",
      "evaluation/env_infos/initial/reward_forward Max        0.235404\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.322688\n",
      "evaluation/env_infos/reward_forward Mean               0.00150335\n",
      "evaluation/env_infos/reward_forward Std                0.0591034\n",
      "evaluation/env_infos/reward_forward Max                1.45049\n",
      "evaluation/env_infos/reward_forward Min               -1.37564\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.123756\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0243426\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0827413\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.200155\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.15121\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0866115\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0584134\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.454575\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.128574\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0795987\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0545285\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.26443\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.65105e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          5.94503e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000399325\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00032212\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.116448\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.257981\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.610584\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.374005\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000771178\n",
      "evaluation/env_infos/torso_velocity Std                0.0731972\n",
      "evaluation/env_infos/torso_velocity Max                2.63574\n",
      "evaluation/env_infos/torso_velocity Min               -1.87523\n",
      "time/data storing (s)                                  0.0169855\n",
      "time/evaluation sampling (s)                          45.5259\n",
      "time/exploration sampling (s)                          2.04533\n",
      "time/logging (s)                                       0.299608\n",
      "time/saving (s)                                        0.0309521\n",
      "time/training (s)                                      4.2333\n",
      "time/epoch (s)                                        52.1521\n",
      "time/total (s)                                      4042.42\n",
      "Epoch                                                 74\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:33:09.016120 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 75 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 77000\n",
      "trainer/QF1 Loss                                       0.290285\n",
      "trainer/QF2 Loss                                       0.305187\n",
      "trainer/Policy Loss                                  -16.6488\n",
      "trainer/Q1 Predictions Mean                           25.2786\n",
      "trainer/Q1 Predictions Std                             1.71065\n",
      "trainer/Q1 Predictions Max                            37.4555\n",
      "trainer/Q1 Predictions Min                            17.8061\n",
      "trainer/Q2 Predictions Mean                           25.2944\n",
      "trainer/Q2 Predictions Std                             1.65101\n",
      "trainer/Q2 Predictions Max                            34.8165\n",
      "trainer/Q2 Predictions Min                            17.4634\n",
      "trainer/Q Targets Mean                                25.0625\n",
      "trainer/Q Targets Std                                  1.57975\n",
      "trainer/Q Targets Max                                 36.273\n",
      "trainer/Q Targets Min                                 19.1827\n",
      "trainer/Log Pis Mean                                   8.82639\n",
      "trainer/Log Pis Std                                    3.46508\n",
      "trainer/Log Pis Max                                   35.543\n",
      "trainer/Log Pis Min                                    1.7213\n",
      "trainer/Policy mu Mean                                -0.0663752\n",
      "trainer/Policy mu Std                                  0.363258\n",
      "trainer/Policy mu Max                                  3.75528\n",
      "trainer/Policy mu Min                                 -3.43086\n",
      "trainer/Policy log std Mean                           -2.40977\n",
      "trainer/Policy log std Std                             0.297867\n",
      "trainer/Policy log std Max                            -1.45258\n",
      "trainer/Policy log std Min                            -4.56392\n",
      "trainer/Alpha                                          0.00780426\n",
      "trainer/Alpha Loss                                     4.01193\n",
      "exploration/num steps total                        77000\n",
      "exploration/num paths total                          137\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.432353\n",
      "exploration/Rewards Std                                0.0844869\n",
      "exploration/Rewards Max                                0.900406\n",
      "exploration/Rewards Min                                0.093158\n",
      "exploration/Returns Mean                             432.353\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              432.353\n",
      "exploration/Returns Min                              432.353\n",
      "exploration/Actions Mean                               0.0178697\n",
      "exploration/Actions Std                                0.376349\n",
      "exploration/Actions Max                                0.667885\n",
      "exploration/Actions Min                               -0.879894\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          432.353\n",
      "exploration/env_infos/final/reward_forward Mean       -0.000401294\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.000401294\n",
      "exploration/env_infos/final/reward_forward Min        -0.000401294\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0217003\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0217003\n",
      "exploration/env_infos/initial/reward_forward Min       0.0217003\n",
      "exploration/env_infos/reward_forward Mean              0.00799938\n",
      "exploration/env_infos/reward_forward Std               0.0645431\n",
      "exploration/env_infos/reward_forward Max               1.22259\n",
      "exploration/env_infos/reward_forward Min              -0.123887\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.57428\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.57428\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.57428\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0995937\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0995937\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0995937\n",
      "exploration/env_infos/reward_ctrl Mean                -0.567832\n",
      "exploration/env_infos/reward_ctrl Std                  0.0844988\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0995937\n",
      "exploration/env_infos/reward_ctrl Min                 -0.906842\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        7.38437e-05\n",
      "exploration/env_infos/final/torso_velocity Std         0.00143123\n",
      "exploration/env_infos/final/torso_velocity Max         0.00201532\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00139249\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.163824\n",
      "exploration/env_infos/initial/torso_velocity Std       0.294952\n",
      "exploration/env_infos/initial/torso_velocity Max       0.574511\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.104739\n",
      "exploration/env_infos/torso_velocity Mean              0.000976337\n",
      "exploration/env_infos/torso_velocity Std               0.0580078\n",
      "exploration/env_infos/torso_velocity Max               1.22259\n",
      "exploration/env_infos/torso_velocity Min              -0.757697\n",
      "evaluation/num steps total                             1.9e+06\n",
      "evaluation/num paths total                          1900\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.600622\n",
      "evaluation/Rewards Std                                 0.138177\n",
      "evaluation/Rewards Max                                 1.61978\n",
      "evaluation/Rewards Min                                -2.24258\n",
      "evaluation/Returns Mean                              600.622\n",
      "evaluation/Returns Std                               127.011\n",
      "evaluation/Returns Max                               824.099\n",
      "evaluation/Returns Min                               416.646\n",
      "evaluation/Actions Mean                               -0.0148471\n",
      "evaluation/Actions Std                                 0.316031\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.997624\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           600.622\n",
      "evaluation/env_infos/final/reward_forward Mean         1.47807e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.84222e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.89904e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.67751e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00145839\n",
      "evaluation/env_infos/initial/reward_forward Std        0.125438\n",
      "evaluation/env_infos/initial/reward_forward Max        0.28664\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.239452\n",
      "evaluation/env_infos/reward_forward Mean              -0.000530406\n",
      "evaluation/env_infos/reward_forward Std                0.0514325\n",
      "evaluation/env_infos/reward_forward Max                1.57155\n",
      "evaluation/env_infos/reward_forward Min               -1.29058\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.40131\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.130357\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.173838\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.58873\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.230165\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.108212\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0834584\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.449139\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.400385\n",
      "evaluation/env_infos/reward_ctrl Std                   0.137214\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0748215\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.24258\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         6.011e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.11441e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.89904e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.67751e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.129293\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.254142\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.645829\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.431477\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00144781\n",
      "evaluation/env_infos/torso_velocity Std                0.061886\n",
      "evaluation/env_infos/torso_velocity Max                1.74395\n",
      "evaluation/env_infos/torso_velocity Min               -1.86105\n",
      "time/data storing (s)                                  0.0229696\n",
      "time/evaluation sampling (s)                          47.6587\n",
      "time/exploration sampling (s)                          2.19853\n",
      "time/logging (s)                                       0.294021\n",
      "time/saving (s)                                        0.0258403\n",
      "time/training (s)                                      4.69945\n",
      "time/epoch (s)                                        54.8995\n",
      "time/total (s)                                      4098.02\n",
      "Epoch                                                 75\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:34:07.067771 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 76 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 78000\n",
      "trainer/QF1 Loss                                       0.560826\n",
      "trainer/QF2 Loss                                       0.663917\n",
      "trainer/Policy Loss                                  -15.8532\n",
      "trainer/Q1 Predictions Mean                           25.3515\n",
      "trainer/Q1 Predictions Std                             1.77551\n",
      "trainer/Q1 Predictions Max                            35.2589\n",
      "trainer/Q1 Predictions Min                            16.2869\n",
      "trainer/Q2 Predictions Mean                           25.3278\n",
      "trainer/Q2 Predictions Std                             1.98492\n",
      "trainer/Q2 Predictions Max                            45.7871\n",
      "trainer/Q2 Predictions Min                            18.3126\n",
      "trainer/Q Targets Mean                                25.2197\n",
      "trainer/Q Targets Std                                  1.87653\n",
      "trainer/Q Targets Max                                 38.3539\n",
      "trainer/Q Targets Min                                 17.4243\n",
      "trainer/Log Pis Mean                                   9.77453\n",
      "trainer/Log Pis Std                                    4.41872\n",
      "trainer/Log Pis Max                                   45.035\n",
      "trainer/Log Pis Min                                   -0.339992\n",
      "trainer/Policy mu Mean                                -0.064097\n",
      "trainer/Policy mu Std                                  0.440097\n",
      "trainer/Policy mu Max                                  5.64074\n",
      "trainer/Policy mu Min                                 -4.62447\n",
      "trainer/Policy log std Mean                           -2.50352\n",
      "trainer/Policy log std Std                             0.360088\n",
      "trainer/Policy log std Max                             0.0936896\n",
      "trainer/Policy log std Min                            -4.67428\n",
      "trainer/Alpha                                          0.00771036\n",
      "trainer/Alpha Loss                                     8.63922\n",
      "exploration/num steps total                        78000\n",
      "exploration/num paths total                          138\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.752459\n",
      "exploration/Rewards Std                                0.0956345\n",
      "exploration/Rewards Max                                1.1495\n",
      "exploration/Rewards Min                               -0.658983\n",
      "exploration/Returns Mean                             752.459\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              752.459\n",
      "exploration/Returns Min                              752.459\n",
      "exploration/Actions Mean                              -0.0775545\n",
      "exploration/Actions Std                                0.236761\n",
      "exploration/Actions Max                                0.742802\n",
      "exploration/Actions Min                               -0.989689\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          752.459\n",
      "exploration/env_infos/final/reward_forward Mean        0.00966403\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00966403\n",
      "exploration/env_infos/final/reward_forward Min         0.00966403\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0141315\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0141315\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0141315\n",
      "exploration/env_infos/reward_forward Mean              0.0205255\n",
      "exploration/env_infos/reward_forward Std               0.0903583\n",
      "exploration/env_infos/reward_forward Max               0.926462\n",
      "exploration/env_infos/reward_forward Min              -0.313795\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.193349\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.193349\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.193349\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.309941\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.309941\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.309941\n",
      "exploration/env_infos/reward_ctrl Mean                -0.248282\n",
      "exploration/env_infos/reward_ctrl Std                  0.0960803\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0642725\n",
      "exploration/env_infos/reward_ctrl Min                 -1.65898\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00311392\n",
      "exploration/env_infos/final/torso_velocity Std         0.00513238\n",
      "exploration/env_infos/final/torso_velocity Max         0.00966403\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00286928\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.109821\n",
      "exploration/env_infos/initial/torso_velocity Std       0.349854\n",
      "exploration/env_infos/initial/torso_velocity Max       0.586615\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.243022\n",
      "exploration/env_infos/torso_velocity Mean              0.00472336\n",
      "exploration/env_infos/torso_velocity Std               0.0818141\n",
      "exploration/env_infos/torso_velocity Max               0.926462\n",
      "exploration/env_infos/torso_velocity Min              -1.41896\n",
      "evaluation/num steps total                             1.92405e+06\n",
      "evaluation/num paths total                          1925\n",
      "evaluation/path length Mean                          962.08\n",
      "evaluation/path length Std                           185.769\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            52\n",
      "evaluation/Rewards Mean                                0.776886\n",
      "evaluation/Rewards Std                                 0.13748\n",
      "evaluation/Rewards Max                                 2.27774\n",
      "evaluation/Rewards Min                                -2.23604\n",
      "evaluation/Returns Mean                              747.427\n",
      "evaluation/Returns Std                               170.947\n",
      "evaluation/Returns Max                               923.751\n",
      "evaluation/Returns Min                                25.3466\n",
      "evaluation/Actions Mean                               -0.0620629\n",
      "evaluation/Actions Std                                 0.229556\n",
      "evaluation/Actions Max                                 0.999972\n",
      "evaluation/Actions Min                                -0.999936\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           747.427\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.035279\n",
      "evaluation/env_infos/final/reward_forward Std          0.172831\n",
      "evaluation/env_infos/final/reward_forward Max          7.23562e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.881974\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0100162\n",
      "evaluation/env_infos/initial/reward_forward Std        0.089979\n",
      "evaluation/env_infos/initial/reward_forward Max        0.165379\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.16004\n",
      "evaluation/env_infos/reward_forward Mean               0.000923048\n",
      "evaluation/env_infos/reward_forward Std                0.107405\n",
      "evaluation/env_infos/reward_forward Max                1.96257\n",
      "evaluation/env_infos/reward_forward Min               -1.47461\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.231974\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.104602\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0492348\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.47291\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.141008\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0887996\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0647292\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.436177\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.22619\n",
      "evaluation/env_infos/reward_ctrl Std                   0.130031\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0414833\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.23604\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0316106\n",
      "evaluation/env_infos/final/torso_velocity Std          0.306534\n",
      "evaluation/env_infos/final/torso_velocity Max          2.35234\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.881974\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.13575\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.237888\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.610535\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.461552\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00033235\n",
      "evaluation/env_infos/torso_velocity Std                0.114683\n",
      "evaluation/env_infos/torso_velocity Max                3.57231\n",
      "evaluation/env_infos/torso_velocity Min               -2.04372\n",
      "time/data storing (s)                                  0.0198629\n",
      "time/evaluation sampling (s)                          49.824\n",
      "time/exploration sampling (s)                          2.19171\n",
      "time/logging (s)                                       0.279578\n",
      "time/saving (s)                                        0.0303098\n",
      "time/training (s)                                      4.87911\n",
      "time/epoch (s)                                        57.2246\n",
      "time/total (s)                                      4156.05\n",
      "Epoch                                                 76\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:35:00.789256 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 77 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 79000\n",
      "trainer/QF1 Loss                                       3.37414\n",
      "trainer/QF2 Loss                                       2.47944\n",
      "trainer/Policy Loss                                  -18.6122\n",
      "trainer/Q1 Predictions Mean                           25.4425\n",
      "trainer/Q1 Predictions Std                             1.80964\n",
      "trainer/Q1 Predictions Max                            33.7803\n",
      "trainer/Q1 Predictions Min                            16.7657\n",
      "trainer/Q2 Predictions Mean                           25.7926\n",
      "trainer/Q2 Predictions Std                             2.00154\n",
      "trainer/Q2 Predictions Max                            35.5339\n",
      "trainer/Q2 Predictions Min                            14.4497\n",
      "trainer/Q Targets Mean                                25.4207\n",
      "trainer/Q Targets Std                                  2.43585\n",
      "trainer/Q Targets Max                                 37.9058\n",
      "trainer/Q Targets Min                                  0.00243938\n",
      "trainer/Log Pis Mean                                   7.07783\n",
      "trainer/Log Pis Std                                    4.79696\n",
      "trainer/Log Pis Max                                   60.1262\n",
      "trainer/Log Pis Min                                   -2.8751\n",
      "trainer/Policy mu Mean                                -0.0827761\n",
      "trainer/Policy mu Std                                  0.484083\n",
      "trainer/Policy mu Max                                  7.2308\n",
      "trainer/Policy mu Min                                 -7.1225\n",
      "trainer/Policy log std Mean                           -2.16372\n",
      "trainer/Policy log std Std                             0.334835\n",
      "trainer/Policy log std Max                             0.283133\n",
      "trainer/Policy log std Min                            -5.07406\n",
      "trainer/Alpha                                          0.00848946\n",
      "trainer/Alpha Loss                                    -4.39537\n",
      "exploration/num steps total                        79000\n",
      "exploration/num paths total                          139\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.825351\n",
      "exploration/Rewards Std                                0.25826\n",
      "exploration/Rewards Max                                1.90073\n",
      "exploration/Rewards Min                               -2.36249\n",
      "exploration/Returns Mean                             825.351\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              825.351\n",
      "exploration/Returns Min                              825.351\n",
      "exploration/Actions Mean                              -0.0557891\n",
      "exploration/Actions Std                                0.220545\n",
      "exploration/Actions Max                                0.999994\n",
      "exploration/Actions Min                               -0.999685\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          825.351\n",
      "exploration/env_infos/final/reward_forward Mean        0.100135\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.100135\n",
      "exploration/env_infos/final/reward_forward Min         0.100135\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.209494\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.209494\n",
      "exploration/env_infos/initial/reward_forward Min      -0.209494\n",
      "exploration/env_infos/reward_forward Mean             -0.0516508\n",
      "exploration/env_infos/reward_forward Std               0.264773\n",
      "exploration/env_infos/reward_forward Max               0.78793\n",
      "exploration/env_infos/reward_forward Min              -1.78287\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.270116\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.270116\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.270116\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.41706\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.41706\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.41706\n",
      "exploration/env_infos/reward_ctrl Mean                -0.20701\n",
      "exploration/env_infos/reward_ctrl Std                  0.224873\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0269494\n",
      "exploration/env_infos/reward_ctrl Min                 -3.36249\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00770751\n",
      "exploration/env_infos/final/torso_velocity Std         0.0822531\n",
      "exploration/env_infos/final/torso_velocity Max         0.100135\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0993886\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0302298\n",
      "exploration/env_infos/initial/torso_velocity Std       0.362772\n",
      "exploration/env_infos/initial/torso_velocity Max       0.542908\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.242724\n",
      "exploration/env_infos/torso_velocity Mean             -0.0395112\n",
      "exploration/env_infos/torso_velocity Std               0.239629\n",
      "exploration/env_infos/torso_velocity Max               1.98005\n",
      "exploration/env_infos/torso_velocity Min              -1.79379\n",
      "evaluation/num steps total                             1.94905e+06\n",
      "evaluation/num paths total                          1950\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.869185\n",
      "evaluation/Rewards Std                                 0.168156\n",
      "evaluation/Rewards Max                                 2.84803\n",
      "evaluation/Rewards Min                                -2.68715\n",
      "evaluation/Returns Mean                              869.185\n",
      "evaluation/Returns Std                                54.8753\n",
      "evaluation/Returns Max                               949.904\n",
      "evaluation/Returns Min                               770.832\n",
      "evaluation/Actions Mean                               -0.0516882\n",
      "evaluation/Actions Std                                 0.175683\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999996\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           869.185\n",
      "evaluation/env_infos/final/reward_forward Mean         7.66534e-08\n",
      "evaluation/env_infos/final/reward_forward Std          5.05618e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.96593e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.42079e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0565706\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132258\n",
      "evaluation/env_infos/initial/reward_forward Max        0.275716\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.328273\n",
      "evaluation/env_infos/reward_forward Mean               0.00232002\n",
      "evaluation/env_infos/reward_forward Std                0.127094\n",
      "evaluation/env_infos/reward_forward Max                2.50208\n",
      "evaluation/env_infos/reward_forward Min               -1.80453\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.121103\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.05509\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0479809\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.2294\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.154713\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.167959\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0289452\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.871967\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.134144\n",
      "evaluation/env_infos/reward_ctrl Std                   0.162333\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0220796\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.68715\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.2511e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.29326e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.96593e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.42079e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0965458\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.266271\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.616215\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.375361\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00433627\n",
      "evaluation/env_infos/torso_velocity Std                0.119827\n",
      "evaluation/env_infos/torso_velocity Max                2.50208\n",
      "evaluation/env_infos/torso_velocity Min               -2.18182\n",
      "time/data storing (s)                                  0.0147655\n",
      "time/evaluation sampling (s)                          46.2371\n",
      "time/exploration sampling (s)                          1.96888\n",
      "time/logging (s)                                       0.284299\n",
      "time/saving (s)                                        0.0294543\n",
      "time/training (s)                                      4.37302\n",
      "time/epoch (s)                                        52.9075\n",
      "time/total (s)                                      4209.78\n",
      "Epoch                                                 77\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:35:56.657923 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 78 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 80000\n",
      "trainer/QF1 Loss                                       0.358457\n",
      "trainer/QF2 Loss                                       0.461516\n",
      "trainer/Policy Loss                                  -16.3491\n",
      "trainer/Q1 Predictions Mean                           25.4854\n",
      "trainer/Q1 Predictions Std                             1.74049\n",
      "trainer/Q1 Predictions Max                            29.5106\n",
      "trainer/Q1 Predictions Min                            16.0316\n",
      "trainer/Q2 Predictions Mean                           25.5348\n",
      "trainer/Q2 Predictions Std                             1.75888\n",
      "trainer/Q2 Predictions Max                            29.2366\n",
      "trainer/Q2 Predictions Min                            15.8117\n",
      "trainer/Q Targets Mean                                25.5356\n",
      "trainer/Q Targets Std                                  1.72192\n",
      "trainer/Q Targets Max                                 29.004\n",
      "trainer/Q Targets Min                                 17.154\n",
      "trainer/Log Pis Mean                                   9.44016\n",
      "trainer/Log Pis Std                                    3.21102\n",
      "trainer/Log Pis Max                                   36.133\n",
      "trainer/Log Pis Min                                    1.46921\n",
      "trainer/Policy mu Mean                                -0.157964\n",
      "trainer/Policy mu Std                                  0.455385\n",
      "trainer/Policy mu Max                                  3.84562\n",
      "trainer/Policy mu Min                                 -4.55794\n",
      "trainer/Policy log std Mean                           -2.37009\n",
      "trainer/Policy log std Std                             0.296713\n",
      "trainer/Policy log std Max                            -0.708775\n",
      "trainer/Policy log std Min                            -4.20794\n",
      "trainer/Alpha                                          0.00808595\n",
      "trainer/Alpha Loss                                     6.94221\n",
      "exploration/num steps total                        80000\n",
      "exploration/num paths total                          140\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.568907\n",
      "exploration/Rewards Std                                0.11345\n",
      "exploration/Rewards Max                                0.898079\n",
      "exploration/Rewards Min                               -0.750721\n",
      "exploration/Returns Mean                             568.907\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              568.907\n",
      "exploration/Returns Min                              568.907\n",
      "exploration/Actions Mean                              -0.128803\n",
      "exploration/Actions Std                                0.302063\n",
      "exploration/Actions Max                                0.681211\n",
      "exploration/Actions Min                               -0.974243\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          568.907\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0106701\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0106701\n",
      "exploration/env_infos/final/reward_forward Min        -0.0106701\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.198514\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.198514\n",
      "exploration/env_infos/initial/reward_forward Min      -0.198514\n",
      "exploration/env_infos/reward_forward Mean             -0.00404823\n",
      "exploration/env_infos/reward_forward Std               0.0459181\n",
      "exploration/env_infos/reward_forward Max               0.181857\n",
      "exploration/env_infos/reward_forward Min              -0.90864\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.389185\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.389185\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.389185\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.368126\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.368126\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.368126\n",
      "exploration/env_infos/reward_ctrl Mean                -0.431328\n",
      "exploration/env_infos/reward_ctrl Std                  0.1133\n",
      "exploration/env_infos/reward_ctrl Max                 -0.101921\n",
      "exploration/env_infos/reward_ctrl Min                 -1.75072\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000435828\n",
      "exploration/env_infos/final/torso_velocity Std         0.00767945\n",
      "exploration/env_infos/final/torso_velocity Max         0.00782866\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0106701\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0155578\n",
      "exploration/env_infos/initial/torso_velocity Std       0.298718\n",
      "exploration/env_infos/initial/torso_velocity Max       0.405684\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.253844\n",
      "exploration/env_infos/torso_velocity Mean             -0.00478458\n",
      "exploration/env_infos/torso_velocity Std               0.0685997\n",
      "exploration/env_infos/torso_velocity Max               0.965852\n",
      "exploration/env_infos/torso_velocity Min              -1.67801\n",
      "evaluation/num steps total                             1.97308e+06\n",
      "evaluation/num paths total                          1975\n",
      "evaluation/path length Mean                          961\n",
      "evaluation/path length Std                           191.06\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            25\n",
      "evaluation/Rewards Mean                                0.368414\n",
      "evaluation/Rewards Std                                 0.223456\n",
      "evaluation/Rewards Max                                 1.93781\n",
      "evaluation/Rewards Min                                -1.70978\n",
      "evaluation/Returns Mean                              354.046\n",
      "evaluation/Returns Std                               215.661\n",
      "evaluation/Returns Max                               799.492\n",
      "evaluation/Returns Min                                 4.09192\n",
      "evaluation/Actions Mean                               -0.165846\n",
      "evaluation/Actions Std                                 0.361812\n",
      "evaluation/Actions Max                                 0.993161\n",
      "evaluation/Actions Min                                -0.989726\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           354.046\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.0235825\n",
      "evaluation/env_infos/final/reward_forward Std          0.114021\n",
      "evaluation/env_infos/final/reward_forward Max          0.000853526\n",
      "evaluation/env_infos/final/reward_forward Min         -0.582141\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0242888\n",
      "evaluation/env_infos/initial/reward_forward Std        0.146088\n",
      "evaluation/env_infos/initial/reward_forward Max        0.261731\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.292271\n",
      "evaluation/env_infos/reward_forward Mean               0.00200625\n",
      "evaluation/env_infos/reward_forward Std                0.0660235\n",
      "evaluation/env_infos/reward_forward Max                1.18229\n",
      "evaluation/env_infos/reward_forward Min               -0.951668\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.660289\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.227013\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.195574\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.09713\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.22453\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.111371\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0806452\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.56531\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.633651\n",
      "evaluation/env_infos/reward_ctrl Std                   0.221108\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0806452\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.70978\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0226101\n",
      "evaluation/env_infos/final/torso_velocity Std          0.22671\n",
      "evaluation/env_infos/final/torso_velocity Max          1.83165\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.582141\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.132588\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.250723\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.665067\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.306074\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000868973\n",
      "evaluation/env_infos/torso_velocity Std                0.0718888\n",
      "evaluation/env_infos/torso_velocity Max                3.51924\n",
      "evaluation/env_infos/torso_velocity Min               -1.8307\n",
      "time/data storing (s)                                  0.0240234\n",
      "time/evaluation sampling (s)                          48.3097\n",
      "time/exploration sampling (s)                          2.10421\n",
      "time/logging (s)                                       0.263775\n",
      "time/saving (s)                                        0.0319572\n",
      "time/training (s)                                      4.3753\n",
      "time/epoch (s)                                        55.109\n",
      "time/total (s)                                      4265.62\n",
      "Epoch                                                 78\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:36:49.449696 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 79 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 81000\n",
      "trainer/QF1 Loss                                       1.03246\n",
      "trainer/QF2 Loss                                       1.08582\n",
      "trainer/Policy Loss                                  -18.7551\n",
      "trainer/Q1 Predictions Mean                           26.22\n",
      "trainer/Q1 Predictions Std                             2.05132\n",
      "trainer/Q1 Predictions Max                            36.1194\n",
      "trainer/Q1 Predictions Min                            14.3388\n",
      "trainer/Q2 Predictions Mean                           26.3381\n",
      "trainer/Q2 Predictions Std                             2.08074\n",
      "trainer/Q2 Predictions Max                            38.0587\n",
      "trainer/Q2 Predictions Min                            12.6111\n",
      "trainer/Q Targets Mean                                26.0289\n",
      "trainer/Q Targets Std                                  2.09879\n",
      "trainer/Q Targets Max                                 39.6354\n",
      "trainer/Q Targets Min                                 16.0985\n",
      "trainer/Log Pis Mean                                   7.65922\n",
      "trainer/Log Pis Std                                    2.66893\n",
      "trainer/Log Pis Max                                   22.4633\n",
      "trainer/Log Pis Min                                   -0.818652\n",
      "trainer/Policy mu Mean                                -0.0609439\n",
      "trainer/Policy mu Std                                  0.34243\n",
      "trainer/Policy mu Max                                  2.22671\n",
      "trainer/Policy mu Min                                 -3.54682\n",
      "trainer/Policy log std Mean                           -2.23835\n",
      "trainer/Policy log std Std                             0.24468\n",
      "trainer/Policy log std Max                            -0.697003\n",
      "trainer/Policy log std Min                            -4.75785\n",
      "trainer/Alpha                                          0.00840543\n",
      "trainer/Alpha Loss                                    -1.62836\n",
      "exploration/num steps total                        81000\n",
      "exploration/num paths total                          141\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.639901\n",
      "exploration/Rewards Std                                0.186727\n",
      "exploration/Rewards Max                                1.69177\n",
      "exploration/Rewards Min                               -1.10937\n",
      "exploration/Returns Mean                             639.901\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              639.901\n",
      "exploration/Returns Min                              639.901\n",
      "exploration/Actions Mean                              -0.094864\n",
      "exploration/Actions Std                                0.292408\n",
      "exploration/Actions Max                                0.99989\n",
      "exploration/Actions Min                               -0.999877\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          639.901\n",
      "exploration/env_infos/final/reward_forward Mean        0.000334323\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.000334323\n",
      "exploration/env_infos/final/reward_forward Min         0.000334323\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.154781\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.154781\n",
      "exploration/env_infos/initial/reward_forward Min      -0.154781\n",
      "exploration/env_infos/reward_forward Mean              0.0289204\n",
      "exploration/env_infos/reward_forward Std               0.183064\n",
      "exploration/env_infos/reward_forward Max               1.60143\n",
      "exploration/env_infos/reward_forward Min              -0.582649\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.271354\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.271354\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.271354\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.625606\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.625606\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.625606\n",
      "exploration/env_infos/reward_ctrl Mean                -0.378007\n",
      "exploration/env_infos/reward_ctrl Std                  0.248397\n",
      "exploration/env_infos/reward_ctrl Max                 -0.110883\n",
      "exploration/env_infos/reward_ctrl Min                 -2.91426\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.000220796\n",
      "exploration/env_infos/final/torso_velocity Std         8.4228e-05\n",
      "exploration/env_infos/final/torso_velocity Max         0.000334323\n",
      "exploration/env_infos/final/torso_velocity Min         0.000132803\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.062251\n",
      "exploration/env_infos/initial/torso_velocity Std       0.250924\n",
      "exploration/env_infos/initial/torso_velocity Max       0.413907\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.154781\n",
      "exploration/env_infos/torso_velocity Mean              0.00713448\n",
      "exploration/env_infos/torso_velocity Std               0.137684\n",
      "exploration/env_infos/torso_velocity Max               1.60143\n",
      "exploration/env_infos/torso_velocity Min              -1.26251\n",
      "evaluation/num steps total                             1.9971e+06\n",
      "evaluation/num paths total                          2000\n",
      "evaluation/path length Mean                          960.76\n",
      "evaluation/path length Std                           192.236\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            19\n",
      "evaluation/Rewards Mean                                0.810367\n",
      "evaluation/Rewards Std                                 0.105789\n",
      "evaluation/Rewards Max                                 2.05235\n",
      "evaluation/Rewards Min                                -2.8333\n",
      "evaluation/Returns Mean                              778.568\n",
      "evaluation/Returns Std                               168.899\n",
      "evaluation/Returns Max                               917.125\n",
      "evaluation/Returns Min                               -12.7877\n",
      "evaluation/Actions Mean                               -0.046571\n",
      "evaluation/Actions Std                                 0.214744\n",
      "evaluation/Actions Max                                 0.999999\n",
      "evaluation/Actions Min                                -0.999989\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           778.568\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.00273881\n",
      "evaluation/env_infos/final/reward_forward Std          0.0124372\n",
      "evaluation/env_infos/final/reward_forward Max          8.36399e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0634833\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0542039\n",
      "evaluation/env_infos/initial/reward_forward Std        0.139479\n",
      "evaluation/env_infos/initial/reward_forward Max        0.185915\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.35111\n",
      "evaluation/env_infos/reward_forward Mean              -7.47604e-05\n",
      "evaluation/env_infos/reward_forward Std                0.105038\n",
      "evaluation/env_infos/reward_forward Max                2.33238\n",
      "evaluation/env_infos/reward_forward Min               -2.21358\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.278534\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.460851\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0722086\n",
      "evaluation/env_infos/final/reward_ctrl Min            -2.52115\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.437715\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.250244\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0928716\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.03735\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.193136\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0988011\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0253717\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.8333\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0176329\n",
      "evaluation/env_infos/final/torso_velocity Std          0.339061\n",
      "evaluation/env_infos/final/torso_velocity Max          2.65571\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.26042\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.093582\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.285063\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.625839\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.389135\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000762271\n",
      "evaluation/env_infos/torso_velocity Std                0.0996917\n",
      "evaluation/env_infos/torso_velocity Max                3.98674\n",
      "evaluation/env_infos/torso_velocity Min               -2.21358\n",
      "time/data storing (s)                                  0.0144636\n",
      "time/evaluation sampling (s)                          45.1818\n",
      "time/exploration sampling (s)                          2.19736\n",
      "time/logging (s)                                       0.283313\n",
      "time/saving (s)                                        0.0314965\n",
      "time/training (s)                                      4.39848\n",
      "time/epoch (s)                                        52.1069\n",
      "time/total (s)                                      4318.43\n",
      "Epoch                                                 79\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:37:42.713340 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 80 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 82000\n",
      "trainer/QF1 Loss                                       0.599627\n",
      "trainer/QF2 Loss                                       1.38351\n",
      "trainer/Policy Loss                                  -17.0276\n",
      "trainer/Q1 Predictions Mean                           25.7761\n",
      "trainer/Q1 Predictions Std                             2.77723\n",
      "trainer/Q1 Predictions Max                            38.0194\n",
      "trainer/Q1 Predictions Min                             4.04484\n",
      "trainer/Q2 Predictions Mean                           25.84\n",
      "trainer/Q2 Predictions Std                             3.596\n",
      "trainer/Q2 Predictions Max                            43.5585\n",
      "trainer/Q2 Predictions Min                            -6.41816\n",
      "trainer/Q Targets Mean                                25.788\n",
      "trainer/Q Targets Std                                  2.86665\n",
      "trainer/Q Targets Max                                 38.1135\n",
      "trainer/Q Targets Min                                 -0.688759\n",
      "trainer/Log Pis Mean                                   9.20411\n",
      "trainer/Log Pis Std                                    4.57806\n",
      "trainer/Log Pis Max                                   68.2042\n",
      "trainer/Log Pis Min                                    1.60964\n",
      "trainer/Policy mu Mean                                 0.190864\n",
      "trainer/Policy mu Std                                  0.525834\n",
      "trainer/Policy mu Max                                  7.79308\n",
      "trainer/Policy mu Min                                 -7.72905\n",
      "trainer/Policy log std Mean                           -2.32225\n",
      "trainer/Policy log std Std                             0.334222\n",
      "trainer/Policy log std Max                             0.743829\n",
      "trainer/Policy log std Min                            -6.07006\n",
      "trainer/Alpha                                          0.00901446\n",
      "trainer/Alpha Loss                                     5.6765\n",
      "exploration/num steps total                        82000\n",
      "exploration/num paths total                          142\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.64509\n",
      "exploration/Rewards Std                                0.0815344\n",
      "exploration/Rewards Max                                0.829047\n",
      "exploration/Rewards Min                                0.0437158\n",
      "exploration/Returns Mean                             645.09\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              645.09\n",
      "exploration/Returns Min                              645.09\n",
      "exploration/Actions Mean                               0.101711\n",
      "exploration/Actions Std                                0.280188\n",
      "exploration/Actions Max                                0.806486\n",
      "exploration/Actions Min                               -0.819606\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          645.09\n",
      "exploration/env_infos/final/reward_forward Mean       -0.064339\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.064339\n",
      "exploration/env_infos/final/reward_forward Min        -0.064339\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0581625\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0581625\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0581625\n",
      "exploration/env_infos/reward_forward Mean              0.00553048\n",
      "exploration/env_infos/reward_forward Std               0.0729207\n",
      "exploration/env_infos/reward_forward Max               0.415552\n",
      "exploration/env_infos/reward_forward Min              -0.484777\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.321528\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.321528\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.321528\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.26285\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.26285\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.26285\n",
      "exploration/env_infos/reward_ctrl Mean                -0.355402\n",
      "exploration/env_infos/reward_ctrl Std                  0.0814874\n",
      "exploration/env_infos/reward_ctrl Max                 -0.170953\n",
      "exploration/env_infos/reward_ctrl Min                 -0.956284\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000989999\n",
      "exploration/env_infos/final/torso_velocity Std         0.0454333\n",
      "exploration/env_infos/final/torso_velocity Max         0.0399824\n",
      "exploration/env_infos/final/torso_velocity Min        -0.064339\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.00387439\n",
      "exploration/env_infos/initial/torso_velocity Std       0.241286\n",
      "exploration/env_infos/initial/torso_velocity Max       0.325482\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.255697\n",
      "exploration/env_infos/torso_velocity Mean              0.00138115\n",
      "exploration/env_infos/torso_velocity Std               0.0619579\n",
      "exploration/env_infos/torso_velocity Max               0.621493\n",
      "exploration/env_infos/torso_velocity Min              -1.2367\n",
      "evaluation/num steps total                             2.0221e+06\n",
      "evaluation/num paths total                          2025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.434186\n",
      "evaluation/Rewards Std                                 0.190589\n",
      "evaluation/Rewards Max                                 1.81622\n",
      "evaluation/Rewards Min                                -2.34825\n",
      "evaluation/Returns Mean                              434.186\n",
      "evaluation/Returns Std                               184.412\n",
      "evaluation/Returns Max                               718.213\n",
      "evaluation/Returns Min                                72.7308\n",
      "evaluation/Actions Mean                                0.180327\n",
      "evaluation/Actions Std                                 0.3305\n",
      "evaluation/Actions Max                                 0.999441\n",
      "evaluation/Actions Min                                -0.999261\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           434.186\n",
      "evaluation/env_infos/final/reward_forward Mean         1.08554e-07\n",
      "evaluation/env_infos/final/reward_forward Std          5.49846e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.04499e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -8.88404e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.03559\n",
      "evaluation/env_infos/initial/reward_forward Std        0.159761\n",
      "evaluation/env_infos/initial/reward_forward Max        0.333478\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.282858\n",
      "evaluation/env_infos/reward_forward Mean               0.00315604\n",
      "evaluation/env_infos/reward_forward Std                0.062964\n",
      "evaluation/env_infos/reward_forward Max                1.65029\n",
      "evaluation/env_infos/reward_forward Min               -1.14246\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.568601\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.185822\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.283167\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.928503\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.517151\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.216774\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.200929\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.05146\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.566993\n",
      "evaluation/env_infos/reward_ctrl Std                   0.188951\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.044401\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.34825\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.18348e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.29472e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.04499e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.24414e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.106466\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.303918\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.666067\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.483324\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0010318\n",
      "evaluation/env_infos/torso_velocity Std                0.0610367\n",
      "evaluation/env_infos/torso_velocity Max                1.65029\n",
      "evaluation/env_infos/torso_velocity Min               -1.74653\n",
      "time/data storing (s)                                  0.0155256\n",
      "time/evaluation sampling (s)                          45.4672\n",
      "time/exploration sampling (s)                          2.07861\n",
      "time/logging (s)                                       0.280998\n",
      "time/saving (s)                                        0.0248694\n",
      "time/training (s)                                      4.57815\n",
      "time/epoch (s)                                        52.4454\n",
      "time/total (s)                                      4371.69\n",
      "Epoch                                                 80\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:38:35.014791 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 81 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 83000\n",
      "trainer/QF1 Loss                                       0.75758\n",
      "trainer/QF2 Loss                                       0.559076\n",
      "trainer/Policy Loss                                  -19.3873\n",
      "trainer/Q1 Predictions Mean                           26.2129\n",
      "trainer/Q1 Predictions Std                             2.46646\n",
      "trainer/Q1 Predictions Max                            28.6343\n",
      "trainer/Q1 Predictions Min                             3.94378\n",
      "trainer/Q2 Predictions Mean                           26.091\n",
      "trainer/Q2 Predictions Std                             2.62927\n",
      "trainer/Q2 Predictions Max                            28.5464\n",
      "trainer/Q2 Predictions Min                             2.04453\n",
      "trainer/Q Targets Mean                                25.9118\n",
      "trainer/Q Targets Std                                  2.70428\n",
      "trainer/Q Targets Max                                 28.4036\n",
      "trainer/Q Targets Min                                  0.180451\n",
      "trainer/Log Pis Mean                                   7.00792\n",
      "trainer/Log Pis Std                                    2.4173\n",
      "trainer/Log Pis Max                                   19.3681\n",
      "trainer/Log Pis Min                                    0.371424\n",
      "trainer/Policy mu Mean                                -0.0620914\n",
      "trainer/Policy mu Std                                  0.340134\n",
      "trainer/Policy mu Max                                  2.0677\n",
      "trainer/Policy mu Min                                 -2.92842\n",
      "trainer/Policy log std Mean                           -2.1762\n",
      "trainer/Policy log std Std                             0.290969\n",
      "trainer/Policy log std Max                            -0.721659\n",
      "trainer/Policy log std Min                            -3.78783\n",
      "trainer/Alpha                                          0.00919246\n",
      "trainer/Alpha Loss                                    -4.64848\n",
      "exploration/num steps total                        83000\n",
      "exploration/num paths total                          143\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.749664\n",
      "exploration/Rewards Std                                0.0914643\n",
      "exploration/Rewards Max                                1.32334\n",
      "exploration/Rewards Min                               -0.248662\n",
      "exploration/Returns Mean                             749.664\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              749.664\n",
      "exploration/Returns Min                              749.664\n",
      "exploration/Actions Mean                              -0.0467258\n",
      "exploration/Actions Std                                0.246825\n",
      "exploration/Actions Max                                0.802166\n",
      "exploration/Actions Min                               -0.971385\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          749.664\n",
      "exploration/env_infos/final/reward_forward Mean        0.0374628\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0374628\n",
      "exploration/env_infos/final/reward_forward Min         0.0374628\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.222112\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.222112\n",
      "exploration/env_infos/initial/reward_forward Min      -0.222112\n",
      "exploration/env_infos/reward_forward Mean              0.0208916\n",
      "exploration/env_infos/reward_forward Std               0.12011\n",
      "exploration/env_infos/reward_forward Max               1.05557\n",
      "exploration/env_infos/reward_forward Min              -0.335708\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.342878\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.342878\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.342878\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.785465\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.785465\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.785465\n",
      "exploration/env_infos/reward_ctrl Mean                -0.252424\n",
      "exploration/env_infos/reward_ctrl Std                  0.0876188\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0378938\n",
      "exploration/env_infos/reward_ctrl Min                 -1.24866\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00289149\n",
      "exploration/env_infos/final/torso_velocity Std         0.0285371\n",
      "exploration/env_infos/final/torso_velocity Max         0.0374628\n",
      "exploration/env_infos/final/torso_velocity Min        -0.023513\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0179003\n",
      "exploration/env_infos/initial/torso_velocity Std       0.367949\n",
      "exploration/env_infos/initial/torso_velocity Max       0.498697\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.330286\n",
      "exploration/env_infos/torso_velocity Mean             -0.00168656\n",
      "exploration/env_infos/torso_velocity Std               0.0991573\n",
      "exploration/env_infos/torso_velocity Max               1.05557\n",
      "exploration/env_infos/torso_velocity Min              -1.13505\n",
      "evaluation/num steps total                             2.0471e+06\n",
      "evaluation/num paths total                          2050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.794452\n",
      "evaluation/Rewards Std                                 0.0629397\n",
      "evaluation/Rewards Max                                 2.28697\n",
      "evaluation/Rewards Min                                -0.967445\n",
      "evaluation/Returns Mean                              794.452\n",
      "evaluation/Returns Std                                41.7527\n",
      "evaluation/Returns Max                               849.718\n",
      "evaluation/Returns Min                               687.787\n",
      "evaluation/Actions Mean                               -0.0547218\n",
      "evaluation/Actions Std                                 0.2205\n",
      "evaluation/Actions Max                                 0.973957\n",
      "evaluation/Actions Min                                -0.998196\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           794.452\n",
      "evaluation/env_infos/final/reward_forward Mean        -2.22569e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.29342e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.73072e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.55761e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0916874\n",
      "evaluation/env_infos/initial/reward_forward Std        0.157359\n",
      "evaluation/env_infos/initial/reward_forward Max        0.182403\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.38803\n",
      "evaluation/env_infos/reward_forward Mean               0.00376993\n",
      "evaluation/env_infos/reward_forward Std                0.0533229\n",
      "evaluation/env_infos/reward_forward Max                1.62801\n",
      "evaluation/env_infos/reward_forward Min               -1.25635\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.204418\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0406222\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.150811\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.308185\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.577543\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.397897\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0607485\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.24124\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.206458\n",
      "evaluation/env_infos/reward_ctrl Std                   0.059475\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0311619\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.96744\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.76081e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.96863e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.38979e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.55761e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0566418\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.310661\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.627668\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.547049\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000827825\n",
      "evaluation/env_infos/torso_velocity Std                0.0566138\n",
      "evaluation/env_infos/torso_velocity Max                1.62801\n",
      "evaluation/env_infos/torso_velocity Min               -1.75808\n",
      "time/data storing (s)                                  0.0150607\n",
      "time/evaluation sampling (s)                          44.6894\n",
      "time/exploration sampling (s)                          2.06716\n",
      "time/logging (s)                                       0.286852\n",
      "time/saving (s)                                        0.0273348\n",
      "time/training (s)                                      4.50112\n",
      "time/epoch (s)                                        51.5869\n",
      "time/total (s)                                      4424\n",
      "Epoch                                                 81\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:39:27.462972 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 82 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 84000\n",
      "trainer/QF1 Loss                                       0.917188\n",
      "trainer/QF2 Loss                                       1.39181\n",
      "trainer/Policy Loss                                  -17.6787\n",
      "trainer/Q1 Predictions Mean                           26.7885\n",
      "trainer/Q1 Predictions Std                             2.24558\n",
      "trainer/Q1 Predictions Max                            42.0311\n",
      "trainer/Q1 Predictions Min                            10.2715\n",
      "trainer/Q2 Predictions Mean                           26.6852\n",
      "trainer/Q2 Predictions Std                             2.03094\n",
      "trainer/Q2 Predictions Max                            35.5095\n",
      "trainer/Q2 Predictions Min                             6.80742\n",
      "trainer/Q Targets Mean                                26.6204\n",
      "trainer/Q Targets Std                                  2.84999\n",
      "trainer/Q Targets Max                                 47.8833\n",
      "trainer/Q Targets Min                                 -0.419588\n",
      "trainer/Log Pis Mean                                   9.34333\n",
      "trainer/Log Pis Std                                    3.78193\n",
      "trainer/Log Pis Max                                   40.7929\n",
      "trainer/Log Pis Min                                    2.32147\n",
      "trainer/Policy mu Mean                                 0.0824123\n",
      "trainer/Policy mu Std                                  0.502255\n",
      "trainer/Policy mu Max                                  5.28649\n",
      "trainer/Policy mu Min                                 -4.91351\n",
      "trainer/Policy log std Mean                           -2.37564\n",
      "trainer/Policy log std Std                             0.274541\n",
      "trainer/Policy log std Max                            -0.84018\n",
      "trainer/Policy log std Min                            -4.01858\n",
      "trainer/Alpha                                          0.00824705\n",
      "trainer/Alpha Loss                                     6.44707\n",
      "exploration/num steps total                        84000\n",
      "exploration/num paths total                          144\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.86152\n",
      "exploration/Rewards Std                                0.0751013\n",
      "exploration/Rewards Max                                1.25566\n",
      "exploration/Rewards Min                                0.156822\n",
      "exploration/Returns Mean                             861.52\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              861.52\n",
      "exploration/Returns Min                              861.52\n",
      "exploration/Actions Mean                               0.0422089\n",
      "exploration/Actions Std                                0.189139\n",
      "exploration/Actions Max                                0.652307\n",
      "exploration/Actions Min                               -0.80257\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          861.52\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0189261\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0189261\n",
      "exploration/env_infos/final/reward_forward Min        -0.0189261\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0893917\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0893917\n",
      "exploration/env_infos/initial/reward_forward Min       0.0893917\n",
      "exploration/env_infos/reward_forward Mean             -0.0145548\n",
      "exploration/env_infos/reward_forward Std               0.134994\n",
      "exploration/env_infos/reward_forward Max               0.486786\n",
      "exploration/env_infos/reward_forward Min              -0.840456\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.159848\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.159848\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.159848\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.17463\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.17463\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.17463\n",
      "exploration/env_infos/reward_ctrl Mean                -0.15022\n",
      "exploration/env_infos/reward_ctrl Std                  0.0570274\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0194966\n",
      "exploration/env_infos/reward_ctrl Min                 -0.843178\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0303128\n",
      "exploration/env_infos/final/torso_velocity Std         0.073754\n",
      "exploration/env_infos/final/torso_velocity Max         0.0537838\n",
      "exploration/env_infos/final/torso_velocity Min        -0.125796\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0555306\n",
      "exploration/env_infos/initial/torso_velocity Std       0.270794\n",
      "exploration/env_infos/initial/torso_velocity Max       0.368954\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.291754\n",
      "exploration/env_infos/torso_velocity Mean             -0.0130704\n",
      "exploration/env_infos/torso_velocity Std               0.15783\n",
      "exploration/env_infos/torso_velocity Max               0.73153\n",
      "exploration/env_infos/torso_velocity Min              -1.38877\n",
      "evaluation/num steps total                             2.0721e+06\n",
      "evaluation/num paths total                          2075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.736225\n",
      "evaluation/Rewards Std                                 0.173128\n",
      "evaluation/Rewards Max                                 1.78714\n",
      "evaluation/Rewards Min                                -1.21688\n",
      "evaluation/Returns Mean                              736.225\n",
      "evaluation/Returns Std                               165.384\n",
      "evaluation/Returns Max                               905.905\n",
      "evaluation/Returns Min                               421.743\n",
      "evaluation/Actions Mean                                0.0791418\n",
      "evaluation/Actions Std                                 0.244447\n",
      "evaluation/Actions Max                                 0.989526\n",
      "evaluation/Actions Min                                -0.983642\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           736.225\n",
      "evaluation/env_infos/final/reward_forward Mean         1.11688e-05\n",
      "evaluation/env_infos/final/reward_forward Std          5.21922e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000266223\n",
      "evaluation/env_infos/final/reward_forward Min         -2.62621e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0670723\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126507\n",
      "evaluation/env_infos/initial/reward_forward Max        0.127099\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.3834\n",
      "evaluation/env_infos/reward_forward Mean               0.00202123\n",
      "evaluation/env_infos/reward_forward Std                0.0461618\n",
      "evaluation/env_infos/reward_forward Max                1.56581\n",
      "evaluation/env_infos/reward_forward Min               -1.06998\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.262157\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.166087\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0919968\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.579596\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.896261\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.625568\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.115843\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.96324\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.26407\n",
      "evaluation/env_infos/reward_ctrl Std                   0.172915\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0469464\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.21688\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.21167e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000214182\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000266223\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00183391\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0592001\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.326033\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.767832\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.549028\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00130443\n",
      "evaluation/env_infos/torso_velocity Std                0.0520604\n",
      "evaluation/env_infos/torso_velocity Max                1.56581\n",
      "evaluation/env_infos/torso_velocity Min               -1.93768\n",
      "time/data storing (s)                                  0.0146708\n",
      "time/evaluation sampling (s)                          45.2397\n",
      "time/exploration sampling (s)                          1.95482\n",
      "time/logging (s)                                       0.279378\n",
      "time/saving (s)                                        0.0275016\n",
      "time/training (s)                                      4.15856\n",
      "time/epoch (s)                                        51.6746\n",
      "time/total (s)                                      4476.43\n",
      "Epoch                                                 82\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:40:20.590284 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 83 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 85000\n",
      "trainer/QF1 Loss                                       2.54981\n",
      "trainer/QF2 Loss                                       1.35916\n",
      "trainer/Policy Loss                                  -18.5962\n",
      "trainer/Q1 Predictions Mean                           26.6288\n",
      "trainer/Q1 Predictions Std                             2.9465\n",
      "trainer/Q1 Predictions Max                            34.7725\n",
      "trainer/Q1 Predictions Min                            -5.14401\n",
      "trainer/Q2 Predictions Mean                           26.6365\n",
      "trainer/Q2 Predictions Std                             2.77388\n",
      "trainer/Q2 Predictions Max                            35.5897\n",
      "trainer/Q2 Predictions Min                             3.56054\n",
      "trainer/Q Targets Mean                                26.4841\n",
      "trainer/Q Targets Std                                  3.37411\n",
      "trainer/Q Targets Max                                 43.9946\n",
      "trainer/Q Targets Min                                 -1.86583\n",
      "trainer/Log Pis Mean                                   8.439\n",
      "trainer/Log Pis Std                                    5.47787\n",
      "trainer/Log Pis Max                                   43.1575\n",
      "trainer/Log Pis Min                                   -1.01422\n",
      "trainer/Policy mu Mean                                 0.00439746\n",
      "trainer/Policy mu Std                                  0.759315\n",
      "trainer/Policy mu Max                                  8.87135\n",
      "trainer/Policy mu Min                                 -6.13861\n",
      "trainer/Policy log std Mean                           -2.11687\n",
      "trainer/Policy log std Std                             0.396314\n",
      "trainer/Policy log std Max                             0.51878\n",
      "trainer/Policy log std Min                            -5.20126\n",
      "trainer/Alpha                                          0.0113984\n",
      "trainer/Alpha Loss                                     1.96425\n",
      "exploration/num steps total                        85000\n",
      "exploration/num paths total                          145\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.766134\n",
      "exploration/Rewards Std                                0.108364\n",
      "exploration/Rewards Max                                1.65062\n",
      "exploration/Rewards Min                               -0.66258\n",
      "exploration/Returns Mean                             766.134\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              766.134\n",
      "exploration/Returns Min                              766.134\n",
      "exploration/Actions Mean                              -0.0400849\n",
      "exploration/Actions Std                                0.242337\n",
      "exploration/Actions Max                                0.946703\n",
      "exploration/Actions Min                               -0.942533\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          766.134\n",
      "exploration/env_infos/final/reward_forward Mean       -0.045705\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.045705\n",
      "exploration/env_infos/final/reward_forward Min        -0.045705\n",
      "exploration/env_infos/initial/reward_forward Mean      0.269339\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.269339\n",
      "exploration/env_infos/initial/reward_forward Min       0.269339\n",
      "exploration/env_infos/reward_forward Mean              6.40578e-05\n",
      "exploration/env_infos/reward_forward Std               0.165886\n",
      "exploration/env_infos/reward_forward Max               0.533399\n",
      "exploration/env_infos/reward_forward Min              -1.24089\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.190012\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.190012\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.190012\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.179158\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.179158\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.179158\n",
      "exploration/env_infos/reward_ctrl Mean                -0.241336\n",
      "exploration/env_infos/reward_ctrl Std                  0.0976721\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0407719\n",
      "exploration/env_infos/reward_ctrl Min                 -1.66258\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0216145\n",
      "exploration/env_infos/final/torso_velocity Std         0.0224265\n",
      "exploration/env_infos/final/torso_velocity Max         0.00829567\n",
      "exploration/env_infos/final/torso_velocity Min        -0.045705\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.13959\n",
      "exploration/env_infos/initial/torso_velocity Std       0.25027\n",
      "exploration/env_infos/initial/torso_velocity Max       0.359893\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.210464\n",
      "exploration/env_infos/torso_velocity Mean             -0.0075258\n",
      "exploration/env_infos/torso_velocity Std               0.151494\n",
      "exploration/env_infos/torso_velocity Max               1.60528\n",
      "exploration/env_infos/torso_velocity Min              -1.24089\n",
      "evaluation/num steps total                             2.0971e+06\n",
      "evaluation/num paths total                          2100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.733803\n",
      "evaluation/Rewards Std                                 0.254288\n",
      "evaluation/Rewards Max                                 2.00628\n",
      "evaluation/Rewards Min                                -2.01241\n",
      "evaluation/Returns Mean                              733.803\n",
      "evaluation/Returns Std                               192.088\n",
      "evaluation/Returns Max                               891.998\n",
      "evaluation/Returns Min                               199.061\n",
      "evaluation/Actions Mean                               -0.039615\n",
      "evaluation/Actions Std                                 0.257798\n",
      "evaluation/Actions Max                                 0.999982\n",
      "evaluation/Actions Min                                -0.999969\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           733.803\n",
      "evaluation/env_infos/final/reward_forward Mean         3.93136e-07\n",
      "evaluation/env_infos/final/reward_forward Std          1.93688e-06\n",
      "evaluation/env_infos/final/reward_forward Max          9.54535e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -7.85981e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.11563\n",
      "evaluation/env_infos/initial/reward_forward Std        0.166904\n",
      "evaluation/env_infos/initial/reward_forward Max        0.221746\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.46365\n",
      "evaluation/env_infos/reward_forward Mean               0.00900889\n",
      "evaluation/env_infos/reward_forward Std                0.131846\n",
      "evaluation/env_infos/reward_forward Max                1.79433\n",
      "evaluation/env_infos/reward_forward Min               -1.73433\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.241429\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.189376\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0964379\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.787676\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.993067\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.718156\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0729429\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -2.09435\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.272117\n",
      "evaluation/env_infos/reward_ctrl Std                   0.254692\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0358131\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.01241\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.29071e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          1.93998e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          1.3122e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.85981e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0476834\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.30352\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.59961\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.549017\n",
      "evaluation/env_infos/torso_velocity Mean               0.00103033\n",
      "evaluation/env_infos/torso_velocity Std                0.148897\n",
      "evaluation/env_infos/torso_velocity Max                2.33396\n",
      "evaluation/env_infos/torso_velocity Min               -2.09726\n",
      "time/data storing (s)                                  0.0150631\n",
      "time/evaluation sampling (s)                          45.5093\n",
      "time/exploration sampling (s)                          1.99317\n",
      "time/logging (s)                                       0.272944\n",
      "time/saving (s)                                        0.0321698\n",
      "time/training (s)                                      4.56519\n",
      "time/epoch (s)                                        52.3879\n",
      "time/total (s)                                      4529.55\n",
      "Epoch                                                 83\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:41:14.024679 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 84 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 86000\n",
      "trainer/QF1 Loss                                       0.921365\n",
      "trainer/QF2 Loss                                       1.38151\n",
      "trainer/Policy Loss                                  -19.8851\n",
      "trainer/Q1 Predictions Mean                           27.3149\n",
      "trainer/Q1 Predictions Std                             2.26112\n",
      "trainer/Q1 Predictions Max                            48.3485\n",
      "trainer/Q1 Predictions Min                            17.5956\n",
      "trainer/Q2 Predictions Mean                           27.192\n",
      "trainer/Q2 Predictions Std                             2.09344\n",
      "trainer/Q2 Predictions Max                            42.0559\n",
      "trainer/Q2 Predictions Min                            15.6188\n",
      "trainer/Q Targets Mean                                27.3612\n",
      "trainer/Q Targets Std                                  2.59787\n",
      "trainer/Q Targets Max                                 52.5973\n",
      "trainer/Q Targets Min                                 17.6351\n",
      "trainer/Log Pis Mean                                   7.83926\n",
      "trainer/Log Pis Std                                    4.17477\n",
      "trainer/Log Pis Max                                   40.3913\n",
      "trainer/Log Pis Min                                   -4.16767\n",
      "trainer/Policy mu Mean                                 0.121674\n",
      "trainer/Policy mu Std                                  0.661278\n",
      "trainer/Policy mu Max                                  5.69655\n",
      "trainer/Policy mu Min                                 -4.27144\n",
      "trainer/Policy log std Mean                           -2.05282\n",
      "trainer/Policy log std Std                             0.383422\n",
      "trainer/Policy log std Max                            -0.0743758\n",
      "trainer/Policy log std Min                            -4.18944\n",
      "trainer/Alpha                                          0.0111152\n",
      "trainer/Alpha Loss                                    -0.723197\n",
      "exploration/num steps total                        86000\n",
      "exploration/num paths total                          146\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.814623\n",
      "exploration/Rewards Std                                0.182215\n",
      "exploration/Rewards Max                                1.62016\n",
      "exploration/Rewards Min                               -1.72814\n",
      "exploration/Returns Mean                             814.623\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              814.623\n",
      "exploration/Returns Min                              814.623\n",
      "exploration/Actions Mean                               0.0390964\n",
      "exploration/Actions Std                                0.237465\n",
      "exploration/Actions Max                                0.946746\n",
      "exploration/Actions Min                               -0.978596\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          814.623\n",
      "exploration/env_infos/final/reward_forward Mean        0.0683184\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0683184\n",
      "exploration/env_infos/final/reward_forward Min         0.0683184\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.26783\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.26783\n",
      "exploration/env_infos/initial/reward_forward Min      -0.26783\n",
      "exploration/env_infos/reward_forward Mean              0.0215041\n",
      "exploration/env_infos/reward_forward Std               0.148422\n",
      "exploration/env_infos/reward_forward Max               0.758834\n",
      "exploration/env_infos/reward_forward Min              -0.373747\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.112372\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.112372\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.112372\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.31984\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.31984\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.31984\n",
      "exploration/env_infos/reward_ctrl Mean                -0.231672\n",
      "exploration/env_infos/reward_ctrl Std                  0.154406\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0213889\n",
      "exploration/env_infos/reward_ctrl Min                 -2.72814\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.019519\n",
      "exploration/env_infos/final/torso_velocity Std         0.0625116\n",
      "exploration/env_infos/final/torso_velocity Max         0.0683184\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0720975\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0925373\n",
      "exploration/env_infos/initial/torso_velocity Std       0.416874\n",
      "exploration/env_infos/initial/torso_velocity Max       0.482583\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.492365\n",
      "exploration/env_infos/torso_velocity Mean              0.00379756\n",
      "exploration/env_infos/torso_velocity Std               0.115169\n",
      "exploration/env_infos/torso_velocity Max               0.758834\n",
      "exploration/env_infos/torso_velocity Min              -0.623636\n",
      "evaluation/num steps total                             2.1221e+06\n",
      "evaluation/num paths total                          2125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.735312\n",
      "evaluation/Rewards Std                                 0.233155\n",
      "evaluation/Rewards Max                                 1.69051\n",
      "evaluation/Rewards Min                                -2.19813\n",
      "evaluation/Returns Mean                              735.312\n",
      "evaluation/Returns Std                               217.272\n",
      "evaluation/Returns Max                               925.803\n",
      "evaluation/Returns Min                               -60.7031\n",
      "evaluation/Actions Mean                                0.0772128\n",
      "evaluation/Actions Std                                 0.245974\n",
      "evaluation/Actions Max                                 0.999729\n",
      "evaluation/Actions Min                                -0.999753\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           735.312\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00409275\n",
      "evaluation/env_infos/final/reward_forward Std          0.0236201\n",
      "evaluation/env_infos/final/reward_forward Max          0.118731\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0164108\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0856116\n",
      "evaluation/env_infos/initial/reward_forward Std        0.142372\n",
      "evaluation/env_infos/initial/reward_forward Max        0.154842\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.36134\n",
      "evaluation/env_infos/reward_forward Mean               0.00144507\n",
      "evaluation/env_infos/reward_forward Std                0.0584867\n",
      "evaluation/env_infos/reward_forward Max                1.65847\n",
      "evaluation/env_infos/reward_forward Min               -1.01888\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.26323\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.218257\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0715199\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.06223\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.877951\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.609104\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0873854\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.76369\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.265859\n",
      "evaluation/env_infos/reward_ctrl Std                   0.23269\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0359134\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.19813\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00240016\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0298422\n",
      "evaluation/env_infos/final/torso_velocity Max          0.118731\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.214075\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0251185\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.305948\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.615063\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.464837\n",
      "evaluation/env_infos/torso_velocity Mean               9.75818e-05\n",
      "evaluation/env_infos/torso_velocity Std                0.0644662\n",
      "evaluation/env_infos/torso_velocity Max                1.65847\n",
      "evaluation/env_infos/torso_velocity Min               -1.80794\n",
      "time/data storing (s)                                  0.0152793\n",
      "time/evaluation sampling (s)                          45.4496\n",
      "time/exploration sampling (s)                          1.92339\n",
      "time/logging (s)                                       0.270606\n",
      "time/saving (s)                                        0.0260746\n",
      "time/training (s)                                      5.00685\n",
      "time/epoch (s)                                        52.6918\n",
      "time/total (s)                                      4582.98\n",
      "Epoch                                                 84\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:42:06.219363 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 85 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 87000\n",
      "trainer/QF1 Loss                                       2.5976\n",
      "trainer/QF2 Loss                                       1.66616\n",
      "trainer/Policy Loss                                  -17.3237\n",
      "trainer/Q1 Predictions Mean                           27.5571\n",
      "trainer/Q1 Predictions Std                             4.10379\n",
      "trainer/Q1 Predictions Max                            80.7083\n",
      "trainer/Q1 Predictions Min                            16.5446\n",
      "trainer/Q2 Predictions Mean                           27.1883\n",
      "trainer/Q2 Predictions Std                             4.13772\n",
      "trainer/Q2 Predictions Max                            76.0166\n",
      "trainer/Q2 Predictions Min                            16.4568\n",
      "trainer/Q Targets Mean                                27.6428\n",
      "trainer/Q Targets Std                                  3.95393\n",
      "trainer/Q Targets Max                                 68.2541\n",
      "trainer/Q Targets Min                                 17.0685\n",
      "trainer/Log Pis Mean                                  10.8042\n",
      "trainer/Log Pis Std                                    6.4171\n",
      "trainer/Log Pis Max                                   48.9878\n",
      "trainer/Log Pis Min                                   -0.5815\n",
      "trainer/Policy mu Mean                                 0.187522\n",
      "trainer/Policy mu Std                                  0.871467\n",
      "trainer/Policy mu Max                                  6.34606\n",
      "trainer/Policy mu Min                                 -5.25748\n",
      "trainer/Policy log std Mean                           -2.21583\n",
      "trainer/Policy log std Std                             0.388606\n",
      "trainer/Policy log std Max                             0.394122\n",
      "trainer/Policy log std Min                            -4.95016\n",
      "trainer/Alpha                                          0.0111412\n",
      "trainer/Alpha Loss                                    12.6214\n",
      "exploration/num steps total                        87000\n",
      "exploration/num paths total                          147\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.683729\n",
      "exploration/Rewards Std                                0.128692\n",
      "exploration/Rewards Max                                1.14062\n",
      "exploration/Rewards Min                               -0.63557\n",
      "exploration/Returns Mean                             683.729\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              683.729\n",
      "exploration/Returns Min                              683.729\n",
      "exploration/Actions Mean                               0.0202938\n",
      "exploration/Actions Std                                0.284014\n",
      "exploration/Actions Max                                0.88512\n",
      "exploration/Actions Min                               -0.792768\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          683.729\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00905347\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00905347\n",
      "exploration/env_infos/final/reward_forward Min        -0.00905347\n",
      "exploration/env_infos/initial/reward_forward Mean      0.176314\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.176314\n",
      "exploration/env_infos/initial/reward_forward Min       0.176314\n",
      "exploration/env_infos/reward_forward Mean             -0.0220909\n",
      "exploration/env_infos/reward_forward Std               0.169091\n",
      "exploration/env_infos/reward_forward Max               0.763801\n",
      "exploration/env_infos/reward_forward Min              -1.21508\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.260184\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.260184\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.260184\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.10344\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.10344\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.10344\n",
      "exploration/env_infos/reward_ctrl Mean                -0.324304\n",
      "exploration/env_infos/reward_ctrl Std                  0.124735\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0408116\n",
      "exploration/env_infos/reward_ctrl Min                 -1.63557\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00676215\n",
      "exploration/env_infos/final/torso_velocity Std         0.00439531\n",
      "exploration/env_infos/final/torso_velocity Max        -0.000612446\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0106205\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0294543\n",
      "exploration/env_infos/initial/torso_velocity Std       0.331652\n",
      "exploration/env_infos/initial/torso_velocity Max       0.341788\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.42974\n",
      "exploration/env_infos/torso_velocity Mean             -0.0124543\n",
      "exploration/env_infos/torso_velocity Std               0.153593\n",
      "exploration/env_infos/torso_velocity Max               1.23074\n",
      "exploration/env_infos/torso_velocity Min              -1.33144\n",
      "evaluation/num steps total                             2.1471e+06\n",
      "evaluation/num paths total                          2150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.445359\n",
      "evaluation/Rewards Std                                 0.356344\n",
      "evaluation/Rewards Max                                 1.53349\n",
      "evaluation/Rewards Min                                -2.19503\n",
      "evaluation/Returns Mean                              445.359\n",
      "evaluation/Returns Std                               311.797\n",
      "evaluation/Returns Max                               915.141\n",
      "evaluation/Returns Min                              -175.387\n",
      "evaluation/Actions Mean                                0.0658853\n",
      "evaluation/Actions Std                                 0.367489\n",
      "evaluation/Actions Max                                 0.999956\n",
      "evaluation/Actions Min                                -0.999859\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           445.359\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0356653\n",
      "evaluation/env_infos/final/reward_forward Std          0.107514\n",
      "evaluation/env_infos/final/reward_forward Max          0.483581\n",
      "evaluation/env_infos/final/reward_forward Min         -1.11298e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0990931\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132281\n",
      "evaluation/env_infos/initial/reward_forward Max        0.158516\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.416959\n",
      "evaluation/env_infos/reward_forward Mean              -0.00655614\n",
      "evaluation/env_infos/reward_forward Std                0.132882\n",
      "evaluation/env_infos/reward_forward Max                1.06395\n",
      "evaluation/env_infos/reward_forward Min               -1.29586\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.521004\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.309598\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0802159\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.16766\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -1.04481\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.665769\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0913599\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.96041\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.557556\n",
      "evaluation/env_infos/reward_ctrl Std                   0.358094\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.03825\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.19503\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0355102\n",
      "evaluation/env_infos/final/torso_velocity Std          0.130562\n",
      "evaluation/env_infos/final/torso_velocity Max          0.598799\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.119418\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0518938\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.296014\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.638111\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.565654\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00157497\n",
      "evaluation/env_infos/torso_velocity Std                0.12147\n",
      "evaluation/env_infos/torso_velocity Max                1.38443\n",
      "evaluation/env_infos/torso_velocity Min               -2.03642\n",
      "time/data storing (s)                                  0.0142434\n",
      "time/evaluation sampling (s)                          44.9526\n",
      "time/exploration sampling (s)                          2.02082\n",
      "time/logging (s)                                       0.272516\n",
      "time/saving (s)                                        0.0259621\n",
      "time/training (s)                                      4.15411\n",
      "time/epoch (s)                                        51.4402\n",
      "time/total (s)                                      4635.18\n",
      "Epoch                                                 85\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:42:58.400177 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 86 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 88000\n",
      "trainer/QF1 Loss                                       1.30642\n",
      "trainer/QF2 Loss                                       1.36787\n",
      "trainer/Policy Loss                                  -20.7067\n",
      "trainer/Q1 Predictions Mean                           27.5576\n",
      "trainer/Q1 Predictions Std                             2.7696\n",
      "trainer/Q1 Predictions Max                            31.2211\n",
      "trainer/Q1 Predictions Min                             5.86984\n",
      "trainer/Q2 Predictions Mean                           27.1803\n",
      "trainer/Q2 Predictions Std                             2.91349\n",
      "trainer/Q2 Predictions Max                            33.6649\n",
      "trainer/Q2 Predictions Min                            -3.93096\n",
      "trainer/Q Targets Mean                                27.4249\n",
      "trainer/Q Targets Std                                  2.93101\n",
      "trainer/Q Targets Max                                 35.6966\n",
      "trainer/Q Targets Min                                 -0.336576\n",
      "trainer/Log Pis Mean                                   6.88617\n",
      "trainer/Log Pis Std                                    2.98172\n",
      "trainer/Log Pis Max                                   27.5569\n",
      "trainer/Log Pis Min                                   -0.975063\n",
      "trainer/Policy mu Mean                                 0.0492306\n",
      "trainer/Policy mu Std                                  0.529632\n",
      "trainer/Policy mu Max                                 10.9632\n",
      "trainer/Policy mu Min                                 -5.73861\n",
      "trainer/Policy log std Mean                           -2.07346\n",
      "trainer/Policy log std Std                             0.33168\n",
      "trainer/Policy log std Max                             2\n",
      "trainer/Policy log std Min                            -3.29904\n",
      "trainer/Alpha                                          0.0116715\n",
      "trainer/Alpha Loss                                    -4.95443\n",
      "exploration/num steps total                        88000\n",
      "exploration/num paths total                          148\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.61447\n",
      "exploration/Rewards Std                                0.401268\n",
      "exploration/Rewards Max                                1.47486\n",
      "exploration/Rewards Min                               -2.02481\n",
      "exploration/Returns Mean                             614.47\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              614.47\n",
      "exploration/Returns Min                              614.47\n",
      "exploration/Actions Mean                              -0.0336161\n",
      "exploration/Actions Std                                0.317846\n",
      "exploration/Actions Max                                0.999957\n",
      "exploration/Actions Min                               -0.996502\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          614.47\n",
      "exploration/env_infos/final/reward_forward Mean        0.159254\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.159254\n",
      "exploration/env_infos/final/reward_forward Min         0.159254\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.107679\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.107679\n",
      "exploration/env_infos/initial/reward_forward Min      -0.107679\n",
      "exploration/env_infos/reward_forward Mean              0.00243847\n",
      "exploration/env_infos/reward_forward Std               0.428503\n",
      "exploration/env_infos/reward_forward Max               1.89529\n",
      "exploration/env_infos/reward_forward Min              -1.38311\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.21761\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.21761\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.21761\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.09688\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.09688\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.09688\n",
      "exploration/env_infos/reward_ctrl Mean                -0.408623\n",
      "exploration/env_infos/reward_ctrl Std                  0.406905\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0468533\n",
      "exploration/env_infos/reward_ctrl Min                 -3.28959\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.136222\n",
      "exploration/env_infos/final/torso_velocity Std         0.329365\n",
      "exploration/env_infos/final/torso_velocity Max         0.527602\n",
      "exploration/env_infos/final/torso_velocity Min        -0.278188\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.104457\n",
      "exploration/env_infos/initial/torso_velocity Std       0.310727\n",
      "exploration/env_infos/initial/torso_velocity Max       0.543805\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.122754\n",
      "exploration/env_infos/torso_velocity Mean              0.0118658\n",
      "exploration/env_infos/torso_velocity Std               0.45224\n",
      "exploration/env_infos/torso_velocity Max               2.3471\n",
      "exploration/env_infos/torso_velocity Min              -1.93231\n",
      "evaluation/num steps total                             2.17116e+06\n",
      "evaluation/num paths total                          2175\n",
      "evaluation/path length Mean                          962.76\n",
      "evaluation/path length Std                           182.438\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            69\n",
      "evaluation/Rewards Mean                                0.779277\n",
      "evaluation/Rewards Std                                 0.216071\n",
      "evaluation/Rewards Max                                 2.45732\n",
      "evaluation/Rewards Min                                -2.73708\n",
      "evaluation/Returns Mean                              750.257\n",
      "evaluation/Returns Std                               169.832\n",
      "evaluation/Returns Max                               901.988\n",
      "evaluation/Returns Min                               -11.1032\n",
      "evaluation/Actions Mean                                0.00579216\n",
      "evaluation/Actions Std                                 0.237111\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -1\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           750.257\n",
      "evaluation/env_infos/final/reward_forward Mean         0.050316\n",
      "evaluation/env_infos/final/reward_forward Std          0.247219\n",
      "evaluation/env_infos/final/reward_forward Max          1.26143\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00353269\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0512592\n",
      "evaluation/env_infos/initial/reward_forward Std        0.112307\n",
      "evaluation/env_infos/initial/reward_forward Max        0.123315\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.254027\n",
      "evaluation/env_infos/reward_forward Mean               0.0115442\n",
      "evaluation/env_infos/reward_forward Std                0.177031\n",
      "evaluation/env_infos/reward_forward Max                2.66976\n",
      "evaluation/env_infos/reward_forward Min               -1.81545\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.279729\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.428089\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0886462\n",
      "evaluation/env_infos/final/reward_ctrl Min            -2.35996\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.638407\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.364001\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.070701\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.25244\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.22502\n",
      "evaluation/env_infos/reward_ctrl Std                   0.221435\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0198021\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.73708\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0526937\n",
      "evaluation/env_infos/final/torso_velocity Std          0.300291\n",
      "evaluation/env_infos/final/torso_velocity Max          2.28227\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00353269\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0490883\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.291929\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.577626\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.487885\n",
      "evaluation/env_infos/torso_velocity Mean               0.00340787\n",
      "evaluation/env_infos/torso_velocity Std                0.172934\n",
      "evaluation/env_infos/torso_velocity Max                3.08441\n",
      "evaluation/env_infos/torso_velocity Min               -2.20918\n",
      "time/data storing (s)                                  0.0151045\n",
      "time/evaluation sampling (s)                          44.8585\n",
      "time/exploration sampling (s)                          1.96846\n",
      "time/logging (s)                                       0.26324\n",
      "time/saving (s)                                        0.0254675\n",
      "time/training (s)                                      4.29615\n",
      "time/epoch (s)                                        51.4269\n",
      "time/total (s)                                      4687.34\n",
      "Epoch                                                 86\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:43:51.007576 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 87 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 89000\n",
      "trainer/QF1 Loss                                       1.22958\n",
      "trainer/QF2 Loss                                       1.05351\n",
      "trainer/Policy Loss                                  -20.069\n",
      "trainer/Q1 Predictions Mean                           27.6012\n",
      "trainer/Q1 Predictions Std                             2.84209\n",
      "trainer/Q1 Predictions Max                            49.1693\n",
      "trainer/Q1 Predictions Min                             4.97076\n",
      "trainer/Q2 Predictions Mean                           27.7095\n",
      "trainer/Q2 Predictions Std                             2.96568\n",
      "trainer/Q2 Predictions Max                            47.226\n",
      "trainer/Q2 Predictions Min                             4.34225\n",
      "trainer/Q Targets Mean                                27.8227\n",
      "trainer/Q Targets Std                                  3.57957\n",
      "trainer/Q Targets Max                                 56.7163\n",
      "trainer/Q Targets Min                                 -0.242457\n",
      "trainer/Log Pis Mean                                   8.05476\n",
      "trainer/Log Pis Std                                    3.03773\n",
      "trainer/Log Pis Max                                   28.5872\n",
      "trainer/Log Pis Min                                   -2.3134\n",
      "trainer/Policy mu Mean                                -0.0700791\n",
      "trainer/Policy mu Std                                  0.439571\n",
      "trainer/Policy mu Max                                  3.23593\n",
      "trainer/Policy mu Min                                 -1.74178\n",
      "trainer/Policy log std Mean                           -2.24151\n",
      "trainer/Policy log std Std                             0.314996\n",
      "trainer/Policy log std Max                            -0.185049\n",
      "trainer/Policy log std Min                            -5.00851\n",
      "trainer/Alpha                                          0.0124449\n",
      "trainer/Alpha Loss                                     0.240118\n",
      "exploration/num steps total                        89000\n",
      "exploration/num paths total                          149\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.56603\n",
      "exploration/Rewards Std                                0.295575\n",
      "exploration/Rewards Max                                2.0535\n",
      "exploration/Rewards Min                               -1.81891\n",
      "exploration/Returns Mean                             566.03\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              566.03\n",
      "exploration/Returns Min                              566.03\n",
      "exploration/Actions Mean                              -0.0380279\n",
      "exploration/Actions Std                                0.334627\n",
      "exploration/Actions Max                                0.999285\n",
      "exploration/Actions Min                               -0.987583\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          566.03\n",
      "exploration/env_infos/final/reward_forward Mean        0.0591455\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0591455\n",
      "exploration/env_infos/final/reward_forward Min         0.0591455\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0682371\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0682371\n",
      "exploration/env_infos/initial/reward_forward Min       0.0682371\n",
      "exploration/env_infos/reward_forward Mean             -0.0280569\n",
      "exploration/env_infos/reward_forward Std               0.397118\n",
      "exploration/env_infos/reward_forward Max               1.93668\n",
      "exploration/env_infos/reward_forward Min              -1.43192\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.465416\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.465416\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.465416\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.599514\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.599514\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.599514\n",
      "exploration/env_infos/reward_ctrl Mean                -0.453686\n",
      "exploration/env_infos/reward_ctrl Std                  0.277867\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0682204\n",
      "exploration/env_infos/reward_ctrl Min                 -2.81891\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.042711\n",
      "exploration/env_infos/final/torso_velocity Std         0.0147748\n",
      "exploration/env_infos/final/torso_velocity Max         0.0591455\n",
      "exploration/env_infos/final/torso_velocity Min         0.0233191\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0750484\n",
      "exploration/env_infos/initial/torso_velocity Std       0.244976\n",
      "exploration/env_infos/initial/torso_velocity Max       0.37843\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.221522\n",
      "exploration/env_infos/torso_velocity Mean             -0.0297761\n",
      "exploration/env_infos/torso_velocity Std               0.342396\n",
      "exploration/env_infos/torso_velocity Max               1.93668\n",
      "exploration/env_infos/torso_velocity Min              -1.74479\n",
      "evaluation/num steps total                             2.19518e+06\n",
      "evaluation/num paths total                          2200\n",
      "evaluation/path length Mean                          960.48\n",
      "evaluation/path length Std                           193.608\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            12\n",
      "evaluation/Rewards Mean                                0.435379\n",
      "evaluation/Rewards Std                                 0.236879\n",
      "evaluation/Rewards Max                                 2.16867\n",
      "evaluation/Rewards Min                                -1.51788\n",
      "evaluation/Returns Mean                              418.173\n",
      "evaluation/Returns Std                               230.977\n",
      "evaluation/Returns Max                               803.784\n",
      "evaluation/Returns Min                                -1.58062\n",
      "evaluation/Actions Mean                               -0.0881197\n",
      "evaluation/Actions Std                                 0.366912\n",
      "evaluation/Actions Max                                 0.999164\n",
      "evaluation/Actions Min                                -0.996206\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           418.173\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0116678\n",
      "evaluation/env_infos/final/reward_forward Std          0.19952\n",
      "evaluation/env_infos/final/reward_forward Max          0.495331\n",
      "evaluation/env_infos/final/reward_forward Min         -0.769802\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0267128\n",
      "evaluation/env_infos/initial/reward_forward Std        0.101033\n",
      "evaluation/env_infos/initial/reward_forward Max        0.211761\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.153042\n",
      "evaluation/env_infos/reward_forward Mean              -0.0108094\n",
      "evaluation/env_infos/reward_forward Std                0.123616\n",
      "evaluation/env_infos/reward_forward Max                1.33829\n",
      "evaluation/env_infos/reward_forward Min               -1.79849\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.607636\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.287398\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.19159\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.58746\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.557029\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.296134\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0935775\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.990638\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.569558\n",
      "evaluation/env_infos/reward_ctrl Std                   0.230769\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0841077\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.51788\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0837062\n",
      "evaluation/env_infos/final/torso_velocity Std          0.464059\n",
      "evaluation/env_infos/final/torso_velocity Max          2.65491\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.769802\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.101799\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.284606\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.657559\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.508555\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00202997\n",
      "evaluation/env_infos/torso_velocity Std                0.140672\n",
      "evaluation/env_infos/torso_velocity Max                3.3345\n",
      "evaluation/env_infos/torso_velocity Min               -1.86566\n",
      "time/data storing (s)                                  0.0150493\n",
      "time/evaluation sampling (s)                          44.6227\n",
      "time/exploration sampling (s)                          2.02563\n",
      "time/logging (s)                                       0.263591\n",
      "time/saving (s)                                        0.0264123\n",
      "time/training (s)                                      4.8581\n",
      "time/epoch (s)                                        51.8115\n",
      "time/total (s)                                      4739.95\n",
      "Epoch                                                 87\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:44:44.779837 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 88 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 90000\n",
      "trainer/QF1 Loss                                       0.835192\n",
      "trainer/QF2 Loss                                       0.565276\n",
      "trainer/Policy Loss                                  -20.4566\n",
      "trainer/Q1 Predictions Mean                           28.1045\n",
      "trainer/Q1 Predictions Std                             2.04739\n",
      "trainer/Q1 Predictions Max                            30.5134\n",
      "trainer/Q1 Predictions Min                            17.2944\n",
      "trainer/Q2 Predictions Mean                           28.0946\n",
      "trainer/Q2 Predictions Std                             1.90973\n",
      "trainer/Q2 Predictions Max                            31.3856\n",
      "trainer/Q2 Predictions Min                            17.2935\n",
      "trainer/Q Targets Mean                                28.0617\n",
      "trainer/Q Targets Std                                  1.96326\n",
      "trainer/Q Targets Max                                 30.9658\n",
      "trainer/Q Targets Min                                 18.2374\n",
      "trainer/Log Pis Mean                                   7.85315\n",
      "trainer/Log Pis Std                                    2.83672\n",
      "trainer/Log Pis Max                                   19.8814\n",
      "trainer/Log Pis Min                                   -4.59582\n",
      "trainer/Policy mu Mean                                -0.123636\n",
      "trainer/Policy mu Std                                  0.379213\n",
      "trainer/Policy mu Max                                  2.22194\n",
      "trainer/Policy mu Min                                 -2.16055\n",
      "trainer/Policy log std Mean                           -2.23714\n",
      "trainer/Policy log std Std                             0.360449\n",
      "trainer/Policy log std Max                            -0.675798\n",
      "trainer/Policy log std Min                            -4.33959\n",
      "trainer/Alpha                                          0.0127204\n",
      "trainer/Alpha Loss                                    -0.641153\n",
      "exploration/num steps total                        90000\n",
      "exploration/num paths total                          150\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.728083\n",
      "exploration/Rewards Std                                0.0835715\n",
      "exploration/Rewards Max                                0.925276\n",
      "exploration/Rewards Min                               -0.0666776\n",
      "exploration/Returns Mean                             728.083\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              728.083\n",
      "exploration/Returns Min                              728.083\n",
      "exploration/Actions Mean                              -0.0730277\n",
      "exploration/Actions Std                                0.250617\n",
      "exploration/Actions Max                                0.694492\n",
      "exploration/Actions Min                               -0.94816\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          728.083\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0105622\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0105622\n",
      "exploration/env_infos/final/reward_forward Min        -0.0105622\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0310068\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0310068\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0310068\n",
      "exploration/env_infos/reward_forward Mean              0.00456756\n",
      "exploration/env_infos/reward_forward Std               0.10279\n",
      "exploration/env_infos/reward_forward Max               0.449392\n",
      "exploration/env_infos/reward_forward Min              -0.289024\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.230296\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.230296\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.230296\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.295733\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.295733\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.295733\n",
      "exploration/env_infos/reward_ctrl Mean                -0.272567\n",
      "exploration/env_infos/reward_ctrl Std                  0.083529\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0747239\n",
      "exploration/env_infos/reward_ctrl Min                 -1.06668\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00480588\n",
      "exploration/env_infos/final/torso_velocity Std         0.00423792\n",
      "exploration/env_infos/final/torso_velocity Max        -0.000482638\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0105622\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0761088\n",
      "exploration/env_infos/initial/torso_velocity Std       0.221788\n",
      "exploration/env_infos/initial/torso_velocity Max       0.38497\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.125637\n",
      "exploration/env_infos/torso_velocity Mean              0.0129533\n",
      "exploration/env_infos/torso_velocity Std               0.110047\n",
      "exploration/env_infos/torso_velocity Max               1.13886\n",
      "exploration/env_infos/torso_velocity Min              -1.12595\n",
      "evaluation/num steps total                             2.22018e+06\n",
      "evaluation/num paths total                          2225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.607076\n",
      "evaluation/Rewards Std                                 0.14531\n",
      "evaluation/Rewards Max                                 2.15107\n",
      "evaluation/Rewards Min                                -0.641825\n",
      "evaluation/Returns Mean                              607.076\n",
      "evaluation/Returns Std                               137.02\n",
      "evaluation/Returns Max                               772.141\n",
      "evaluation/Returns Min                               385.698\n",
      "evaluation/Actions Mean                               -0.0904624\n",
      "evaluation/Actions Std                                 0.300676\n",
      "evaluation/Actions Max                                 0.852773\n",
      "evaluation/Actions Min                                -0.997695\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           607.076\n",
      "evaluation/env_infos/final/reward_forward Mean         1.92154e-05\n",
      "evaluation/env_infos/final/reward_forward Std          9.5693e-05\n",
      "evaluation/env_infos/final/reward_forward Max          0.000488006\n",
      "evaluation/env_infos/final/reward_forward Min         -1.15727e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0725765\n",
      "evaluation/env_infos/initial/reward_forward Std        0.139375\n",
      "evaluation/env_infos/initial/reward_forward Max        0.312501\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.196867\n",
      "evaluation/env_infos/reward_forward Mean               0.003825\n",
      "evaluation/env_infos/reward_forward Std                0.0518321\n",
      "evaluation/env_infos/reward_forward Max                1.9226\n",
      "evaluation/env_infos/reward_forward Min               -1.1961\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.396653\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.138138\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.225451\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.619404\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.300163\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.131107\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.137596\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.527741\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.394358\n",
      "evaluation/env_infos/reward_ctrl Std                   0.143017\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0844453\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.75692\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.06934e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          6.01842e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000488006\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.15727e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.14731\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.26024\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.619266\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.294194\n",
      "evaluation/env_infos/torso_velocity Mean               0.000680608\n",
      "evaluation/env_infos/torso_velocity Std                0.055712\n",
      "evaluation/env_infos/torso_velocity Max                1.9226\n",
      "evaluation/env_infos/torso_velocity Min               -1.7467\n",
      "time/data storing (s)                                  0.0151953\n",
      "time/evaluation sampling (s)                          46.1633\n",
      "time/exploration sampling (s)                          2.04978\n",
      "time/logging (s)                                       0.289568\n",
      "time/saving (s)                                        0.0341621\n",
      "time/training (s)                                      4.49361\n",
      "time/epoch (s)                                        53.0456\n",
      "time/total (s)                                      4793.75\n",
      "Epoch                                                 88\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:45:37.897222 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 89 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 91000\n",
      "trainer/QF1 Loss                                       1.06005\n",
      "trainer/QF2 Loss                                       1.03829\n",
      "trainer/Policy Loss                                  -19.7546\n",
      "trainer/Q1 Predictions Mean                           27.991\n",
      "trainer/Q1 Predictions Std                             2.61355\n",
      "trainer/Q1 Predictions Max                            32.041\n",
      "trainer/Q1 Predictions Min                            16.646\n",
      "trainer/Q2 Predictions Mean                           27.8403\n",
      "trainer/Q2 Predictions Std                             2.68036\n",
      "trainer/Q2 Predictions Max                            30.7472\n",
      "trainer/Q2 Predictions Min                            16.6283\n",
      "trainer/Q Targets Mean                                28.0384\n",
      "trainer/Q Targets Std                                  2.8237\n",
      "trainer/Q Targets Max                                 34.0997\n",
      "trainer/Q Targets Min                                 14.6906\n",
      "trainer/Log Pis Mean                                   8.41124\n",
      "trainer/Log Pis Std                                    2.7885\n",
      "trainer/Log Pis Max                                   17.4331\n",
      "trainer/Log Pis Min                                   -1.70371\n",
      "trainer/Policy mu Mean                                -0.0346807\n",
      "trainer/Policy mu Std                                  0.349986\n",
      "trainer/Policy mu Max                                  1.93412\n",
      "trainer/Policy mu Min                                 -2.27178\n",
      "trainer/Policy log std Mean                           -2.3443\n",
      "trainer/Policy log std Std                             0.335955\n",
      "trainer/Policy log std Max                            -0.608258\n",
      "trainer/Policy log std Min                            -4.65521\n",
      "trainer/Alpha                                          0.0108282\n",
      "trainer/Alpha Loss                                     1.86096\n",
      "exploration/num steps total                        91000\n",
      "exploration/num paths total                          151\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.821506\n",
      "exploration/Rewards Std                                0.0986769\n",
      "exploration/Rewards Max                                1.60919\n",
      "exploration/Rewards Min                               -0.560781\n",
      "exploration/Returns Mean                             821.506\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              821.506\n",
      "exploration/Returns Min                              821.506\n",
      "exploration/Actions Mean                              -0.00398939\n",
      "exploration/Actions Std                                0.215946\n",
      "exploration/Actions Max                                0.899726\n",
      "exploration/Actions Min                               -0.887398\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          821.506\n",
      "exploration/env_infos/final/reward_forward Mean        0.0149555\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0149555\n",
      "exploration/env_infos/final/reward_forward Min         0.0149555\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0757864\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0757864\n",
      "exploration/env_infos/initial/reward_forward Min       0.0757864\n",
      "exploration/env_infos/reward_forward Mean             -0.00721324\n",
      "exploration/env_infos/reward_forward Std               0.140381\n",
      "exploration/env_infos/reward_forward Max               0.736362\n",
      "exploration/env_infos/reward_forward Min              -1.13824\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.201848\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.201848\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.201848\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.13606\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.13606\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.13606\n",
      "exploration/env_infos/reward_ctrl Mean                -0.186595\n",
      "exploration/env_infos/reward_ctrl Std                  0.080451\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0501914\n",
      "exploration/env_infos/reward_ctrl Min                 -1.56078\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0159541\n",
      "exploration/env_infos/final/torso_velocity Std         0.041185\n",
      "exploration/env_infos/final/torso_velocity Max         0.0149555\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0741612\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.214877\n",
      "exploration/env_infos/initial/torso_velocity Std       0.205558\n",
      "exploration/env_infos/initial/torso_velocity Max       0.505491\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0633532\n",
      "exploration/env_infos/torso_velocity Mean             -0.00861309\n",
      "exploration/env_infos/torso_velocity Std               0.170904\n",
      "exploration/env_infos/torso_velocity Max               1.05908\n",
      "exploration/env_infos/torso_velocity Min              -1.43496\n",
      "evaluation/num steps total                             2.24518e+06\n",
      "evaluation/num paths total                          2250\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.781384\n",
      "evaluation/Rewards Std                                 0.121931\n",
      "evaluation/Rewards Max                                 1.73987\n",
      "evaluation/Rewards Min                                -2.24404\n",
      "evaluation/Returns Mean                              781.384\n",
      "evaluation/Returns Std                                73.443\n",
      "evaluation/Returns Max                               867.319\n",
      "evaluation/Returns Min                               583.901\n",
      "evaluation/Actions Mean                               -0.0271467\n",
      "evaluation/Actions Std                                 0.232677\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999871\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           781.384\n",
      "evaluation/env_infos/final/reward_forward Mean         7.91926e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.24589e-07\n",
      "evaluation/env_infos/final/reward_forward Max          4.37615e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -3.96141e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00992513\n",
      "evaluation/env_infos/initial/reward_forward Std        0.104501\n",
      "evaluation/env_infos/initial/reward_forward Max        0.153209\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.267876\n",
      "evaluation/env_infos/reward_forward Mean              -0.00110096\n",
      "evaluation/env_infos/reward_forward Std                0.0502966\n",
      "evaluation/env_infos/reward_forward Max                1.03649\n",
      "evaluation/env_infos/reward_forward Min               -1.1404\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.212381\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0731414\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.127387\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.413626\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.27683\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.203362\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0592764\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.834444\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.219502\n",
      "evaluation/env_infos/reward_ctrl Std                   0.120553\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.050038\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.24404\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.18788e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.79411e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.06588e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.9389e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123485\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.287253\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.708561\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.40384\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00129273\n",
      "evaluation/env_infos/torso_velocity Std                0.0720196\n",
      "evaluation/env_infos/torso_velocity Max                1.69499\n",
      "evaluation/env_infos/torso_velocity Min               -2.00538\n",
      "time/data storing (s)                                  0.0146512\n",
      "time/evaluation sampling (s)                          45.583\n",
      "time/exploration sampling (s)                          2.00763\n",
      "time/logging (s)                                       0.278768\n",
      "time/saving (s)                                        0.024444\n",
      "time/training (s)                                      4.39743\n",
      "time/epoch (s)                                        52.306\n",
      "time/total (s)                                      4846.85\n",
      "Epoch                                                 89\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:46:30.961866 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 90 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 92000\n",
      "trainer/QF1 Loss                                       1.54107\n",
      "trainer/QF2 Loss                                       0.591371\n",
      "trainer/Policy Loss                                  -22.3485\n",
      "trainer/Q1 Predictions Mean                           29.3536\n",
      "trainer/Q1 Predictions Std                             5.7951\n",
      "trainer/Q1 Predictions Max                           113.892\n",
      "trainer/Q1 Predictions Min                            16.8762\n",
      "trainer/Q2 Predictions Mean                           29.0201\n",
      "trainer/Q2 Predictions Std                             4.92032\n",
      "trainer/Q2 Predictions Max                            98.8583\n",
      "trainer/Q2 Predictions Min                            16.4872\n",
      "trainer/Q Targets Mean                                28.8449\n",
      "trainer/Q Targets Std                                  4.95018\n",
      "trainer/Q Targets Max                                 99.6334\n",
      "trainer/Q Targets Min                                 16.4005\n",
      "trainer/Log Pis Mean                                   6.90044\n",
      "trainer/Log Pis Std                                    3.24607\n",
      "trainer/Log Pis Max                                   38.512\n",
      "trainer/Log Pis Min                                   -1.55591\n",
      "trainer/Policy mu Mean                                -0.0235394\n",
      "trainer/Policy mu Std                                  0.354396\n",
      "trainer/Policy mu Max                                  6.39866\n",
      "trainer/Policy mu Min                                 -2.47257\n",
      "trainer/Policy log std Mean                           -2.21773\n",
      "trainer/Policy log std Std                             0.293403\n",
      "trainer/Policy log std Max                            -0.128565\n",
      "trainer/Policy log std Min                            -5.08134\n",
      "trainer/Alpha                                          0.0101192\n",
      "trainer/Alpha Loss                                    -5.04798\n",
      "exploration/num steps total                        92000\n",
      "exploration/num paths total                          152\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.786901\n",
      "exploration/Rewards Std                                0.241809\n",
      "exploration/Rewards Max                                1.73943\n",
      "exploration/Rewards Min                               -0.689134\n",
      "exploration/Returns Mean                             786.901\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              786.901\n",
      "exploration/Returns Min                              786.901\n",
      "exploration/Actions Mean                              -0.0205873\n",
      "exploration/Actions Std                                0.247098\n",
      "exploration/Actions Max                                0.972708\n",
      "exploration/Actions Min                               -0.882464\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          786.901\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0195152\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0195152\n",
      "exploration/env_infos/final/reward_forward Min        -0.0195152\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0100247\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0100247\n",
      "exploration/env_infos/initial/reward_forward Min       0.0100247\n",
      "exploration/env_infos/reward_forward Mean              0.0306667\n",
      "exploration/env_infos/reward_forward Std               0.340615\n",
      "exploration/env_infos/reward_forward Max               1.10648\n",
      "exploration/env_infos/reward_forward Min              -0.89184\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.444101\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.444101\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.444101\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.150535\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.150535\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.150535\n",
      "exploration/env_infos/reward_ctrl Mean                -0.245926\n",
      "exploration/env_infos/reward_ctrl Std                  0.201722\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0138833\n",
      "exploration/env_infos/reward_ctrl Min                 -1.68913\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.016109\n",
      "exploration/env_infos/final/torso_velocity Std         0.433089\n",
      "exploration/env_infos/final/torso_velocity Max         0.516009\n",
      "exploration/env_infos/final/torso_velocity Min        -0.544821\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.164491\n",
      "exploration/env_infos/initial/torso_velocity Std       0.301507\n",
      "exploration/env_infos/initial/torso_velocity Max       0.585911\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.102464\n",
      "exploration/env_infos/torso_velocity Mean             -0.00118973\n",
      "exploration/env_infos/torso_velocity Std               0.318179\n",
      "exploration/env_infos/torso_velocity Max               1.24398\n",
      "exploration/env_infos/torso_velocity Min              -2.07291\n",
      "evaluation/num steps total                             2.2692e+06\n",
      "evaluation/num paths total                          2275\n",
      "evaluation/path length Mean                          960.84\n",
      "evaluation/path length Std                           191.844\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            21\n",
      "evaluation/Rewards Mean                                0.856832\n",
      "evaluation/Rewards Std                                 0.0952552\n",
      "evaluation/Rewards Max                                 1.93281\n",
      "evaluation/Rewards Min                                -1.93225\n",
      "evaluation/Returns Mean                              823.278\n",
      "evaluation/Returns Std                               171.131\n",
      "evaluation/Returns Max                               906.787\n",
      "evaluation/Returns Min                                 9.32938\n",
      "evaluation/Actions Mean                               -0.00959734\n",
      "evaluation/Actions Std                                 0.190926\n",
      "evaluation/Actions Max                                 0.996815\n",
      "evaluation/Actions Min                                -0.990685\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           823.278\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.00260073\n",
      "evaluation/env_infos/final/reward_forward Std          0.0542666\n",
      "evaluation/env_infos/final/reward_forward Max          0.143954\n",
      "evaluation/env_infos/final/reward_forward Min         -0.229454\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0375993\n",
      "evaluation/env_infos/initial/reward_forward Std        0.108598\n",
      "evaluation/env_infos/initial/reward_forward Max        0.189906\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.269487\n",
      "evaluation/env_infos/reward_forward Mean              -0.0046482\n",
      "evaluation/env_infos/reward_forward Std                0.117391\n",
      "evaluation/env_infos/reward_forward Max                1.32209\n",
      "evaluation/env_infos/reward_forward Min               -1.63057\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.1413\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0475767\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0790861\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.269558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.131592\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.121229\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.045889\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.534896\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.146179\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0910977\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.025855\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.93225\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.0112264\n",
      "evaluation/env_infos/final/torso_velocity Std          0.148185\n",
      "evaluation/env_infos/final/torso_velocity Max          0.919517\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.676788\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.122652\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.247505\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.60692\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.460442\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00505769\n",
      "evaluation/env_infos/torso_velocity Std                0.134537\n",
      "evaluation/env_infos/torso_velocity Max                2.57226\n",
      "evaluation/env_infos/torso_velocity Min               -1.82684\n",
      "time/data storing (s)                                  0.0140181\n",
      "time/evaluation sampling (s)                          45.7778\n",
      "time/exploration sampling (s)                          1.90404\n",
      "time/logging (s)                                       0.269488\n",
      "time/saving (s)                                        0.0271004\n",
      "time/training (s)                                      4.29829\n",
      "time/epoch (s)                                        52.2907\n",
      "time/total (s)                                      4899.9\n",
      "Epoch                                                 90\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:47:25.371703 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 91 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 93000\n",
      "trainer/QF1 Loss                                       0.602789\n",
      "trainer/QF2 Loss                                       0.567931\n",
      "trainer/Policy Loss                                  -21.2358\n",
      "trainer/Q1 Predictions Mean                           29.0848\n",
      "trainer/Q1 Predictions Std                             1.63833\n",
      "trainer/Q1 Predictions Max                            32.2953\n",
      "trainer/Q1 Predictions Min                            18.828\n",
      "trainer/Q2 Predictions Mean                           28.9095\n",
      "trainer/Q2 Predictions Std                             1.65638\n",
      "trainer/Q2 Predictions Max                            31.748\n",
      "trainer/Q2 Predictions Min                            19.9482\n",
      "trainer/Q Targets Mean                                28.8666\n",
      "trainer/Q Targets Std                                  1.67819\n",
      "trainer/Q Targets Max                                 32.0127\n",
      "trainer/Q Targets Min                                 18.9197\n",
      "trainer/Log Pis Mean                                   8.01031\n",
      "trainer/Log Pis Std                                    3.23379\n",
      "trainer/Log Pis Max                                   29.4098\n",
      "trainer/Log Pis Min                                   -2.26766\n",
      "trainer/Policy mu Mean                                -0.0656548\n",
      "trainer/Policy mu Std                                  0.434023\n",
      "trainer/Policy mu Max                                  2.742\n",
      "trainer/Policy mu Min                                 -6.24217\n",
      "trainer/Policy log std Mean                           -2.24702\n",
      "trainer/Policy log std Std                             0.259042\n",
      "trainer/Policy log std Max                            -0.859734\n",
      "trainer/Policy log std Min                            -4.07156\n",
      "trainer/Alpha                                          0.0101363\n",
      "trainer/Alpha Loss                                     0.0473565\n",
      "exploration/num steps total                        93000\n",
      "exploration/num paths total                          153\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.886009\n",
      "exploration/Rewards Std                                0.0982745\n",
      "exploration/Rewards Max                                1.52701\n",
      "exploration/Rewards Min                               -0.143722\n",
      "exploration/Returns Mean                             886.009\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              886.009\n",
      "exploration/Returns Min                              886.009\n",
      "exploration/Actions Mean                              -0.0314028\n",
      "exploration/Actions Std                                0.177626\n",
      "exploration/Actions Max                                0.866426\n",
      "exploration/Actions Min                               -0.644252\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          886.009\n",
      "exploration/env_infos/final/reward_forward Mean        0.316496\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.316496\n",
      "exploration/env_infos/final/reward_forward Min         0.316496\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0594194\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0594194\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0594194\n",
      "exploration/env_infos/reward_forward Mean             -0.00665836\n",
      "exploration/env_infos/reward_forward Std               0.23826\n",
      "exploration/env_infos/reward_forward Max               1.28932\n",
      "exploration/env_infos/reward_forward Min              -1.04568\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.134658\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.134658\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.134658\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.154242\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.154242\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.154242\n",
      "exploration/env_infos/reward_ctrl Mean                -0.130149\n",
      "exploration/env_infos/reward_ctrl Std                  0.0726604\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0118122\n",
      "exploration/env_infos/reward_ctrl Min                 -1.14372\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.158766\n",
      "exploration/env_infos/final/torso_velocity Std         0.352124\n",
      "exploration/env_infos/final/torso_velocity Max         0.316496\n",
      "exploration/env_infos/final/torso_velocity Min        -0.525167\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.101861\n",
      "exploration/env_infos/initial/torso_velocity Std       0.234114\n",
      "exploration/env_infos/initial/torso_velocity Max       0.432913\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.06791\n",
      "exploration/env_infos/torso_velocity Mean             -0.00742126\n",
      "exploration/env_infos/torso_velocity Std               0.217758\n",
      "exploration/env_infos/torso_velocity Max               1.28932\n",
      "exploration/env_infos/torso_velocity Min              -1.53158\n",
      "evaluation/num steps total                             2.2942e+06\n",
      "evaluation/num paths total                          2300\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.632975\n",
      "evaluation/Rewards Std                                 0.201597\n",
      "evaluation/Rewards Max                                 2.01419\n",
      "evaluation/Rewards Min                                -0.828457\n",
      "evaluation/Returns Mean                              632.975\n",
      "evaluation/Returns Std                               196.932\n",
      "evaluation/Returns Max                               914.93\n",
      "evaluation/Returns Min                               216.068\n",
      "evaluation/Actions Mean                               -0.0718831\n",
      "evaluation/Actions Std                                 0.294626\n",
      "evaluation/Actions Max                                 0.993013\n",
      "evaluation/Actions Min                                -0.996126\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           632.975\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.46215e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.44182e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.86609e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -7.63422e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00635946\n",
      "evaluation/env_infos/initial/reward_forward Std        0.104832\n",
      "evaluation/env_infos/initial/reward_forward Max        0.248719\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.14793\n",
      "evaluation/env_infos/reward_forward Mean              -0.000892535\n",
      "evaluation/env_infos/reward_forward Std                0.0618431\n",
      "evaluation/env_infos/reward_forward Max                1.37177\n",
      "evaluation/env_infos/reward_forward Min               -1.72769\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.36556\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.197709\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0788367\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.784146\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.368507\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.186539\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0813926\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.699166\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.367888\n",
      "evaluation/env_infos/reward_ctrl Std                   0.200385\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0370718\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.82846\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.3688e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.67181e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          2.12873e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -7.63422e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0955491\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.270665\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.58127\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.377606\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00213826\n",
      "evaluation/env_infos/torso_velocity Std                0.0638514\n",
      "evaluation/env_infos/torso_velocity Max                1.37177\n",
      "evaluation/env_infos/torso_velocity Min               -1.73281\n",
      "time/data storing (s)                                  0.0154102\n",
      "time/evaluation sampling (s)                          46.5825\n",
      "time/exploration sampling (s)                          2.08175\n",
      "time/logging (s)                                       0.275476\n",
      "time/saving (s)                                        0.028116\n",
      "time/training (s)                                      4.64714\n",
      "time/epoch (s)                                        53.6304\n",
      "time/total (s)                                      4954.31\n",
      "Epoch                                                 91\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:48:21.975232 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 92 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 94000\n",
      "trainer/QF1 Loss                                       1.01578\n",
      "trainer/QF2 Loss                                       1.06524\n",
      "trainer/Policy Loss                                  -21.4514\n",
      "trainer/Q1 Predictions Mean                           29.2001\n",
      "trainer/Q1 Predictions Std                             2.81031\n",
      "trainer/Q1 Predictions Max                            39.3994\n",
      "trainer/Q1 Predictions Min                            -0.280394\n",
      "trainer/Q2 Predictions Mean                           29.0521\n",
      "trainer/Q2 Predictions Std                             2.79149\n",
      "trainer/Q2 Predictions Max                            39.3764\n",
      "trainer/Q2 Predictions Min                             0.626943\n",
      "trainer/Q Targets Mean                                29.0772\n",
      "trainer/Q Targets Std                                  2.93015\n",
      "trainer/Q Targets Max                                 41.7796\n",
      "trainer/Q Targets Min                                 -0.674402\n",
      "trainer/Log Pis Mean                                   7.84155\n",
      "trainer/Log Pis Std                                    2.9118\n",
      "trainer/Log Pis Max                                   27.5627\n",
      "trainer/Log Pis Min                                    0.757397\n",
      "trainer/Policy mu Mean                                -0.100468\n",
      "trainer/Policy mu Std                                  0.386022\n",
      "trainer/Policy mu Max                                  3.68311\n",
      "trainer/Policy mu Min                                 -2.50421\n",
      "trainer/Policy log std Mean                           -2.25663\n",
      "trainer/Policy log std Std                             0.305278\n",
      "trainer/Policy log std Max                            -1.01541\n",
      "trainer/Policy log std Min                            -6.89295\n",
      "trainer/Alpha                                          0.00895664\n",
      "trainer/Alpha Loss                                    -0.746959\n",
      "exploration/num steps total                        94000\n",
      "exploration/num paths total                          154\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.0230594\n",
      "exploration/Rewards Std                                0.245758\n",
      "exploration/Rewards Max                                1.10824\n",
      "exploration/Rewards Min                               -1.39537\n",
      "exploration/Returns Mean                             -23.0594\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              -23.0594\n",
      "exploration/Returns Min                              -23.0594\n",
      "exploration/Actions Mean                              -0.2862\n",
      "exploration/Actions Std                                0.417546\n",
      "exploration/Actions Max                                0.999975\n",
      "exploration/Actions Min                               -0.994211\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          -23.0594\n",
      "exploration/env_infos/final/reward_forward Mean        0.00198386\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.00198386\n",
      "exploration/env_infos/final/reward_forward Min         0.00198386\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0438191\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0438191\n",
      "exploration/env_infos/initial/reward_forward Min       0.0438191\n",
      "exploration/env_infos/reward_forward Mean              0.0363306\n",
      "exploration/env_infos/reward_forward Std               0.212318\n",
      "exploration/env_infos/reward_forward Max               1.88692\n",
      "exploration/env_infos/reward_forward Min              -0.741489\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.811722\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.811722\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.811722\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.042749\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.042749\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.042749\n",
      "exploration/env_infos/reward_ctrl Mean                -1.02502\n",
      "exploration/env_infos/reward_ctrl Std                  0.242648\n",
      "exploration/env_infos/reward_ctrl Max                 -0.042749\n",
      "exploration/env_infos/reward_ctrl Min                 -2.39537\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00404231\n",
      "exploration/env_infos/final/torso_velocity Std         0.00730373\n",
      "exploration/env_infos/final/torso_velocity Max         0.00198386\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0143204\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.129532\n",
      "exploration/env_infos/initial/torso_velocity Std       0.239208\n",
      "exploration/env_infos/initial/torso_velocity Max       0.455797\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.11102\n",
      "exploration/env_infos/torso_velocity Mean              0.0112953\n",
      "exploration/env_infos/torso_velocity Std               0.160325\n",
      "exploration/env_infos/torso_velocity Max               2.07638\n",
      "exploration/env_infos/torso_velocity Min              -1.35437\n",
      "evaluation/num steps total                             2.3192e+06\n",
      "evaluation/num paths total                          2325\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.68059\n",
      "evaluation/Rewards Std                                 0.1532\n",
      "evaluation/Rewards Max                                 2.25478\n",
      "evaluation/Rewards Min                                -1.20863\n",
      "evaluation/Returns Mean                              680.59\n",
      "evaluation/Returns Std                                60.1556\n",
      "evaluation/Returns Max                               791.359\n",
      "evaluation/Returns Min                               579.193\n",
      "evaluation/Actions Mean                               -0.0504853\n",
      "evaluation/Actions Std                                 0.283493\n",
      "evaluation/Actions Max                                 0.998525\n",
      "evaluation/Actions Min                                -0.993574\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           680.59\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0310406\n",
      "evaluation/env_infos/final/reward_forward Std          0.158816\n",
      "evaluation/env_infos/final/reward_forward Max          0.673218\n",
      "evaluation/env_infos/final/reward_forward Min         -0.189624\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00255596\n",
      "evaluation/env_infos/initial/reward_forward Std        0.103781\n",
      "evaluation/env_infos/initial/reward_forward Max        0.223271\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.132611\n",
      "evaluation/env_infos/reward_forward Mean               0.00295387\n",
      "evaluation/env_infos/reward_forward Std                0.168275\n",
      "evaluation/env_infos/reward_forward Max                1.55053\n",
      "evaluation/env_infos/reward_forward Min               -1.29931\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.327392\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0740468\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.204236\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.479393\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.289155\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.175582\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0445635\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.654988\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.331667\n",
      "evaluation/env_infos/reward_ctrl Std                   0.147994\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0385973\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.20863\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00352916\n",
      "evaluation/env_infos/final/torso_velocity Std          0.131159\n",
      "evaluation/env_infos/final/torso_velocity Max          0.673218\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.444174\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.126718\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.27888\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.672092\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.353529\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00418508\n",
      "evaluation/env_infos/torso_velocity Std                0.190817\n",
      "evaluation/env_infos/torso_velocity Max                2.59362\n",
      "evaluation/env_infos/torso_velocity Min               -1.90975\n",
      "time/data storing (s)                                  0.0161741\n",
      "time/evaluation sampling (s)                          48.4077\n",
      "time/exploration sampling (s)                          2.16624\n",
      "time/logging (s)                                       0.282215\n",
      "time/saving (s)                                        0.0286042\n",
      "time/training (s)                                      4.88258\n",
      "time/epoch (s)                                        55.7835\n",
      "time/total (s)                                      5010.92\n",
      "Epoch                                                 92\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:49:17.641282 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 93 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 95000\n",
      "trainer/QF1 Loss                                       0.891489\n",
      "trainer/QF2 Loss                                       1.37655\n",
      "trainer/Policy Loss                                  -20.1494\n",
      "trainer/Q1 Predictions Mean                           28.9672\n",
      "trainer/Q1 Predictions Std                             2.56322\n",
      "trainer/Q1 Predictions Max                            33.6816\n",
      "trainer/Q1 Predictions Min                            17.5005\n",
      "trainer/Q2 Predictions Mean                           28.3928\n",
      "trainer/Q2 Predictions Std                             2.47831\n",
      "trainer/Q2 Predictions Max                            35.1689\n",
      "trainer/Q2 Predictions Min                            17.0617\n",
      "trainer/Q Targets Mean                                29.0147\n",
      "trainer/Q Targets Std                                  2.76263\n",
      "trainer/Q Targets Max                                 35.6079\n",
      "trainer/Q Targets Min                                 15.3724\n",
      "trainer/Log Pis Mean                                   8.76682\n",
      "trainer/Log Pis Std                                    3.04911\n",
      "trainer/Log Pis Max                                   28.3209\n",
      "trainer/Log Pis Min                                   -1.92922\n",
      "trainer/Policy mu Mean                                -0.184065\n",
      "trainer/Policy mu Std                                  0.433694\n",
      "trainer/Policy mu Max                                  2.64648\n",
      "trainer/Policy mu Min                                 -3.45137\n",
      "trainer/Policy log std Mean                           -2.30958\n",
      "trainer/Policy log std Std                             0.31108\n",
      "trainer/Policy log std Max                            -0.733391\n",
      "trainer/Policy log std Min                            -4.99282\n",
      "trainer/Alpha                                          0.0105525\n",
      "trainer/Alpha Loss                                     3.49173\n",
      "exploration/num steps total                        95000\n",
      "exploration/num paths total                          155\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.710645\n",
      "exploration/Rewards Std                                0.169326\n",
      "exploration/Rewards Max                                1.14015\n",
      "exploration/Rewards Min                               -1.44446\n",
      "exploration/Returns Mean                             710.645\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              710.645\n",
      "exploration/Returns Min                              710.645\n",
      "exploration/Actions Mean                              -0.0651003\n",
      "exploration/Actions Std                                0.266315\n",
      "exploration/Actions Max                                0.995015\n",
      "exploration/Actions Min                               -0.957834\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          710.645\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0160621\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0160621\n",
      "exploration/env_infos/final/reward_forward Min        -0.0160621\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0159954\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0159954\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0159954\n",
      "exploration/env_infos/reward_forward Mean              0.03515\n",
      "exploration/env_infos/reward_forward Std               0.153679\n",
      "exploration/env_infos/reward_forward Max               0.832047\n",
      "exploration/env_infos/reward_forward Min              -0.822881\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.317873\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.317873\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.317873\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.483068\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.483068\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.483068\n",
      "exploration/env_infos/reward_ctrl Mean                -0.300648\n",
      "exploration/env_infos/reward_ctrl Std                  0.164701\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0420492\n",
      "exploration/env_infos/reward_ctrl Min                 -2.44446\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00481023\n",
      "exploration/env_infos/final/torso_velocity Std         0.0113238\n",
      "exploration/env_infos/final/torso_velocity Max         0.0106844\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0160621\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0651917\n",
      "exploration/env_infos/initial/torso_velocity Std       0.354222\n",
      "exploration/env_infos/initial/torso_velocity Max       0.533881\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.322311\n",
      "exploration/env_infos/torso_velocity Mean              0.00988977\n",
      "exploration/env_infos/torso_velocity Std               0.172341\n",
      "exploration/env_infos/torso_velocity Max               1.05095\n",
      "exploration/env_infos/torso_velocity Min              -1.14096\n",
      "evaluation/num steps total                             2.3442e+06\n",
      "evaluation/num paths total                          2350\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.474951\n",
      "evaluation/Rewards Std                                 0.221698\n",
      "evaluation/Rewards Max                                 1.81814\n",
      "evaluation/Rewards Min                                -1.65288\n",
      "evaluation/Returns Mean                              474.951\n",
      "evaluation/Returns Std                               207.729\n",
      "evaluation/Returns Max                               773.665\n",
      "evaluation/Returns Min                               118.606\n",
      "evaluation/Actions Mean                               -0.119195\n",
      "evaluation/Actions Std                                 0.342676\n",
      "evaluation/Actions Max                                 0.999425\n",
      "evaluation/Actions Min                                -0.999809\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           474.951\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000648862\n",
      "evaluation/env_infos/final/reward_forward Std          0.00323426\n",
      "evaluation/env_infos/final/reward_forward Max          0.0164913\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000268574\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00961335\n",
      "evaluation/env_infos/initial/reward_forward Std        0.108273\n",
      "evaluation/env_infos/initial/reward_forward Max        0.198915\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.182783\n",
      "evaluation/env_infos/reward_forward Mean               0.00179083\n",
      "evaluation/env_infos/reward_forward Std                0.0620848\n",
      "evaluation/env_infos/reward_forward Max                1.31125\n",
      "evaluation/env_infos/reward_forward Min               -1.32626\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.525144\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.211475\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.207667\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.886399\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.430994\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.285956\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0985517\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.18389\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.526538\n",
      "evaluation/env_infos/reward_ctrl Std                   0.219994\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.089377\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.65288\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.000121146\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0024704\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0164913\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00946831\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0996824\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.275234\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.620789\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.492842\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00126317\n",
      "evaluation/env_infos/torso_velocity Std                0.0730744\n",
      "evaluation/env_infos/torso_velocity Max                1.82444\n",
      "evaluation/env_infos/torso_velocity Min               -2.15231\n",
      "time/data storing (s)                                  0.0154275\n",
      "time/evaluation sampling (s)                          47.8429\n",
      "time/exploration sampling (s)                          2.24731\n",
      "time/logging (s)                                       0.273255\n",
      "time/saving (s)                                        0.0255553\n",
      "time/training (s)                                      4.38757\n",
      "time/epoch (s)                                        54.7921\n",
      "time/total (s)                                      5066.57\n",
      "Epoch                                                 93\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:50:11.961481 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 94 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 96000\n",
      "trainer/QF1 Loss                                       0.774962\n",
      "trainer/QF2 Loss                                       0.667552\n",
      "trainer/Policy Loss                                  -21.8476\n",
      "trainer/Q1 Predictions Mean                           29.1846\n",
      "trainer/Q1 Predictions Std                             2.59562\n",
      "trainer/Q1 Predictions Max                            33.7538\n",
      "trainer/Q1 Predictions Min                            17.0957\n",
      "trainer/Q2 Predictions Mean                           29.3575\n",
      "trainer/Q2 Predictions Std                             2.57097\n",
      "trainer/Q2 Predictions Max                            32.5126\n",
      "trainer/Q2 Predictions Min                            16.8723\n",
      "trainer/Q Targets Mean                                29.3238\n",
      "trainer/Q Targets Std                                  2.64266\n",
      "trainer/Q Targets Max                                 33.6072\n",
      "trainer/Q Targets Min                                 16.9426\n",
      "trainer/Log Pis Mean                                   7.66072\n",
      "trainer/Log Pis Std                                    2.62204\n",
      "trainer/Log Pis Max                                   24.9682\n",
      "trainer/Log Pis Min                                    0.108491\n",
      "trainer/Policy mu Mean                                -0.134095\n",
      "trainer/Policy mu Std                                  0.457135\n",
      "trainer/Policy mu Max                                  1.68344\n",
      "trainer/Policy mu Min                                 -4.63972\n",
      "trainer/Policy log std Mean                           -2.16138\n",
      "trainer/Policy log std Std                             0.3481\n",
      "trainer/Policy log std Max                             0.970707\n",
      "trainer/Policy log std Min                            -4.49827\n",
      "trainer/Alpha                                          0.0123954\n",
      "trainer/Alpha Loss                                    -1.48953\n",
      "exploration/num steps total                        96000\n",
      "exploration/num paths total                          156\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.192032\n",
      "exploration/Rewards Std                                0.158835\n",
      "exploration/Rewards Max                                0.861359\n",
      "exploration/Rewards Min                               -1.37304\n",
      "exploration/Returns Mean                             192.032\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              192.032\n",
      "exploration/Returns Min                              192.032\n",
      "exploration/Actions Mean                              -0.164211\n",
      "exploration/Actions Std                                0.41856\n",
      "exploration/Actions Max                                0.920519\n",
      "exploration/Actions Min                               -0.947015\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          192.032\n",
      "exploration/env_infos/final/reward_forward Mean        0.0355479\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0355479\n",
      "exploration/env_infos/final/reward_forward Min         0.0355479\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0514201\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0514201\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0514201\n",
      "exploration/env_infos/reward_forward Mean              0.0170166\n",
      "exploration/env_infos/reward_forward Std               0.14404\n",
      "exploration/env_infos/reward_forward Max               1.61562\n",
      "exploration/env_infos/reward_forward Min              -0.634362\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.695288\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.695288\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.695288\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.621088\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.621088\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.621088\n",
      "exploration/env_infos/reward_ctrl Mean                -0.808629\n",
      "exploration/env_infos/reward_ctrl Std                  0.157865\n",
      "exploration/env_infos/reward_ctrl Max                 -0.138641\n",
      "exploration/env_infos/reward_ctrl Min                 -2.37304\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0607129\n",
      "exploration/env_infos/final/torso_velocity Std         0.0559343\n",
      "exploration/env_infos/final/torso_velocity Max         0.138242\n",
      "exploration/env_infos/final/torso_velocity Min         0.00834915\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0117257\n",
      "exploration/env_infos/initial/torso_velocity Std       0.317675\n",
      "exploration/env_infos/initial/torso_velocity Max       0.395671\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.379428\n",
      "exploration/env_infos/torso_velocity Mean              0.00694143\n",
      "exploration/env_infos/torso_velocity Std               0.12768\n",
      "exploration/env_infos/torso_velocity Max               1.61562\n",
      "exploration/env_infos/torso_velocity Min              -1.33029\n",
      "evaluation/num steps total                             2.3692e+06\n",
      "evaluation/num paths total                          2375\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.431685\n",
      "evaluation/Rewards Std                                 0.252808\n",
      "evaluation/Rewards Max                                 1.48019\n",
      "evaluation/Rewards Min                                -1.92678\n",
      "evaluation/Returns Mean                              431.685\n",
      "evaluation/Returns Std                               230.769\n",
      "evaluation/Returns Max                               773.908\n",
      "evaluation/Returns Min                                50.6304\n",
      "evaluation/Actions Mean                               -0.114434\n",
      "evaluation/Actions Std                                 0.36071\n",
      "evaluation/Actions Max                                 0.973278\n",
      "evaluation/Actions Min                                -0.948255\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           431.685\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0240443\n",
      "evaluation/env_infos/final/reward_forward Std          0.0850254\n",
      "evaluation/env_infos/final/reward_forward Max          0.385759\n",
      "evaluation/env_infos/final/reward_forward Min         -8.38823e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0420894\n",
      "evaluation/env_infos/initial/reward_forward Std        0.187342\n",
      "evaluation/env_infos/initial/reward_forward Max        0.400886\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.395672\n",
      "evaluation/env_infos/reward_forward Mean               0.00350526\n",
      "evaluation/env_infos/reward_forward Std                0.0830429\n",
      "evaluation/env_infos/reward_forward Max                1.3917\n",
      "evaluation/env_infos/reward_forward Min               -1.97149\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.592988\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.237295\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.220265\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.953555\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.563615\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.323061\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.070735\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -1.16442\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.572826\n",
      "evaluation/env_infos/reward_ctrl Std                   0.246889\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0694169\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.92678\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00443683\n",
      "evaluation/env_infos/final/torso_velocity Std          0.0652023\n",
      "evaluation/env_infos/final/torso_velocity Max          0.385759\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.23969\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.0847442\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.305012\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.665261\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.529152\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000227292\n",
      "evaluation/env_infos/torso_velocity Std                0.079779\n",
      "evaluation/env_infos/torso_velocity Max                1.58272\n",
      "evaluation/env_infos/torso_velocity Min               -1.97149\n",
      "time/data storing (s)                                  0.0155884\n",
      "time/evaluation sampling (s)                          46.2237\n",
      "time/exploration sampling (s)                          2.44084\n",
      "time/logging (s)                                       0.27193\n",
      "time/saving (s)                                        0.0276223\n",
      "time/training (s)                                      4.54316\n",
      "time/epoch (s)                                        53.5229\n",
      "time/total (s)                                      5120.89\n",
      "Epoch                                                 94\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:51:05.781400 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 95 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 97000\n",
      "trainer/QF1 Loss                                       0.442707\n",
      "trainer/QF2 Loss                                       0.481544\n",
      "trainer/Policy Loss                                  -22.2195\n",
      "trainer/Q1 Predictions Mean                           29.8155\n",
      "trainer/Q1 Predictions Std                             2.42681\n",
      "trainer/Q1 Predictions Max                            33.3951\n",
      "trainer/Q1 Predictions Min                            16.1632\n",
      "trainer/Q2 Predictions Mean                           29.7618\n",
      "trainer/Q2 Predictions Std                             2.42966\n",
      "trainer/Q2 Predictions Max                            32.8384\n",
      "trainer/Q2 Predictions Min                            17.0635\n",
      "trainer/Q Targets Mean                                29.7483\n",
      "trainer/Q Targets Std                                  2.29401\n",
      "trainer/Q Targets Max                                 33.3324\n",
      "trainer/Q Targets Min                                 17.3075\n",
      "trainer/Log Pis Mean                                   7.85096\n",
      "trainer/Log Pis Std                                    2.64832\n",
      "trainer/Log Pis Max                                   22.3352\n",
      "trainer/Log Pis Min                                   -2.75627\n",
      "trainer/Policy mu Mean                                -0.104245\n",
      "trainer/Policy mu Std                                  0.421516\n",
      "trainer/Policy mu Max                                  3.25085\n",
      "trainer/Policy mu Min                                 -3.39361\n",
      "trainer/Policy log std Mean                           -2.22718\n",
      "trainer/Policy log std Std                             0.330307\n",
      "trainer/Policy log std Max                            -0.736335\n",
      "trainer/Policy log std Min                            -4.06923\n",
      "trainer/Alpha                                          0.0111254\n",
      "trainer/Alpha Loss                                    -0.67053\n",
      "exploration/num steps total                        97000\n",
      "exploration/num paths total                          157\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.473701\n",
      "exploration/Rewards Std                                0.14204\n",
      "exploration/Rewards Max                                1.3661\n",
      "exploration/Rewards Min                               -0.693816\n",
      "exploration/Returns Mean                             473.701\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              473.701\n",
      "exploration/Returns Min                              473.701\n",
      "exploration/Actions Mean                              -0.124121\n",
      "exploration/Actions Std                                0.347376\n",
      "exploration/Actions Max                                0.877297\n",
      "exploration/Actions Min                               -0.972604\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          473.701\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0435584\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0435584\n",
      "exploration/env_infos/final/reward_forward Min        -0.0435584\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00698313\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00698313\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00698313\n",
      "exploration/env_infos/reward_forward Mean              0.00597306\n",
      "exploration/env_infos/reward_forward Std               0.0592959\n",
      "exploration/env_infos/reward_forward Max               1.19464\n",
      "exploration/env_infos/reward_forward Min              -0.524755\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.533033\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.533033\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.533033\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.948412\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.948412\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.948412\n",
      "exploration/env_infos/reward_ctrl Mean                -0.544305\n",
      "exploration/env_infos/reward_ctrl Std                  0.127334\n",
      "exploration/env_infos/reward_ctrl Max                 -0.196307\n",
      "exploration/env_infos/reward_ctrl Min                 -1.69382\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0362465\n",
      "exploration/env_infos/final/torso_velocity Std         0.0723103\n",
      "exploration/env_infos/final/torso_velocity Max         0.131525\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0435584\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.01454\n",
      "exploration/env_infos/initial/torso_velocity Std       0.272418\n",
      "exploration/env_infos/initial/torso_velocity Max       0.315259\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.351896\n",
      "exploration/env_infos/torso_velocity Mean              0.00219856\n",
      "exploration/env_infos/torso_velocity Std               0.0966927\n",
      "exploration/env_infos/torso_velocity Max               1.19464\n",
      "exploration/env_infos/torso_velocity Min              -1.47236\n",
      "evaluation/num steps total                             2.3942e+06\n",
      "evaluation/num paths total                          2400\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.742894\n",
      "evaluation/Rewards Std                                 0.239819\n",
      "evaluation/Rewards Max                                 2.79919\n",
      "evaluation/Rewards Min                                -1.68424\n",
      "evaluation/Returns Mean                              742.894\n",
      "evaluation/Returns Std                               175.664\n",
      "evaluation/Returns Max                               926.091\n",
      "evaluation/Returns Min                               387.053\n",
      "evaluation/Actions Mean                               -0.116569\n",
      "evaluation/Actions Std                                 0.231286\n",
      "evaluation/Actions Max                                 0.999954\n",
      "evaluation/Actions Min                                -0.999846\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           742.894\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0113558\n",
      "evaluation/env_infos/final/reward_forward Std          0.115224\n",
      "evaluation/env_infos/final/reward_forward Max          0.254755\n",
      "evaluation/env_infos/final/reward_forward Min         -0.414484\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0668442\n",
      "evaluation/env_infos/initial/reward_forward Std        0.149172\n",
      "evaluation/env_infos/initial/reward_forward Max        0.236117\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.333188\n",
      "evaluation/env_infos/reward_forward Mean               0.0134649\n",
      "evaluation/env_infos/reward_forward Std                0.249596\n",
      "evaluation/env_infos/reward_forward Max                1.72989\n",
      "evaluation/env_infos/reward_forward Min               -1.69436\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.228509\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.184507\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0489185\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.613061\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.466357\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.289702\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.029555\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.898116\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.268326\n",
      "evaluation/env_infos/reward_ctrl Std                   0.233666\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0254046\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.68424\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00722698\n",
      "evaluation/env_infos/final/torso_velocity Std          0.130631\n",
      "evaluation/env_infos/final/torso_velocity Max          0.280521\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.819049\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.107146\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.298755\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.689854\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.333188\n",
      "evaluation/env_infos/torso_velocity Mean               0.00182048\n",
      "evaluation/env_infos/torso_velocity Std                0.224\n",
      "evaluation/env_infos/torso_velocity Max                2.45568\n",
      "evaluation/env_infos/torso_velocity Min               -2.09067\n",
      "time/data storing (s)                                  0.0147786\n",
      "time/evaluation sampling (s)                          45.9878\n",
      "time/exploration sampling (s)                          2.04343\n",
      "time/logging (s)                                       0.271172\n",
      "time/saving (s)                                        0.0272194\n",
      "time/training (s)                                      4.61339\n",
      "time/epoch (s)                                        52.9578\n",
      "time/total (s)                                      5174.71\n",
      "Epoch                                                 95\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:51:59.962793 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 96 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 98000\n",
      "trainer/QF1 Loss                                       0.366334\n",
      "trainer/QF2 Loss                                       0.342818\n",
      "trainer/Policy Loss                                  -22.7056\n",
      "trainer/Q1 Predictions Mean                           30.0208\n",
      "trainer/Q1 Predictions Std                             2.50225\n",
      "trainer/Q1 Predictions Max                            33.8971\n",
      "trainer/Q1 Predictions Min                            17.0381\n",
      "trainer/Q2 Predictions Mean                           30.0065\n",
      "trainer/Q2 Predictions Std                             2.43193\n",
      "trainer/Q2 Predictions Max                            33.856\n",
      "trainer/Q2 Predictions Min                            17.9606\n",
      "trainer/Q Targets Mean                                30.0169\n",
      "trainer/Q Targets Std                                  2.47302\n",
      "trainer/Q Targets Max                                 34.5818\n",
      "trainer/Q Targets Min                                 17.3812\n",
      "trainer/Log Pis Mean                                   7.41925\n",
      "trainer/Log Pis Std                                    2.17612\n",
      "trainer/Log Pis Max                                   12.8846\n",
      "trainer/Log Pis Min                                    0.439477\n",
      "trainer/Policy mu Mean                                 0.0372214\n",
      "trainer/Policy mu Std                                  0.318119\n",
      "trainer/Policy mu Max                                  1.78572\n",
      "trainer/Policy mu Min                                 -2.81098\n",
      "trainer/Policy log std Mean                           -2.2409\n",
      "trainer/Policy log std Std                             0.262638\n",
      "trainer/Policy log std Max                            -0.503394\n",
      "trainer/Policy log std Min                            -3.21463\n",
      "trainer/Alpha                                          0.00980405\n",
      "trainer/Alpha Loss                                    -2.68503\n",
      "exploration/num steps total                        98000\n",
      "exploration/num paths total                          158\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.817686\n",
      "exploration/Rewards Std                                0.0991494\n",
      "exploration/Rewards Max                                1.15305\n",
      "exploration/Rewards Min                               -0.295257\n",
      "exploration/Returns Mean                             817.686\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              817.686\n",
      "exploration/Returns Min                              817.686\n",
      "exploration/Actions Mean                               0.0445031\n",
      "exploration/Actions Std                                0.212225\n",
      "exploration/Actions Max                                0.81643\n",
      "exploration/Actions Min                               -0.78861\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          817.686\n",
      "exploration/env_infos/final/reward_forward Mean        0.0113848\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0113848\n",
      "exploration/env_infos/final/reward_forward Min         0.0113848\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0706613\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0706613\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0706613\n",
      "exploration/env_infos/reward_forward Mean             -0.00172277\n",
      "exploration/env_infos/reward_forward Std               0.136813\n",
      "exploration/env_infos/reward_forward Max               0.520868\n",
      "exploration/env_infos/reward_forward Min              -0.888762\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.113849\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.113849\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.113849\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.102443\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.102443\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.102443\n",
      "exploration/env_infos/reward_ctrl Mean                -0.18808\n",
      "exploration/env_infos/reward_ctrl Std                  0.0929985\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0378778\n",
      "exploration/env_infos/reward_ctrl Min                 -1.29526\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00392984\n",
      "exploration/env_infos/final/torso_velocity Std         0.00544335\n",
      "exploration/env_infos/final/torso_velocity Max         0.0113848\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00145991\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0163308\n",
      "exploration/env_infos/initial/torso_velocity Std       0.263105\n",
      "exploration/env_infos/initial/torso_velocity Max       0.329617\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.307948\n",
      "exploration/env_infos/torso_velocity Mean             -0.0050805\n",
      "exploration/env_infos/torso_velocity Std               0.114066\n",
      "exploration/env_infos/torso_velocity Max               0.569632\n",
      "exploration/env_infos/torso_velocity Min              -1.1627\n",
      "evaluation/num steps total                             2.4192e+06\n",
      "evaluation/num paths total                          2425\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.877454\n",
      "evaluation/Rewards Std                                 0.085983\n",
      "evaluation/Rewards Max                                 1.98749\n",
      "evaluation/Rewards Min                                -1.82086\n",
      "evaluation/Returns Mean                              877.454\n",
      "evaluation/Returns Std                                50.2209\n",
      "evaluation/Returns Max                               964.73\n",
      "evaluation/Returns Min                               788.955\n",
      "evaluation/Actions Mean                                0.025756\n",
      "evaluation/Actions Std                                 0.17529\n",
      "evaluation/Actions Max                                 0.999979\n",
      "evaluation/Actions Min                                -0.996781\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           877.454\n",
      "evaluation/env_infos/final/reward_forward Mean         0.00106003\n",
      "evaluation/env_infos/final/reward_forward Std          0.00994166\n",
      "evaluation/env_infos/final/reward_forward Max          0.0460211\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0195209\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0649835\n",
      "evaluation/env_infos/initial/reward_forward Std        0.151065\n",
      "evaluation/env_infos/initial/reward_forward Max        0.331755\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.246135\n",
      "evaluation/env_infos/reward_forward Mean              -0.00307402\n",
      "evaluation/env_infos/reward_forward Std                0.0993961\n",
      "evaluation/env_infos/reward_forward Max                1.34465\n",
      "evaluation/env_infos/reward_forward Min               -2.93417\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.120129\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0497673\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0395299\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.210326\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.148811\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0758072\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0650574\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.413564\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.12556\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0758811\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0213556\n",
      "evaluation/env_infos/reward_ctrl Min                  -2.82086\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00120723\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00952468\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0460211\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.0514855\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.147146\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.233215\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.556421\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.547848\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00430398\n",
      "evaluation/env_infos/torso_velocity Std                0.110915\n",
      "evaluation/env_infos/torso_velocity Max                2.20699\n",
      "evaluation/env_infos/torso_velocity Min               -2.93417\n",
      "time/data storing (s)                                  0.0149145\n",
      "time/evaluation sampling (s)                          46.3932\n",
      "time/exploration sampling (s)                          2.05266\n",
      "time/logging (s)                                       0.272004\n",
      "time/saving (s)                                        0.0259331\n",
      "time/training (s)                                      4.58935\n",
      "time/epoch (s)                                        53.3481\n",
      "time/total (s)                                      5228.89\n",
      "Epoch                                                 96\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:52:55.648821 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 97 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 99000\n",
      "trainer/QF1 Loss                                       0.582572\n",
      "trainer/QF2 Loss                                       0.547562\n",
      "trainer/Policy Loss                                  -23.4781\n",
      "trainer/Q1 Predictions Mean                           30.4439\n",
      "trainer/Q1 Predictions Std                             2.58589\n",
      "trainer/Q1 Predictions Max                            39.5294\n",
      "trainer/Q1 Predictions Min                            18.903\n",
      "trainer/Q2 Predictions Mean                           30.4046\n",
      "trainer/Q2 Predictions Std                             2.48942\n",
      "trainer/Q2 Predictions Max                            35.037\n",
      "trainer/Q2 Predictions Min                            18.583\n",
      "trainer/Q Targets Mean                                30.2176\n",
      "trainer/Q Targets Std                                  2.50739\n",
      "trainer/Q Targets Max                                 37.3431\n",
      "trainer/Q Targets Min                                 17.4735\n",
      "trainer/Log Pis Mean                                   7.23579\n",
      "trainer/Log Pis Std                                    2.57899\n",
      "trainer/Log Pis Max                                   21.9413\n",
      "trainer/Log Pis Min                                    0.0559347\n",
      "trainer/Policy mu Mean                                 0.00700522\n",
      "trainer/Policy mu Std                                  0.292783\n",
      "trainer/Policy mu Max                                  2.2847\n",
      "trainer/Policy mu Min                                 -2.05166\n",
      "trainer/Policy log std Mean                           -2.23959\n",
      "trainer/Policy log std Std                             0.24707\n",
      "trainer/Policy log std Max                            -0.64863\n",
      "trainer/Policy log std Min                            -4.28708\n",
      "trainer/Alpha                                          0.0104642\n",
      "trainer/Alpha Loss                                    -3.48413\n",
      "exploration/num steps total                        99000\n",
      "exploration/num paths total                          159\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.816789\n",
      "exploration/Rewards Std                                0.215591\n",
      "exploration/Rewards Max                                1.38276\n",
      "exploration/Rewards Min                               -1.46299\n",
      "exploration/Returns Mean                             816.789\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              816.789\n",
      "exploration/Returns Min                              816.789\n",
      "exploration/Actions Mean                               0.0411979\n",
      "exploration/Actions Std                                0.212295\n",
      "exploration/Actions Max                                0.999857\n",
      "exploration/Actions Min                               -0.990309\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          816.789\n",
      "exploration/env_infos/final/reward_forward Mean        0.0202029\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0202029\n",
      "exploration/env_infos/final/reward_forward Min         0.0202029\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0590815\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0590815\n",
      "exploration/env_infos/initial/reward_forward Min       0.0590815\n",
      "exploration/env_infos/reward_forward Mean              0.0033211\n",
      "exploration/env_infos/reward_forward Std               0.262793\n",
      "exploration/env_infos/reward_forward Max               1.21913\n",
      "exploration/env_infos/reward_forward Min              -1.58824\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0633775\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0633775\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0633775\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.113268\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.113268\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.113268\n",
      "exploration/env_infos/reward_ctrl Mean                -0.187065\n",
      "exploration/env_infos/reward_ctrl Std                  0.213539\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0131581\n",
      "exploration/env_infos/reward_ctrl Min                 -2.46299\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00999942\n",
      "exploration/env_infos/final/torso_velocity Std         0.0334821\n",
      "exploration/env_infos/final/torso_velocity Max         0.0449414\n",
      "exploration/env_infos/final/torso_velocity Min        -0.035146\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.165841\n",
      "exploration/env_infos/initial/torso_velocity Std       0.268365\n",
      "exploration/env_infos/initial/torso_velocity Max       0.534628\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0961861\n",
      "exploration/env_infos/torso_velocity Mean             -0.0225507\n",
      "exploration/env_infos/torso_velocity Std               0.270243\n",
      "exploration/env_infos/torso_velocity Max               2.14623\n",
      "exploration/env_infos/torso_velocity Min              -1.81661\n",
      "evaluation/num steps total                             2.44327e+06\n",
      "evaluation/num paths total                          2450\n",
      "evaluation/path length Mean                          962.84\n",
      "evaluation/path length Std                           182.046\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                            71\n",
      "evaluation/Rewards Mean                                0.800446\n",
      "evaluation/Rewards Std                                 0.267109\n",
      "evaluation/Rewards Max                                 2.77381\n",
      "evaluation/Rewards Min                                -2.93801\n",
      "evaluation/Returns Mean                              770.702\n",
      "evaluation/Returns Std                               190.248\n",
      "evaluation/Returns Max                               919.893\n",
      "evaluation/Returns Min                                23.5653\n",
      "evaluation/Actions Mean                                0.0220417\n",
      "evaluation/Actions Std                                 0.230371\n",
      "evaluation/Actions Max                                 1\n",
      "evaluation/Actions Min                                -0.999738\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           770.702\n",
      "evaluation/env_infos/final/reward_forward Mean        -0.0336408\n",
      "evaluation/env_infos/final/reward_forward Std          0.111979\n",
      "evaluation/env_infos/final/reward_forward Max          1.12587e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -0.437005\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.069351\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132819\n",
      "evaluation/env_infos/initial/reward_forward Max        0.260116\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.315296\n",
      "evaluation/env_infos/reward_forward Mean               0.00494667\n",
      "evaluation/env_infos/reward_forward Std                0.217498\n",
      "evaluation/env_infos/reward_forward Max                3.23853\n",
      "evaluation/env_infos/reward_forward Min               -1.74042\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.238524\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.324471\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0520464\n",
      "evaluation/env_infos/final/reward_ctrl Min            -1.62624\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.200237\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.136234\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0375198\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.62661\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.214226\n",
      "evaluation/env_infos/reward_ctrl Std                   0.260993\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.010548\n",
      "evaluation/env_infos/reward_ctrl Min                  -3.93801\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00591507\n",
      "evaluation/env_infos/final/torso_velocity Std          0.110993\n",
      "evaluation/env_infos/final/torso_velocity Max          0.66381\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.437005\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.116795\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.27755\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.624498\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.370562\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00465526\n",
      "evaluation/env_infos/torso_velocity Std                0.237036\n",
      "evaluation/env_infos/torso_velocity Max                3.23853\n",
      "evaluation/env_infos/torso_velocity Min               -2.66663\n",
      "time/data storing (s)                                  0.0143728\n",
      "time/evaluation sampling (s)                          47.6145\n",
      "time/exploration sampling (s)                          2.22402\n",
      "time/logging (s)                                       0.266342\n",
      "time/saving (s)                                        0.0249076\n",
      "time/training (s)                                      4.73067\n",
      "time/epoch (s)                                        54.8748\n",
      "time/total (s)                                      5284.57\n",
      "Epoch                                                 97\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:53:49.876312 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 98 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 100000\n",
      "trainer/QF1 Loss                                        0.780065\n",
      "trainer/QF2 Loss                                        0.807664\n",
      "trainer/Policy Loss                                   -22.5177\n",
      "trainer/Q1 Predictions Mean                            30.4479\n",
      "trainer/Q1 Predictions Std                              2.30739\n",
      "trainer/Q1 Predictions Max                             32.7842\n",
      "trainer/Q1 Predictions Min                             16.254\n",
      "trainer/Q2 Predictions Mean                            30.4827\n",
      "trainer/Q2 Predictions Std                              2.39804\n",
      "trainer/Q2 Predictions Max                             32.9088\n",
      "trainer/Q2 Predictions Min                             16.1655\n",
      "trainer/Q Targets Mean                                 30.589\n",
      "trainer/Q Targets Std                                   2.46199\n",
      "trainer/Q Targets Max                                  41.8928\n",
      "trainer/Q Targets Min                                  15.7298\n",
      "trainer/Log Pis Mean                                    8.12458\n",
      "trainer/Log Pis Std                                     2.22247\n",
      "trainer/Log Pis Max                                    16.5207\n",
      "trainer/Log Pis Min                                     1.56079\n",
      "trainer/Policy mu Mean                                 -0.0531097\n",
      "trainer/Policy mu Std                                   0.299409\n",
      "trainer/Policy mu Max                                   1.57745\n",
      "trainer/Policy mu Min                                  -2.97391\n",
      "trainer/Policy log std Mean                            -2.36512\n",
      "trainer/Policy log std Std                              0.246181\n",
      "trainer/Policy log std Max                             -0.300492\n",
      "trainer/Policy log std Min                             -3.63885\n",
      "trainer/Alpha                                           0.00936329\n",
      "trainer/Alpha Loss                                      0.582042\n",
      "exploration/num steps total                        100000\n",
      "exploration/num paths total                           160\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.791156\n",
      "exploration/Rewards Std                                 0.216789\n",
      "exploration/Rewards Max                                 2.02107\n",
      "exploration/Rewards Min                                -1.28583\n",
      "exploration/Returns Mean                              791.156\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               791.156\n",
      "exploration/Returns Min                               791.156\n",
      "exploration/Actions Mean                               -0.110525\n",
      "exploration/Actions Std                                 0.207136\n",
      "exploration/Actions Max                                 0.999779\n",
      "exploration/Actions Min                                -0.986979\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           791.156\n",
      "exploration/env_infos/final/reward_forward Mean         0.0266344\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0266344\n",
      "exploration/env_infos/final/reward_forward Min          0.0266344\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0739493\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0739493\n",
      "exploration/env_infos/initial/reward_forward Min        0.0739493\n",
      "exploration/env_infos/reward_forward Mean              -0.0105848\n",
      "exploration/env_infos/reward_forward Std                0.262628\n",
      "exploration/env_infos/reward_forward Max                1.93455\n",
      "exploration/env_infos/reward_forward Min               -1.68219\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.186536\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.186536\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.186536\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.347756\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.347756\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.347756\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220484\n",
      "exploration/env_infos/reward_ctrl Std                   0.22153\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0446887\n",
      "exploration/env_infos/reward_ctrl Min                  -2.39079\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.000690015\n",
      "exploration/env_infos/final/torso_velocity Std          0.0240899\n",
      "exploration/env_infos/final/torso_velocity Max          0.0266344\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0314042\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0957809\n",
      "exploration/env_infos/initial/torso_velocity Std        0.230479\n",
      "exploration/env_infos/initial/torso_velocity Max        0.0739493\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.421633\n",
      "exploration/env_infos/torso_velocity Mean              -0.0163742\n",
      "exploration/env_infos/torso_velocity Std                0.231958\n",
      "exploration/env_infos/torso_velocity Max                1.93455\n",
      "exploration/env_infos/torso_velocity Min               -2.21671\n",
      "evaluation/num steps total                              2.46827e+06\n",
      "evaluation/num paths total                           2475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.838342\n",
      "evaluation/Rewards Std                                  0.146129\n",
      "evaluation/Rewards Max                                  2.54493\n",
      "evaluation/Rewards Min                                 -1.65458\n",
      "evaluation/Returns Mean                               838.342\n",
      "evaluation/Returns Std                                 36.7863\n",
      "evaluation/Returns Max                                913.529\n",
      "evaluation/Returns Min                                784.528\n",
      "evaluation/Actions Mean                                 0.014886\n",
      "evaluation/Actions Std                                  0.203504\n",
      "evaluation/Actions Max                                  0.99999\n",
      "evaluation/Actions Min                                 -0.999682\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            838.342\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.34525e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.8144e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.08176e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.20435e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0734587\n",
      "evaluation/env_infos/initial/reward_forward Std         0.173937\n",
      "evaluation/env_infos/initial/reward_forward Max         0.219604\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.429634\n",
      "evaluation/env_infos/reward_forward Mean               -0.00311874\n",
      "evaluation/env_infos/reward_forward Std                 0.15157\n",
      "evaluation/env_infos/reward_forward Max                 2.09757\n",
      "evaluation/env_infos/reward_forward Min                -1.53097\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.14822\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0319089\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0856609\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.194149\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.187693\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.128643\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0275158\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.529773\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.166542\n",
      "evaluation/env_infos/reward_ctrl Std                    0.140301\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0119575\n",
      "evaluation/env_infos/reward_ctrl Min                   -2.65458\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.74525e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.85862e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.9894e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.69428e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.112797\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.277074\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.697113\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.429634\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00517032\n",
      "evaluation/env_infos/torso_velocity Std                 0.156191\n",
      "evaluation/env_infos/torso_velocity Max                 2.09757\n",
      "evaluation/env_infos/torso_velocity Min                -2.46761\n",
      "time/data storing (s)                                   0.0148472\n",
      "time/evaluation sampling (s)                           46.5039\n",
      "time/exploration sampling (s)                           2.00586\n",
      "time/logging (s)                                        0.279366\n",
      "time/saving (s)                                         0.0288392\n",
      "time/training (s)                                       4.59865\n",
      "time/epoch (s)                                         53.4315\n",
      "time/total (s)                                       5338.8\n",
      "Epoch                                                  98\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:54:45.607955 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_10_24_53_0000--s-10] Epoch 99 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 101000\n",
      "trainer/QF1 Loss                                        0.891498\n",
      "trainer/QF2 Loss                                        1.0873\n",
      "trainer/Policy Loss                                   -22.2747\n",
      "trainer/Q1 Predictions Mean                            30.5505\n",
      "trainer/Q1 Predictions Std                              2.81738\n",
      "trainer/Q1 Predictions Max                             36.8492\n",
      "trainer/Q1 Predictions Min                             17.9023\n",
      "trainer/Q2 Predictions Mean                            30.5615\n",
      "trainer/Q2 Predictions Std                              2.73543\n",
      "trainer/Q2 Predictions Max                             35.845\n",
      "trainer/Q2 Predictions Min                             18.804\n",
      "trainer/Q Targets Mean                                 30.5392\n",
      "trainer/Q Targets Std                                   3.05019\n",
      "trainer/Q Targets Max                                  42.316\n",
      "trainer/Q Targets Min                                  18.3273\n",
      "trainer/Log Pis Mean                                    8.53974\n",
      "trainer/Log Pis Std                                     2.6632\n",
      "trainer/Log Pis Max                                    19.2631\n",
      "trainer/Log Pis Min                                     1.56831\n",
      "trainer/Policy mu Mean                                  0.0447411\n",
      "trainer/Policy mu Std                                   0.396213\n",
      "trainer/Policy mu Max                                   3.02117\n",
      "trainer/Policy mu Min                                  -2.44221\n",
      "trainer/Policy log std Mean                            -2.31965\n",
      "trainer/Policy log std Std                              0.338464\n",
      "trainer/Policy log std Max                             -0.592036\n",
      "trainer/Policy log std Min                             -3.91669\n",
      "trainer/Alpha                                           0.0118595\n",
      "trainer/Alpha Loss                                      2.39443\n",
      "exploration/num steps total                        101000\n",
      "exploration/num paths total                           161\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.826222\n",
      "exploration/Rewards Std                                 0.0745758\n",
      "exploration/Rewards Max                                 1.05464\n",
      "exploration/Rewards Min                                -0.0529647\n",
      "exploration/Returns Mean                              826.222\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               826.222\n",
      "exploration/Returns Min                               826.222\n",
      "exploration/Actions Mean                                0.108873\n",
      "exploration/Actions Std                                 0.181315\n",
      "exploration/Actions Max                                 0.716425\n",
      "exploration/Actions Min                                -0.932793\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           826.222\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00453897\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00453897\n",
      "exploration/env_infos/final/reward_forward Min         -0.00453897\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0199973\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0199973\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0199973\n",
      "exploration/env_infos/reward_forward Mean              -0.00881804\n",
      "exploration/env_infos/reward_forward Std                0.0866335\n",
      "exploration/env_infos/reward_forward Max                0.446662\n",
      "exploration/env_infos/reward_forward Min               -1.31839\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.270551\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.270551\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.270551\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.457753\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.457753\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.457753\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.178914\n",
      "exploration/env_infos/reward_ctrl Std                   0.0703279\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0442989\n",
      "exploration/env_infos/reward_ctrl Min                  -1.05296\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00175575\n",
      "exploration/env_infos/final/torso_velocity Std          0.00450128\n",
      "exploration/env_infos/final/torso_velocity Max          0.00572448\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00453897\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.165865\n",
      "exploration/env_infos/initial/torso_velocity Std        0.264894\n",
      "exploration/env_infos/initial/torso_velocity Max        0.540479\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.022886\n",
      "exploration/env_infos/torso_velocity Mean              -0.0005843\n",
      "exploration/env_infos/torso_velocity Std                0.0789591\n",
      "exploration/env_infos/torso_velocity Max                0.861095\n",
      "exploration/env_infos/torso_velocity Min               -1.31839\n",
      "evaluation/num steps total                              2.49327e+06\n",
      "evaluation/num paths total                           2500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.782827\n",
      "evaluation/Rewards Std                                  0.11512\n",
      "evaluation/Rewards Max                                  1.90746\n",
      "evaluation/Rewards Min                                 -1.50325\n",
      "evaluation/Returns Mean                               782.827\n",
      "evaluation/Returns Std                                 74.281\n",
      "evaluation/Returns Max                                925.767\n",
      "evaluation/Returns Min                                625.099\n",
      "evaluation/Actions Mean                                 0.0848623\n",
      "evaluation/Actions Std                                  0.218444\n",
      "evaluation/Actions Max                                  0.999866\n",
      "evaluation/Actions Min                                 -0.991816\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            782.827\n",
      "evaluation/env_infos/final/reward_forward Mean          6.18928e-06\n",
      "evaluation/env_infos/final/reward_forward Std           5.37633e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000252531\n",
      "evaluation/env_infos/final/reward_forward Min          -9.14383e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0430997\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129815\n",
      "evaluation/env_infos/initial/reward_forward Max         0.247219\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.192949\n",
      "evaluation/env_infos/reward_forward Mean               -0.00871643\n",
      "evaluation/env_infos/reward_forward Std                 0.11302\n",
      "evaluation/env_infos/reward_forward Max                 1.03114\n",
      "evaluation/env_infos/reward_forward Min                -1.83576\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.216191\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0846714\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.067521\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.374221\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.165429\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0918647\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0602713\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.554352\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.219678\n",
      "evaluation/env_infos/reward_ctrl Std                    0.112797\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0343267\n",
      "evaluation/env_infos/reward_ctrl Min                   -2.50325\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.22744e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           4.1381e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000252531\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000112544\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137902\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.237139\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.694929\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.436516\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00376541\n",
      "evaluation/env_infos/torso_velocity Std                 0.109346\n",
      "evaluation/env_infos/torso_velocity Max                 2.85267\n",
      "evaluation/env_infos/torso_velocity Min                -2.04454\n",
      "time/data storing (s)                                   0.0145546\n",
      "time/evaluation sampling (s)                           47.4856\n",
      "time/exploration sampling (s)                           2.24554\n",
      "time/logging (s)                                        0.269748\n",
      "time/saving (s)                                         0.0268039\n",
      "time/training (s)                                       4.8372\n",
      "time/epoch (s)                                         54.8794\n",
      "time/total (s)                                       5394.52\n",
      "Epoch                                                  99\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[19198]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c264778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c2e5740). One of the two will be used. Which one is undefined.\n",
      "objc[19198]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c264700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c2e5768). One of the two will be used. Which one is undefined.\n",
      "objc[19198]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c2647a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c2e57b8). One of the two will be used. Which one is undefined.\n",
      "objc[19198]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1c264818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1c2e5830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 11:54:52.902622 PDT | Variant:\n",
      "2021-05-25 11:54:52.903312 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": false,\n",
      "    \"n_sampled_latents\": 1,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": false,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 11:55:51.049075 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      22.3946\n",
      "trainer/QF2 Loss                                      22.3754\n",
      "trainer/Policy Loss                                   -5.3293\n",
      "trainer/Q1 Predictions Mean                            0.00216844\n",
      "trainer/Q1 Predictions Std                             0.00365869\n",
      "trainer/Q1 Predictions Max                             0.0143304\n",
      "trainer/Q1 Predictions Min                            -0.00711921\n",
      "trainer/Q2 Predictions Mean                            0.00424809\n",
      "trainer/Q2 Predictions Std                             0.00320595\n",
      "trainer/Q2 Predictions Max                             0.0149196\n",
      "trainer/Q2 Predictions Min                            -0.00269154\n",
      "trainer/Q Targets Mean                                 4.65146\n",
      "trainer/Q Targets Std                                  0.882386\n",
      "trainer/Q Targets Max                                  7.32847\n",
      "trainer/Q Targets Min                                 -0.591982\n",
      "trainer/Log Pis Mean                                  -5.32836\n",
      "trainer/Log Pis Std                                    0.608577\n",
      "trainer/Log Pis Max                                   -3.4595\n",
      "trainer/Log Pis Min                                   -7.6402\n",
      "trainer/Policy mu Mean                                -0.000278591\n",
      "trainer/Policy mu Std                                  0.00210892\n",
      "trainer/Policy mu Max                                  0.00576516\n",
      "trainer/Policy mu Min                                 -0.00681759\n",
      "trainer/Policy log std Mean                           -6.0388e-05\n",
      "trainer/Policy log std Std                             0.00229982\n",
      "trainer/Policy log std Max                             0.00987175\n",
      "trainer/Policy log std Min                            -0.0072079\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           15\n",
      "exploration/path length Mean                          90.9091\n",
      "exploration/path length Std                          110.381\n",
      "exploration/path length Max                          427\n",
      "exploration/path length Min                            8\n",
      "exploration/Rewards Mean                              -0.50347\n",
      "exploration/Rewards Std                                0.478441\n",
      "exploration/Rewards Max                                1.27923\n",
      "exploration/Rewards Min                               -1.89958\n",
      "exploration/Returns Mean                             -45.77\n",
      "exploration/Returns Std                               53.3174\n",
      "exploration/Returns Max                               -2.39649\n",
      "exploration/Returns Min                             -206.682\n",
      "exploration/Actions Mean                               0.00462324\n",
      "exploration/Actions Std                                0.62244\n",
      "exploration/Actions Max                                0.999662\n",
      "exploration/Actions Min                               -0.999063\n",
      "exploration/Num Paths                                 11\n",
      "exploration/Average Returns                          -45.77\n",
      "exploration/env_infos/final/reward_forward Mean       -0.247262\n",
      "exploration/env_infos/final/reward_forward Std         1.00002\n",
      "exploration/env_infos/final/reward_forward Max         1.54729\n",
      "exploration/env_infos/final/reward_forward Min        -1.90155\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0124261\n",
      "exploration/env_infos/initial/reward_forward Std       0.177582\n",
      "exploration/env_infos/initial/reward_forward Max       0.268075\n",
      "exploration/env_infos/initial/reward_forward Min      -0.359573\n",
      "exploration/env_infos/reward_forward Mean              0.00925785\n",
      "exploration/env_infos/reward_forward Std               0.621288\n",
      "exploration/env_infos/reward_forward Max               2.19612\n",
      "exploration/env_infos/reward_forward Min              -1.90155\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.43183\n",
      "exploration/env_infos/final/reward_ctrl Std            0.474869\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.575174\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.4169\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.44752\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.382094\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.915727\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.10685\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54981\n",
      "exploration/env_infos/reward_ctrl Std                  0.438075\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477931\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89958\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.279406\n",
      "exploration/env_infos/final/torso_velocity Std         1.14345\n",
      "exploration/env_infos/final/torso_velocity Max         3.13565\n",
      "exploration/env_infos/final/torso_velocity Min        -1.90155\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.138253\n",
      "exploration/env_infos/initial/torso_velocity Std       0.274377\n",
      "exploration/env_infos/initial/torso_velocity Max       0.584581\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.359573\n",
      "exploration/env_infos/torso_velocity Mean              0.00347145\n",
      "exploration/env_infos/torso_velocity Std               0.676345\n",
      "exploration/env_infos/torso_velocity Max               3.53082\n",
      "exploration/env_infos/torso_velocity Min              -2.56684\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.0008\n",
      "evaluation/Rewards Std                                 0.0159093\n",
      "evaluation/Rewards Max                                 2.1846\n",
      "evaluation/Rewards Min                                 0.999984\n",
      "evaluation/Returns Mean                             1000.8\n",
      "evaluation/Returns Std                                 1.11559\n",
      "evaluation/Returns Max                              1004.38\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000140735\n",
      "evaluation/Actions Std                                 0.00116236\n",
      "evaluation/Actions Max                                 0.00346672\n",
      "evaluation/Actions Min                                -0.0037738\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.8\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000288318\n",
      "evaluation/env_infos/final/reward_forward Std          0.000288771\n",
      "evaluation/env_infos/final/reward_forward Max          0.00120917\n",
      "evaluation/env_infos/final/reward_forward Min         -8.16482e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_forward Mean      -0.0279349\n",
      "evaluation/env_infos/initial/reward_forward Std        0.187545\n",
      "evaluation/env_infos/initial/reward_forward Max        0.501563\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.347508\n",
      "evaluation/env_infos/reward_forward Mean               0.00284841\n",
      "evaluation/env_infos/reward_forward Std                0.046714\n",
      "evaluation/env_infos/reward_forward Max                1.55246\n",
      "evaluation/env_infos/reward_forward Min               -1.11447\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.4697e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.82475e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.67969e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.68605e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.3028e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           4.7236e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.5549e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.09396e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.48356e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.26324e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -3.23397e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.56163e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000149995\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000380257\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00240535\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000147477\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.133301\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.24673\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.656886\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.347508\n",
      "evaluation/env_infos/torso_velocity Mean              -4.08281e-05\n",
      "evaluation/env_infos/torso_velocity Std                0.0525121\n",
      "evaluation/env_infos/torso_velocity Max                1.55246\n",
      "evaluation/env_infos/torso_velocity Min               -1.73161\n",
      "time/data storing (s)                                  0.0394038\n",
      "time/evaluation sampling (s)                          50.1413\n",
      "time/exploration sampling (s)                          1.95665\n",
      "time/logging (s)                                       0.28003\n",
      "time/saving (s)                                        0.0696526\n",
      "time/training (s)                                      3.73383\n",
      "time/epoch (s)                                        56.2208\n",
      "time/total (s)                                        61.8093\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 11:56:46.523904 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.515764\n",
      "trainer/QF2 Loss                                       0.514882\n",
      "trainer/Policy Loss                                   -9.3181\n",
      "trainer/Q1 Predictions Mean                            3.84864\n",
      "trainer/Q1 Predictions Std                             0.388451\n",
      "trainer/Q1 Predictions Max                             4.82483\n",
      "trainer/Q1 Predictions Min                             2.30664\n",
      "trainer/Q2 Predictions Mean                            3.85279\n",
      "trainer/Q2 Predictions Std                             0.390617\n",
      "trainer/Q2 Predictions Max                             4.95365\n",
      "trainer/Q2 Predictions Min                             2.33145\n",
      "trainer/Q Targets Mean                                 4.0323\n",
      "trainer/Q Targets Std                                  0.648057\n",
      "trainer/Q Targets Max                                  6.51269\n",
      "trainer/Q Targets Min                                  0.0151227\n",
      "trainer/Log Pis Mean                                  -5.47688\n",
      "trainer/Log Pis Std                                    0.456775\n",
      "trainer/Log Pis Max                                   -4.54934\n",
      "trainer/Log Pis Min                                   -9.68812\n",
      "trainer/Policy mu Mean                                 0.000603101\n",
      "trainer/Policy mu Std                                  0.0241185\n",
      "trainer/Policy mu Max                                  0.116309\n",
      "trainer/Policy mu Min                                 -0.0925935\n",
      "trainer/Policy log std Mean                           -0.106356\n",
      "trainer/Policy log std Std                             0.0240229\n",
      "trainer/Policy log std Max                            -0.061342\n",
      "trainer/Policy log std Min                            -0.264703\n",
      "trainer/Alpha                                          0.738555\n",
      "trainer/Alpha Loss                                    -4.04385\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           16\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                              -0.402287\n",
      "exploration/Rewards Std                                0.440871\n",
      "exploration/Rewards Max                                1.10654\n",
      "exploration/Rewards Min                               -1.81465\n",
      "exploration/Returns Mean                            -402.287\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             -402.287\n",
      "exploration/Returns Min                             -402.287\n",
      "exploration/Actions Mean                              -0.00723596\n",
      "exploration/Actions Std                                0.600456\n",
      "exploration/Actions Max                                0.998394\n",
      "exploration/Actions Min                               -0.998898\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         -402.287\n",
      "exploration/env_infos/final/reward_forward Mean       -0.309412\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.309412\n",
      "exploration/env_infos/final/reward_forward Min        -0.309412\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0390346\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0390346\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0390346\n",
      "exploration/env_infos/reward_forward Mean             -0.00100373\n",
      "exploration/env_infos/reward_forward Std               0.403434\n",
      "exploration/env_infos/reward_forward Max               1.59008\n",
      "exploration/env_infos/reward_forward Min              -1.66518\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.45315\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -1.45315\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.45315\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.58645\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -1.58645\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.58645\n",
      "exploration/env_infos/reward_ctrl Mean                -1.4424\n",
      "exploration/env_infos/reward_ctrl Std                  0.412144\n",
      "exploration/env_infos/reward_ctrl Max                 -0.41729\n",
      "exploration/env_infos/reward_ctrl Min                 -2.81465\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.137679\n",
      "exploration/env_infos/final/torso_velocity Std         0.272563\n",
      "exploration/env_infos/final/torso_velocity Max         0.247046\n",
      "exploration/env_infos/final/torso_velocity Min        -0.350672\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.029157\n",
      "exploration/env_infos/initial/torso_velocity Std       0.255892\n",
      "exploration/env_infos/initial/torso_velocity Max       0.371041\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.244535\n",
      "exploration/env_infos/torso_velocity Mean             -0.00664078\n",
      "exploration/env_infos/torso_velocity Std               0.407216\n",
      "exploration/env_infos/torso_velocity Max               2.34955\n",
      "exploration/env_infos/torso_velocity Min              -2.80493\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00029\n",
      "evaluation/Rewards Std                                 0.0346614\n",
      "evaluation/Rewards Max                                 2.54779\n",
      "evaluation/Rewards Min                                 0.996219\n",
      "evaluation/Returns Mean                             1000.29\n",
      "evaluation/Returns Std                                 2.44973\n",
      "evaluation/Returns Max                              1008.07\n",
      "evaluation/Returns Min                               998.637\n",
      "evaluation/Actions Mean                               -0.00177207\n",
      "evaluation/Actions Std                                 0.0173565\n",
      "evaluation/Actions Max                                 0.0378069\n",
      "evaluation/Actions Min                                -0.0562981\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.29\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000499454\n",
      "evaluation/env_infos/final/reward_forward Std          0.000505827\n",
      "evaluation/env_infos/final/reward_forward Max          0.00101742\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000769973\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0010196\n",
      "evaluation/env_infos/initial/reward_forward Std        0.139011\n",
      "evaluation/env_infos/initial/reward_forward Max        0.303574\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.301093\n",
      "evaluation/env_infos/reward_forward Mean              -0.000934673\n",
      "evaluation/env_infos/reward_forward Std                0.0452156\n",
      "evaluation/env_infos/reward_forward Max                1.15461\n",
      "evaluation/env_infos/reward_forward Min               -1.27668\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00122861\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000146474\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.000982993\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00138118\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.000978635\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000107349\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000807781\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00113087\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00121756\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000177694\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000775114\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00378104\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000244462\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000393384\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00132802\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000769973\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.143807\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.229678\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.608413\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.301093\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000524425\n",
      "evaluation/env_infos/torso_velocity Std                0.0622178\n",
      "evaluation/env_infos/torso_velocity Max                1.3557\n",
      "evaluation/env_infos/torso_velocity Min               -1.86404\n",
      "time/data storing (s)                                  0.0300501\n",
      "time/evaluation sampling (s)                          49.0954\n",
      "time/exploration sampling (s)                          1.824\n",
      "time/logging (s)                                       0.27182\n",
      "time/saving (s)                                        0.0360317\n",
      "time/training (s)                                      3.89701\n",
      "time/epoch (s)                                        55.1543\n",
      "time/total (s)                                       117.276\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:57:41.642200 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.51094\n",
      "trainer/QF2 Loss                                       0.507098\n",
      "trainer/Policy Loss                                   -9.14573\n",
      "trainer/Q1 Predictions Mean                            3.69534\n",
      "trainer/Q1 Predictions Std                             0.299982\n",
      "trainer/Q1 Predictions Max                             4.73361\n",
      "trainer/Q1 Predictions Min                             2.37177\n",
      "trainer/Q2 Predictions Mean                            3.69205\n",
      "trainer/Q2 Predictions Std                             0.295432\n",
      "trainer/Q2 Predictions Max                             4.51555\n",
      "trainer/Q2 Predictions Min                             2.36094\n",
      "trainer/Q Targets Mean                                 3.76424\n",
      "trainer/Q Targets Std                                  0.644668\n",
      "trainer/Q Targets Max                                  6.37521\n",
      "trainer/Q Targets Min                                 -0.106633\n",
      "trainer/Log Pis Mean                                  -5.46011\n",
      "trainer/Log Pis Std                                    0.344301\n",
      "trainer/Log Pis Max                                   -4.58458\n",
      "trainer/Log Pis Min                                   -7.84094\n",
      "trainer/Policy mu Mean                                -0.00828545\n",
      "trainer/Policy mu Std                                  0.0220741\n",
      "trainer/Policy mu Max                                  0.0742789\n",
      "trainer/Policy mu Min                                 -0.0797819\n",
      "trainer/Policy log std Mean                           -0.128009\n",
      "trainer/Policy log std Std                             0.0188673\n",
      "trainer/Policy log std Max                            -0.0814248\n",
      "trainer/Policy log std Min                            -0.213469\n",
      "trainer/Alpha                                          0.547069\n",
      "trainer/Alpha Loss                                    -8.07846\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           25\n",
      "exploration/path length Mean                         111.111\n",
      "exploration/path length Std                           87.2804\n",
      "exploration/path length Max                          330\n",
      "exploration/path length Min                           14\n",
      "exploration/Rewards Mean                              -0.344117\n",
      "exploration/Rewards Std                                0.489034\n",
      "exploration/Rewards Max                                2.49869\n",
      "exploration/Rewards Min                               -1.70052\n",
      "exploration/Returns Mean                             -38.2352\n",
      "exploration/Returns Std                               31.9909\n",
      "exploration/Returns Max                               -3.67313\n",
      "exploration/Returns Min                             -119.096\n",
      "exploration/Actions Mean                              -0.00562809\n",
      "exploration/Actions Std                                0.590956\n",
      "exploration/Actions Max                                0.998017\n",
      "exploration/Actions Min                               -0.996286\n",
      "exploration/Num Paths                                  9\n",
      "exploration/Average Returns                          -38.2352\n",
      "exploration/env_infos/final/reward_forward Mean       -0.189747\n",
      "exploration/env_infos/final/reward_forward Std         1.23038\n",
      "exploration/env_infos/final/reward_forward Max         1.86464\n",
      "exploration/env_infos/final/reward_forward Min        -1.99715\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.141419\n",
      "exploration/env_infos/initial/reward_forward Std       0.201544\n",
      "exploration/env_infos/initial/reward_forward Max       0.286209\n",
      "exploration/env_infos/initial/reward_forward Min      -0.500215\n",
      "exploration/env_infos/reward_forward Mean              0.0447392\n",
      "exploration/env_infos/reward_forward Std               0.680678\n",
      "exploration/env_infos/reward_forward Max               2.1871\n",
      "exploration/env_infos/reward_forward Min              -2.21547\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.31847\n",
      "exploration/env_infos/final/reward_ctrl Std            0.435838\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.524001\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.05216\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.35015\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.437905\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.737422\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.93231\n",
      "exploration/env_infos/reward_ctrl Mean                -1.39704\n",
      "exploration/env_infos/reward_ctrl Std                  0.421426\n",
      "exploration/env_infos/reward_ctrl Max                 -0.201561\n",
      "exploration/env_infos/reward_ctrl Min                 -2.70052\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.652684\n",
      "exploration/env_infos/final/torso_velocity Std         1.27213\n",
      "exploration/env_infos/final/torso_velocity Max         3.50338\n",
      "exploration/env_infos/final/torso_velocity Min        -1.99715\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0780282\n",
      "exploration/env_infos/initial/torso_velocity Std       0.28892\n",
      "exploration/env_infos/initial/torso_velocity Max       0.594703\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.500215\n",
      "exploration/env_infos/torso_velocity Mean              0.0684777\n",
      "exploration/env_infos/torso_velocity Std               0.722985\n",
      "exploration/env_infos/torso_velocity Max               4.22033\n",
      "exploration/env_infos/torso_velocity Min              -2.21547\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999565\n",
      "evaluation/Rewards Std                                 0.0131533\n",
      "evaluation/Rewards Max                                 2.19579\n",
      "evaluation/Rewards Min                                 0.997047\n",
      "evaluation/Returns Mean                              999.565\n",
      "evaluation/Returns Std                                 0.916874\n",
      "evaluation/Returns Max                              1003.05\n",
      "evaluation/Returns Min                               998.9\n",
      "evaluation/Actions Mean                               -0.00494853\n",
      "evaluation/Actions Std                                 0.0156482\n",
      "evaluation/Actions Max                                 0.0360513\n",
      "evaluation/Actions Min                                -0.0430073\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.565\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.90022e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.00015304\n",
      "evaluation/env_infos/final/reward_forward Max          4.83066e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000589256\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00384194\n",
      "evaluation/env_infos/initial/reward_forward Std        0.113187\n",
      "evaluation/env_infos/initial/reward_forward Max        0.236228\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.212855\n",
      "evaluation/env_infos/reward_forward Mean              -0.00223588\n",
      "evaluation/env_infos/reward_forward Std                0.0489651\n",
      "evaluation/env_infos/reward_forward Max                1.19147\n",
      "evaluation/env_infos/reward_forward Min               -1.48622\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00106367\n",
      "evaluation/env_infos/final/reward_ctrl Std             3.21126e-05\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00100121\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00111242\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00116313\n",
      "evaluation/env_infos/initial/reward_ctrl Std           7.33539e-05\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00103156\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00132944\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00107742\n",
      "evaluation/env_infos/reward_ctrl Std                   8.10787e-05\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00063475\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.0029535\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.8887e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000177274\n",
      "evaluation/env_infos/final/torso_velocity Max          4.83066e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.00104061\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.117447\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.229435\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.676533\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.383994\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00302658\n",
      "evaluation/env_infos/torso_velocity Std                0.0554876\n",
      "evaluation/env_infos/torso_velocity Max                1.19147\n",
      "evaluation/env_infos/torso_velocity Min               -1.89076\n",
      "time/data storing (s)                                  0.038609\n",
      "time/evaluation sampling (s)                          48.8545\n",
      "time/exploration sampling (s)                          1.97191\n",
      "time/logging (s)                                       0.274673\n",
      "time/saving (s)                                        0.0256489\n",
      "time/training (s)                                      3.75067\n",
      "time/epoch (s)                                        54.916\n",
      "time/total (s)                                       172.397\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:58:34.952621 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.414458\n",
      "trainer/QF2 Loss                                        0.411235\n",
      "trainer/Policy Loss                                    -9.21571\n",
      "trainer/Q1 Predictions Mean                             3.75487\n",
      "trainer/Q1 Predictions Std                              0.251152\n",
      "trainer/Q1 Predictions Max                              4.58926\n",
      "trainer/Q1 Predictions Min                              2.75524\n",
      "trainer/Q2 Predictions Mean                             3.7548\n",
      "trainer/Q2 Predictions Std                              0.255502\n",
      "trainer/Q2 Predictions Max                              4.60865\n",
      "trainer/Q2 Predictions Min                              2.73313\n",
      "trainer/Q Targets Mean                                  3.7249\n",
      "trainer/Q Targets Std                                   0.63487\n",
      "trainer/Q Targets Max                                   6.32421\n",
      "trainer/Q Targets Min                                  -0.829927\n",
      "trainer/Log Pis Mean                                   -5.48353\n",
      "trainer/Log Pis Std                                     0.421542\n",
      "trainer/Log Pis Max                                    -4.69968\n",
      "trainer/Log Pis Min                                    -9.59654\n",
      "trainer/Policy mu Mean                                 -0.00868573\n",
      "trainer/Policy mu Std                                   0.0272104\n",
      "trainer/Policy mu Max                                   0.0554869\n",
      "trainer/Policy mu Min                                  -0.14012\n",
      "trainer/Policy log std Mean                            -0.137305\n",
      "trainer/Policy log std Std                              0.0166444\n",
      "trainer/Policy log std Max                             -0.0999286\n",
      "trainer/Policy log std Min                             -0.218727\n",
      "trainer/Alpha                                           0.405265\n",
      "trainer/Alpha Loss                                    -12.1381\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            32\n",
      "exploration/path length Mean                          142.857\n",
      "exploration/path length Std                           124.652\n",
      "exploration/path length Max                           423\n",
      "exploration/path length Min                            19\n",
      "exploration/Rewards Mean                               -0.315056\n",
      "exploration/Rewards Std                                 0.476423\n",
      "exploration/Rewards Max                                 2.23645\n",
      "exploration/Rewards Min                                -1.68558\n",
      "exploration/Returns Mean                              -45.0079\n",
      "exploration/Returns Std                                44.4294\n",
      "exploration/Returns Max                                -4.89336\n",
      "exploration/Returns Min                              -140.656\n",
      "exploration/Actions Mean                               -0.000884001\n",
      "exploration/Actions Std                                 0.58627\n",
      "exploration/Actions Max                                 0.997501\n",
      "exploration/Actions Min                                -0.993615\n",
      "exploration/Num Paths                                   7\n",
      "exploration/Average Returns                           -45.0079\n",
      "exploration/env_infos/final/reward_forward Mean        -0.164915\n",
      "exploration/env_infos/final/reward_forward Std          0.968755\n",
      "exploration/env_infos/final/reward_forward Max          0.986741\n",
      "exploration/env_infos/final/reward_forward Min         -1.47248\n",
      "exploration/env_infos/initial/reward_forward Mean       0.11124\n",
      "exploration/env_infos/initial/reward_forward Std        0.188384\n",
      "exploration/env_infos/initial/reward_forward Max        0.485053\n",
      "exploration/env_infos/initial/reward_forward Min       -0.132968\n",
      "exploration/env_infos/reward_forward Mean              -0.113497\n",
      "exploration/env_infos/reward_forward Std                0.67097\n",
      "exploration/env_infos/reward_forward Max                1.78185\n",
      "exploration/env_infos/reward_forward Min               -2.70832\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.10644\n",
      "exploration/env_infos/final/reward_ctrl Std             0.392629\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.599918\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.54852\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.37491\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.28052\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.954721\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.71408\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.37485\n",
      "exploration/env_infos/reward_ctrl Std                   0.404667\n",
      "exploration/env_infos/reward_ctrl Max                  -0.334459\n",
      "exploration/env_infos/reward_ctrl Min                  -2.68558\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.396752\n",
      "exploration/env_infos/final/torso_velocity Std          1.00707\n",
      "exploration/env_infos/final/torso_velocity Max          2.57213\n",
      "exploration/env_infos/final/torso_velocity Min         -1.47248\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.161916\n",
      "exploration/env_infos/initial/torso_velocity Std        0.229276\n",
      "exploration/env_infos/initial/torso_velocity Max        0.560915\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.213692\n",
      "exploration/env_infos/torso_velocity Mean              -0.0413683\n",
      "exploration/env_infos/torso_velocity Std                0.680989\n",
      "exploration/env_infos/torso_velocity Max                3.75984\n",
      "exploration/env_infos/torso_velocity Min               -2.71324\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.997831\n",
      "evaluation/Rewards Std                                  0.0172638\n",
      "evaluation/Rewards Max                                  2.13808\n",
      "evaluation/Rewards Min                                  0.996409\n",
      "evaluation/Returns Mean                               997.831\n",
      "evaluation/Returns Std                                  1.22238\n",
      "evaluation/Returns Max                               1000.85\n",
      "evaluation/Returns Min                                996.61\n",
      "evaluation/Actions Mean                                -0.00969224\n",
      "evaluation/Actions Std                                  0.025531\n",
      "evaluation/Actions Max                                  0.0495051\n",
      "evaluation/Actions Min                                 -0.0589557\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            997.831\n",
      "evaluation/env_infos/final/reward_forward Mean          2.61e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.44023e-05\n",
      "evaluation/env_infos/final/reward_forward Max           7.31386e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -7.94558e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0595734\n",
      "evaluation/env_infos/initial/reward_forward Std         0.116144\n",
      "evaluation/env_infos/initial/reward_forward Max         0.257996\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.179817\n",
      "evaluation/env_infos/reward_forward Mean                0.000531277\n",
      "evaluation/env_infos/reward_forward Std                 0.0458716\n",
      "evaluation/env_infos/reward_forward Max                 0.975097\n",
      "evaluation/env_infos/reward_forward Min                -1.2906\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00298884\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000317549\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00242056\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00339998\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00251759\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000400549\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00187121\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00297138\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00298308\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000321144\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00149554\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00359113\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.65534e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000287448\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00220296\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.94558e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.185257\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227057\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.676839\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.184569\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000636872\n",
      "evaluation/env_infos/torso_velocity Std                 0.0517504\n",
      "evaluation/env_infos/torso_velocity Max                 1.29586\n",
      "evaluation/env_infos/torso_velocity Min                -1.9411\n",
      "time/data storing (s)                                   0.0338828\n",
      "time/evaluation sampling (s)                           47.5152\n",
      "time/exploration sampling (s)                           1.88207\n",
      "time/logging (s)                                        0.282045\n",
      "time/saving (s)                                         0.02699\n",
      "time/training (s)                                       3.37281\n",
      "time/epoch (s)                                         53.113\n",
      "time/total (s)                                        225.714\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 11:59:26.369209 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.347563\n",
      "trainer/QF2 Loss                                        0.359824\n",
      "trainer/Policy Loss                                    -9.24935\n",
      "trainer/Q1 Predictions Mean                             3.82111\n",
      "trainer/Q1 Predictions Std                              0.24304\n",
      "trainer/Q1 Predictions Max                              4.58188\n",
      "trainer/Q1 Predictions Min                              3.17998\n",
      "trainer/Q2 Predictions Mean                             3.80873\n",
      "trainer/Q2 Predictions Std                              0.241334\n",
      "trainer/Q2 Predictions Max                              4.51914\n",
      "trainer/Q2 Predictions Min                              3.16487\n",
      "trainer/Q Targets Mean                                  3.772\n",
      "trainer/Q Targets Std                                   0.613904\n",
      "trainer/Q Targets Max                                   6.5795\n",
      "trainer/Q Targets Min                                  -0.642651\n",
      "trainer/Log Pis Mean                                   -5.4631\n",
      "trainer/Log Pis Std                                     0.362704\n",
      "trainer/Log Pis Max                                    -4.40807\n",
      "trainer/Log Pis Min                                    -7.27474\n",
      "trainer/Policy mu Mean                                 -0.00729035\n",
      "trainer/Policy mu Std                                   0.039141\n",
      "trainer/Policy mu Max                                   0.0927948\n",
      "trainer/Policy mu Min                                  -0.168976\n",
      "trainer/Policy log std Mean                            -0.119763\n",
      "trainer/Policy log std Std                              0.0118808\n",
      "trainer/Policy log std Max                             -0.0930377\n",
      "trainer/Policy log std Min                             -0.17812\n",
      "trainer/Alpha                                           0.300203\n",
      "trainer/Alpha Loss                                    -16.1597\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            37\n",
      "exploration/path length Mean                          200\n",
      "exploration/path length Std                           354.536\n",
      "exploration/path length Max                           909\n",
      "exploration/path length Min                            14\n",
      "exploration/Rewards Mean                               -0.340409\n",
      "exploration/Rewards Std                                 0.471329\n",
      "exploration/Rewards Max                                 2.07691\n",
      "exploration/Rewards Min                                -1.8493\n",
      "exploration/Returns Mean                              -68.0818\n",
      "exploration/Returns Std                               129.331\n",
      "exploration/Returns Max                                 7.64948\n",
      "exploration/Returns Min                              -326.395\n",
      "exploration/Actions Mean                               -0.016662\n",
      "exploration/Actions Std                                 0.59102\n",
      "exploration/Actions Max                                 0.996331\n",
      "exploration/Actions Min                                -0.999097\n",
      "exploration/Num Paths                                   5\n",
      "exploration/Average Returns                           -68.0818\n",
      "exploration/env_infos/final/reward_forward Mean        -0.624245\n",
      "exploration/env_infos/final/reward_forward Std          1.43043\n",
      "exploration/env_infos/final/reward_forward Max          1.33378\n",
      "exploration/env_infos/final/reward_forward Min         -2.49781\n",
      "exploration/env_infos/initial/reward_forward Mean       0.026795\n",
      "exploration/env_infos/initial/reward_forward Std        0.142202\n",
      "exploration/env_infos/initial/reward_forward Max        0.258407\n",
      "exploration/env_infos/initial/reward_forward Min       -0.151078\n",
      "exploration/env_infos/reward_forward Mean              -0.0874263\n",
      "exploration/env_infos/reward_forward Std                0.560351\n",
      "exploration/env_infos/reward_forward Max                1.36791\n",
      "exploration/env_infos/reward_forward Min               -2.76283\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.34722\n",
      "exploration/env_infos/final/reward_ctrl Std             0.499382\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.780029\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.11722\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.5446\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.360425\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.17726\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.19464\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.39833\n",
      "exploration/env_infos/reward_ctrl Std                   0.409417\n",
      "exploration/env_infos/reward_ctrl Max                  -0.288899\n",
      "exploration/env_infos/reward_ctrl Min                  -2.8493\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.20671\n",
      "exploration/env_infos/final/torso_velocity Std          1.2897\n",
      "exploration/env_infos/final/torso_velocity Max          2.32645\n",
      "exploration/env_infos/final/torso_velocity Min         -2.49781\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.146234\n",
      "exploration/env_infos/initial/torso_velocity Std        0.275846\n",
      "exploration/env_infos/initial/torso_velocity Max        0.598964\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.361802\n",
      "exploration/env_infos/torso_velocity Mean              -0.019735\n",
      "exploration/env_infos/torso_velocity Std                0.50898\n",
      "exploration/env_infos/torso_velocity Max                2.66029\n",
      "exploration/env_infos/torso_velocity Min               -2.99628\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.994871\n",
      "evaluation/Rewards Std                                  0.0220517\n",
      "evaluation/Rewards Max                                  2.43058\n",
      "evaluation/Rewards Min                                  0.991596\n",
      "evaluation/Returns Mean                               994.871\n",
      "evaluation/Returns Std                                  1.18447\n",
      "evaluation/Returns Max                                999.229\n",
      "evaluation/Returns Min                                993.672\n",
      "evaluation/Actions Mean                                -0.00752581\n",
      "evaluation/Actions Std                                  0.0375493\n",
      "evaluation/Actions Max                                  0.0638056\n",
      "evaluation/Actions Min                                 -0.110268\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            994.871\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.89715e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.00017724\n",
      "evaluation/env_infos/final/reward_forward Max           0.000247277\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000580726\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00756816\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112475\n",
      "evaluation/env_infos/initial/reward_forward Max         0.231625\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.266554\n",
      "evaluation/env_infos/reward_forward Mean                0.00139851\n",
      "evaluation/env_infos/reward_forward Std                 0.0404766\n",
      "evaluation/env_infos/reward_forward Max                 1.1187\n",
      "evaluation/env_infos/reward_forward Min                -1.19725\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00586763\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000461932\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00498012\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00638571\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00523204\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000431412\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0044381\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00578431\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00586635\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000471641\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00330329\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00840437\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.01076e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000235138\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0012644\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000580726\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152224\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.25726\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.712042\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.342486\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000564983\n",
      "evaluation/env_infos/torso_velocity Std                 0.0530215\n",
      "evaluation/env_infos/torso_velocity Max                 1.56505\n",
      "evaluation/env_infos/torso_velocity Min                -1.72075\n",
      "time/data storing (s)                                   0.0310322\n",
      "time/evaluation sampling (s)                           45.4049\n",
      "time/exploration sampling (s)                           1.88017\n",
      "time/logging (s)                                        0.269891\n",
      "time/saving (s)                                         0.0266308\n",
      "time/training (s)                                       3.58204\n",
      "time/epoch (s)                                         51.1946\n",
      "time/total (s)                                        277.119\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:00:21.803244 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.355843\n",
      "trainer/QF2 Loss                                        0.351762\n",
      "trainer/Policy Loss                                    -9.14466\n",
      "trainer/Q1 Predictions Mean                             3.71174\n",
      "trainer/Q1 Predictions Std                              0.273209\n",
      "trainer/Q1 Predictions Max                              4.65897\n",
      "trainer/Q1 Predictions Min                              3.13643\n",
      "trainer/Q2 Predictions Mean                             3.69668\n",
      "trainer/Q2 Predictions Std                              0.272177\n",
      "trainer/Q2 Predictions Max                              4.6127\n",
      "trainer/Q2 Predictions Min                              2.99153\n",
      "trainer/Q Targets Mean                                  3.71382\n",
      "trainer/Q Targets Std                                   0.641016\n",
      "trainer/Q Targets Max                                   5.84868\n",
      "trainer/Q Targets Min                                  -0.405301\n",
      "trainer/Log Pis Mean                                   -5.45858\n",
      "trainer/Log Pis Std                                     0.553415\n",
      "trainer/Log Pis Max                                    -3.94425\n",
      "trainer/Log Pis Min                                    -9.82236\n",
      "trainer/Policy mu Mean                                 -0.00781913\n",
      "trainer/Policy mu Std                                   0.0783435\n",
      "trainer/Policy mu Max                                   0.289396\n",
      "trainer/Policy mu Min                                  -0.255829\n",
      "trainer/Policy log std Mean                            -0.136003\n",
      "trainer/Policy log std Std                              0.0196324\n",
      "trainer/Policy log std Max                             -0.0714607\n",
      "trainer/Policy log std Min                             -0.227597\n",
      "trainer/Alpha                                           0.222438\n",
      "trainer/Alpha Loss                                    -20.1893\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            46\n",
      "exploration/path length Mean                          111.111\n",
      "exploration/path length Std                           163.927\n",
      "exploration/path length Max                           518\n",
      "exploration/path length Min                            12\n",
      "exploration/Rewards Mean                               -0.375729\n",
      "exploration/Rewards Std                                 0.459206\n",
      "exploration/Rewards Max                                 1.72816\n",
      "exploration/Rewards Min                                -2.11543\n",
      "exploration/Returns Mean                              -41.7477\n",
      "exploration/Returns Std                                62.3973\n",
      "exploration/Returns Max                                -3.66542\n",
      "exploration/Returns Min                              -196.287\n",
      "exploration/Actions Mean                               -0.00760253\n",
      "exploration/Actions Std                                 0.593154\n",
      "exploration/Actions Max                                 0.995159\n",
      "exploration/Actions Min                                -0.998058\n",
      "exploration/Num Paths                                   9\n",
      "exploration/Average Returns                           -41.7477\n",
      "exploration/env_infos/final/reward_forward Mean         0.177841\n",
      "exploration/env_infos/final/reward_forward Std          0.725025\n",
      "exploration/env_infos/final/reward_forward Max          1.24452\n",
      "exploration/env_infos/final/reward_forward Min         -0.998941\n",
      "exploration/env_infos/initial/reward_forward Mean       0.097291\n",
      "exploration/env_infos/initial/reward_forward Std        0.155432\n",
      "exploration/env_infos/initial/reward_forward Max        0.26549\n",
      "exploration/env_infos/initial/reward_forward Min       -0.218177\n",
      "exploration/env_infos/reward_forward Mean               0.0198563\n",
      "exploration/env_infos/reward_forward Std                0.644664\n",
      "exploration/env_infos/reward_forward Max                1.83712\n",
      "exploration/env_infos/reward_forward Min               -2.32418\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.28579\n",
      "exploration/env_infos/final/reward_ctrl Std             0.307008\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.801954\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.68677\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.4068\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.560503\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.855391\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.20845\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.40756\n",
      "exploration/env_infos/reward_ctrl Std                   0.426162\n",
      "exploration/env_infos/reward_ctrl Max                  -0.320383\n",
      "exploration/env_infos/reward_ctrl Min                  -3.11543\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.499395\n",
      "exploration/env_infos/final/torso_velocity Std          0.83126\n",
      "exploration/env_infos/final/torso_velocity Max          2.94576\n",
      "exploration/env_infos/final/torso_velocity Min         -0.998941\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.13988\n",
      "exploration/env_infos/initial/torso_velocity Std        0.232618\n",
      "exploration/env_infos/initial/torso_velocity Max        0.52537\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.317513\n",
      "exploration/env_infos/torso_velocity Mean               0.0133856\n",
      "exploration/env_infos/torso_velocity Std                0.648698\n",
      "exploration/env_infos/torso_velocity Max                3.97675\n",
      "exploration/env_infos/torso_velocity Min               -2.51376\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.990205\n",
      "evaluation/Rewards Std                                  0.0135022\n",
      "evaluation/Rewards Max                                  2.31545\n",
      "evaluation/Rewards Min                                  0.952622\n",
      "evaluation/Returns Mean                               990.205\n",
      "evaluation/Returns Std                                  0.774118\n",
      "evaluation/Returns Max                                992.427\n",
      "evaluation/Returns Min                                989.117\n",
      "evaluation/Actions Mean                                -0.00972301\n",
      "evaluation/Actions Std                                  0.0491454\n",
      "evaluation/Actions Max                                  0.153809\n",
      "evaluation/Actions Min                                 -0.142936\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            990.205\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000402455\n",
      "evaluation/env_infos/final/reward_forward Std           0.00197428\n",
      "evaluation/env_infos/final/reward_forward Max           0.0100744\n",
      "evaluation/env_infos/final/reward_forward Min          -1.34039e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0266902\n",
      "evaluation/env_infos/initial/reward_forward Std         0.114822\n",
      "evaluation/env_infos/initial/reward_forward Max         0.141522\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.286926\n",
      "evaluation/env_infos/reward_forward Mean               -0.00360617\n",
      "evaluation/env_infos/reward_forward Std                 0.05405\n",
      "evaluation/env_infos/reward_forward Max                 0.773038\n",
      "evaluation/env_infos/reward_forward Min                -1.57834\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00992715\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000534873\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00850049\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0107805\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00918222\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00144551\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00703165\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0123018\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0100392\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00143903\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00703165\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0473779\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.9519e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00136184\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0100744\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00604794\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126611\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.247212\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.571766\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.286926\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00211009\n",
      "evaluation/env_infos/torso_velocity Std                 0.0536137\n",
      "evaluation/env_infos/torso_velocity Max                 1.4571\n",
      "evaluation/env_infos/torso_velocity Min                -1.82569\n",
      "time/data storing (s)                                   0.0377513\n",
      "time/evaluation sampling (s)                           49.5409\n",
      "time/exploration sampling (s)                           1.95615\n",
      "time/logging (s)                                        0.273246\n",
      "time/saving (s)                                         0.0246578\n",
      "time/training (s)                                       3.3927\n",
      "time/epoch (s)                                         55.2254\n",
      "time/total (s)                                        332.556\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:01:13.093725 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.579423\n",
      "trainer/QF2 Loss                                        0.575744\n",
      "trainer/Policy Loss                                    -9.24286\n",
      "trainer/Q1 Predictions Mean                             3.84922\n",
      "trainer/Q1 Predictions Std                              0.30419\n",
      "trainer/Q1 Predictions Max                              4.67076\n",
      "trainer/Q1 Predictions Min                              2.9966\n",
      "trainer/Q2 Predictions Mean                             3.8593\n",
      "trainer/Q2 Predictions Std                              0.306961\n",
      "trainer/Q2 Predictions Max                              4.88718\n",
      "trainer/Q2 Predictions Min                              3.07833\n",
      "trainer/Q Targets Mean                                  3.73701\n",
      "trainer/Q Targets Std                                   0.84316\n",
      "trainer/Q Targets Max                                   6.7721\n",
      "trainer/Q Targets Min                                  -1.4169\n",
      "trainer/Log Pis Mean                                   -5.41368\n",
      "trainer/Log Pis Std                                     0.547568\n",
      "trainer/Log Pis Max                                    -3.45447\n",
      "trainer/Log Pis Min                                    -9.92574\n",
      "trainer/Policy mu Mean                                 -0.0169701\n",
      "trainer/Policy mu Std                                   0.0997567\n",
      "trainer/Policy mu Max                                   0.437909\n",
      "trainer/Policy mu Min                                  -0.414455\n",
      "trainer/Policy log std Mean                            -0.159885\n",
      "trainer/Policy log std Std                              0.0234526\n",
      "trainer/Policy log std Max                             -0.105241\n",
      "trainer/Policy log std Min                             -0.282664\n",
      "trainer/Alpha                                           0.164848\n",
      "trainer/Alpha Loss                                    -24.1412\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            58\n",
      "exploration/path length Mean                           83.3333\n",
      "exploration/path length Std                            57.0385\n",
      "exploration/path length Max                           238\n",
      "exploration/path length Min                            12\n",
      "exploration/Rewards Mean                               -0.25153\n",
      "exploration/Rewards Std                                 0.527071\n",
      "exploration/Rewards Max                                 2.33328\n",
      "exploration/Rewards Min                                -1.52912\n",
      "exploration/Returns Mean                              -20.9609\n",
      "exploration/Returns Std                                21.207\n",
      "exploration/Returns Max                                -3.61499\n",
      "exploration/Returns Min                               -81.345\n",
      "exploration/Actions Mean                               -0.0175783\n",
      "exploration/Actions Std                                 0.579603\n",
      "exploration/Actions Max                                 0.996699\n",
      "exploration/Actions Min                                -0.995234\n",
      "exploration/Num Paths                                  12\n",
      "exploration/Average Returns                           -20.9609\n",
      "exploration/env_infos/final/reward_forward Mean         0.270855\n",
      "exploration/env_infos/final/reward_forward Std          1.12292\n",
      "exploration/env_infos/final/reward_forward Max          2.06048\n",
      "exploration/env_infos/final/reward_forward Min         -1.77741\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0256924\n",
      "exploration/env_infos/initial/reward_forward Std        0.187898\n",
      "exploration/env_infos/initial/reward_forward Max        0.306634\n",
      "exploration/env_infos/initial/reward_forward Min       -0.313361\n",
      "exploration/env_infos/reward_forward Mean               0.0330064\n",
      "exploration/env_infos/reward_forward Std                0.680937\n",
      "exploration/env_infos/reward_forward Max                2.21026\n",
      "exploration/env_infos/reward_forward Min               -2.64605\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.44624\n",
      "exploration/env_infos/final/reward_ctrl Std             0.413332\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.710636\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.32145\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.57991\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.406769\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.908374\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.31856\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.345\n",
      "exploration/env_infos/reward_ctrl Std                   0.41569\n",
      "exploration/env_infos/reward_ctrl Max                  -0.177123\n",
      "exploration/env_infos/reward_ctrl Min                  -2.52912\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.518116\n",
      "exploration/env_infos/final/torso_velocity Std          1.12207\n",
      "exploration/env_infos/final/torso_velocity Max          2.74922\n",
      "exploration/env_infos/final/torso_velocity Min         -1.77741\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.173685\n",
      "exploration/env_infos/initial/torso_velocity Std        0.241528\n",
      "exploration/env_infos/initial/torso_velocity Max        0.655557\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.313361\n",
      "exploration/env_infos/torso_velocity Mean               0.0185853\n",
      "exploration/env_infos/torso_velocity Std                0.739979\n",
      "exploration/env_infos/torso_velocity Max                3.56985\n",
      "exploration/env_infos/torso_velocity Min               -2.64605\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.98167\n",
      "evaluation/Rewards Std                                  0.018857\n",
      "evaluation/Rewards Max                                  2.05773\n",
      "evaluation/Rewards Min                                  0.94159\n",
      "evaluation/Returns Mean                               981.67\n",
      "evaluation/Returns Std                                  2.25833\n",
      "evaluation/Returns Max                                988.749\n",
      "evaluation/Returns Min                                978.78\n",
      "evaluation/Actions Mean                                -0.0126643\n",
      "evaluation/Actions Std                                  0.0685498\n",
      "evaluation/Actions Max                                  0.20325\n",
      "evaluation/Actions Min                                 -0.183184\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            981.67\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00800028\n",
      "evaluation/env_infos/final/reward_forward Std           0.0271528\n",
      "evaluation/env_infos/final/reward_forward Max           0.104712\n",
      "evaluation/env_infos/final/reward_forward Min          -9.74571e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00089079\n",
      "evaluation/env_infos/initial/reward_forward Std         0.137521\n",
      "evaluation/env_infos/initial/reward_forward Max         0.246657\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.267057\n",
      "evaluation/env_infos/reward_forward Mean               -0.00138305\n",
      "evaluation/env_infos/reward_forward Std                 0.0549562\n",
      "evaluation/env_infos/reward_forward Max                 1.43317\n",
      "evaluation/env_infos/reward_forward Min                -1.00844\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0189543\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00176933\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0154943\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0215217\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.015411\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00348493\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0120414\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0239454\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0194378\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00265633\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0106682\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0584097\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00403788\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0214524\n",
      "evaluation/env_infos/final/torso_velocity Max           0.123098\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.022882\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146736\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246492\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.661225\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.270125\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00146498\n",
      "evaluation/env_infos/torso_velocity Std                 0.0527347\n",
      "evaluation/env_infos/torso_velocity Max                 1.43317\n",
      "evaluation/env_infos/torso_velocity Min                -1.77078\n",
      "time/data storing (s)                                   0.0368773\n",
      "time/evaluation sampling (s)                           45.499\n",
      "time/exploration sampling (s)                           1.83494\n",
      "time/logging (s)                                        0.292891\n",
      "time/saving (s)                                         0.0266538\n",
      "time/training (s)                                       3.40373\n",
      "time/epoch (s)                                         51.0941\n",
      "time/total (s)                                        383.866\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:02:04.789384 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.249001\n",
      "trainer/QF2 Loss                                        0.264629\n",
      "trainer/Policy Loss                                    -8.96245\n",
      "trainer/Q1 Predictions Mean                             3.74123\n",
      "trainer/Q1 Predictions Std                              0.354192\n",
      "trainer/Q1 Predictions Max                              4.89485\n",
      "trainer/Q1 Predictions Min                              2.92183\n",
      "trainer/Q2 Predictions Mean                             3.7393\n",
      "trainer/Q2 Predictions Std                              0.35151\n",
      "trainer/Q2 Predictions Max                              4.88253\n",
      "trainer/Q2 Predictions Min                              2.84279\n",
      "trainer/Q Targets Mean                                  3.75667\n",
      "trainer/Q Targets Std                                   0.58052\n",
      "trainer/Q Targets Max                                   6.38501\n",
      "trainer/Q Targets Min                                   2.63673\n",
      "trainer/Log Pis Mean                                   -5.24095\n",
      "trainer/Log Pis Std                                     0.662515\n",
      "trainer/Log Pis Max                                    -3.27299\n",
      "trainer/Log Pis Min                                    -8.59854\n",
      "trainer/Policy mu Mean                                 -0.0293777\n",
      "trainer/Policy mu Std                                   0.157917\n",
      "trainer/Policy mu Max                                   0.653579\n",
      "trainer/Policy mu Min                                  -0.738611\n",
      "trainer/Policy log std Mean                            -0.253293\n",
      "trainer/Policy log std Std                              0.0721178\n",
      "trainer/Policy log std Max                             -0.137134\n",
      "trainer/Policy log std Min                             -0.548884\n",
      "trainer/Alpha                                           0.122341\n",
      "trainer/Alpha Loss                                    -27.7793\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            71\n",
      "exploration/path length Mean                           76.9231\n",
      "exploration/path length Std                            93.8759\n",
      "exploration/path length Max                           374\n",
      "exploration/path length Min                            11\n",
      "exploration/Rewards Mean                               -0.167936\n",
      "exploration/Rewards Std                                 0.525697\n",
      "exploration/Rewards Max                                 2.76579\n",
      "exploration/Rewards Min                                -1.63764\n",
      "exploration/Returns Mean                              -12.9181\n",
      "exploration/Returns Std                                16.0112\n",
      "exploration/Returns Max                                 5.42432\n",
      "exploration/Returns Min                               -57.5528\n",
      "exploration/Actions Mean                               -0.0132087\n",
      "exploration/Actions Std                                 0.558673\n",
      "exploration/Actions Max                                 0.991913\n",
      "exploration/Actions Min                                -0.992564\n",
      "exploration/Num Paths                                  13\n",
      "exploration/Average Returns                           -12.9181\n",
      "exploration/env_infos/final/reward_forward Mean        -0.434984\n",
      "exploration/env_infos/final/reward_forward Std          1.10731\n",
      "exploration/env_infos/final/reward_forward Max          1.64043\n",
      "exploration/env_infos/final/reward_forward Min         -2.22702\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0663878\n",
      "exploration/env_infos/initial/reward_forward Std        0.150222\n",
      "exploration/env_infos/initial/reward_forward Max        0.273489\n",
      "exploration/env_infos/initial/reward_forward Min       -0.199797\n",
      "exploration/env_infos/reward_forward Mean              -0.0449821\n",
      "exploration/env_infos/reward_forward Std                0.83874\n",
      "exploration/env_infos/reward_forward Max                2.32095\n",
      "exploration/env_infos/reward_forward Min               -2.3447\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.28593\n",
      "exploration/env_infos/final/reward_ctrl Std             0.375833\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.738829\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.98893\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.16624\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.469688\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.164035\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.09699\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.24916\n",
      "exploration/env_infos/reward_ctrl Std                   0.39812\n",
      "exploration/env_infos/reward_ctrl Max                  -0.164035\n",
      "exploration/env_infos/reward_ctrl Min                  -2.63764\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.457273\n",
      "exploration/env_infos/final/torso_velocity Std          1.30823\n",
      "exploration/env_infos/final/torso_velocity Max          3.16487\n",
      "exploration/env_infos/final/torso_velocity Min         -2.22702\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.16966\n",
      "exploration/env_infos/initial/torso_velocity Std        0.214086\n",
      "exploration/env_infos/initial/torso_velocity Max        0.603206\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.199797\n",
      "exploration/env_infos/torso_velocity Mean               0.0214552\n",
      "exploration/env_infos/torso_velocity Std                0.825315\n",
      "exploration/env_infos/torso_velocity Max                4.23826\n",
      "exploration/env_infos/torso_velocity Min               -2.35432\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.962328\n",
      "evaluation/Rewards Std                                  0.0294716\n",
      "evaluation/Rewards Max                                  2.09642\n",
      "evaluation/Rewards Min                                  0.477633\n",
      "evaluation/Returns Mean                               962.328\n",
      "evaluation/Returns Std                                  5.2236\n",
      "evaluation/Returns Max                                981.359\n",
      "evaluation/Returns Min                                954.596\n",
      "evaluation/Actions Mean                                -0.0264347\n",
      "evaluation/Actions Std                                  0.0961321\n",
      "evaluation/Actions Max                                  0.658593\n",
      "evaluation/Actions Min                                 -0.595885\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            962.328\n",
      "evaluation/env_infos/final/reward_forward Mean          7.49947e-06\n",
      "evaluation/env_infos/final/reward_forward Std           0.00637887\n",
      "evaluation/env_infos/final/reward_forward Max           0.0226973\n",
      "evaluation/env_infos/final/reward_forward Min          -0.022376\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0334523\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118741\n",
      "evaluation/env_infos/initial/reward_forward Max         0.190985\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.367681\n",
      "evaluation/env_infos/reward_forward Mean               -0.00125646\n",
      "evaluation/env_infos/reward_forward Std                 0.0625113\n",
      "evaluation/env_infos/reward_forward Max                 0.759198\n",
      "evaluation/env_infos/reward_forward Min                -1.17121\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0381157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_ctrl Std              0.00645541\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0230325\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0520445\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0349044\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00436316\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0281782\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0444218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0397607\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0171568\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0142418\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.522367\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.03751e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00508586\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0226973\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.022376\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135417\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239706\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.653362\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.367681\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00111497\n",
      "evaluation/env_infos/torso_velocity Std                 0.0594028\n",
      "evaluation/env_infos/torso_velocity Max                 1.49762\n",
      "evaluation/env_infos/torso_velocity Min                -2.26306\n",
      "time/data storing (s)                                   0.0424413\n",
      "time/evaluation sampling (s)                           45.2465\n",
      "time/exploration sampling (s)                           1.84583\n",
      "time/logging (s)                                        0.29072\n",
      "time/saving (s)                                         0.0267584\n",
      "time/training (s)                                       4.00588\n",
      "time/epoch (s)                                         51.4581\n",
      "time/total (s)                                        435.559\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 12:02:55.928322 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  20000\n",
      "trainer/QF1 Loss                                        0.273302\n",
      "trainer/QF2 Loss                                        0.318101\n",
      "trainer/Policy Loss                                    -8.23499\n",
      "trainer/Q1 Predictions Mean                             3.48445\n",
      "trainer/Q1 Predictions Std                              0.403992\n",
      "trainer/Q1 Predictions Max                              5.04788\n",
      "trainer/Q1 Predictions Min                              2.17087\n",
      "trainer/Q2 Predictions Mean                             3.46584\n",
      "trainer/Q2 Predictions Std                              0.372497\n",
      "trainer/Q2 Predictions Max                              5.14031\n",
      "trainer/Q2 Predictions Min                              2.50129\n",
      "trainer/Q Targets Mean                                  3.62252\n",
      "trainer/Q Targets Std                                   0.652004\n",
      "trainer/Q Targets Max                                   6.50436\n",
      "trainer/Q Targets Min                                  -0.485075\n",
      "trainer/Log Pis Mean                                   -4.71192\n",
      "trainer/Log Pis Std                                     1.11754\n",
      "trainer/Log Pis Max                                    -1.32489\n",
      "trainer/Log Pis Min                                    -9.49254\n",
      "trainer/Policy mu Mean                                  0.0115734\n",
      "trainer/Policy mu Std                                   0.198706\n",
      "trainer/Policy mu Max                                   0.844213\n",
      "trainer/Policy mu Min                                  -0.974396\n",
      "trainer/Policy log std Mean                            -0.415657\n",
      "trainer/Policy log std Std                              0.154115\n",
      "trainer/Policy log std Max                             -0.165718\n",
      "trainer/Policy log std Min                             -1.01086\n",
      "trainer/Alpha                                           0.0914037\n",
      "trainer/Alpha Loss                                    -30.3765\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            79\n",
      "exploration/path length Mean                          125\n",
      "exploration/path length Std                            86.0697\n",
      "exploration/path length Max                           278\n",
      "exploration/path length Min                            23\n",
      "exploration/Rewards Mean                               -0.0126352\n",
      "exploration/Rewards Std                                 0.438689\n",
      "exploration/Rewards Max                                 1.9005\n",
      "exploration/Rewards Min                                -1.22158\n",
      "exploration/Returns Mean                               -1.5794\n",
      "exploration/Returns Std                                10.7314\n",
      "exploration/Returns Max                                16.3177\n",
      "exploration/Returns Min                               -16.5973\n",
      "exploration/Actions Mean                                0.0182665\n",
      "exploration/Actions Std                                 0.519977\n",
      "exploration/Actions Max                                 0.99278\n",
      "exploration/Actions Min                                -0.988834\n",
      "exploration/Num Paths                                   8\n",
      "exploration/Average Returns                            -1.5794\n",
      "exploration/env_infos/final/reward_forward Mean         0.0634874\n",
      "exploration/env_infos/final/reward_forward Std          0.918973\n",
      "exploration/env_infos/final/reward_forward Max          1.55137\n",
      "exploration/env_infos/final/reward_forward Min         -1.18904\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0559374\n",
      "exploration/env_infos/initial/reward_forward Std        0.225035\n",
      "exploration/env_infos/initial/reward_forward Max        0.253688\n",
      "exploration/env_infos/initial/reward_forward Min       -0.508833\n",
      "exploration/env_infos/reward_forward Mean              -0.0112998\n",
      "exploration/env_infos/reward_forward Std                0.707264\n",
      "exploration/env_infos/reward_forward Max                2.59467\n",
      "exploration/env_infos/reward_forward Min               -2.37134\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.905486\n",
      "exploration/env_infos/final/reward_ctrl Std             0.398396\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.380128\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.65519\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.977953\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.366628\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.470367\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.56448\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.08284\n",
      "exploration/env_infos/reward_ctrl Std                   0.364536\n",
      "exploration/env_infos/reward_ctrl Max                  -0.118921\n",
      "exploration/env_infos/reward_ctrl Min                  -2.22158\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.3024\n",
      "exploration/env_infos/final/torso_velocity Std          0.865496\n",
      "exploration/env_infos/final/torso_velocity Max          1.55137\n",
      "exploration/env_infos/final/torso_velocity Min         -1.26113\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.118748\n",
      "exploration/env_infos/initial/torso_velocity Std        0.312693\n",
      "exploration/env_infos/initial/torso_velocity Max        0.737818\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.508833\n",
      "exploration/env_infos/torso_velocity Mean               0.0278171\n",
      "exploration/env_infos/torso_velocity Std                0.771893\n",
      "exploration/env_infos/torso_velocity Max                3.04939\n",
      "exploration/env_infos/torso_velocity Min               -2.37134\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966618\n",
      "evaluation/Rewards Std                                  0.0596087\n",
      "evaluation/Rewards Max                                  1.92936\n",
      "evaluation/Rewards Min                                  0.360232\n",
      "evaluation/Returns Mean                               966.618\n",
      "evaluation/Returns Std                                 13.3864\n",
      "evaluation/Returns Max                                996.163\n",
      "evaluation/Returns Min                                943.331\n",
      "evaluation/Actions Mean                                 0.00671833\n",
      "evaluation/Actions Std                                  0.0993418\n",
      "evaluation/Actions Max                                  0.569314\n",
      "evaluation/Actions Min                                 -0.602234\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.618\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0138795\n",
      "evaluation/env_infos/final/reward_forward Std           0.117871\n",
      "evaluation/env_infos/final/reward_forward Max           0.272717\n",
      "evaluation/env_infos/final/reward_forward Min          -0.35379\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00269251\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0842951\n",
      "evaluation/env_infos/initial/reward_forward Max         0.131585\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.181995\n",
      "evaluation/env_infos/reward_forward Mean                0.0227993\n",
      "evaluation/env_infos/reward_forward Std                 0.195292\n",
      "evaluation/env_infos/reward_forward Max                 1.27491\n",
      "evaluation/env_infos/reward_forward Min                -1.75413\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.032576\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0111141\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0194556\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.063434\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0380574\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0115969\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0204588\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0660709\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0396557\n",
      "evaluation/env_infos/reward_ctrl Std                    0.028897\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00862238\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.639768\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00813138\n",
      "evaluation/env_infos/final/torso_velocity Std           0.121194\n",
      "evaluation/env_infos/final/torso_velocity Max           0.367269\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.509665\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144828\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.25573\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.687027\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.35907\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00225196\n",
      "evaluation/env_infos/torso_velocity Std                 0.194485\n",
      "evaluation/env_infos/torso_velocity Max                 2.51717\n",
      "evaluation/env_infos/torso_velocity Min                -1.76451\n",
      "time/data storing (s)                                   0.034555\n",
      "time/evaluation sampling (s)                           44.8348\n",
      "time/exploration sampling (s)                           1.82975\n",
      "time/logging (s)                                        0.273771\n",
      "time/saving (s)                                         0.0252787\n",
      "time/training (s)                                       3.87495\n",
      "time/epoch (s)                                         50.8731\n",
      "time/total (s)                                        486.68\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:03:47.738506 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  22000\n",
      "trainer/QF1 Loss                                        0.248644\n",
      "trainer/QF2 Loss                                        0.285232\n",
      "trainer/Policy Loss                                    -7.12807\n",
      "trainer/Q1 Predictions Mean                             3.47372\n",
      "trainer/Q1 Predictions Std                              0.483184\n",
      "trainer/Q1 Predictions Max                              5.12166\n",
      "trainer/Q1 Predictions Min                              2.13631\n",
      "trainer/Q2 Predictions Mean                             3.48718\n",
      "trainer/Q2 Predictions Std                              0.447456\n",
      "trainer/Q2 Predictions Max                              5.0356\n",
      "trainer/Q2 Predictions Min                              2.15065\n",
      "trainer/Q Targets Mean                                  3.53407\n",
      "trainer/Q Targets Std                                   0.712619\n",
      "trainer/Q Targets Max                                   6.06322\n",
      "trainer/Q Targets Min                                  -0.655188\n",
      "trainer/Log Pis Mean                                   -3.38265\n",
      "trainer/Log Pis Std                                     1.31431\n",
      "trainer/Log Pis Max                                    -0.402747\n",
      "trainer/Log Pis Min                                    -8.73829\n",
      "trainer/Policy mu Mean                                  0.0062923\n",
      "trainer/Policy mu Std                                   0.165211\n",
      "trainer/Policy mu Max                                   0.611671\n",
      "trainer/Policy mu Min                                  -0.82376\n",
      "trainer/Policy log std Mean                            -0.756741\n",
      "trainer/Policy log std Std                              0.253337\n",
      "trainer/Policy log std Max                             -0.28487\n",
      "trainer/Policy log std Min                             -1.59612\n",
      "trainer/Alpha                                           0.0696253\n",
      "trainer/Alpha Loss                                    -30.3012\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                            83\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           167.271\n",
      "exploration/path length Max                           524\n",
      "exploration/path length Min                           107\n",
      "exploration/Rewards Mean                                0.361577\n",
      "exploration/Rewards Std                                 0.412605\n",
      "exploration/Rewards Max                                 2.94891\n",
      "exploration/Rewards Min                                -0.82037\n",
      "exploration/Returns Mean                               90.3942\n",
      "exploration/Returns Std                                57.0956\n",
      "exploration/Returns Max                               184.851\n",
      "exploration/Returns Min                                32.5541\n",
      "exploration/Actions Mean                                0.0236962\n",
      "exploration/Actions Std                                 0.421378\n",
      "exploration/Actions Max                                 0.971767\n",
      "exploration/Actions Min                                -0.988436\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                            90.3942\n",
      "exploration/env_infos/final/reward_forward Mean         0.26815\n",
      "exploration/env_infos/final/reward_forward Std          0.364203\n",
      "exploration/env_infos/final/reward_forward Max          0.643136\n",
      "exploration/env_infos/final/reward_forward Min         -0.232983\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.11628\n",
      "exploration/env_infos/initial/reward_forward Std        0.229718\n",
      "exploration/env_infos/initial/reward_forward Max        0.0975571\n",
      "exploration/env_infos/initial/reward_forward Min       -0.482646\n",
      "exploration/env_infos/reward_forward Mean               0.0300686\n",
      "exploration/env_infos/reward_forward Std                0.682381\n",
      "exploration/env_infos/reward_forward Max                2.60736\n",
      "exploration/env_infos/reward_forward Min               -1.84527\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.579455\n",
      "exploration/env_infos/final/reward_ctrl Std             0.149378\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.411879\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.768782\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.523792\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.239094\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.264471\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.858186\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.712483\n",
      "exploration/env_infos/reward_ctrl Std                   0.277122\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0479875\n",
      "exploration/env_infos/reward_ctrl Min                  -1.82037\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.707407\n",
      "exploration/env_infos/final/torso_velocity Std          0.611312\n",
      "exploration/env_infos/final/torso_velocity Max          2.12141\n",
      "exploration/env_infos/final/torso_velocity Min         -0.232983\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.102087\n",
      "exploration/env_infos/initial/torso_velocity Std        0.237263\n",
      "exploration/env_infos/initial/torso_velocity Max        0.524161\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.482646\n",
      "exploration/env_infos/torso_velocity Mean               0.0389228\n",
      "exploration/env_infos/torso_velocity Std                0.751395\n",
      "exploration/env_infos/torso_velocity Max                2.60736\n",
      "exploration/env_infos/torso_velocity Min               -2.44361\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943565\n",
      "evaluation/Rewards Std                                  0.0360236\n",
      "evaluation/Rewards Max                                  1.96145\n",
      "evaluation/Rewards Min                                  0.771504\n",
      "evaluation/Returns Mean                               943.565\n",
      "evaluation/Returns Std                                 23.348\n",
      "evaluation/Returns Max                                980.94\n",
      "evaluation/Returns Min                                910.603\n",
      "evaluation/Actions Mean                                 0.0281693\n",
      "evaluation/Actions Std                                  0.11678\n",
      "evaluation/Actions Max                                  0.474282\n",
      "evaluation/Actions Min                                 -0.460489\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.565\n",
      "evaluation/env_infos/final/reward_forward Mean          4.0105e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.00019709\n",
      "evaluation/env_infos/final/reward_forward Max           0.00100564\n",
      "evaluation/env_infos/final/reward_forward Min          -8.46804e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.104246\n",
      "evaluation/env_infos/initial/reward_forward Std         0.13743\n",
      "evaluation/env_infos/initial/reward_forward Max         0.376804\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.115249\n",
      "evaluation/env_infos/reward_forward Mean                0.00050563\n",
      "evaluation/env_infos/reward_forward Std                 0.0670175\n",
      "evaluation/env_infos/reward_forward Max                 1.56274\n",
      "evaluation/env_infos/reward_forward Min                -1.66041\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0570775\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0242205\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0185314\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0890013\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0739339\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0256954\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0335567\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.115468\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.057724\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0251298\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0137618\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.228496\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.83875e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000188361\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00100595\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.46804e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138683\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230891\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.585385\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.300143\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00235968\n",
      "evaluation/env_infos/torso_velocity Std                 0.0672268\n",
      "evaluation/env_infos/torso_velocity Max                 1.56274\n",
      "evaluation/env_infos/torso_velocity Min                -1.81445\n",
      "time/data storing (s)                                   0.0323149\n",
      "time/evaluation sampling (s)                           45.3382\n",
      "time/exploration sampling (s)                           1.88209\n",
      "time/logging (s)                                        0.301963\n",
      "time/saving (s)                                         0.0268119\n",
      "time/training (s)                                       4.01195\n",
      "time/epoch (s)                                         51.5933\n",
      "time/total (s)                                        538.518\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:04:39.096287 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  24000\n",
      "trainer/QF1 Loss                                        0.264892\n",
      "trainer/QF2 Loss                                        0.274543\n",
      "trainer/Policy Loss                                    -4.00645\n",
      "trainer/Q1 Predictions Mean                             3.57729\n",
      "trainer/Q1 Predictions Std                              0.521272\n",
      "trainer/Q1 Predictions Max                              5.49465\n",
      "trainer/Q1 Predictions Min                              2.09698\n",
      "trainer/Q2 Predictions Mean                             3.60995\n",
      "trainer/Q2 Predictions Std                              0.492255\n",
      "trainer/Q2 Predictions Max                              5.01101\n",
      "trainer/Q2 Predictions Min                              2.36419\n",
      "trainer/Q Targets Mean                                  3.61688\n",
      "trainer/Q Targets Std                                   0.71426\n",
      "trainer/Q Targets Max                                   6.6972\n",
      "trainer/Q Targets Min                                  -0.104061\n",
      "trainer/Log Pis Mean                                    0.119588\n",
      "trainer/Log Pis Std                                     2.0244\n",
      "trainer/Log Pis Max                                     5.16719\n",
      "trainer/Log Pis Min                                    -7.79015\n",
      "trainer/Policy mu Mean                                  0.0517183\n",
      "trainer/Policy mu Std                                   0.17908\n",
      "trainer/Policy mu Max                                   1.28348\n",
      "trainer/Policy mu Min                                  -0.605673\n",
      "trainer/Policy log std Mean                            -1.30966\n",
      "trainer/Policy log std Std                              0.345599\n",
      "trainer/Policy log std Max                             -0.480946\n",
      "trainer/Policy log std Min                             -2.13511\n",
      "trainer/Alpha                                           0.055286\n",
      "trainer/Alpha Loss                                    -22.7994\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                            84\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.703755\n",
      "exploration/Rewards Std                                 0.266661\n",
      "exploration/Rewards Max                                 2.449\n",
      "exploration/Rewards Min                                 0.0345557\n",
      "exploration/Returns Mean                              703.755\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               703.755\n",
      "exploration/Returns Min                               703.755\n",
      "exploration/Actions Mean                                0.0521094\n",
      "exploration/Actions Std                                 0.291212\n",
      "exploration/Actions Max                                 0.933904\n",
      "exploration/Actions Min                                -0.930295\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           703.755\n",
      "exploration/env_infos/final/reward_forward Mean         0.400192\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.400192\n",
      "exploration/env_infos/final/reward_forward Min          0.400192\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0683659\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0683659\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0683659\n",
      "exploration/env_infos/reward_forward Mean              -0.00677636\n",
      "exploration/env_infos/reward_forward Std                0.482964\n",
      "exploration/env_infos/reward_forward Max                1.34809\n",
      "exploration/env_infos/reward_forward Min               -1.53531\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.225132\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.225132\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.225132\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.519495\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.519495\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.519495\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.35008\n",
      "exploration/env_infos/reward_ctrl Std                   0.167676\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0330057\n",
      "exploration/env_infos/reward_ctrl Min                  -0.965444\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.439683\n",
      "exploration/env_infos/final/torso_velocity Std          0.418298\n",
      "exploration/env_infos/final/torso_velocity Max          0.970594\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0517368\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.00773472\n",
      "exploration/env_infos/initial/torso_velocity Std        0.294777\n",
      "exploration/env_infos/initial/torso_velocity Max        0.400746\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.309175\n",
      "exploration/env_infos/torso_velocity Mean              -0.0262095\n",
      "exploration/env_infos/torso_velocity Std                0.54073\n",
      "exploration/env_infos/torso_velocity Max                1.94053\n",
      "exploration/env_infos/torso_velocity Min               -1.75203\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966799\n",
      "evaluation/Rewards Std                                  0.0362282\n",
      "evaluation/Rewards Max                                  2.42641\n",
      "evaluation/Rewards Min                                  0.656891\n",
      "evaluation/Returns Mean                               966.799\n",
      "evaluation/Returns Std                                  2.90342\n",
      "evaluation/Returns Max                                972.756\n",
      "evaluation/Returns Min                                962.487\n",
      "evaluation/Actions Mean                                 0.0335311\n",
      "evaluation/Actions Std                                  0.0876123\n",
      "evaluation/Actions Max                                  0.633651\n",
      "evaluation/Actions Min                                 -0.409137\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.799\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00058689\n",
      "evaluation/env_infos/final/reward_forward Std           0.00287585\n",
      "evaluation/env_infos/final/reward_forward Max           5.29887e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0146756\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0395886\n",
      "evaluation/env_infos/initial/reward_forward Std         0.137642\n",
      "evaluation/env_infos/initial/reward_forward Max         0.219389\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.341642\n",
      "evaluation/env_infos/reward_forward Mean                0.000724821\n",
      "evaluation/env_infos/reward_forward Std                 0.0625864\n",
      "evaluation/env_infos/reward_forward Max                 1.27369\n",
      "evaluation/env_infos/reward_forward Min                -1.40184\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0337088\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00202589\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0287146\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0378888\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0519343\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0117123\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0307145\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0829329\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.035201\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0141199\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0155182\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.343109\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000714907\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00709951\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0593583\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0146756\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.139054\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244879\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.690703\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.341642\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00298467\n",
      "evaluation/env_infos/torso_velocity Std                 0.0809855\n",
      "evaluation/env_infos/torso_velocity Max                 1.46584\n",
      "evaluation/env_infos/torso_velocity Min                -1.85038\n",
      "time/data storing (s)                                   0.030543\n",
      "time/evaluation sampling (s)                           45.266\n",
      "time/exploration sampling (s)                           1.76557\n",
      "time/logging (s)                                        0.2789\n",
      "time/saving (s)                                         0.025748\n",
      "time/training (s)                                       3.70283\n",
      "time/epoch (s)                                         51.0696\n",
      "time/total (s)                                        589.853\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:05:30.632272 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  26000\n",
      "trainer/QF1 Loss                                        0.182161\n",
      "trainer/QF2 Loss                                        0.222136\n",
      "trainer/Policy Loss                                    -2.30313\n",
      "trainer/Q1 Predictions Mean                             3.78047\n",
      "trainer/Q1 Predictions Std                              0.724249\n",
      "trainer/Q1 Predictions Max                              5.54728\n",
      "trainer/Q1 Predictions Min                              1.8337\n",
      "trainer/Q2 Predictions Mean                             3.7768\n",
      "trainer/Q2 Predictions Std                              0.693106\n",
      "trainer/Q2 Predictions Max                              5.31218\n",
      "trainer/Q2 Predictions Min                              1.99568\n",
      "trainer/Q Targets Mean                                  3.86169\n",
      "trainer/Q Targets Std                                   0.847774\n",
      "trainer/Q Targets Max                                   6.97176\n",
      "trainer/Q Targets Min                                  -0.829927\n",
      "trainer/Log Pis Mean                                    2.2406\n",
      "trainer/Log Pis Std                                     2.16179\n",
      "trainer/Log Pis Max                                     6.25903\n",
      "trainer/Log Pis Min                                    -5.51989\n",
      "trainer/Policy mu Mean                                  0.0128346\n",
      "trainer/Policy mu Std                                   0.14808\n",
      "trainer/Policy mu Max                                   0.794201\n",
      "trainer/Policy mu Min                                  -0.908182\n",
      "trainer/Policy log std Mean                            -1.63016\n",
      "trainer/Policy log std Std                              0.265219\n",
      "trainer/Policy log std Max                             -0.421631\n",
      "trainer/Policy log std Min                             -2.2379\n",
      "trainer/Alpha                                           0.0463829\n",
      "trainer/Alpha Loss                                    -17.6774\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                            85\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.847771\n",
      "exploration/Rewards Std                                 0.177009\n",
      "exploration/Rewards Max                                 1.91606\n",
      "exploration/Rewards Min                                 0.369536\n",
      "exploration/Returns Mean                              847.771\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               847.771\n",
      "exploration/Returns Min                               847.771\n",
      "exploration/Actions Mean                                0.0106009\n",
      "exploration/Actions Std                                 0.216409\n",
      "exploration/Actions Max                                 0.79728\n",
      "exploration/Actions Min                                -0.81761\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           847.771\n",
      "exploration/env_infos/final/reward_forward Mean        -0.059509\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.059509\n",
      "exploration/env_infos/final/reward_forward Min         -0.059509\n",
      "exploration/env_infos/initial/reward_forward Mean       0.073365\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.073365\n",
      "exploration/env_infos/initial/reward_forward Min        0.073365\n",
      "exploration/env_infos/reward_forward Mean              -0.104299\n",
      "exploration/env_infos/reward_forward Std                0.418333\n",
      "exploration/env_infos/reward_forward Max                1.4175\n",
      "exploration/env_infos/reward_forward Min               -1.70691\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.161057\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.161057\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.161057\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.167564\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.167564\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.167564\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.18778\n",
      "exploration/env_infos/reward_ctrl Std                   0.103566\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0124016\n",
      "exploration/env_infos/reward_ctrl Min                  -0.630464\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.169742\n",
      "exploration/env_infos/final/torso_velocity Std          0.234174\n",
      "exploration/env_infos/final/torso_velocity Max          0.491343\n",
      "exploration/env_infos/final/torso_velocity Min         -0.059509\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.081753\n",
      "exploration/env_infos/initial/torso_velocity Std        0.303957\n",
      "exploration/env_infos/initial/torso_velocity Max        0.458146\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.286252\n",
      "exploration/env_infos/torso_velocity Mean              -0.019935\n",
      "exploration/env_infos/torso_velocity Std                0.456538\n",
      "exploration/env_infos/torso_velocity Max                2.07877\n",
      "exploration/env_infos/torso_velocity Min               -1.71993\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978739\n",
      "evaluation/Rewards Std                                  0.0245129\n",
      "evaluation/Rewards Max                                  2.20915\n",
      "evaluation/Rewards Min                                  0.728217\n",
      "evaluation/Returns Mean                               978.739\n",
      "evaluation/Returns Std                                  6.4281\n",
      "evaluation/Returns Max                                989.419\n",
      "evaluation/Returns Min                                968.371\n",
      "evaluation/Actions Mean                                 0.00660502\n",
      "evaluation/Actions Std                                  0.0741827\n",
      "evaluation/Actions Max                                  0.429395\n",
      "evaluation/Actions Min                                 -0.592477\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.739\n",
      "evaluation/env_infos/final/reward_forward Mean          1.26333e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.13408e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.10985e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.71753e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0363271\n",
      "evaluation/env_infos/initial/reward_forward Std         0.150183\n",
      "evaluation/env_infos/initial/reward_forward Max         0.262209\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.329544\n",
      "evaluation/env_infos/reward_forward Mean               -3.29753e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0474349\n",
      "evaluation/env_infos/reward_forward Max                 0.87221\n",
      "evaluation/env_infos/reward_forward Min                -1.39968\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0215411\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00611737\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0114142\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0314784\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0189613\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0040984\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0127279\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0289085\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0221868\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0105298\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00294921\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.316102\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          5.24861e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.68808e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.10985e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.41433e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.12273\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253252\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.617079\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.329544\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00164567\n",
      "evaluation/env_infos/torso_velocity Std                 0.0649412\n",
      "evaluation/env_infos/torso_velocity Max                 1.4924\n",
      "evaluation/env_infos/torso_velocity Min                -1.75812\n",
      "time/data storing (s)                                   0.0294903\n",
      "time/evaluation sampling (s)                           45.4644\n",
      "time/exploration sampling (s)                           1.80069\n",
      "time/logging (s)                                        0.280436\n",
      "time/saving (s)                                         0.0246896\n",
      "time/training (s)                                       3.68662\n",
      "time/epoch (s)                                         51.2863\n",
      "time/total (s)                                        641.39\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:06:22.873375 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  28000\n",
      "trainer/QF1 Loss                                        0.268735\n",
      "trainer/QF2 Loss                                        0.261642\n",
      "trainer/Policy Loss                                    -1.75178\n",
      "trainer/Q1 Predictions Mean                             4.30291\n",
      "trainer/Q1 Predictions Std                              0.730236\n",
      "trainer/Q1 Predictions Max                              5.78692\n",
      "trainer/Q1 Predictions Min                              2.46234\n",
      "trainer/Q2 Predictions Mean                             4.24672\n",
      "trainer/Q2 Predictions Std                              0.733047\n",
      "trainer/Q2 Predictions Max                              5.74803\n",
      "trainer/Q2 Predictions Min                              2.18918\n",
      "trainer/Q Targets Mean                                  4.22702\n",
      "trainer/Q Targets Std                                   0.884574\n",
      "trainer/Q Targets Max                                   7.53765\n",
      "trainer/Q Targets Min                                  -0.686767\n",
      "trainer/Log Pis Mean                                    3.31674\n",
      "trainer/Log Pis Std                                     2.22509\n",
      "trainer/Log Pis Max                                     8.9022\n",
      "trainer/Log Pis Min                                    -4.70424\n",
      "trainer/Policy mu Mean                                 -0.00596795\n",
      "trainer/Policy mu Std                                   0.129446\n",
      "trainer/Policy mu Max                                   0.536588\n",
      "trainer/Policy mu Min                                  -0.71301\n",
      "trainer/Policy log std Mean                            -1.81863\n",
      "trainer/Policy log std Std                              0.214287\n",
      "trainer/Policy log std Max                             -0.902758\n",
      "trainer/Policy log std Min                             -2.51842\n",
      "trainer/Alpha                                           0.040443\n",
      "trainer/Alpha Loss                                    -15.0172\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                            86\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.88563\n",
      "exploration/Rewards Std                                 0.0826853\n",
      "exploration/Rewards Max                                 1.44903\n",
      "exploration/Rewards Min                                 0.604304\n",
      "exploration/Returns Mean                              885.63\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               885.63\n",
      "exploration/Returns Min                               885.63\n",
      "exploration/Actions Mean                               -0.00974919\n",
      "exploration/Actions Std                                 0.174987\n",
      "exploration/Actions Max                                 0.628733\n",
      "exploration/Actions Min                                -0.621182\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           885.63\n",
      "exploration/env_infos/final/reward_forward Mean        -0.242317\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.242317\n",
      "exploration/env_infos/final/reward_forward Min         -0.242317\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.129049\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.129049\n",
      "exploration/env_infos/initial/reward_forward Min       -0.129049\n",
      "exploration/env_infos/reward_forward Mean              -0.082641\n",
      "exploration/env_infos/reward_forward Std                0.405682\n",
      "exploration/env_infos/reward_forward Max                1.3065\n",
      "exploration/env_infos/reward_forward Min               -1.33383\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.146904\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.146904\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.146904\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.119517\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.119517\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.119517\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.122862\n",
      "exploration/env_infos/reward_ctrl Std                   0.0614996\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00757255\n",
      "exploration/env_infos/reward_ctrl Min                  -0.395696\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0701809\n",
      "exploration/env_infos/final/torso_velocity Std          0.167983\n",
      "exploration/env_infos/final/torso_velocity Max          0.157678\n",
      "exploration/env_infos/final/torso_velocity Min         -0.242317\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.150523\n",
      "exploration/env_infos/initial/torso_velocity Std        0.249999\n",
      "exploration/env_infos/initial/torso_velocity Max        0.477737\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.129049\n",
      "exploration/env_infos/torso_velocity Mean              -0.0247921\n",
      "exploration/env_infos/torso_velocity Std                0.435126\n",
      "exploration/env_infos/torso_velocity Max                1.65256\n",
      "exploration/env_infos/torso_velocity Min               -1.58856\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978575\n",
      "evaluation/Rewards Std                                  0.0444449\n",
      "evaluation/Rewards Max                                  2.86714\n",
      "evaluation/Rewards Min                                  0.830494\n",
      "evaluation/Returns Mean                               978.575\n",
      "evaluation/Returns Std                                 12.4467\n",
      "evaluation/Returns Max                               1015.32\n",
      "evaluation/Returns Min                                967.565\n",
      "evaluation/Actions Mean                                -0.0130002\n",
      "evaluation/Actions Std                                  0.0815849\n",
      "evaluation/Actions Max                                  0.395395\n",
      "evaluation/Actions Min                                 -0.418799\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.575\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00636059\n",
      "evaluation/env_infos/final/reward_forward Std           0.0270548\n",
      "evaluation/env_infos/final/reward_forward Max           0.0396358\n",
      "evaluation/env_infos/final/reward_forward Min          -0.119419\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0106334\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136637\n",
      "evaluation/env_infos/initial/reward_forward Max         0.327027\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.348199\n",
      "evaluation/env_infos/reward_forward Mean               -0.000994667\n",
      "evaluation/env_infos/reward_forward Std                 0.0713337\n",
      "evaluation/env_infos/reward_forward Max                 1.05983\n",
      "evaluation/env_infos/reward_forward Min                -1.84912\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.026416\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00577246\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0142381\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0324512\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0244219\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00591367\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0146373\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0366604\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0273004\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00814335\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0101927\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.169506\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00148105\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0321303\n",
      "evaluation/env_infos/final/torso_velocity Max           0.100971\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.182452\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135724\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.232563\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.638245\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.348199\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000855169\n",
      "evaluation/env_infos/torso_velocity Std                 0.0899752\n",
      "evaluation/env_infos/torso_velocity Max                 1.41841\n",
      "evaluation/env_infos/torso_velocity Min                -1.85479\n",
      "time/data storing (s)                                   0.0301121\n",
      "time/evaluation sampling (s)                           45.9996\n",
      "time/exploration sampling (s)                           1.97877\n",
      "time/logging (s)                                        0.281584\n",
      "time/saving (s)                                         0.0266279\n",
      "time/training (s)                                       3.66855\n",
      "time/epoch (s)                                         51.9853\n",
      "time/total (s)                                        693.632\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:07:20.467359 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  30000\n",
      "trainer/QF1 Loss                                        0.296368\n",
      "trainer/QF2 Loss                                        0.289419\n",
      "trainer/Policy Loss                                    -1.03162\n",
      "trainer/Q1 Predictions Mean                             4.51145\n",
      "trainer/Q1 Predictions Std                              0.850042\n",
      "trainer/Q1 Predictions Max                              6.42181\n",
      "trainer/Q1 Predictions Min                              0.794296\n",
      "trainer/Q2 Predictions Mean                             4.61932\n",
      "trainer/Q2 Predictions Std                              0.820895\n",
      "trainer/Q2 Predictions Max                              6.68537\n",
      "trainer/Q2 Predictions Min                              1.47721\n",
      "trainer/Q Targets Mean                                  4.53034\n",
      "trainer/Q Targets Std                                   0.968778\n",
      "trainer/Q Targets Max                                   7.31369\n",
      "trainer/Q Targets Min                                  -0.405301\n",
      "trainer/Log Pis Mean                                    4.40095\n",
      "trainer/Log Pis Std                                     1.94747\n",
      "trainer/Log Pis Max                                     8.69942\n",
      "trainer/Log Pis Min                                    -0.995101\n",
      "trainer/Policy mu Mean                                 -0.00757681\n",
      "trainer/Policy mu Std                                   0.11645\n",
      "trainer/Policy mu Max                                   0.524941\n",
      "trainer/Policy mu Min                                  -0.595375\n",
      "trainer/Policy log std Mean                            -1.92591\n",
      "trainer/Policy log std Std                              0.185131\n",
      "trainer/Policy log std Max                             -0.987625\n",
      "trainer/Policy log std Min                             -2.57411\n",
      "trainer/Alpha                                           0.0360257\n",
      "trainer/Alpha Loss                                    -11.9575\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                            87\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938471\n",
      "exploration/Rewards Std                                 0.158868\n",
      "exploration/Rewards Max                                 2.06585\n",
      "exploration/Rewards Min                                 0.618325\n",
      "exploration/Returns Mean                              938.471\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.471\n",
      "exploration/Returns Min                               938.471\n",
      "exploration/Actions Mean                               -0.00493731\n",
      "exploration/Actions Std                                 0.157632\n",
      "exploration/Actions Max                                 0.571474\n",
      "exploration/Actions Min                                -0.62658\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.471\n",
      "exploration/env_infos/final/reward_forward Mean        -0.697876\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.697876\n",
      "exploration/env_infos/final/reward_forward Min         -0.697876\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0638175\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0638175\n",
      "exploration/env_infos/initial/reward_forward Min        0.0638175\n",
      "exploration/env_infos/reward_forward Mean              -0.00696057\n",
      "exploration/env_infos/reward_forward Std                0.415228\n",
      "exploration/env_infos/reward_forward Max                1.79768\n",
      "exploration/env_infos/reward_forward Min               -1.1767\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.122943\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.122943\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.122943\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.142484\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.142484\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.142484\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0994889\n",
      "exploration/env_infos/reward_ctrl Std                   0.0523989\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00812061\n",
      "exploration/env_infos/reward_ctrl Min                  -0.381675\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.31892\n",
      "exploration/env_infos/final/torso_velocity Std          0.383638\n",
      "exploration/env_infos/final/torso_velocity Max          0.206805\n",
      "exploration/env_infos/final/torso_velocity Min         -0.697876\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.171059\n",
      "exploration/env_infos/initial/torso_velocity Std        0.187787\n",
      "exploration/env_infos/initial/torso_velocity Max        0.435085\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0142742\n",
      "exploration/env_infos/torso_velocity Mean               0.00630568\n",
      "exploration/env_infos/torso_velocity Std                0.403802\n",
      "exploration/env_infos/torso_velocity Max                1.79768\n",
      "exploration/env_infos/torso_velocity Min               -1.48883\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.977443\n",
      "evaluation/Rewards Std                                  0.0352207\n",
      "evaluation/Rewards Max                                  2.83853\n",
      "evaluation/Rewards Min                                  0.901803\n",
      "evaluation/Returns Mean                               977.443\n",
      "evaluation/Returns Std                                  7.83995\n",
      "evaluation/Returns Max                                992.044\n",
      "evaluation/Returns Min                                965.346\n",
      "evaluation/Actions Mean                                -0.00689161\n",
      "evaluation/Actions Std                                  0.077689\n",
      "evaluation/Actions Max                                  0.266748\n",
      "evaluation/Actions Min                                 -0.382064\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            977.443\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.23568e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000159079\n",
      "evaluation/env_infos/final/reward_forward Max           1.11302e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000811674\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0532225\n",
      "evaluation/env_infos/initial/reward_forward Std         0.133152\n",
      "evaluation/env_infos/initial/reward_forward Max         0.317452\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.260186\n",
      "evaluation/env_infos/reward_forward Mean                0.00169806\n",
      "evaluation/env_infos/reward_forward Std                 0.0575001\n",
      "evaluation/env_infos/reward_forward Max                 1.34126\n",
      "evaluation/env_infos/reward_forward Min                -1.26433\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0242248\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00714681\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00800164\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0353694\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0148381\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00758551\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00593802\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0334461\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0243323\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00756069\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00463157\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0981972\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.28955e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000225857\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00178311\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000811674\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.16118\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218594\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.619129\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.260186\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00212826\n",
      "evaluation/env_infos/torso_velocity Std                 0.0634295\n",
      "evaluation/env_infos/torso_velocity Max                 1.34126\n",
      "evaluation/env_infos/torso_velocity Min                -1.82843\n",
      "time/data storing (s)                                   0.0309598\n",
      "time/evaluation sampling (s)                           49.0195\n",
      "time/exploration sampling (s)                           2.20052\n",
      "time/logging (s)                                        0.345273\n",
      "time/saving (s)                                         0.0247432\n",
      "time/training (s)                                       5.75661\n",
      "time/epoch (s)                                         57.3776\n",
      "time/total (s)                                        751.289\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:08:28.599183 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  32000\n",
      "trainer/QF1 Loss                                        0.381161\n",
      "trainer/QF2 Loss                                        0.406135\n",
      "trainer/Policy Loss                                    -1.2034\n",
      "trainer/Q1 Predictions Mean                             4.8459\n",
      "trainer/Q1 Predictions Std                              0.825141\n",
      "trainer/Q1 Predictions Max                              6.88477\n",
      "trainer/Q1 Predictions Min                              3.03213\n",
      "trainer/Q2 Predictions Mean                             4.80967\n",
      "trainer/Q2 Predictions Std                              0.792659\n",
      "trainer/Q2 Predictions Max                              6.79892\n",
      "trainer/Q2 Predictions Min                              3.00856\n",
      "trainer/Q Targets Mean                                  5.06568\n",
      "trainer/Q Targets Std                                   0.990262\n",
      "trainer/Q Targets Max                                   7.26135\n",
      "trainer/Q Targets Min                                   0.0287774\n",
      "trainer/Log Pis Mean                                    4.39198\n",
      "trainer/Log Pis Std                                     2.33569\n",
      "trainer/Log Pis Max                                     9.23807\n",
      "trainer/Log Pis Min                                    -4.56426\n",
      "trainer/Policy mu Mean                                 -0.00438575\n",
      "trainer/Policy mu Std                                   0.118576\n",
      "trainer/Policy mu Max                                   0.417852\n",
      "trainer/Policy mu Min                                  -0.742183\n",
      "trainer/Policy log std Mean                            -1.9506\n",
      "trainer/Policy log std Std                              0.169079\n",
      "trainer/Policy log std Max                             -1.17775\n",
      "trainer/Policy log std Min                             -2.73043\n",
      "trainer/Alpha                                           0.0325578\n",
      "trainer/Alpha Loss                                    -12.3528\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                            88\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.943243\n",
      "exploration/Rewards Std                                 0.164182\n",
      "exploration/Rewards Max                                 2.66957\n",
      "exploration/Rewards Min                                 0.67963\n",
      "exploration/Returns Mean                              943.243\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               943.243\n",
      "exploration/Returns Min                               943.243\n",
      "exploration/Actions Mean                               -0.012875\n",
      "exploration/Actions Std                                 0.1559\n",
      "exploration/Actions Max                                 0.502957\n",
      "exploration/Actions Min                                -0.625344\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           943.243\n",
      "exploration/env_infos/final/reward_forward Mean        -0.335133\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.335133\n",
      "exploration/env_infos/final/reward_forward Min         -0.335133\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0175908\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0175908\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0175908\n",
      "exploration/env_infos/reward_forward Mean              -0.0427723\n",
      "exploration/env_infos/reward_forward Std                0.28365\n",
      "exploration/env_infos/reward_forward Max                0.85386\n",
      "exploration/env_infos/reward_forward Min               -1.34551\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.128384\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.128384\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.128384\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0404309\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0404309\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0404309\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0978821\n",
      "exploration/env_infos/reward_ctrl Std                   0.0499674\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0124209\n",
      "exploration/env_infos/reward_ctrl Min                  -0.32037\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.247588\n",
      "exploration/env_infos/final/torso_velocity Std          0.136398\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0549577\n",
      "exploration/env_infos/final/torso_velocity Min         -0.352673\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.198358\n",
      "exploration/env_infos/initial/torso_velocity Std        0.207065\n",
      "exploration/env_infos/initial/torso_velocity Max        0.477617\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0175908\n",
      "exploration/env_infos/torso_velocity Mean              -0.0259967\n",
      "exploration/env_infos/torso_velocity Std                0.291965\n",
      "exploration/env_infos/torso_velocity Max                1.20999\n",
      "exploration/env_infos/torso_velocity Min               -1.80702\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.961893\n",
      "evaluation/Rewards Std                                  0.0282728\n",
      "evaluation/Rewards Max                                  2.11315\n",
      "evaluation/Rewards Min                                  0.842932\n",
      "evaluation/Returns Mean                               961.893\n",
      "evaluation/Returns Std                                  5.60675\n",
      "evaluation/Returns Max                                971.828\n",
      "evaluation/Returns Min                                953.907\n",
      "evaluation/Actions Mean                                -0.00964495\n",
      "evaluation/Actions Std                                  0.0990729\n",
      "evaluation/Actions Max                                  0.30992\n",
      "evaluation/Actions Min                                 -0.425861\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            961.893\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000461523\n",
      "evaluation/env_infos/final/reward_forward Std           0.0027307\n",
      "evaluation/env_infos/final/reward_forward Max           0.012843\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00375158\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00558238\n",
      "evaluation/env_infos/initial/reward_forward Std         0.101213\n",
      "evaluation/env_infos/initial/reward_forward Max         0.142307\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.265662\n",
      "evaluation/env_infos/reward_forward Mean                0.00332516\n",
      "evaluation/env_infos/reward_forward Std                 0.0627703\n",
      "evaluation/env_infos/reward_forward Max                 1.21302\n",
      "evaluation/env_infos/reward_forward Min                -1.03567\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0388703\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00507284\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.028219\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.045963\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0193733\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00649055\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.010759\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0333236\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0396338\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00668765\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.010759\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.157068\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000177213\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00242662\n",
      "evaluation/env_infos/final/torso_velocity Max           0.012843\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0113344\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137406\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.222965\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.633458\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.354341\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000592812\n",
      "evaluation/env_infos/torso_velocity Std                 0.0744897\n",
      "evaluation/env_infos/torso_velocity Max                 1.24554\n",
      "evaluation/env_infos/torso_velocity Min                -1.86619\n",
      "time/data storing (s)                                   0.0311864\n",
      "time/evaluation sampling (s)                           59.9318\n",
      "time/exploration sampling (s)                           1.93853\n",
      "time/logging (s)                                        0.321251\n",
      "time/saving (s)                                         0.0295828\n",
      "time/training (s)                                       5.58387\n",
      "time/epoch (s)                                         67.8363\n",
      "time/total (s)                                        819.397\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:09:25.254711 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  34000\n",
      "trainer/QF1 Loss                                        0.300632\n",
      "trainer/QF2 Loss                                        0.320806\n",
      "trainer/Policy Loss                                    -0.278559\n",
      "trainer/Q1 Predictions Mean                             5.26529\n",
      "trainer/Q1 Predictions Std                              0.912456\n",
      "trainer/Q1 Predictions Max                              7.15795\n",
      "trainer/Q1 Predictions Min                              2.16367\n",
      "trainer/Q2 Predictions Mean                             5.34872\n",
      "trainer/Q2 Predictions Std                              0.895365\n",
      "trainer/Q2 Predictions Max                              7.14301\n",
      "trainer/Q2 Predictions Min                              2.96545\n",
      "trainer/Q Targets Mean                                  5.36611\n",
      "trainer/Q Targets Std                                   1.02795\n",
      "trainer/Q Targets Max                                   8.20096\n",
      "trainer/Q Targets Min                                  -0.231836\n",
      "trainer/Log Pis Mean                                    5.7395\n",
      "trainer/Log Pis Std                                     2.29284\n",
      "trainer/Log Pis Max                                    10.5507\n",
      "trainer/Log Pis Min                                    -4.13355\n",
      "trainer/Policy mu Mean                                  0.0085037\n",
      "trainer/Policy mu Std                                   0.127188\n",
      "trainer/Policy mu Max                                   0.744402\n",
      "trainer/Policy mu Min                                  -0.807306\n",
      "trainer/Policy log std Mean                            -2.09141\n",
      "trainer/Policy log std Std                              0.17473\n",
      "trainer/Policy log std Max                             -1.36996\n",
      "trainer/Policy log std Min                             -2.54341\n",
      "trainer/Alpha                                           0.0296433\n",
      "trainer/Alpha Loss                                     -7.95155\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                            89\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.928452\n",
      "exploration/Rewards Std                                 0.111297\n",
      "exploration/Rewards Max                                 2.09144\n",
      "exploration/Rewards Min                                 0.722431\n",
      "exploration/Returns Mean                              928.452\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               928.452\n",
      "exploration/Returns Min                               928.452\n",
      "exploration/Actions Mean                               -0.0122315\n",
      "exploration/Actions Std                                 0.151904\n",
      "exploration/Actions Max                                 0.529964\n",
      "exploration/Actions Min                                -0.541728\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           928.452\n",
      "exploration/env_infos/final/reward_forward Mean        -0.113137\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.113137\n",
      "exploration/env_infos/final/reward_forward Min         -0.113137\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0998483\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0998483\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0998483\n",
      "exploration/env_infos/reward_forward Mean              -0.0388611\n",
      "exploration/env_infos/reward_forward Std                0.291871\n",
      "exploration/env_infos/reward_forward Max                0.814003\n",
      "exploration/env_infos/reward_forward Min               -1.21606\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.221174\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.221174\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.221174\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0724987\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0724987\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0724987\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0928973\n",
      "exploration/env_infos/reward_ctrl Std                   0.0437894\n",
      "exploration/env_infos/reward_ctrl Max                  -0.010011\n",
      "exploration/env_infos/reward_ctrl Min                  -0.294012\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0375018\n",
      "exploration/env_infos/final/torso_velocity Std          0.154555\n",
      "exploration/env_infos/final/torso_velocity Max          0.177911\n",
      "exploration/env_infos/final/torso_velocity Min         -0.177279\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0922513\n",
      "exploration/env_infos/initial/torso_velocity Std        0.179706\n",
      "exploration/env_infos/initial/torso_velocity Max        0.332401\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0998483\n",
      "exploration/env_infos/torso_velocity Mean              -0.00610878\n",
      "exploration/env_infos/torso_velocity Std                0.315542\n",
      "exploration/env_infos/torso_velocity Max                1.17565\n",
      "exploration/env_infos/torso_velocity Min               -1.33151\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.972968\n",
      "evaluation/Rewards Std                                  0.0189101\n",
      "evaluation/Rewards Max                                  2.25666\n",
      "evaluation/Rewards Min                                  0.833713\n",
      "evaluation/Returns Mean                               972.968\n",
      "evaluation/Returns Std                                  2.78668\n",
      "evaluation/Returns Max                                977.642\n",
      "evaluation/Returns Min                                968.915\n",
      "evaluation/Actions Mean                                 0.0172942\n",
      "evaluation/Actions Std                                  0.0813546\n",
      "evaluation/Actions Max                                  0.419931\n",
      "evaluation/Actions Min                                 -0.316044\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            972.968\n",
      "evaluation/env_infos/final/reward_forward Mean          1.64165e-06\n",
      "evaluation/env_infos/final/reward_forward Std           6.80203e-06\n",
      "evaluation/env_infos/final/reward_forward Max           3.45661e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -6.01607e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0179021\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14059\n",
      "evaluation/env_infos/initial/reward_forward Max         0.258641\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275051\n",
      "evaluation/env_infos/reward_forward Mean               -6.6657e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0516411\n",
      "evaluation/env_infos/reward_forward Max                 1.25722\n",
      "evaluation/env_infos/reward_forward Min                -1.71288\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0272999\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00295294\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0223529\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0308079\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0176387\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00320561\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0124484\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0248973\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0276706\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00578324\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0124484\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.166287\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.43554e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           7.47407e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           3.45661e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.25272e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.13453\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.236558\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.569574\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310523\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0017334\n",
      "evaluation/env_infos/torso_velocity Std                 0.064134\n",
      "evaluation/env_infos/torso_velocity Max                 1.5492\n",
      "evaluation/env_infos/torso_velocity Min                -1.99539\n",
      "time/data storing (s)                                   0.0298871\n",
      "time/evaluation sampling (s)                           49.8423\n",
      "time/exploration sampling (s)                           1.92028\n",
      "time/logging (s)                                        0.271541\n",
      "time/saving (s)                                         0.0335893\n",
      "time/training (s)                                       4.15823\n",
      "time/epoch (s)                                         56.2558\n",
      "time/total (s)                                        876.002\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:10:22.162613 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  36000\n",
      "trainer/QF1 Loss                                        0.194337\n",
      "trainer/QF2 Loss                                        0.201024\n",
      "trainer/Policy Loss                                    -0.890788\n",
      "trainer/Q1 Predictions Mean                             5.77779\n",
      "trainer/Q1 Predictions Std                              0.839228\n",
      "trainer/Q1 Predictions Max                              7.33883\n",
      "trainer/Q1 Predictions Min                              3.14317\n",
      "trainer/Q2 Predictions Mean                             5.86857\n",
      "trainer/Q2 Predictions Std                              0.846114\n",
      "trainer/Q2 Predictions Max                              7.47887\n",
      "trainer/Q2 Predictions Min                              3.24023\n",
      "trainer/Q Targets Mean                                  5.8258\n",
      "trainer/Q Targets Std                                   0.92908\n",
      "trainer/Q Targets Max                                   8.64945\n",
      "trainer/Q Targets Min                                  -0.137529\n",
      "trainer/Log Pis Mean                                    5.65168\n",
      "trainer/Log Pis Std                                     2.31576\n",
      "trainer/Log Pis Max                                    10.0071\n",
      "trainer/Log Pis Min                                    -4.14347\n",
      "trainer/Policy mu Mean                                  0.011298\n",
      "trainer/Policy mu Std                                   0.130674\n",
      "trainer/Policy mu Max                                   0.680499\n",
      "trainer/Policy mu Min                                  -0.435038\n",
      "trainer/Policy log std Mean                            -2.08418\n",
      "trainer/Policy log std Std                              0.182403\n",
      "trainer/Policy log std Max                             -1.2098\n",
      "trainer/Policy log std Min                             -2.57551\n",
      "trainer/Alpha                                           0.0271907\n",
      "trainer/Alpha Loss                                     -8.4636\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                            90\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.913196\n",
      "exploration/Rewards Std                                 0.107902\n",
      "exploration/Rewards Max                                 1.79343\n",
      "exploration/Rewards Min                                 0.701298\n",
      "exploration/Returns Mean                              913.196\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               913.196\n",
      "exploration/Returns Min                               913.196\n",
      "exploration/Actions Mean                                0.000753973\n",
      "exploration/Actions Std                                 0.166426\n",
      "exploration/Actions Max                                 0.504931\n",
      "exploration/Actions Min                                -0.541411\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           913.196\n",
      "exploration/env_infos/final/reward_forward Mean         0.038835\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.038835\n",
      "exploration/env_infos/final/reward_forward Min          0.038835\n",
      "exploration/env_infos/initial/reward_forward Mean       0.137439\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.137439\n",
      "exploration/env_infos/initial/reward_forward Min        0.137439\n",
      "exploration/env_infos/reward_forward Mean               0.0267612\n",
      "exploration/env_infos/reward_forward Std                0.151583\n",
      "exploration/env_infos/reward_forward Max                1.02481\n",
      "exploration/env_infos/reward_forward Min               -0.648209\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.115098\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.115098\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.115098\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0402854\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0402854\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0402854\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.110793\n",
      "exploration/env_infos/reward_ctrl Std                   0.0441742\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0164224\n",
      "exploration/env_infos/reward_ctrl Min                  -0.298702\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0248836\n",
      "exploration/env_infos/final/torso_velocity Std          0.054868\n",
      "exploration/env_infos/final/torso_velocity Max          0.038835\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0950925\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.228053\n",
      "exploration/env_infos/initial/torso_velocity Std        0.143402\n",
      "exploration/env_infos/initial/torso_velocity Max        0.430485\n",
      "exploration/env_infos/initial/torso_velocity Min        0.116236\n",
      "exploration/env_infos/torso_velocity Mean               0.00385003\n",
      "exploration/env_infos/torso_velocity Std                0.18243\n",
      "exploration/env_infos/torso_velocity Max                1.02481\n",
      "exploration/env_infos/torso_velocity Min               -1.24391\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.954563\n",
      "evaluation/Rewards Std                                  0.0666018\n",
      "evaluation/Rewards Max                                  2.13031\n",
      "evaluation/Rewards Min                                  0.793405\n",
      "evaluation/Returns Mean                               954.563\n",
      "evaluation/Returns Std                                 25.5997\n",
      "evaluation/Returns Max                               1031.31\n",
      "evaluation/Returns Min                                932.882\n",
      "evaluation/Actions Mean                                 0.0109535\n",
      "evaluation/Actions Std                                  0.118154\n",
      "evaluation/Actions Max                                  0.509178\n",
      "evaluation/Actions Min                                 -0.299812\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            954.563\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0288206\n",
      "evaluation/env_infos/final/reward_forward Std           0.12271\n",
      "evaluation/env_infos/final/reward_forward Max           0.172288\n",
      "evaluation/env_infos/final/reward_forward Min          -0.482827\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00395149\n",
      "evaluation/env_infos/initial/reward_forward Std         0.151243\n",
      "evaluation/env_infos/initial/reward_forward Max         0.220918\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.3412\n",
      "evaluation/env_infos/reward_forward Mean               -0.00246772\n",
      "evaluation/env_infos/reward_forward Std                 0.122585\n",
      "evaluation/env_infos/reward_forward Max                 1.6635\n",
      "evaluation/env_infos/reward_forward Min                -1.54599\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0552049\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00978321\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0292575\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0672884\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.028184\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0047389\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0178064\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0379481\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.056321\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00935146\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0178064\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.206595\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00800112\n",
      "evaluation/env_infos/final/torso_velocity Std           0.151123\n",
      "evaluation/env_infos/final/torso_velocity Max           0.82574\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.482827\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141767\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244972\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.728014\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.3412\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0047839\n",
      "evaluation/env_infos/torso_velocity Std                 0.123214\n",
      "evaluation/env_infos/torso_velocity Max                 1.6635\n",
      "evaluation/env_infos/torso_velocity Min                -2.13484\n",
      "time/data storing (s)                                   0.0309061\n",
      "time/evaluation sampling (s)                           49.8841\n",
      "time/exploration sampling (s)                           2.2064\n",
      "time/logging (s)                                        0.278874\n",
      "time/saving (s)                                         0.0269714\n",
      "time/training (s)                                       4.20023\n",
      "time/epoch (s)                                         56.6275\n",
      "time/total (s)                                        932.917\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:11:14.290134 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  38000\n",
      "trainer/QF1 Loss                                        0.183719\n",
      "trainer/QF2 Loss                                        0.167997\n",
      "trainer/Policy Loss                                    -1.1403\n",
      "trainer/Q1 Predictions Mean                             6.13642\n",
      "trainer/Q1 Predictions Std                              0.899793\n",
      "trainer/Q1 Predictions Max                              8.07254\n",
      "trainer/Q1 Predictions Min                              3.33919\n",
      "trainer/Q2 Predictions Mean                             6.16882\n",
      "trainer/Q2 Predictions Std                              0.904202\n",
      "trainer/Q2 Predictions Max                              8.02191\n",
      "trainer/Q2 Predictions Min                              3.38662\n",
      "trainer/Q Targets Mean                                  6.24681\n",
      "trainer/Q Targets Std                                   0.955549\n",
      "trainer/Q Targets Max                                   8.98952\n",
      "trainer/Q Targets Min                                   3.66701\n",
      "trainer/Log Pis Mean                                    5.69895\n",
      "trainer/Log Pis Std                                     2.31357\n",
      "trainer/Log Pis Max                                    10.2147\n",
      "trainer/Log Pis Min                                    -2.49073\n",
      "trainer/Policy mu Mean                                 -0.00402562\n",
      "trainer/Policy mu Std                                   0.11927\n",
      "trainer/Policy mu Max                                   0.477601\n",
      "trainer/Policy mu Min                                  -0.591545\n",
      "trainer/Policy log std Mean                            -2.10326\n",
      "trainer/Policy log std Std                              0.171516\n",
      "trainer/Policy log std Max                             -1.42905\n",
      "trainer/Policy log std Min                             -2.57793\n",
      "trainer/Alpha                                           0.0252166\n",
      "trainer/Alpha Loss                                     -8.46678\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                            91\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.928281\n",
      "exploration/Rewards Std                                 0.0801434\n",
      "exploration/Rewards Max                                 1.45364\n",
      "exploration/Rewards Min                                 0.71405\n",
      "exploration/Returns Mean                              928.281\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               928.281\n",
      "exploration/Returns Min                               928.281\n",
      "exploration/Actions Mean                               -0.00269231\n",
      "exploration/Actions Std                                 0.147979\n",
      "exploration/Actions Max                                 0.449772\n",
      "exploration/Actions Min                                -0.526673\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           928.281\n",
      "exploration/env_infos/final/reward_forward Mean         0.337683\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.337683\n",
      "exploration/env_infos/final/reward_forward Min          0.337683\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00459017\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.00459017\n",
      "exploration/env_infos/initial/reward_forward Min        0.00459017\n",
      "exploration/env_infos/reward_forward Mean              -0.0150595\n",
      "exploration/env_infos/reward_forward Std                0.272434\n",
      "exploration/env_infos/reward_forward Max                0.946624\n",
      "exploration/env_infos/reward_forward Min               -0.814719\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.115377\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.115377\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.115377\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0119101\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0119101\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0119101\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0876198\n",
      "exploration/env_infos/reward_ctrl Std                   0.0389573\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0119101\n",
      "exploration/env_infos/reward_ctrl Min                  -0.28595\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0127108\n",
      "exploration/env_infos/final/torso_velocity Std          0.280581\n",
      "exploration/env_infos/final/torso_velocity Max          0.337683\n",
      "exploration/env_infos/final/torso_velocity Min         -0.349174\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164439\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201643\n",
      "exploration/env_infos/initial/torso_velocity Max        0.448878\n",
      "exploration/env_infos/initial/torso_velocity Min        0.00459017\n",
      "exploration/env_infos/torso_velocity Mean              -0.0075968\n",
      "exploration/env_infos/torso_velocity Std                0.240673\n",
      "exploration/env_infos/torso_velocity Max                1.06262\n",
      "exploration/env_infos/torso_velocity Min               -1.14005\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.944176\n",
      "evaluation/Rewards Std                                  0.0268534\n",
      "evaluation/Rewards Max                                  2.24971\n",
      "evaluation/Rewards Min                                  0.816599\n",
      "evaluation/Returns Mean                               944.176\n",
      "evaluation/Returns Std                                 12.3998\n",
      "evaluation/Returns Max                                962.053\n",
      "evaluation/Returns Min                                923.883\n",
      "evaluation/Actions Mean                                -0.0142602\n",
      "evaluation/Actions Std                                  0.118213\n",
      "evaluation/Actions Max                                  0.500074\n",
      "evaluation/Actions Min                                 -0.379853\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            944.176\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.22543e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.70222e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.37424e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.89849e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0445193\n",
      "evaluation/env_infos/initial/reward_forward Std         0.131763\n",
      "evaluation/env_infos/initial/reward_forward Max         0.308008\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.404327\n",
      "evaluation/env_infos/reward_forward Mean                0.0047108\n",
      "evaluation/env_infos/reward_forward Std                 0.0648379\n",
      "evaluation/env_infos/reward_forward Max                 1.3235\n",
      "evaluation/env_infos/reward_forward Min                -1.29892\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.056836\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0123729\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0378386\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0769558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0132145\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0100686\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00303113\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0372689\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0567107\n",
      "evaluation/env_infos/reward_ctrl Std                    0.012722\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00303113\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.183401\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.81889e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.99103e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           3.79607e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.89849e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.122691\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.256916\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.778248\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.404327\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00101372\n",
      "evaluation/env_infos/torso_velocity Std                 0.0728291\n",
      "evaluation/env_infos/torso_velocity Max                 1.37039\n",
      "evaluation/env_infos/torso_velocity Min                -1.78641\n",
      "time/data storing (s)                                   0.0352152\n",
      "time/evaluation sampling (s)                           45.3036\n",
      "time/exploration sampling (s)                           1.97523\n",
      "time/logging (s)                                        0.27252\n",
      "time/saving (s)                                         0.0315272\n",
      "time/training (s)                                       4.21411\n",
      "time/epoch (s)                                         51.8322\n",
      "time/total (s)                                        985.037\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:12:07.176002 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  40000\n",
      "trainer/QF1 Loss                                        0.310785\n",
      "trainer/QF2 Loss                                        0.36666\n",
      "trainer/Policy Loss                                    -1.04149\n",
      "trainer/Q1 Predictions Mean                             6.58828\n",
      "trainer/Q1 Predictions Std                              0.986356\n",
      "trainer/Q1 Predictions Max                              8.42634\n",
      "trainer/Q1 Predictions Min                              3.93897\n",
      "trainer/Q2 Predictions Mean                             6.78781\n",
      "trainer/Q2 Predictions Std                              0.977504\n",
      "trainer/Q2 Predictions Max                              8.36618\n",
      "trainer/Q2 Predictions Min                              4.12266\n",
      "trainer/Q Targets Mean                                  6.52803\n",
      "trainer/Q Targets Std                                   1.07639\n",
      "trainer/Q Targets Max                                   9.66765\n",
      "trainer/Q Targets Min                                   0.147768\n",
      "trainer/Log Pis Mean                                    6.32455\n",
      "trainer/Log Pis Std                                     2.20487\n",
      "trainer/Log Pis Max                                    10.9042\n",
      "trainer/Log Pis Min                                    -3.86689\n",
      "trainer/Policy mu Mean                                  0.0192318\n",
      "trainer/Policy mu Std                                   0.147316\n",
      "trainer/Policy mu Max                                   0.644149\n",
      "trainer/Policy mu Min                                  -0.658961\n",
      "trainer/Policy log std Mean                            -2.17696\n",
      "trainer/Policy log std Std                              0.179194\n",
      "trainer/Policy log std Max                             -1.35602\n",
      "trainer/Policy log std Min                             -2.66288\n",
      "trainer/Alpha                                           0.0235551\n",
      "trainer/Alpha Loss                                     -6.27929\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                            92\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.903723\n",
      "exploration/Rewards Std                                 0.0640232\n",
      "exploration/Rewards Max                                 1.30252\n",
      "exploration/Rewards Min                                 0.672688\n",
      "exploration/Returns Mean                              903.723\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               903.723\n",
      "exploration/Returns Min                               903.723\n",
      "exploration/Actions Mean                                0.0106143\n",
      "exploration/Actions Std                                 0.16546\n",
      "exploration/Actions Max                                 0.462974\n",
      "exploration/Actions Min                                -0.625595\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           903.723\n",
      "exploration/env_infos/final/reward_forward Mean        -0.116361\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.116361\n",
      "exploration/env_infos/final/reward_forward Min         -0.116361\n",
      "exploration/env_infos/initial/reward_forward Mean       0.019638\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.019638\n",
      "exploration/env_infos/initial/reward_forward Min        0.019638\n",
      "exploration/env_infos/reward_forward Mean               0.00389997\n",
      "exploration/env_infos/reward_forward Std                0.15812\n",
      "exploration/env_infos/reward_forward Max                0.697714\n",
      "exploration/env_infos/reward_forward Min               -0.529868\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.107523\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.107523\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.107523\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0192535\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0192535\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0192535\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109959\n",
      "exploration/env_infos/reward_ctrl Std                   0.0402182\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0192535\n",
      "exploration/env_infos/reward_ctrl Min                  -0.327312\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0401911\n",
      "exploration/env_infos/final/torso_velocity Std          0.0598839\n",
      "exploration/env_infos/final/torso_velocity Max          0.0299519\n",
      "exploration/env_infos/final/torso_velocity Min         -0.116361\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0986932\n",
      "exploration/env_infos/initial/torso_velocity Std        0.309569\n",
      "exploration/env_infos/initial/torso_velocity Max        0.511131\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.234689\n",
      "exploration/env_infos/torso_velocity Mean              -0.00557072\n",
      "exploration/env_infos/torso_velocity Std                0.138467\n",
      "exploration/env_infos/torso_velocity Max                0.940492\n",
      "exploration/env_infos/torso_velocity Min               -1.37367\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.932742\n",
      "evaluation/Rewards Std                                  0.0289228\n",
      "evaluation/Rewards Max                                  2.4704\n",
      "evaluation/Rewards Min                                  0.822764\n",
      "evaluation/Returns Mean                               932.742\n",
      "evaluation/Returns Std                                  5.72776\n",
      "evaluation/Returns Max                                943.279\n",
      "evaluation/Returns Min                                924.414\n",
      "evaluation/Actions Mean                                 0.00989474\n",
      "evaluation/Actions Std                                  0.130288\n",
      "evaluation/Actions Max                                  0.383093\n",
      "evaluation/Actions Min                                 -0.445855\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            932.742\n",
      "evaluation/env_infos/final/reward_forward Mean          7.92848e-09\n",
      "evaluation/env_infos/final/reward_forward Std           3.14621e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.21392e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.49831e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0515789\n",
      "evaluation/env_infos/initial/reward_forward Std         0.111976\n",
      "evaluation/env_infos/initial/reward_forward Max         0.249717\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.177668\n",
      "evaluation/env_infos/reward_forward Mean                0.00278258\n",
      "evaluation/env_infos/reward_forward Std                 0.0509596\n",
      "evaluation/env_infos/reward_forward Max                 1.16695\n",
      "evaluation/env_infos/reward_forward Min                -1.12535\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0682193\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00572678\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0566291\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0770558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0243262\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00765119\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0128718\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0412374\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0682914\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00666803\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0128718\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.177236\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.38727e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.39714e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.21392e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.49831e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.156008\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.209895\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.582182\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.284053\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00109238\n",
      "evaluation/env_infos/torso_velocity Std                 0.063015\n",
      "evaluation/env_infos/torso_velocity Max                 1.38822\n",
      "evaluation/env_infos/torso_velocity Min                -2.04138\n",
      "time/data storing (s)                                   0.0305385\n",
      "time/evaluation sampling (s)                           45.7274\n",
      "time/exploration sampling (s)                           2.04069\n",
      "time/logging (s)                                        0.280074\n",
      "time/saving (s)                                         0.0273928\n",
      "time/training (s)                                       4.49517\n",
      "time/epoch (s)                                         52.6013\n",
      "time/total (s)                                       1037.93\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:12:59.871433 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  42000\n",
      "trainer/QF1 Loss                                        0.25968\n",
      "trainer/QF2 Loss                                        0.320367\n",
      "trainer/Policy Loss                                    -0.773347\n",
      "trainer/Q1 Predictions Mean                             7.11573\n",
      "trainer/Q1 Predictions Std                              0.97456\n",
      "trainer/Q1 Predictions Max                              9.05389\n",
      "trainer/Q1 Predictions Min                              3.30805\n",
      "trainer/Q2 Predictions Mean                             7.18588\n",
      "trainer/Q2 Predictions Std                              0.977719\n",
      "trainer/Q2 Predictions Max                              9.09722\n",
      "trainer/Q2 Predictions Min                              3.95725\n",
      "trainer/Q Targets Mean                                  6.9365\n",
      "trainer/Q Targets Std                                   0.988073\n",
      "trainer/Q Targets Max                                   9.44717\n",
      "trainer/Q Targets Min                                   3.46019\n",
      "trainer/Log Pis Mean                                    7.09106\n",
      "trainer/Log Pis Std                                     2.19127\n",
      "trainer/Log Pis Max                                    11.1915\n",
      "trainer/Log Pis Min                                    -2.79071\n",
      "trainer/Policy mu Mean                                  0.0240871\n",
      "trainer/Policy mu Std                                   0.179974\n",
      "trainer/Policy mu Max                                   1.27914\n",
      "trainer/Policy mu Min                                  -1.49184\n",
      "trainer/Policy log std Mean                            -2.25665\n",
      "trainer/Policy log std Std                              0.175762\n",
      "trainer/Policy log std Max                             -1.00136\n",
      "trainer/Policy log std Min                             -2.88368\n",
      "trainer/Alpha                                           0.0222991\n",
      "trainer/Alpha Loss                                     -3.45642\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                            93\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.850112\n",
      "exploration/Rewards Std                                 0.0655646\n",
      "exploration/Rewards Max                                 1.80927\n",
      "exploration/Rewards Min                                 0.651987\n",
      "exploration/Returns Mean                              850.112\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               850.112\n",
      "exploration/Returns Min                               850.112\n",
      "exploration/Actions Mean                               -0.00943282\n",
      "exploration/Actions Std                                 0.195697\n",
      "exploration/Actions Max                                 0.669241\n",
      "exploration/Actions Min                                -0.557905\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           850.112\n",
      "exploration/env_infos/final/reward_forward Mean        -0.207249\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.207249\n",
      "exploration/env_infos/final/reward_forward Min         -0.207249\n",
      "exploration/env_infos/initial/reward_forward Mean       0.070832\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.070832\n",
      "exploration/env_infos/initial/reward_forward Min        0.070832\n",
      "exploration/env_infos/reward_forward Mean              -0.0136804\n",
      "exploration/env_infos/reward_forward Std                0.0964521\n",
      "exploration/env_infos/reward_forward Max                0.332042\n",
      "exploration/env_infos/reward_forward Min               -0.938268\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.107073\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.107073\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.107073\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.096743\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.096743\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.096743\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.153545\n",
      "exploration/env_infos/reward_ctrl Std                   0.0568597\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0295446\n",
      "exploration/env_infos/reward_ctrl Min                  -0.353492\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0863837\n",
      "exploration/env_infos/final/torso_velocity Std          0.129357\n",
      "exploration/env_infos/final/torso_velocity Max          0.0929763\n",
      "exploration/env_infos/final/torso_velocity Min         -0.207249\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.183963\n",
      "exploration/env_infos/initial/torso_velocity Std        0.250789\n",
      "exploration/env_infos/initial/torso_velocity Max        0.531636\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0505796\n",
      "exploration/env_infos/torso_velocity Mean              -0.00697537\n",
      "exploration/env_infos/torso_velocity Std                0.112345\n",
      "exploration/env_infos/torso_velocity Max                0.85994\n",
      "exploration/env_infos/torso_velocity Min               -1.52608\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.907551\n",
      "evaluation/Rewards Std                                  0.0309814\n",
      "evaluation/Rewards Max                                  2.23376\n",
      "evaluation/Rewards Min                                  0.608824\n",
      "evaluation/Returns Mean                               907.551\n",
      "evaluation/Returns Std                                 11.1476\n",
      "evaluation/Returns Max                                923.038\n",
      "evaluation/Returns Min                                883.863\n",
      "evaluation/Actions Mean                                 0.0162106\n",
      "evaluation/Actions Std                                  0.152263\n",
      "evaluation/Actions Max                                  0.576314\n",
      "evaluation/Actions Min                                 -0.400837\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            907.551\n",
      "evaluation/env_infos/final/reward_forward Mean          5.78529e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.02022e-05\n",
      "evaluation/env_infos/final/reward_forward Max           9.41569e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -7.81499e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0133583\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118315\n",
      "evaluation/env_infos/initial/reward_forward Max         0.197237\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.291399\n",
      "evaluation/env_infos/reward_forward Mean                0.00470363\n",
      "evaluation/env_infos/reward_forward Std                 0.059992\n",
      "evaluation/env_infos/reward_forward Max                 1.5331\n",
      "evaluation/env_infos/reward_forward Min                -1.10228\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0937468\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0110158\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0766035\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.116216\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0340875\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0101513\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0206292\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0588644\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0937875\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0121023\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0206292\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.391176\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.75456e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           6.08408e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000169614\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00043634\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152279\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225279\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.646228\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.291399\n",
      "evaluation/env_infos/torso_velocity Mean                0.000219832\n",
      "evaluation/env_infos/torso_velocity Std                 0.0635821\n",
      "evaluation/env_infos/torso_velocity Max                 1.5331\n",
      "evaluation/env_infos/torso_velocity Min                -1.74988\n",
      "time/data storing (s)                                   0.0370319\n",
      "time/evaluation sampling (s)                           45.7609\n",
      "time/exploration sampling (s)                           2.24584\n",
      "time/logging (s)                                        0.273731\n",
      "time/saving (s)                                         0.0272217\n",
      "time/training (s)                                       4.0403\n",
      "time/epoch (s)                                         52.385\n",
      "time/total (s)                                       1090.62\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:13:52.358244 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  44000\n",
      "trainer/QF1 Loss                                        0.215311\n",
      "trainer/QF2 Loss                                        0.221515\n",
      "trainer/Policy Loss                                    -0.971748\n",
      "trainer/Q1 Predictions Mean                             7.10852\n",
      "trainer/Q1 Predictions Std                              0.985366\n",
      "trainer/Q1 Predictions Max                              8.61061\n",
      "trainer/Q1 Predictions Min                              4.01539\n",
      "trainer/Q2 Predictions Mean                             7.15635\n",
      "trainer/Q2 Predictions Std                              0.988611\n",
      "trainer/Q2 Predictions Max                              8.88196\n",
      "trainer/Q2 Predictions Min                              4.3371\n",
      "trainer/Q Targets Mean                                  7.33176\n",
      "trainer/Q Targets Std                                   1.0479\n",
      "trainer/Q Targets Max                                   8.90488\n",
      "trainer/Q Targets Min                                   3.21066\n",
      "trainer/Log Pis Mean                                    6.78211\n",
      "trainer/Log Pis Std                                     2.24566\n",
      "trainer/Log Pis Max                                    10.9656\n",
      "trainer/Log Pis Min                                    -1.18357\n",
      "trainer/Policy mu Mean                                  0.0145023\n",
      "trainer/Policy mu Std                                   0.155899\n",
      "trainer/Policy mu Max                                   0.841411\n",
      "trainer/Policy mu Min                                  -0.848009\n",
      "trainer/Policy log std Mean                            -2.23982\n",
      "trainer/Policy log std Std                              0.201333\n",
      "trainer/Policy log std Max                             -1.0725\n",
      "trainer/Policy log std Min                             -2.78346\n",
      "trainer/Alpha                                           0.021303\n",
      "trainer/Alpha Loss                                     -4.68708\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                            94\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.925028\n",
      "exploration/Rewards Std                                 0.0433769\n",
      "exploration/Rewards Max                                 1.29647\n",
      "exploration/Rewards Min                                 0.679307\n",
      "exploration/Returns Mean                              925.028\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               925.028\n",
      "exploration/Returns Min                               925.028\n",
      "exploration/Actions Mean                                0.0302043\n",
      "exploration/Actions Std                                 0.138564\n",
      "exploration/Actions Max                                 0.51541\n",
      "exploration/Actions Min                                -0.453539\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           925.028\n",
      "exploration/env_infos/final/reward_forward Mean         0.0310023\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0310023\n",
      "exploration/env_infos/final/reward_forward Min          0.0310023\n",
      "exploration/env_infos/initial/reward_forward Mean       0.178328\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.178328\n",
      "exploration/env_infos/initial/reward_forward Min        0.178328\n",
      "exploration/env_infos/reward_forward Mean               0.00459589\n",
      "exploration/env_infos/reward_forward Std                0.170954\n",
      "exploration/env_infos/reward_forward Max                0.801538\n",
      "exploration/env_infos/reward_forward Min               -1.10106\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0788722\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0788722\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0788722\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0409112\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0409112\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0409112\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0804488\n",
      "exploration/env_infos/reward_ctrl Std                   0.0330247\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00481836\n",
      "exploration/env_infos/reward_ctrl Min                  -0.320693\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0158384\n",
      "exploration/env_infos/final/torso_velocity Std          0.0701128\n",
      "exploration/env_infos/final/torso_velocity Max          0.0364258\n",
      "exploration/env_infos/final/torso_velocity Min         -0.114943\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.200985\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0666218\n",
      "exploration/env_infos/initial/torso_velocity Max        0.291513\n",
      "exploration/env_infos/initial/torso_velocity Min        0.133113\n",
      "exploration/env_infos/torso_velocity Mean               0.000656033\n",
      "exploration/env_infos/torso_velocity Std                0.164608\n",
      "exploration/env_infos/torso_velocity Max                0.801538\n",
      "exploration/env_infos/torso_velocity Min               -1.6792\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943597\n",
      "evaluation/Rewards Std                                  0.0286408\n",
      "evaluation/Rewards Max                                  2.44992\n",
      "evaluation/Rewards Min                                  0.714275\n",
      "evaluation/Returns Mean                               943.597\n",
      "evaluation/Returns Std                                  8.74651\n",
      "evaluation/Returns Max                                957.112\n",
      "evaluation/Returns Min                                922.845\n",
      "evaluation/Actions Mean                                 0.0103694\n",
      "evaluation/Actions Std                                  0.119509\n",
      "evaluation/Actions Max                                  0.580538\n",
      "evaluation/Actions Min                                 -0.434185\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.597\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.46523e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.34165e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.07718e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.45232e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0576744\n",
      "evaluation/env_infos/initial/reward_forward Std         0.144805\n",
      "evaluation/env_infos/initial/reward_forward Max         0.326229\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.335551\n",
      "evaluation/env_infos/reward_forward Mean                0.0036185\n",
      "evaluation/env_infos/reward_forward Std                 0.0553445\n",
      "evaluation/env_infos/reward_forward Max                 1.06439\n",
      "evaluation/env_infos/reward_forward Min                -1.0912\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0573684\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00928192\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.043363\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0779409\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0185025\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00719801\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00915981\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0410157\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0575598\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0106329\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00915981\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.285725\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.53922e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.2807e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.07718e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.45232e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146323\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.23276\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.667524\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.335551\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000842433\n",
      "evaluation/env_infos/torso_velocity Std                 0.0653028\n",
      "evaluation/env_infos/torso_velocity Max                 1.16528\n",
      "evaluation/env_infos/torso_velocity Min                -1.85003\n",
      "time/data storing (s)                                   0.0319222\n",
      "time/evaluation sampling (s)                           44.7077\n",
      "time/exploration sampling (s)                           2.36105\n",
      "time/logging (s)                                        0.371955\n",
      "time/saving (s)                                         0.02665\n",
      "time/training (s)                                       4.77866\n",
      "time/epoch (s)                                         52.278\n",
      "time/total (s)                                       1143.2\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:14:46.078511 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  46000\n",
      "trainer/QF1 Loss                                        0.258338\n",
      "trainer/QF2 Loss                                        0.265541\n",
      "trainer/Policy Loss                                    -1.46917\n",
      "trainer/Q1 Predictions Mean                             7.94268\n",
      "trainer/Q1 Predictions Std                              1.05242\n",
      "trainer/Q1 Predictions Max                              9.71267\n",
      "trainer/Q1 Predictions Min                              3.75192\n",
      "trainer/Q2 Predictions Mean                             7.86068\n",
      "trainer/Q2 Predictions Std                              1.01091\n",
      "trainer/Q2 Predictions Max                              9.7086\n",
      "trainer/Q2 Predictions Min                              4.18418\n",
      "trainer/Q Targets Mean                                  7.80421\n",
      "trainer/Q Targets Std                                   1.13083\n",
      "trainer/Q Targets Max                                   9.82065\n",
      "trainer/Q Targets Min                                   3.3683\n",
      "trainer/Log Pis Mean                                    6.98141\n",
      "trainer/Log Pis Std                                     2.37525\n",
      "trainer/Log Pis Max                                    11.8557\n",
      "trainer/Log Pis Min                                    -3.11576\n",
      "trainer/Policy mu Mean                                  0.000962648\n",
      "trainer/Policy mu Std                                   0.145242\n",
      "trainer/Policy mu Max                                   0.816488\n",
      "trainer/Policy mu Min                                  -0.717819\n",
      "trainer/Policy log std Mean                            -2.26448\n",
      "trainer/Policy log std Std                              0.201078\n",
      "trainer/Policy log std Max                             -1.36063\n",
      "trainer/Policy log std Min                             -2.98147\n",
      "trainer/Alpha                                           0.020395\n",
      "trainer/Alpha Loss                                     -3.96443\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                            95\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.945152\n",
      "exploration/Rewards Std                                 0.0507356\n",
      "exploration/Rewards Max                                 1.46813\n",
      "exploration/Rewards Min                                 0.694237\n",
      "exploration/Returns Mean                              945.152\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               945.152\n",
      "exploration/Returns Min                               945.152\n",
      "exploration/Actions Mean                                0.0307412\n",
      "exploration/Actions Std                                 0.121556\n",
      "exploration/Actions Max                                 0.571849\n",
      "exploration/Actions Min                                -0.449532\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           945.152\n",
      "exploration/env_infos/final/reward_forward Mean         0.299616\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.299616\n",
      "exploration/env_infos/final/reward_forward Min          0.299616\n",
      "exploration/env_infos/initial/reward_forward Mean       0.259848\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.259848\n",
      "exploration/env_infos/initial/reward_forward Min        0.259848\n",
      "exploration/env_infos/reward_forward Mean               0.0267234\n",
      "exploration/env_infos/reward_forward Std                0.230685\n",
      "exploration/env_infos/reward_forward Max                0.870168\n",
      "exploration/env_infos/reward_forward Min               -0.89865\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0715737\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0715737\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0715737\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0325406\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0325406\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0325406\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0628835\n",
      "exploration/env_infos/reward_ctrl Std                   0.0289996\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0127385\n",
      "exploration/env_infos/reward_ctrl Min                  -0.305763\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.096044\n",
      "exploration/env_infos/final/torso_velocity Std          0.144104\n",
      "exploration/env_infos/final/torso_velocity Max          0.299616\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0139742\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.238287\n",
      "exploration/env_infos/initial/torso_velocity Std        0.225773\n",
      "exploration/env_infos/initial/torso_velocity Max        0.503391\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0483765\n",
      "exploration/env_infos/torso_velocity Mean               0.0045693\n",
      "exploration/env_infos/torso_velocity Std                0.166226\n",
      "exploration/env_infos/torso_velocity Max                0.870168\n",
      "exploration/env_infos/torso_velocity Min               -1.63319\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.950413\n",
      "evaluation/Rewards Std                                  0.0362794\n",
      "evaluation/Rewards Max                                  2.49039\n",
      "evaluation/Rewards Min                                  0.673822\n",
      "evaluation/Returns Mean                               950.413\n",
      "evaluation/Returns Std                                 21.3682\n",
      "evaluation/Returns Max                                973.83\n",
      "evaluation/Returns Min                                901.083\n",
      "evaluation/Actions Mean                                 0.00186176\n",
      "evaluation/Actions Std                                  0.112664\n",
      "evaluation/Actions Max                                  0.564931\n",
      "evaluation/Actions Min                                 -0.422691\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            950.413\n",
      "evaluation/env_infos/final/reward_forward Mean          1.87681e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.92174e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.07942e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.89316e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00583291\n",
      "evaluation/env_infos/initial/reward_forward Std         0.111204\n",
      "evaluation/env_infos/initial/reward_forward Max         0.288146\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.249087\n",
      "evaluation/env_infos/reward_forward Mean                0.000639721\n",
      "evaluation/env_infos/reward_forward Std                 0.0536882\n",
      "evaluation/env_infos/reward_forward Max                 1.15385\n",
      "evaluation/env_infos/reward_forward Min                -1.54841\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0506151\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.022555\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0255437\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.103832\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.014936\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0071335\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00592073\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0303582\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0507865\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0235287\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00592073\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.326178\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.87354e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.98403e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.07942e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.10445e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.12138\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.23587\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.600157\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.285847\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00244173\n",
      "evaluation/env_infos/torso_velocity Std                 0.0627378\n",
      "evaluation/env_infos/torso_velocity Max                 1.43119\n",
      "evaluation/env_infos/torso_velocity Min                -1.8185\n",
      "time/data storing (s)                                   0.0312383\n",
      "time/evaluation sampling (s)                           46.2609\n",
      "time/exploration sampling (s)                           2.26514\n",
      "time/logging (s)                                        0.304226\n",
      "time/saving (s)                                         0.0269403\n",
      "time/training (s)                                       4.43902\n",
      "time/epoch (s)                                         53.3275\n",
      "time/total (s)                                       1196.86\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:15:39.845219 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  48000\n",
      "trainer/QF1 Loss                                        0.298648\n",
      "trainer/QF2 Loss                                        0.316541\n",
      "trainer/Policy Loss                                    -1.34313\n",
      "trainer/Q1 Predictions Mean                             7.8232\n",
      "trainer/Q1 Predictions Std                              1.0774\n",
      "trainer/Q1 Predictions Max                              9.46294\n",
      "trainer/Q1 Predictions Min                              2.20953\n",
      "trainer/Q2 Predictions Mean                             7.81103\n",
      "trainer/Q2 Predictions Std                              1.08467\n",
      "trainer/Q2 Predictions Max                              9.57892\n",
      "trainer/Q2 Predictions Min                              2.29869\n",
      "trainer/Q Targets Mean                                  8.06632\n",
      "trainer/Q Targets Std                                   1.1826\n",
      "trainer/Q Targets Max                                  10.4356\n",
      "trainer/Q Targets Min                                   1.60703\n",
      "trainer/Log Pis Mean                                    7.04221\n",
      "trainer/Log Pis Std                                     2.20974\n",
      "trainer/Log Pis Max                                    10.9576\n",
      "trainer/Log Pis Min                                    -0.341683\n",
      "trainer/Policy mu Mean                                  0.0115897\n",
      "trainer/Policy mu Std                                   0.159952\n",
      "trainer/Policy mu Max                                   1.00794\n",
      "trainer/Policy mu Min                                  -0.667647\n",
      "trainer/Policy log std Mean                            -2.25702\n",
      "trainer/Policy log std Std                              0.189703\n",
      "trainer/Policy log std Max                             -1.40835\n",
      "trainer/Policy log std Min                             -2.86738\n",
      "trainer/Alpha                                           0.0195864\n",
      "trainer/Alpha Loss                                     -3.76643\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                            96\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.929268\n",
      "exploration/Rewards Std                                 0.0940117\n",
      "exploration/Rewards Max                                 1.9052\n",
      "exploration/Rewards Min                                 0.740211\n",
      "exploration/Returns Mean                              929.268\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               929.268\n",
      "exploration/Returns Min                               929.268\n",
      "exploration/Actions Mean                                0.0326718\n",
      "exploration/Actions Std                                 0.147428\n",
      "exploration/Actions Max                                 0.441338\n",
      "exploration/Actions Min                                -0.392585\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           929.268\n",
      "exploration/env_infos/final/reward_forward Mean         0.286169\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.286169\n",
      "exploration/env_infos/final/reward_forward Min          0.286169\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0747004\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0747004\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0747004\n",
      "exploration/env_infos/reward_forward Mean               0.0283003\n",
      "exploration/env_infos/reward_forward Std                0.275464\n",
      "exploration/env_infos/reward_forward Max                1.05651\n",
      "exploration/env_infos/reward_forward Min               -0.791161\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.115237\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.115237\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.115237\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0560538\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0560538\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0560538\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0912093\n",
      "exploration/env_infos/reward_ctrl Std                   0.0332759\n",
      "exploration/env_infos/reward_ctrl Max                  -0.013774\n",
      "exploration/env_infos/reward_ctrl Min                  -0.259789\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0297692\n",
      "exploration/env_infos/final/torso_velocity Std          0.181636\n",
      "exploration/env_infos/final/torso_velocity Max          0.286169\n",
      "exploration/env_infos/final/torso_velocity Min         -0.111913\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0691587\n",
      "exploration/env_infos/initial/torso_velocity Std        0.291001\n",
      "exploration/env_infos/initial/torso_velocity Max        0.475006\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.192829\n",
      "exploration/env_infos/torso_velocity Mean               0.00458317\n",
      "exploration/env_infos/torso_velocity Std                0.232564\n",
      "exploration/env_infos/torso_velocity Max                1.05651\n",
      "exploration/env_infos/torso_velocity Min               -1.57075\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.929135\n",
      "evaluation/Rewards Std                                  0.0296358\n",
      "evaluation/Rewards Max                                  1.96251\n",
      "evaluation/Rewards Min                                  0.628053\n",
      "evaluation/Returns Mean                               929.135\n",
      "evaluation/Returns Std                                 12.89\n",
      "evaluation/Returns Max                                945.982\n",
      "evaluation/Returns Min                                900.826\n",
      "evaluation/Actions Mean                                 0.00869503\n",
      "evaluation/Actions Std                                  0.133947\n",
      "evaluation/Actions Max                                  0.623203\n",
      "evaluation/Actions Min                                 -0.370977\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            929.135\n",
      "evaluation/env_infos/final/reward_forward Mean          9.10246e-06\n",
      "evaluation/env_infos/final/reward_forward Std           5.50239e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000274816\n",
      "evaluation/env_infos/final/reward_forward Min          -4.72746e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0214791\n",
      "evaluation/env_infos/initial/reward_forward Std         0.15603\n",
      "evaluation/env_infos/initial/reward_forward Max         0.284615\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.398642\n",
      "evaluation/env_infos/reward_forward Mean                0.0034897\n",
      "evaluation/env_infos/reward_forward Std                 0.0563415\n",
      "evaluation/env_infos/reward_forward Max                 1.74491\n",
      "evaluation/env_infos/reward_forward Min                -1.32443\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0718868\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0131324\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0533434\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0990399\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0272729\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00771123\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0182606\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0467768\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0720701\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0139715\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0182606\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.371947\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.88248e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           7.68027e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000274816\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000588406\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.119216\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.241526\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.685661\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.398642\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00119238\n",
      "evaluation/env_infos/torso_velocity Std                 0.0643699\n",
      "evaluation/env_infos/torso_velocity Max                 1.74491\n",
      "evaluation/env_infos/torso_velocity Min                -1.79023\n",
      "time/data storing (s)                                   0.0296707\n",
      "time/evaluation sampling (s)                           46.7482\n",
      "time/exploration sampling (s)                           2.04791\n",
      "time/logging (s)                                        0.274479\n",
      "time/saving (s)                                         0.0266573\n",
      "time/training (s)                                       4.25132\n",
      "time/epoch (s)                                         53.3783\n",
      "time/total (s)                                       1250.59\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:16:32.918655 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  50000\n",
      "trainer/QF1 Loss                                        0.221858\n",
      "trainer/QF2 Loss                                        0.200312\n",
      "trainer/Policy Loss                                    -1.42402\n",
      "trainer/Q1 Predictions Mean                             8.49892\n",
      "trainer/Q1 Predictions Std                              1.00859\n",
      "trainer/Q1 Predictions Max                             10.134\n",
      "trainer/Q1 Predictions Min                              4.63529\n",
      "trainer/Q2 Predictions Mean                             8.55118\n",
      "trainer/Q2 Predictions Std                              1.03252\n",
      "trainer/Q2 Predictions Max                             10.207\n",
      "trainer/Q2 Predictions Min                              4.67886\n",
      "trainer/Q Targets Mean                                  8.48099\n",
      "trainer/Q Targets Std                                   1.06618\n",
      "trainer/Q Targets Max                                  10.3675\n",
      "trainer/Q Targets Min                                   4.17236\n",
      "trainer/Log Pis Mean                                    7.65048\n",
      "trainer/Log Pis Std                                     2.1134\n",
      "trainer/Log Pis Max                                    12.4907\n",
      "trainer/Log Pis Min                                     0.789652\n",
      "trainer/Policy mu Mean                                 -0.0206489\n",
      "trainer/Policy mu Std                                   0.145864\n",
      "trainer/Policy mu Max                                   0.688875\n",
      "trainer/Policy mu Min                                  -1.67748\n",
      "trainer/Policy log std Mean                            -2.33538\n",
      "trainer/Policy log std Std                              0.190442\n",
      "trainer/Policy log std Max                             -1.37409\n",
      "trainer/Policy log std Min                             -2.94241\n",
      "trainer/Alpha                                           0.0187976\n",
      "trainer/Alpha Loss                                     -1.38888\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                            97\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.91989\n",
      "exploration/Rewards Std                                 0.111507\n",
      "exploration/Rewards Max                                 1.99191\n",
      "exploration/Rewards Min                                 0.739709\n",
      "exploration/Returns Mean                              919.89\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               919.89\n",
      "exploration/Returns Min                               919.89\n",
      "exploration/Actions Mean                               -0.030183\n",
      "exploration/Actions Std                                 0.154209\n",
      "exploration/Actions Max                                 0.465796\n",
      "exploration/Actions Min                                -0.5445\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           919.89\n",
      "exploration/env_infos/final/reward_forward Mean         0.312665\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.312665\n",
      "exploration/env_infos/final/reward_forward Min          0.312665\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.224532\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.224532\n",
      "exploration/env_infos/initial/reward_forward Min       -0.224532\n",
      "exploration/env_infos/reward_forward Mean              -0.00810929\n",
      "exploration/env_infos/reward_forward Std                0.180743\n",
      "exploration/env_infos/reward_forward Max                1.04469\n",
      "exploration/env_infos/reward_forward Min               -0.654103\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0350907\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0350907\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0350907\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0423064\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0423064\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0423064\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0987659\n",
      "exploration/env_infos/reward_ctrl Std                   0.0392371\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0181151\n",
      "exploration/env_infos/reward_ctrl Min                  -0.260291\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00947836\n",
      "exploration/env_infos/final/torso_velocity Std          0.249376\n",
      "exploration/env_infos/final/torso_velocity Max          0.312665\n",
      "exploration/env_infos/final/torso_velocity Min         -0.294849\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0798601\n",
      "exploration/env_infos/initial/torso_velocity Std        0.234575\n",
      "exploration/env_infos/initial/torso_velocity Max        0.346281\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.224532\n",
      "exploration/env_infos/torso_velocity Mean              -0.00796961\n",
      "exploration/env_infos/torso_velocity Std                0.14721\n",
      "exploration/env_infos/torso_velocity Max                1.04469\n",
      "exploration/env_infos/torso_velocity Min               -1.45962\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943888\n",
      "evaluation/Rewards Std                                  0.0311403\n",
      "evaluation/Rewards Max                                  1.89726\n",
      "evaluation/Rewards Min                                  0.754109\n",
      "evaluation/Returns Mean                               943.888\n",
      "evaluation/Returns Std                                 19.2965\n",
      "evaluation/Returns Max                                971.779\n",
      "evaluation/Returns Min                                910.564\n",
      "evaluation/Actions Mean                                -0.0272095\n",
      "evaluation/Actions Std                                  0.116451\n",
      "evaluation/Actions Max                                  0.361982\n",
      "evaluation/Actions Min                                 -0.511021\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.888\n",
      "evaluation/env_infos/final/reward_forward Mean          3.92406e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000132669\n",
      "evaluation/env_infos/final/reward_forward Max           0.000523949\n",
      "evaluation/env_infos/final/reward_forward Min          -1.04841e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.026014\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122262\n",
      "evaluation/env_infos/initial/reward_forward Max         0.250495\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.295481\n",
      "evaluation/env_infos/reward_forward Mean                0.00687958\n",
      "evaluation/env_infos/reward_forward Std                 0.0688564\n",
      "evaluation/env_infos/reward_forward Max                 1.50128\n",
      "evaluation/env_infos/reward_forward Min                -0.919888\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0566379\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0195727\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0276531\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0892722\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0123634\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00540169\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00658714\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0298708\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0572051\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0203173\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00658714\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.245891\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.2256e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000121274\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000523949\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000683643\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152845\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244398\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.728404\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.295481\n",
      "evaluation/env_infos/torso_velocity Mean                0.000191475\n",
      "evaluation/env_infos/torso_velocity Std                 0.0755387\n",
      "evaluation/env_infos/torso_velocity Max                 1.76424\n",
      "evaluation/env_infos/torso_velocity Min                -1.86939\n",
      "time/data storing (s)                                   0.0312051\n",
      "time/evaluation sampling (s)                           45.8297\n",
      "time/exploration sampling (s)                           2.1133\n",
      "time/logging (s)                                        0.276868\n",
      "time/saving (s)                                         0.0253421\n",
      "time/training (s)                                       4.46542\n",
      "time/epoch (s)                                         52.7419\n",
      "time/total (s)                                       1303.67\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:17:25.306284 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  52000\n",
      "trainer/QF1 Loss                                        0.338197\n",
      "trainer/QF2 Loss                                        0.373757\n",
      "trainer/Policy Loss                                    -1.93844\n",
      "trainer/Q1 Predictions Mean                             9.08365\n",
      "trainer/Q1 Predictions Std                              0.94792\n",
      "trainer/Q1 Predictions Max                             10.8473\n",
      "trainer/Q1 Predictions Min                              5.95734\n",
      "trainer/Q2 Predictions Mean                             8.99583\n",
      "trainer/Q2 Predictions Std                              0.933104\n",
      "trainer/Q2 Predictions Max                             10.7334\n",
      "trainer/Q2 Predictions Min                              5.57832\n",
      "trainer/Q Targets Mean                                  8.95464\n",
      "trainer/Q Targets Std                                   1.08812\n",
      "trainer/Q Targets Max                                  10.9079\n",
      "trainer/Q Targets Min                                   0.0287774\n",
      "trainer/Log Pis Mean                                    7.59287\n",
      "trainer/Log Pis Std                                     2.34708\n",
      "trainer/Log Pis Max                                    13.4457\n",
      "trainer/Log Pis Min                                    -0.652321\n",
      "trainer/Policy mu Mean                                 -0.0162507\n",
      "trainer/Policy mu Std                                   0.13647\n",
      "trainer/Policy mu Max                                   0.740939\n",
      "trainer/Policy mu Min                                  -1.02291\n",
      "trainer/Policy log std Mean                            -2.33478\n",
      "trainer/Policy log std Std                              0.194696\n",
      "trainer/Policy log std Max                             -1.60124\n",
      "trainer/Policy log std Min                             -3.03542\n",
      "trainer/Alpha                                           0.0182194\n",
      "trainer/Alpha Loss                                     -1.63055\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                            98\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.91645\n",
      "exploration/Rewards Std                                 0.0574887\n",
      "exploration/Rewards Max                                 1.54682\n",
      "exploration/Rewards Min                                 0.76299\n",
      "exploration/Returns Mean                              916.45\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.45\n",
      "exploration/Returns Min                               916.45\n",
      "exploration/Actions Mean                               -0.0174024\n",
      "exploration/Actions Std                                 0.149985\n",
      "exploration/Actions Max                                 0.463622\n",
      "exploration/Actions Min                                -0.58183\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.45\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0137078\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0137078\n",
      "exploration/env_infos/final/reward_forward Min         -0.0137078\n",
      "exploration/env_infos/initial/reward_forward Mean       0.130175\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.130175\n",
      "exploration/env_infos/initial/reward_forward Min        0.130175\n",
      "exploration/env_infos/reward_forward Mean               0.00487101\n",
      "exploration/env_infos/reward_forward Std                0.134432\n",
      "exploration/env_infos/reward_forward Max                0.543558\n",
      "exploration/env_infos/reward_forward Min               -0.976583\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.078807\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.078807\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.078807\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0622449\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0622449\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0622449\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0911937\n",
      "exploration/env_infos/reward_ctrl Std                   0.0397209\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00701882\n",
      "exploration/env_infos/reward_ctrl Min                  -0.23701\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0158785\n",
      "exploration/env_infos/final/torso_velocity Std          0.0209243\n",
      "exploration/env_infos/final/torso_velocity Max          0.0311555\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0137078\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.153256\n",
      "exploration/env_infos/initial/torso_velocity Std        0.115935\n",
      "exploration/env_infos/initial/torso_velocity Max        0.305373\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0242197\n",
      "exploration/env_infos/torso_velocity Mean               0.000533576\n",
      "exploration/env_infos/torso_velocity Std                0.150243\n",
      "exploration/env_infos/torso_velocity Max                0.923282\n",
      "exploration/env_infos/torso_velocity Min               -1.58257\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.964989\n",
      "evaluation/Rewards Std                                  0.0287859\n",
      "evaluation/Rewards Max                                  2.67096\n",
      "evaluation/Rewards Min                                  0.568074\n",
      "evaluation/Returns Mean                               964.989\n",
      "evaluation/Returns Std                                 11.2929\n",
      "evaluation/Returns Max                                983.079\n",
      "evaluation/Returns Min                                942.394\n",
      "evaluation/Actions Mean                                -0.0114227\n",
      "evaluation/Actions Std                                  0.0941141\n",
      "evaluation/Actions Max                                  0.585545\n",
      "evaluation/Actions Min                                 -0.497733\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            964.989\n",
      "evaluation/env_infos/final/reward_forward Mean          9.30578e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.71428e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.1404e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.33318e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00395425\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132365\n",
      "evaluation/env_infos/initial/reward_forward Max         0.330956\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.23536\n",
      "evaluation/env_infos/reward_forward Mean                0.001718\n",
      "evaluation/env_infos/reward_forward Std                 0.0477569\n",
      "evaluation/env_infos/reward_forward Max                 1.22088\n",
      "evaluation/env_infos/reward_forward Min                -1.27079\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0355203\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0115061\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0164371\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0575001\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0148767\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00579193\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00639946\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0275417\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0359518\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0133348\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00639946\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.431926\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.94874e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.1006e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.1404e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.03452e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138735\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.23027\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.63535\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.23536\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00245428\n",
      "evaluation/env_infos/torso_velocity Std                 0.0619252\n",
      "evaluation/env_infos/torso_velocity Max                 1.22088\n",
      "evaluation/env_infos/torso_velocity Min                -1.90857\n",
      "time/data storing (s)                                   0.0307865\n",
      "time/evaluation sampling (s)                           45.3216\n",
      "time/exploration sampling (s)                           2.03964\n",
      "time/logging (s)                                        0.281251\n",
      "time/saving (s)                                         0.0251946\n",
      "time/training (s)                                       4.35939\n",
      "time/epoch (s)                                         52.0578\n",
      "time/total (s)                                       1356.06\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:18:18.967700 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  54000\n",
      "trainer/QF1 Loss                                        0.274661\n",
      "trainer/QF2 Loss                                        0.240071\n",
      "trainer/Policy Loss                                    -2.4179\n",
      "trainer/Q1 Predictions Mean                             9.26398\n",
      "trainer/Q1 Predictions Std                              1.27985\n",
      "trainer/Q1 Predictions Max                             11.0936\n",
      "trainer/Q1 Predictions Min                              0.0919854\n",
      "trainer/Q2 Predictions Mean                             9.14529\n",
      "trainer/Q2 Predictions Std                              1.18032\n",
      "trainer/Q2 Predictions Max                             11.0682\n",
      "trainer/Q2 Predictions Min                              2.50886\n",
      "trainer/Q Targets Mean                                  9.11936\n",
      "trainer/Q Targets Std                                   1.28332\n",
      "trainer/Q Targets Max                                  11.5347\n",
      "trainer/Q Targets Min                                  -0.513291\n",
      "trainer/Log Pis Mean                                    7.33695\n",
      "trainer/Log Pis Std                                     2.1974\n",
      "trainer/Log Pis Max                                    12.904\n",
      "trainer/Log Pis Min                                     0.11482\n",
      "trainer/Policy mu Mean                                 -0.00646802\n",
      "trainer/Policy mu Std                                   0.162329\n",
      "trainer/Policy mu Max                                   1.62563\n",
      "trainer/Policy mu Min                                  -1.89625\n",
      "trainer/Policy log std Mean                            -2.27264\n",
      "trainer/Policy log std Std                              0.215767\n",
      "trainer/Policy log std Max                             -1.28433\n",
      "trainer/Policy log std Min                             -2.91594\n",
      "trainer/Alpha                                           0.017531\n",
      "trainer/Alpha Loss                                     -2.68103\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                            99\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.94885\n",
      "exploration/Rewards Std                                 0.0949492\n",
      "exploration/Rewards Max                                 1.53705\n",
      "exploration/Rewards Min                                 0.648419\n",
      "exploration/Returns Mean                              948.85\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               948.85\n",
      "exploration/Returns Min                               948.85\n",
      "exploration/Actions Mean                               -0.0160491\n",
      "exploration/Actions Std                                 0.135068\n",
      "exploration/Actions Max                                 0.474161\n",
      "exploration/Actions Min                                -0.490596\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           948.85\n",
      "exploration/env_infos/final/reward_forward Mean         0.0791791\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0791791\n",
      "exploration/env_infos/final/reward_forward Min          0.0791791\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0807135\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0807135\n",
      "exploration/env_infos/initial/reward_forward Min        0.0807135\n",
      "exploration/env_infos/reward_forward Mean              -0.00113907\n",
      "exploration/env_infos/reward_forward Std                0.303563\n",
      "exploration/env_infos/reward_forward Max                1.30044\n",
      "exploration/env_infos/reward_forward Min               -1.24682\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0582736\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0582736\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0582736\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0365588\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0365588\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0365588\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0740034\n",
      "exploration/env_infos/reward_ctrl Std                   0.0358947\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00981342\n",
      "exploration/env_infos/reward_ctrl Min                  -0.351581\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.11772\n",
      "exploration/env_infos/final/torso_velocity Std          0.180606\n",
      "exploration/env_infos/final/torso_velocity Max          0.0791791\n",
      "exploration/env_infos/final/torso_velocity Min         -0.357062\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.181976\n",
      "exploration/env_infos/initial/torso_velocity Std        0.220996\n",
      "exploration/env_infos/initial/torso_velocity Max        0.488669\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0234557\n",
      "exploration/env_infos/torso_velocity Mean              -0.00839028\n",
      "exploration/env_infos/torso_velocity Std                0.234039\n",
      "exploration/env_infos/torso_velocity Max                1.30044\n",
      "exploration/env_infos/torso_velocity Min               -1.35104\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978872\n",
      "evaluation/Rewards Std                                  0.0284774\n",
      "evaluation/Rewards Max                                  3.04542\n",
      "evaluation/Rewards Min                                  0.761057\n",
      "evaluation/Returns Mean                               978.872\n",
      "evaluation/Returns Std                                  4.37634\n",
      "evaluation/Returns Max                                985.701\n",
      "evaluation/Returns Min                                967.193\n",
      "evaluation/Actions Mean                                 0.00475207\n",
      "evaluation/Actions Std                                  0.0740483\n",
      "evaluation/Actions Max                                  0.549542\n",
      "evaluation/Actions Min                                 -0.402094\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.872\n",
      "evaluation/env_infos/final/reward_forward Mean          1.69114e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.45399e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.60641e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.08342e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00914677\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125982\n",
      "evaluation/env_infos/initial/reward_forward Max         0.259924\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.354108\n",
      "evaluation/env_infos/reward_forward Mean               -0.00149174\n",
      "evaluation/env_infos/reward_forward Std                 0.0514214\n",
      "evaluation/env_infos/reward_forward Max                 1.16252\n",
      "evaluation/env_infos/reward_forward Min                -1.32346\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0215506\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00451806\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0139315\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0331462\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0120855\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00292682\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00784754\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0187216\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0220229\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00740345\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00784754\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.238943\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.86425e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.22239e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.60641e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.08342e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149169\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.215827\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.598733\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.354108\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00208286\n",
      "evaluation/env_infos/torso_velocity Std                 0.0617916\n",
      "evaluation/env_infos/torso_velocity Max                 1.57307\n",
      "evaluation/env_infos/torso_velocity Min                -2.0549\n",
      "time/data storing (s)                                   0.0429896\n",
      "time/evaluation sampling (s)                           46.0149\n",
      "time/exploration sampling (s)                           2.21442\n",
      "time/logging (s)                                        0.297492\n",
      "time/saving (s)                                         0.0248931\n",
      "time/training (s)                                       4.73491\n",
      "time/epoch (s)                                         53.3296\n",
      "time/total (s)                                       1409.73\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:19:14.513177 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 26 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  56000\n",
      "trainer/QF1 Loss                                        0.60489\n",
      "trainer/QF2 Loss                                        0.593843\n",
      "trainer/Policy Loss                                    -2.3466\n",
      "trainer/Q1 Predictions Mean                             9.57151\n",
      "trainer/Q1 Predictions Std                              1.10426\n",
      "trainer/Q1 Predictions Max                             11.6537\n",
      "trainer/Q1 Predictions Min                              4.9947\n",
      "trainer/Q2 Predictions Mean                             9.44636\n",
      "trainer/Q2 Predictions Std                              1.08987\n",
      "trainer/Q2 Predictions Max                             11.4025\n",
      "trainer/Q2 Predictions Min                              4.44507\n",
      "trainer/Q Targets Mean                                  9.42227\n",
      "trainer/Q Targets Std                                   1.36767\n",
      "trainer/Q Targets Max                                  11.2466\n",
      "trainer/Q Targets Min                                   0.319411\n",
      "trainer/Log Pis Mean                                    7.7078\n",
      "trainer/Log Pis Std                                     2.38781\n",
      "trainer/Log Pis Max                                    13.1106\n",
      "trainer/Log Pis Min                                     0.982458\n",
      "trainer/Policy mu Mean                                  0.0132366\n",
      "trainer/Policy mu Std                                   0.132313\n",
      "trainer/Policy mu Max                                   0.697949\n",
      "trainer/Policy mu Min                                  -0.814282\n",
      "trainer/Policy log std Mean                            -2.35676\n",
      "trainer/Policy log std Std                              0.220417\n",
      "trainer/Policy log std Max                             -1.49215\n",
      "trainer/Policy log std Min                             -3.20026\n",
      "trainer/Alpha                                           0.016837\n",
      "trainer/Alpha Loss                                     -1.19331\n",
      "exploration/num steps total                         28000\n",
      "exploration/num paths total                           100\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.985022\n",
      "exploration/Rewards Std                                 0.134026\n",
      "exploration/Rewards Max                                 2.05155\n",
      "exploration/Rewards Min                                 0.805803\n",
      "exploration/Returns Mean                              985.022\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               985.022\n",
      "exploration/Returns Min                               985.022\n",
      "exploration/Actions Mean                                0.0201152\n",
      "exploration/Actions Std                                 0.113461\n",
      "exploration/Actions Max                                 0.436337\n",
      "exploration/Actions Min                                -0.427516\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           985.022\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0468976\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0468976\n",
      "exploration/env_infos/final/reward_forward Min         -0.0468976\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0712913\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0712913\n",
      "exploration/env_infos/initial/reward_forward Min        0.0712913\n",
      "exploration/env_infos/reward_forward Mean              -0.0169006\n",
      "exploration/env_infos/reward_forward Std                0.254104\n",
      "exploration/env_infos/reward_forward Max                1.28147\n",
      "exploration/env_infos/reward_forward Min               -1.16382\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0412127\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0412127\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0412127\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0232152\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0232152\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0232152\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.053112\n",
      "exploration/env_infos/reward_ctrl Std                   0.0264368\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00669568\n",
      "exploration/env_infos/reward_ctrl Min                  -0.253227\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0218268\n",
      "exploration/env_infos/final/torso_velocity Std          0.0310383\n",
      "exploration/env_infos/final/torso_velocity Max          0.0219121\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0468976\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.187198\n",
      "exploration/env_infos/initial/torso_velocity Std        0.267156\n",
      "exploration/env_infos/initial/torso_velocity Max        0.556571\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0662695\n",
      "exploration/env_infos/torso_velocity Mean              -0.00317221\n",
      "exploration/env_infos/torso_velocity Std                0.235648\n",
      "exploration/env_infos/torso_velocity Max                1.28147\n",
      "exploration/env_infos/torso_velocity Min               -1.24681\n",
      "evaluation/num steps total                         675000\n",
      "evaluation/num paths total                            675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.972398\n",
      "evaluation/Rewards Std                                  0.0280235\n",
      "evaluation/Rewards Max                                  2.07118\n",
      "evaluation/Rewards Min                                  0.368798\n",
      "evaluation/Returns Mean                               972.398\n",
      "evaluation/Returns Std                                  7.44215\n",
      "evaluation/Returns Max                                987.033\n",
      "evaluation/Returns Min                                953.635\n",
      "evaluation/Actions Mean                                 0.00353271\n",
      "evaluation/Actions Std                                  0.0850037\n",
      "evaluation/Actions Max                                  0.634309\n",
      "evaluation/Actions Min                                 -0.627396\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            972.398\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000324801\n",
      "evaluation/env_infos/final/reward_forward Std           0.00168506\n",
      "evaluation/env_infos/final/reward_forward Max           0.00856864\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000448575\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.036073\n",
      "evaluation/env_infos/initial/reward_forward Std         0.100633\n",
      "evaluation/env_infos/initial/reward_forward Max         0.224404\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.147466\n",
      "evaluation/env_infos/reward_forward Mean                0.00574836\n",
      "evaluation/env_infos/reward_forward Std                 0.0717315\n",
      "evaluation/env_infos/reward_forward Max                 1.95647\n",
      "evaluation/env_infos/reward_forward Min                -0.958786\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0285343\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00724053\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0151073\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0463253\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0157644\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00469731\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00970678\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.028988\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0289524\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0119465\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00923756\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.631202\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000166837\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00107127\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00856864\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000448575\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.164917\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231483\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.628321\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.147466\n",
      "evaluation/env_infos/torso_velocity Mean                0.000527392\n",
      "evaluation/env_infos/torso_velocity Std                 0.0707796\n",
      "evaluation/env_infos/torso_velocity Max                 1.95647\n",
      "evaluation/env_infos/torso_velocity Min                -1.84215\n",
      "time/data storing (s)                                   0.0299351\n",
      "time/evaluation sampling (s)                           48.5238\n",
      "time/exploration sampling (s)                           2.0194\n",
      "time/logging (s)                                        0.292368\n",
      "time/saving (s)                                         0.0260628\n",
      "time/training (s)                                       4.3047\n",
      "time/epoch (s)                                         55.1963\n",
      "time/total (s)                                       1465.27\n",
      "Epoch                                                  26\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:20:11.243876 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 27 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  58000\n",
      "trainer/QF1 Loss                                        0.190677\n",
      "trainer/QF2 Loss                                        0.23105\n",
      "trainer/Policy Loss                                    -2.18879\n",
      "trainer/Q1 Predictions Mean                             9.63454\n",
      "trainer/Q1 Predictions Std                              1.31353\n",
      "trainer/Q1 Predictions Max                             12.0085\n",
      "trainer/Q1 Predictions Min                             -0.878654\n",
      "trainer/Q2 Predictions Mean                             9.66842\n",
      "trainer/Q2 Predictions Std                              1.18718\n",
      "trainer/Q2 Predictions Max                             11.9147\n",
      "trainer/Q2 Predictions Min                              2.3272\n",
      "trainer/Q Targets Mean                                  9.76128\n",
      "trainer/Q Targets Std                                   1.30382\n",
      "trainer/Q Targets Max                                  11.7057\n",
      "trainer/Q Targets Min                                  -0.513291\n",
      "trainer/Log Pis Mean                                    8.01644\n",
      "trainer/Log Pis Std                                     2.53301\n",
      "trainer/Log Pis Max                                    13.7902\n",
      "trainer/Log Pis Min                                    -0.794762\n",
      "trainer/Policy mu Mean                                 -0.00577631\n",
      "trainer/Policy mu Std                                   0.158652\n",
      "trainer/Policy mu Max                                   1.22983\n",
      "trainer/Policy mu Min                                  -1.79328\n",
      "trainer/Policy log std Mean                            -2.39594\n",
      "trainer/Policy log std Std                              0.204993\n",
      "trainer/Policy log std Max                             -1.34308\n",
      "trainer/Policy log std Min                             -3.14995\n",
      "trainer/Alpha                                           0.0164414\n",
      "trainer/Alpha Loss                                      0.0675455\n",
      "exploration/num steps total                         29000\n",
      "exploration/num paths total                           101\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.958933\n",
      "exploration/Rewards Std                                 0.135262\n",
      "exploration/Rewards Max                                 2.10827\n",
      "exploration/Rewards Min                                 0.65329\n",
      "exploration/Returns Mean                              958.933\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               958.933\n",
      "exploration/Returns Min                               958.933\n",
      "exploration/Actions Mean                               -0.00558458\n",
      "exploration/Actions Std                                 0.135306\n",
      "exploration/Actions Max                                 0.586026\n",
      "exploration/Actions Min                                -0.560369\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           958.933\n",
      "exploration/env_infos/final/reward_forward Mean         0.170489\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.170489\n",
      "exploration/env_infos/final/reward_forward Min          0.170489\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0398076\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0398076\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0398076\n",
      "exploration/env_infos/reward_forward Mean               0.026274\n",
      "exploration/env_infos/reward_forward Std                0.290047\n",
      "exploration/env_infos/reward_forward Max                1.15054\n",
      "exploration/env_infos/reward_forward Min               -0.91438\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.126642\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.126642\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.126642\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0278865\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0278865\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0278865\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0733558\n",
      "exploration/env_infos/reward_ctrl Std                   0.0340667\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0102388\n",
      "exploration/env_infos/reward_ctrl Min                  -0.34671\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0129644\n",
      "exploration/env_infos/final/torso_velocity Std          0.169907\n",
      "exploration/env_infos/final/torso_velocity Max          0.170489\n",
      "exploration/env_infos/final/torso_velocity Min         -0.239083\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0978353\n",
      "exploration/env_infos/initial/torso_velocity Std        0.199818\n",
      "exploration/env_infos/initial/torso_velocity Max        0.380389\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0470754\n",
      "exploration/env_infos/torso_velocity Mean               0.0117159\n",
      "exploration/env_infos/torso_velocity Std                0.237096\n",
      "exploration/env_infos/torso_velocity Max                1.15054\n",
      "exploration/env_infos/torso_velocity Min               -0.91438\n",
      "evaluation/num steps total                         700000\n",
      "evaluation/num paths total                            700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.962388\n",
      "evaluation/Rewards Std                                  0.0342831\n",
      "evaluation/Rewards Max                                  2.93198\n",
      "evaluation/Rewards Min                                  0.705855\n",
      "evaluation/Returns Mean                               962.388\n",
      "evaluation/Returns Std                                  7.50088\n",
      "evaluation/Returns Max                                974.487\n",
      "evaluation/Returns Min                                947.841\n",
      "evaluation/Actions Mean                                -0.010114\n",
      "evaluation/Actions Std                                  0.0979741\n",
      "evaluation/Actions Max                                  0.51419\n",
      "evaluation/Actions Min                                 -0.555922\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            962.388\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00247115\n",
      "evaluation/env_infos/final/reward_forward Std           0.00856968\n",
      "evaluation/env_infos/final/reward_forward Max           5.93301e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0372329\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.007754\n",
      "evaluation/env_infos/initial/reward_forward Std         0.109161\n",
      "evaluation/env_infos/initial/reward_forward Max         0.302499\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.14772\n",
      "evaluation/env_infos/reward_forward Mean                0.00310993\n",
      "evaluation/env_infos/reward_forward Std                 0.0526109\n",
      "evaluation/env_infos/reward_forward Max                 1.51862\n",
      "evaluation/env_infos/reward_forward Min                -0.814788\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0381229\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00735406\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0255783\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0515707\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0159139\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00931954\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0071481\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0435282\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0388049\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0100557\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0071481\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.294145\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000973923\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00984518\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0490538\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0372329\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141283\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244233\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.69327\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.241918\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000288532\n",
      "evaluation/env_infos/torso_velocity Std                 0.060374\n",
      "evaluation/env_infos/torso_velocity Max                 1.51862\n",
      "evaluation/env_infos/torso_velocity Min                -1.97074\n",
      "time/data storing (s)                                   0.0436552\n",
      "time/evaluation sampling (s)                           49.4848\n",
      "time/exploration sampling (s)                           2.08715\n",
      "time/logging (s)                                        0.29758\n",
      "time/saving (s)                                         0.0264536\n",
      "time/training (s)                                       4.42929\n",
      "time/epoch (s)                                         56.3689\n",
      "time/total (s)                                       1522.01\n",
      "Epoch                                                  27\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:21:08.959158 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 28 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  60000\n",
      "trainer/QF1 Loss                                        0.219019\n",
      "trainer/QF2 Loss                                        0.168766\n",
      "trainer/Policy Loss                                    -3.165\n",
      "trainer/Q1 Predictions Mean                            10.0365\n",
      "trainer/Q1 Predictions Std                              0.997427\n",
      "trainer/Q1 Predictions Max                             12.0811\n",
      "trainer/Q1 Predictions Min                              4.62817\n",
      "trainer/Q2 Predictions Mean                            10.1355\n",
      "trainer/Q2 Predictions Std                              0.97276\n",
      "trainer/Q2 Predictions Max                             12.2786\n",
      "trainer/Q2 Predictions Min                              5.02198\n",
      "trainer/Q Targets Mean                                 10.302\n",
      "trainer/Q Targets Std                                   1.06993\n",
      "trainer/Q Targets Max                                  11.8277\n",
      "trainer/Q Targets Min                                   3.42718\n",
      "trainer/Log Pis Mean                                    7.31454\n",
      "trainer/Log Pis Std                                     2.23295\n",
      "trainer/Log Pis Max                                    15.1546\n",
      "trainer/Log Pis Min                                     1.3958\n",
      "trainer/Policy mu Mean                                 -0.0258355\n",
      "trainer/Policy mu Std                                   0.13707\n",
      "trainer/Policy mu Max                                   0.748486\n",
      "trainer/Policy mu Min                                  -0.672184\n",
      "trainer/Policy log std Mean                            -2.3066\n",
      "trainer/Policy log std Std                              0.204626\n",
      "trainer/Policy log std Max                             -1.37761\n",
      "trainer/Policy log std Min                             -3.1225\n",
      "trainer/Alpha                                           0.0159734\n",
      "trainer/Alpha Loss                                     -2.83543\n",
      "exploration/num steps total                         30000\n",
      "exploration/num paths total                           102\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.906628\n",
      "exploration/Rewards Std                                 0.0493107\n",
      "exploration/Rewards Max                                 1.21475\n",
      "exploration/Rewards Min                                 0.735035\n",
      "exploration/Returns Mean                              906.628\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               906.628\n",
      "exploration/Returns Min                               906.628\n",
      "exploration/Actions Mean                               -0.0199914\n",
      "exploration/Actions Std                                 0.155108\n",
      "exploration/Actions Max                                 0.517234\n",
      "exploration/Actions Min                                -0.560603\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           906.628\n",
      "exploration/env_infos/final/reward_forward Mean         0.160016\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.160016\n",
      "exploration/env_infos/final/reward_forward Min          0.160016\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.16799\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.16799\n",
      "exploration/env_infos/initial/reward_forward Min       -0.16799\n",
      "exploration/env_infos/reward_forward Mean               0.0247393\n",
      "exploration/env_infos/reward_forward Std                0.130197\n",
      "exploration/env_infos/reward_forward Max                0.523875\n",
      "exploration/env_infos/reward_forward Min               -0.52838\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.108491\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.108491\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.108491\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0595238\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0595238\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0595238\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0978329\n",
      "exploration/env_infos/reward_ctrl Std                   0.0402395\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0169122\n",
      "exploration/env_infos/reward_ctrl Min                  -0.264965\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0543165\n",
      "exploration/env_infos/final/torso_velocity Std          0.0831586\n",
      "exploration/env_infos/final/torso_velocity Max          0.160016\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0431845\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.116277\n",
      "exploration/env_infos/initial/torso_velocity Std        0.264513\n",
      "exploration/env_infos/initial/torso_velocity Max        0.468993\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.16799\n",
      "exploration/env_infos/torso_velocity Mean               0.00428427\n",
      "exploration/env_infos/torso_velocity Std                0.130577\n",
      "exploration/env_infos/torso_velocity Max                0.552862\n",
      "exploration/env_infos/torso_velocity Min               -1.64807\n",
      "evaluation/num steps total                         725000\n",
      "evaluation/num paths total                            725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.941873\n",
      "evaluation/Rewards Std                                  0.015465\n",
      "evaluation/Rewards Max                                  1.79184\n",
      "evaluation/Rewards Min                                  0.709179\n",
      "evaluation/Returns Mean                               941.873\n",
      "evaluation/Returns Std                                  5.94614\n",
      "evaluation/Returns Max                                949.732\n",
      "evaluation/Returns Min                                932.688\n",
      "evaluation/Actions Mean                                -0.014324\n",
      "evaluation/Actions Std                                  0.12016\n",
      "evaluation/Actions Max                                  0.55851\n",
      "evaluation/Actions Min                                 -0.506415\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            941.873\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.54446e-08\n",
      "evaluation/env_infos/final/reward_forward Std           6.04182e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.4948e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.06392e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0723794\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129576\n",
      "evaluation/env_infos/initial/reward_forward Max         0.200727\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.342887\n",
      "evaluation/env_infos/reward_forward Mean               -0.00237213\n",
      "evaluation/env_infos/reward_forward Std                 0.0604528\n",
      "evaluation/env_infos/reward_forward Max                 1.31324\n",
      "evaluation/env_infos/reward_forward Min                -1.96576\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0582818\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00601859\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0506631\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0673053\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0383346\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00365888\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0321845\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0462511\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0585748\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00765552\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0319382\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.294832\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.0839e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           6.39752e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           4.38534e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.06392e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.0926779\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.262161\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.620232\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.342887\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00413592\n",
      "evaluation/env_infos/torso_velocity Std                 0.0658655\n",
      "evaluation/env_infos/torso_velocity Max                 1.31324\n",
      "evaluation/env_infos/torso_velocity Min                -1.96576\n",
      "time/data storing (s)                                   0.0302521\n",
      "time/evaluation sampling (s)                           50.3712\n",
      "time/exploration sampling (s)                           2.23701\n",
      "time/logging (s)                                        0.271674\n",
      "time/saving (s)                                         0.0250915\n",
      "time/training (s)                                       4.37345\n",
      "time/epoch (s)                                         57.3087\n",
      "time/total (s)                                       1579.7\n",
      "Epoch                                                  28\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:22:03.247882 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 29 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  62000\n",
      "trainer/QF1 Loss                                        0.266816\n",
      "trainer/QF2 Loss                                        0.328823\n",
      "trainer/Policy Loss                                    -2.93171\n",
      "trainer/Q1 Predictions Mean                            10.5891\n",
      "trainer/Q1 Predictions Std                              1.26884\n",
      "trainer/Q1 Predictions Max                             12.8301\n",
      "trainer/Q1 Predictions Min                              2.89685\n",
      "trainer/Q2 Predictions Mean                            10.5154\n",
      "trainer/Q2 Predictions Std                              1.1653\n",
      "trainer/Q2 Predictions Max                             12.2278\n",
      "trainer/Q2 Predictions Min                              5.42511\n",
      "trainer/Q Targets Mean                                 10.5185\n",
      "trainer/Q Targets Std                                   1.29397\n",
      "trainer/Q Targets Max                                  12.6803\n",
      "trainer/Q Targets Min                                  -0.000990629\n",
      "trainer/Log Pis Mean                                    8.11682\n",
      "trainer/Log Pis Std                                     2.7097\n",
      "trainer/Log Pis Max                                    15.2826\n",
      "trainer/Log Pis Min                                    -2.56316\n",
      "trainer/Policy mu Mean                                  0.00988084\n",
      "trainer/Policy mu Std                                   0.139014\n",
      "trainer/Policy mu Max                                   1.14038\n",
      "trainer/Policy mu Min                                  -1.13807\n",
      "trainer/Policy log std Mean                            -2.441\n",
      "trainer/Policy log std Std                              0.211687\n",
      "trainer/Policy log std Max                             -1.71464\n",
      "trainer/Policy log std Min                             -3.72932\n",
      "trainer/Alpha                                           0.0153674\n",
      "trainer/Alpha Loss                                      0.48776\n",
      "exploration/num steps total                         31000\n",
      "exploration/num paths total                           103\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.967192\n",
      "exploration/Rewards Std                                 0.0858739\n",
      "exploration/Rewards Max                                 2.05807\n",
      "exploration/Rewards Min                                 0.725754\n",
      "exploration/Returns Mean                              967.192\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               967.192\n",
      "exploration/Returns Min                               967.192\n",
      "exploration/Actions Mean                                0.0202933\n",
      "exploration/Actions Std                                 0.107255\n",
      "exploration/Actions Max                                 0.482947\n",
      "exploration/Actions Min                                -0.346567\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           967.192\n",
      "exploration/env_infos/final/reward_forward Mean         0.0766915\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0766915\n",
      "exploration/env_infos/final/reward_forward Min          0.0766915\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0407492\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0407492\n",
      "exploration/env_infos/initial/reward_forward Min        0.0407492\n",
      "exploration/env_infos/reward_forward Mean               0.00921789\n",
      "exploration/env_infos/reward_forward Std                0.118054\n",
      "exploration/env_infos/reward_forward Max                0.695482\n",
      "exploration/env_infos/reward_forward Min               -0.487691\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0437797\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0437797\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0437797\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0506331\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0506331\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0506331\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0476615\n",
      "exploration/env_infos/reward_ctrl Std                   0.0224727\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00610356\n",
      "exploration/env_infos/reward_ctrl Min                  -0.274246\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0269576\n",
      "exploration/env_infos/final/torso_velocity Std          0.0353143\n",
      "exploration/env_infos/final/torso_velocity Max          0.0766915\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0018524\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0691505\n",
      "exploration/env_infos/initial/torso_velocity Std        0.217435\n",
      "exploration/env_infos/initial/torso_velocity Max        0.348515\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.181813\n",
      "exploration/env_infos/torso_velocity Mean               0.00457809\n",
      "exploration/env_infos/torso_velocity Std                0.160418\n",
      "exploration/env_infos/torso_velocity Max                1.26624\n",
      "exploration/env_infos/torso_velocity Min               -1.31787\n",
      "evaluation/num steps total                         750000\n",
      "evaluation/num paths total                            750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.970725\n",
      "evaluation/Rewards Std                                  0.0216158\n",
      "evaluation/Rewards Max                                  1.76614\n",
      "evaluation/Rewards Min                                  0.47831\n",
      "evaluation/Returns Mean                               970.725\n",
      "evaluation/Returns Std                                  7.68168\n",
      "evaluation/Returns Max                                984.685\n",
      "evaluation/Returns Min                                961.928\n",
      "evaluation/Actions Mean                                 0.0185964\n",
      "evaluation/Actions Std                                  0.084649\n",
      "evaluation/Actions Max                                  0.587239\n",
      "evaluation/Actions Min                                 -0.482435\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            970.725\n",
      "evaluation/env_infos/final/reward_forward Mean         -8.37794e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000433089\n",
      "evaluation/env_infos/final/reward_forward Max           0.000108904\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0022029\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.000929749\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121226\n",
      "evaluation/env_infos/initial/reward_forward Max         0.275148\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.321151\n",
      "evaluation/env_infos/reward_forward Mean               -0.0012189\n",
      "evaluation/env_infos/reward_forward Std                 0.0482093\n",
      "evaluation/env_infos/reward_forward Max                 1.39551\n",
      "evaluation/env_infos/reward_forward Min                -1.28659\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0286869\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00828366\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0147643\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0394659\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0304577\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00634651\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0181526\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0480112\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0300451\n",
      "evaluation/env_infos/reward_ctrl Std                    0.012446\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00645331\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.52169\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.51417e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000442428\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000362859\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00316268\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.142413\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242784\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.598887\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.321151\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0031144\n",
      "evaluation/env_infos/torso_velocity Std                 0.0633703\n",
      "evaluation/env_infos/torso_velocity Max                 1.39551\n",
      "evaluation/env_infos/torso_velocity Min                -1.64255\n",
      "time/data storing (s)                                   0.0307893\n",
      "time/evaluation sampling (s)                           47.0703\n",
      "time/exploration sampling (s)                           2.16606\n",
      "time/logging (s)                                        0.272424\n",
      "time/saving (s)                                         0.0250199\n",
      "time/training (s)                                       4.35842\n",
      "time/epoch (s)                                         53.923\n",
      "time/total (s)                                       1633.99\n",
      "Epoch                                                  29\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:22:55.545784 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 30 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  64000\n",
      "trainer/QF1 Loss                                        0.373559\n",
      "trainer/QF2 Loss                                        0.419781\n",
      "trainer/Policy Loss                                    -4.11334\n",
      "trainer/Q1 Predictions Mean                            11.0612\n",
      "trainer/Q1 Predictions Std                              1.14817\n",
      "trainer/Q1 Predictions Max                             12.7751\n",
      "trainer/Q1 Predictions Min                              6.52361\n",
      "trainer/Q2 Predictions Mean                            11.1182\n",
      "trainer/Q2 Predictions Std                              1.10355\n",
      "trainer/Q2 Predictions Max                             13.0472\n",
      "trainer/Q2 Predictions Min                              6.7161\n",
      "trainer/Q Targets Mean                                 11.0118\n",
      "trainer/Q Targets Std                                   1.19392\n",
      "trainer/Q Targets Max                                  13.0302\n",
      "trainer/Q Targets Min                                   1.79184\n",
      "trainer/Log Pis Mean                                    7.42817\n",
      "trainer/Log Pis Std                                     2.34986\n",
      "trainer/Log Pis Max                                    12.8678\n",
      "trainer/Log Pis Min                                    -0.308901\n",
      "trainer/Policy mu Mean                                 -0.00349199\n",
      "trainer/Policy mu Std                                   0.137452\n",
      "trainer/Policy mu Max                                   0.715599\n",
      "trainer/Policy mu Min                                  -1.86304\n",
      "trainer/Policy log std Mean                            -2.32913\n",
      "trainer/Policy log std Std                              0.200934\n",
      "trainer/Policy log std Max                             -1.00331\n",
      "trainer/Policy log std Min                             -3.01496\n",
      "trainer/Alpha                                           0.0149253\n",
      "trainer/Alpha Loss                                     -2.40421\n",
      "exploration/num steps total                         32000\n",
      "exploration/num paths total                           104\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.980709\n",
      "exploration/Rewards Std                                 0.0986834\n",
      "exploration/Rewards Max                                 1.80966\n",
      "exploration/Rewards Min                                 0.704641\n",
      "exploration/Returns Mean                              980.709\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               980.709\n",
      "exploration/Returns Min                               980.709\n",
      "exploration/Actions Mean                                0.00796138\n",
      "exploration/Actions Std                                 0.103987\n",
      "exploration/Actions Max                                 0.495445\n",
      "exploration/Actions Min                                -0.411167\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           980.709\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0217276\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0217276\n",
      "exploration/env_infos/final/reward_forward Min         -0.0217276\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.327471\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.327471\n",
      "exploration/env_infos/initial/reward_forward Min       -0.327471\n",
      "exploration/env_infos/reward_forward Mean               0.0181473\n",
      "exploration/env_infos/reward_forward Std                0.259975\n",
      "exploration/env_infos/reward_forward Max                1.15359\n",
      "exploration/env_infos/reward_forward Min               -1.11196\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0581055\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0581055\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0581055\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0662222\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0662222\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0662222\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0435069\n",
      "exploration/env_infos/reward_ctrl Std                   0.0241536\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00397063\n",
      "exploration/env_infos/reward_ctrl Min                  -0.295359\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00835179\n",
      "exploration/env_infos/final/torso_velocity Std          0.0414673\n",
      "exploration/env_infos/final/torso_velocity Max          0.0669889\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0217276\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0200997\n",
      "exploration/env_infos/initial/torso_velocity Std        0.295173\n",
      "exploration/env_infos/initial/torso_velocity Max        0.394101\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.327471\n",
      "exploration/env_infos/torso_velocity Mean               0.00605131\n",
      "exploration/env_infos/torso_velocity Std                0.21998\n",
      "exploration/env_infos/torso_velocity Max                1.15359\n",
      "exploration/env_infos/torso_velocity Min               -1.69314\n",
      "evaluation/num steps total                         775000\n",
      "evaluation/num paths total                            775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.980297\n",
      "evaluation/Rewards Std                                  0.0330499\n",
      "evaluation/Rewards Max                                  2.85889\n",
      "evaluation/Rewards Min                                  0.240765\n",
      "evaluation/Returns Mean                               980.297\n",
      "evaluation/Returns Std                                  5.03677\n",
      "evaluation/Returns Max                                988.205\n",
      "evaluation/Returns Min                                969.248\n",
      "evaluation/Actions Mean                                 0.00150183\n",
      "evaluation/Actions Std                                  0.0724114\n",
      "evaluation/Actions Max                                  0.644658\n",
      "evaluation/Actions Min                                 -0.624781\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            980.297\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.98143e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.12961e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.1415e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.86292e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0084947\n",
      "evaluation/env_infos/initial/reward_forward Std         0.106571\n",
      "evaluation/env_infos/initial/reward_forward Max         0.136206\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.226775\n",
      "evaluation/env_infos/reward_forward Mean               -0.00222311\n",
      "evaluation/env_infos/reward_forward Std                 0.0626041\n",
      "evaluation/env_infos/reward_forward Max                 1.25105\n",
      "evaluation/env_infos/reward_forward Min                -1.81589\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0204624\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0044936\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0117367\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0304277\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.026607\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0090827\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0125348\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0505808\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0209827\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0101955\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00711753\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.759235\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.20451e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.05286e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.67783e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.86292e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14513\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22282\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.68953\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.226775\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00322609\n",
      "evaluation/env_infos/torso_velocity Std                 0.0674423\n",
      "evaluation/env_infos/torso_velocity Max                 1.25105\n",
      "evaluation/env_infos/torso_velocity Min                -1.81589\n",
      "time/data storing (s)                                   0.0297773\n",
      "time/evaluation sampling (s)                           45.3859\n",
      "time/exploration sampling (s)                           1.98519\n",
      "time/logging (s)                                        0.271029\n",
      "time/saving (s)                                         0.0247653\n",
      "time/training (s)                                       4.21513\n",
      "time/epoch (s)                                         51.9118\n",
      "time/total (s)                                       1686.28\n",
      "Epoch                                                  30\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:23:48.499491 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 31 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  66000\n",
      "trainer/QF1 Loss                                        0.586071\n",
      "trainer/QF2 Loss                                        0.730694\n",
      "trainer/Policy Loss                                    -4.07638\n",
      "trainer/Q1 Predictions Mean                            11.1842\n",
      "trainer/Q1 Predictions Std                              1.26659\n",
      "trainer/Q1 Predictions Max                             13.0223\n",
      "trainer/Q1 Predictions Min                              3.29589\n",
      "trainer/Q2 Predictions Mean                            11.3025\n",
      "trainer/Q2 Predictions Std                              1.19504\n",
      "trainer/Q2 Predictions Max                             13.2075\n",
      "trainer/Q2 Predictions Min                              5.08814\n",
      "trainer/Q Targets Mean                                 11.2443\n",
      "trainer/Q Targets Std                                   1.56737\n",
      "trainer/Q Targets Max                                  13.0846\n",
      "trainer/Q Targets Min                                  -0.729072\n",
      "trainer/Log Pis Mean                                    7.54152\n",
      "trainer/Log Pis Std                                     2.43307\n",
      "trainer/Log Pis Max                                    12.7158\n",
      "trainer/Log Pis Min                                    -0.226569\n",
      "trainer/Policy mu Mean                                 -0.0055012\n",
      "trainer/Policy mu Std                                   0.129493\n",
      "trainer/Policy mu Max                                   0.803572\n",
      "trainer/Policy mu Min                                  -1.37251\n",
      "trainer/Policy log std Mean                            -2.32754\n",
      "trainer/Policy log std Std                              0.236242\n",
      "trainer/Policy log std Max                             -1.43835\n",
      "trainer/Policy log std Min                             -3.05375\n",
      "trainer/Alpha                                           0.0146716\n",
      "trainer/Alpha Loss                                     -1.93557\n",
      "exploration/num steps total                         33000\n",
      "exploration/num paths total                           105\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.921888\n",
      "exploration/Rewards Std                                 0.0494871\n",
      "exploration/Rewards Max                                 1.38139\n",
      "exploration/Rewards Min                                 0.777677\n",
      "exploration/Returns Mean                              921.888\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               921.888\n",
      "exploration/Returns Min                               921.888\n",
      "exploration/Actions Mean                               -0.0101769\n",
      "exploration/Actions Std                                 0.144842\n",
      "exploration/Actions Max                                 0.558337\n",
      "exploration/Actions Min                                -0.415806\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           921.888\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0170878\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0170878\n",
      "exploration/env_infos/final/reward_forward Min         -0.0170878\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.116146\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.116146\n",
      "exploration/env_infos/initial/reward_forward Min       -0.116146\n",
      "exploration/env_infos/reward_forward Mean               0.015725\n",
      "exploration/env_infos/reward_forward Std                0.131796\n",
      "exploration/env_infos/reward_forward Max                0.706431\n",
      "exploration/env_infos/reward_forward Min               -0.792525\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0514885\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0514885\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0514885\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0405596\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0405596\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0405596\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0843313\n",
      "exploration/env_infos/reward_ctrl Std                   0.0380022\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00844738\n",
      "exploration/env_infos/reward_ctrl Min                  -0.231339\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0110761\n",
      "exploration/env_infos/final/torso_velocity Std          0.00484224\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00523027\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0170878\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.159076\n",
      "exploration/env_infos/initial/torso_velocity Std        0.221956\n",
      "exploration/env_infos/initial/torso_velocity Max        0.427401\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.116146\n",
      "exploration/env_infos/torso_velocity Mean              -0.00360596\n",
      "exploration/env_infos/torso_velocity Std                0.159097\n",
      "exploration/env_infos/torso_velocity Max                0.892666\n",
      "exploration/env_infos/torso_velocity Min               -1.78201\n",
      "evaluation/num steps total                         800000\n",
      "evaluation/num paths total                            800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.965791\n",
      "evaluation/Rewards Std                                  0.0350637\n",
      "evaluation/Rewards Max                                  2.67117\n",
      "evaluation/Rewards Min                                  0.776649\n",
      "evaluation/Returns Mean                               965.791\n",
      "evaluation/Returns Std                                  5.29296\n",
      "evaluation/Returns Max                                970.989\n",
      "evaluation/Returns Min                                946.423\n",
      "evaluation/Actions Mean                                -0.00329043\n",
      "evaluation/Actions Std                                  0.094355\n",
      "evaluation/Actions Max                                  0.602215\n",
      "evaluation/Actions Min                                 -0.38691\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            965.791\n",
      "evaluation/env_infos/final/reward_forward Mean         -4.43007e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.28312e-05\n",
      "evaluation/env_infos/final/reward_forward Max           1.39194e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00011457\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0378327\n",
      "evaluation/env_infos/initial/reward_forward Std         0.117828\n",
      "evaluation/env_infos/initial/reward_forward Max         0.195427\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.233172\n",
      "evaluation/env_infos/reward_forward Mean               -0.000385901\n",
      "evaluation/env_infos/reward_forward Std                 0.057325\n",
      "evaluation/env_infos/reward_forward Max                 1.04195\n",
      "evaluation/env_infos/reward_forward Min                -1.71292\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0356558\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00549941\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.029666\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0536129\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.026477\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00683436\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00952023\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0424388\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0356547\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00672904\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00215751\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.277438\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.49722e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           3.45005e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000178926\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000162773\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.121562\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.27096\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.7448\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.316996\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00353254\n",
      "evaluation/env_infos/torso_velocity Std                 0.0650344\n",
      "evaluation/env_infos/torso_velocity Max                 1.57569\n",
      "evaluation/env_infos/torso_velocity Min                -1.85589\n",
      "time/data storing (s)                                   0.0296037\n",
      "time/evaluation sampling (s)                           45.5645\n",
      "time/exploration sampling (s)                           2.0159\n",
      "time/logging (s)                                        0.279388\n",
      "time/saving (s)                                         0.0301839\n",
      "time/training (s)                                       4.65994\n",
      "time/epoch (s)                                         52.5795\n",
      "time/total (s)                                       1739.24\n",
      "Epoch                                                  31\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:24:40.633954 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 32 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  68000\n",
      "trainer/QF1 Loss                                        0.329159\n",
      "trainer/QF2 Loss                                        0.358542\n",
      "trainer/Policy Loss                                    -3.73039\n",
      "trainer/Q1 Predictions Mean                            11.5215\n",
      "trainer/Q1 Predictions Std                              1.13201\n",
      "trainer/Q1 Predictions Max                             13.6716\n",
      "trainer/Q1 Predictions Min                              7.71295\n",
      "trainer/Q2 Predictions Mean                            11.5262\n",
      "trainer/Q2 Predictions Std                              1.15805\n",
      "trainer/Q2 Predictions Max                             13.6222\n",
      "trainer/Q2 Predictions Min                              7.41094\n",
      "trainer/Q Targets Mean                                 11.6458\n",
      "trainer/Q Targets Std                                   1.24154\n",
      "trainer/Q Targets Max                                  13.4742\n",
      "trainer/Q Targets Min                                   5.86763\n",
      "trainer/Log Pis Mean                                    8.16522\n",
      "trainer/Log Pis Std                                     2.32329\n",
      "trainer/Log Pis Max                                    14.9205\n",
      "trainer/Log Pis Min                                    -3.55226\n",
      "trainer/Policy mu Mean                                  0.0353272\n",
      "trainer/Policy mu Std                                   0.148467\n",
      "trainer/Policy mu Max                                   1.06231\n",
      "trainer/Policy mu Min                                  -1.13506\n",
      "trainer/Policy log std Mean                            -2.39627\n",
      "trainer/Policy log std Std                              0.205208\n",
      "trainer/Policy log std Max                             -1.64545\n",
      "trainer/Policy log std Min                             -3.19321\n",
      "trainer/Alpha                                           0.0143762\n",
      "trainer/Alpha Loss                                      0.700854\n",
      "exploration/num steps total                         34000\n",
      "exploration/num paths total                           106\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.940279\n",
      "exploration/Rewards Std                                 0.0581976\n",
      "exploration/Rewards Max                                 1.33455\n",
      "exploration/Rewards Min                                 0.469103\n",
      "exploration/Returns Mean                              940.279\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               940.279\n",
      "exploration/Returns Min                               940.279\n",
      "exploration/Actions Mean                                0.037867\n",
      "exploration/Actions Std                                 0.127831\n",
      "exploration/Actions Max                                 0.724595\n",
      "exploration/Actions Min                                -0.41953\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           940.279\n",
      "exploration/env_infos/final/reward_forward Mean         0.0144547\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0144547\n",
      "exploration/env_infos/final/reward_forward Min          0.0144547\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0851494\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0851494\n",
      "exploration/env_infos/initial/reward_forward Min        0.0851494\n",
      "exploration/env_infos/reward_forward Mean               0.00364579\n",
      "exploration/env_infos/reward_forward Std                0.153055\n",
      "exploration/env_infos/reward_forward Max                1.10608\n",
      "exploration/env_infos/reward_forward Min               -0.496027\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0493505\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0493505\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0493505\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0254111\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0254111\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0254111\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0710985\n",
      "exploration/env_infos/reward_ctrl Std                   0.0362691\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00956927\n",
      "exploration/env_infos/reward_ctrl Min                  -0.530897\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0169129\n",
      "exploration/env_infos/final/torso_velocity Std          0.00687826\n",
      "exploration/env_infos/final/torso_velocity Max          0.0262927\n",
      "exploration/env_infos/final/torso_velocity Min          0.0099913\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.22481\n",
      "exploration/env_infos/initial/torso_velocity Std        0.265786\n",
      "exploration/env_infos/initial/torso_velocity Max        0.596857\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00757466\n",
      "exploration/env_infos/torso_velocity Mean               0.00256105\n",
      "exploration/env_infos/torso_velocity Std                0.162553\n",
      "exploration/env_infos/torso_velocity Max                1.10608\n",
      "exploration/env_infos/torso_velocity Min               -1.29972\n",
      "evaluation/num steps total                         825000\n",
      "evaluation/num paths total                            825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.968145\n",
      "evaluation/Rewards Std                                  0.0284492\n",
      "evaluation/Rewards Max                                  2.29297\n",
      "evaluation/Rewards Min                                  0.539869\n",
      "evaluation/Returns Mean                               968.145\n",
      "evaluation/Returns Std                                 10.6617\n",
      "evaluation/Returns Max                                984.86\n",
      "evaluation/Returns Min                                943.098\n",
      "evaluation/Actions Mean                                 0.0354193\n",
      "evaluation/Actions Std                                  0.0832144\n",
      "evaluation/Actions Max                                  0.717801\n",
      "evaluation/Actions Min                                 -0.402069\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            968.145\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.12729e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.11242e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.09251e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.62241e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0421547\n",
      "evaluation/env_infos/initial/reward_forward Std         0.109503\n",
      "evaluation/env_infos/initial/reward_forward Max         0.247739\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.218442\n",
      "evaluation/env_infos/reward_forward Mean                0.00123648\n",
      "evaluation/env_infos/reward_forward Std                 0.0468846\n",
      "evaluation/env_infos/reward_forward Max                 1.12051\n",
      "evaluation/env_infos/reward_forward Min                -1.24926\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0320369\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0106863\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0175427\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0567246\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0203285\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00871444\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0105202\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0411005\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0327167\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0150719\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00812908\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.460131\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.00179e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.41287e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.18677e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.62241e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144417\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238411\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614804\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.298843\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00172499\n",
      "evaluation/env_infos/torso_velocity Std                 0.0615898\n",
      "evaluation/env_infos/torso_velocity Max                 1.73804\n",
      "evaluation/env_infos/torso_velocity Min                -1.70438\n",
      "time/data storing (s)                                   0.0306923\n",
      "time/evaluation sampling (s)                           45.2958\n",
      "time/exploration sampling (s)                           1.96265\n",
      "time/logging (s)                                        0.271421\n",
      "time/saving (s)                                         0.0262024\n",
      "time/training (s)                                       4.13324\n",
      "time/epoch (s)                                         51.72\n",
      "time/total (s)                                       1791.37\n",
      "Epoch                                                  32\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:25:32.169449 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 33 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  70000\n",
      "trainer/QF1 Loss                                        0.212766\n",
      "trainer/QF2 Loss                                        0.275536\n",
      "trainer/Policy Loss                                    -4.51982\n",
      "trainer/Q1 Predictions Mean                            12.03\n",
      "trainer/Q1 Predictions Std                              1.12535\n",
      "trainer/Q1 Predictions Max                             13.9839\n",
      "trainer/Q1 Predictions Min                              6.49373\n",
      "trainer/Q2 Predictions Mean                            11.9946\n",
      "trainer/Q2 Predictions Std                              1.18273\n",
      "trainer/Q2 Predictions Max                             13.7753\n",
      "trainer/Q2 Predictions Min                              5.72067\n",
      "trainer/Q Targets Mean                                 12.1473\n",
      "trainer/Q Targets Std                                   1.13528\n",
      "trainer/Q Targets Max                                  14.8879\n",
      "trainer/Q Targets Min                                   7.60094\n",
      "trainer/Log Pis Mean                                    7.8481\n",
      "trainer/Log Pis Std                                     2.14182\n",
      "trainer/Log Pis Max                                    13.0469\n",
      "trainer/Log Pis Min                                    -0.893311\n",
      "trainer/Policy mu Mean                                  0.028254\n",
      "trainer/Policy mu Std                                   0.132362\n",
      "trainer/Policy mu Max                                   0.93135\n",
      "trainer/Policy mu Min                                  -1.53102\n",
      "trainer/Policy log std Mean                            -2.36711\n",
      "trainer/Policy log std Std                              0.227196\n",
      "trainer/Policy log std Max                             -1.16943\n",
      "trainer/Policy log std Min                             -3.00798\n",
      "trainer/Alpha                                           0.0139537\n",
      "trainer/Alpha Loss                                     -0.648859\n",
      "exploration/num steps total                         35000\n",
      "exploration/num paths total                           107\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.967485\n",
      "exploration/Rewards Std                                 0.135279\n",
      "exploration/Rewards Max                                 2.17556\n",
      "exploration/Rewards Min                                 0.780996\n",
      "exploration/Returns Mean                              967.485\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               967.485\n",
      "exploration/Returns Min                               967.485\n",
      "exploration/Actions Mean                                0.0170892\n",
      "exploration/Actions Std                                 0.126782\n",
      "exploration/Actions Max                                 0.459988\n",
      "exploration/Actions Min                                -0.452648\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           967.485\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0484603\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0484603\n",
      "exploration/env_infos/final/reward_forward Min         -0.0484603\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0213036\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0213036\n",
      "exploration/env_infos/initial/reward_forward Min        0.0213036\n",
      "exploration/env_infos/reward_forward Mean              -0.0403926\n",
      "exploration/env_infos/reward_forward Std                0.206103\n",
      "exploration/env_infos/reward_forward Max                0.857412\n",
      "exploration/env_infos/reward_forward Min               -1.23676\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0599396\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0599396\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0599396\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0410841\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0410841\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0410841\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0654628\n",
      "exploration/env_infos/reward_ctrl Std                   0.0317736\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00599764\n",
      "exploration/env_infos/reward_ctrl Min                  -0.219004\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0299587\n",
      "exploration/env_infos/final/torso_velocity Std          0.021654\n",
      "exploration/env_infos/final/torso_velocity Max          0.000425268\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0484603\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.169184\n",
      "exploration/env_infos/initial/torso_velocity Std        0.232062\n",
      "exploration/env_infos/initial/torso_velocity Max        0.496853\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0106036\n",
      "exploration/env_infos/torso_velocity Mean              -0.0130533\n",
      "exploration/env_infos/torso_velocity Std                0.184201\n",
      "exploration/env_infos/torso_velocity Max                0.857412\n",
      "exploration/env_infos/torso_velocity Min               -1.60813\n",
      "evaluation/num steps total                         850000\n",
      "evaluation/num paths total                            850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.97191\n",
      "evaluation/Rewards Std                                  0.0269641\n",
      "evaluation/Rewards Max                                  2.42535\n",
      "evaluation/Rewards Min                                  0.771676\n",
      "evaluation/Returns Mean                               971.91\n",
      "evaluation/Returns Std                                  7.88458\n",
      "evaluation/Returns Max                                984.042\n",
      "evaluation/Returns Min                                946.725\n",
      "evaluation/Actions Mean                                 0.0324913\n",
      "evaluation/Actions Std                                  0.0789332\n",
      "evaluation/Actions Max                                  0.539832\n",
      "evaluation/Actions Min                                 -0.375179\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            971.91\n",
      "evaluation/env_infos/final/reward_forward Mean          1.96678e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.94305e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.36408e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.872e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0401778\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132953\n",
      "evaluation/env_infos/initial/reward_forward Max         0.26215\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.28887\n",
      "evaluation/env_infos/reward_forward Mean               -0.00286991\n",
      "evaluation/env_infos/reward_forward Std                 0.0595451\n",
      "evaluation/env_infos/reward_forward Max                 1.26543\n",
      "evaluation/env_infos/reward_forward Min                -1.45813\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0286745\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0075066\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0177815\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0526419\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.030113\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00626164\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0171526\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.041465\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0291446\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00958632\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00480593\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.302926\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.61207e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.16806e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.36408e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.872e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.121335\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.259374\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.632794\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.28887\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00356469\n",
      "evaluation/env_infos/torso_velocity Std                 0.0628766\n",
      "evaluation/env_infos/torso_velocity Max                 1.32376\n",
      "evaluation/env_infos/torso_velocity Min                -1.677\n",
      "time/data storing (s)                                   0.0291701\n",
      "time/evaluation sampling (s)                           44.7274\n",
      "time/exploration sampling (s)                           1.99912\n",
      "time/logging (s)                                        0.272928\n",
      "time/saving (s)                                         0.0264056\n",
      "time/training (s)                                       4.08508\n",
      "time/epoch (s)                                         51.1401\n",
      "time/total (s)                                       1842.91\n",
      "Epoch                                                  33\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:26:24.309975 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 34 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  72000\n",
      "trainer/QF1 Loss                                        0.322054\n",
      "trainer/QF2 Loss                                        0.254445\n",
      "trainer/Policy Loss                                    -4.36941\n",
      "trainer/Q1 Predictions Mean                            12.263\n",
      "trainer/Q1 Predictions Std                              1.18803\n",
      "trainer/Q1 Predictions Max                             14.0896\n",
      "trainer/Q1 Predictions Min                              7.97018\n",
      "trainer/Q2 Predictions Mean                            12.1505\n",
      "trainer/Q2 Predictions Std                              1.15871\n",
      "trainer/Q2 Predictions Max                             13.9141\n",
      "trainer/Q2 Predictions Min                              7.70038\n",
      "trainer/Q Targets Mean                                 12.3135\n",
      "trainer/Q Targets Std                                   1.19036\n",
      "trainer/Q Targets Max                                  14.6404\n",
      "trainer/Q Targets Min                                   8.2571\n",
      "trainer/Log Pis Mean                                    8.23534\n",
      "trainer/Log Pis Std                                     2.51202\n",
      "trainer/Log Pis Max                                    18.6405\n",
      "trainer/Log Pis Min                                     1.62353\n",
      "trainer/Policy mu Mean                                  0.00164821\n",
      "trainer/Policy mu Std                                   0.156483\n",
      "trainer/Policy mu Max                                   1.91089\n",
      "trainer/Policy mu Min                                  -0.669991\n",
      "trainer/Policy log std Mean                            -2.3947\n",
      "trainer/Policy log std Std                              0.240092\n",
      "trainer/Policy log std Max                             -1.54903\n",
      "trainer/Policy log std Min                             -3.73802\n",
      "trainer/Alpha                                           0.0136817\n",
      "trainer/Alpha Loss                                      1.00999\n",
      "exploration/num steps total                         36000\n",
      "exploration/num paths total                           108\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.926073\n",
      "exploration/Rewards Std                                 0.0335889\n",
      "exploration/Rewards Max                                 1.28665\n",
      "exploration/Rewards Min                                 0.804945\n",
      "exploration/Returns Mean                              926.073\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               926.073\n",
      "exploration/Returns Min                               926.073\n",
      "exploration/Actions Mean                                0.0302615\n",
      "exploration/Actions Std                                 0.135412\n",
      "exploration/Actions Max                                 0.384505\n",
      "exploration/Actions Min                                -0.420725\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           926.073\n",
      "exploration/env_infos/final/reward_forward Mean         0.0742907\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0742907\n",
      "exploration/env_infos/final/reward_forward Min          0.0742907\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.172276\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.172276\n",
      "exploration/env_infos/initial/reward_forward Min       -0.172276\n",
      "exploration/env_infos/reward_forward Mean              -0.00088885\n",
      "exploration/env_infos/reward_forward Std                0.117051\n",
      "exploration/env_infos/reward_forward Max                0.661549\n",
      "exploration/env_infos/reward_forward Min               -0.476437\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.110991\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.110991\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.110991\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0828547\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0828547\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0828547\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0770091\n",
      "exploration/env_infos/reward_ctrl Std                   0.0273855\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0147652\n",
      "exploration/env_infos/reward_ctrl Min                  -0.195055\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0835754\n",
      "exploration/env_infos/final/torso_velocity Std          0.0372511\n",
      "exploration/env_infos/final/torso_velocity Max          0.133127\n",
      "exploration/env_infos/final/torso_velocity Min          0.0433088\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.19223\n",
      "exploration/env_infos/initial/torso_velocity Std        0.423403\n",
      "exploration/env_infos/initial/torso_velocity Max        0.785891\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.172276\n",
      "exploration/env_infos/torso_velocity Mean              -0.00879336\n",
      "exploration/env_infos/torso_velocity Std                0.120466\n",
      "exploration/env_infos/torso_velocity Max                0.785891\n",
      "exploration/env_infos/torso_velocity Min               -1.98378\n",
      "evaluation/num steps total                         875000\n",
      "evaluation/num paths total                            875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.954194\n",
      "evaluation/Rewards Std                                  0.0305448\n",
      "evaluation/Rewards Max                                  2.73915\n",
      "evaluation/Rewards Min                                  0.697085\n",
      "evaluation/Returns Mean                               954.194\n",
      "evaluation/Returns Std                                  8.17418\n",
      "evaluation/Returns Max                                971.538\n",
      "evaluation/Returns Min                                942.327\n",
      "evaluation/Actions Mean                                -0.000525568\n",
      "evaluation/Actions Std                                  0.108311\n",
      "evaluation/Actions Max                                  0.549732\n",
      "evaluation/Actions Min                                 -0.569763\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            954.194\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.42414e-05\n",
      "evaluation/env_infos/final/reward_forward Std           5.10849e-05\n",
      "evaluation/env_infos/final/reward_forward Max           6.88293e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000204751\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0560252\n",
      "evaluation/env_infos/initial/reward_forward Std         0.101105\n",
      "evaluation/env_infos/initial/reward_forward Max         0.147911\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275404\n",
      "evaluation/env_infos/reward_forward Mean               -0.00237777\n",
      "evaluation/env_infos/reward_forward Std                 0.050716\n",
      "evaluation/env_infos/reward_forward Max                 1.40964\n",
      "evaluation/env_infos/reward_forward Min                -1.34003\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0461237\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00813761\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0299055\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0581598\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0279118\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00502379\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0184566\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0366283\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0469263\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0102665\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.017123\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.302915\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.4814e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           5.64959e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000152945\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000204751\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.115403\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.249902\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.626317\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.284031\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00357935\n",
      "evaluation/env_infos/torso_velocity Std                 0.0595694\n",
      "evaluation/env_infos/torso_velocity Max                 1.40964\n",
      "evaluation/env_infos/torso_velocity Min                -1.6877\n",
      "time/data storing (s)                                   0.0335182\n",
      "time/evaluation sampling (s)                           45.368\n",
      "time/exploration sampling (s)                           1.99564\n",
      "time/logging (s)                                        0.286851\n",
      "time/saving (s)                                         0.0270481\n",
      "time/training (s)                                       4.04429\n",
      "time/epoch (s)                                         51.7554\n",
      "time/total (s)                                       1895.06\n",
      "Epoch                                                  34\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:27:15.947814 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 35 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  74000\n",
      "trainer/QF1 Loss                                        0.158004\n",
      "trainer/QF2 Loss                                        0.167808\n",
      "trainer/Policy Loss                                    -5.46531\n",
      "trainer/Q1 Predictions Mean                            13.0304\n",
      "trainer/Q1 Predictions Std                              1.08387\n",
      "trainer/Q1 Predictions Max                             15.1614\n",
      "trainer/Q1 Predictions Min                              9.85219\n",
      "trainer/Q2 Predictions Mean                            13.0065\n",
      "trainer/Q2 Predictions Std                              1.07389\n",
      "trainer/Q2 Predictions Max                             15.2272\n",
      "trainer/Q2 Predictions Min                              7.85868\n",
      "trainer/Q Targets Mean                                 12.8959\n",
      "trainer/Q Targets Std                                   1.06926\n",
      "trainer/Q Targets Max                                  15.3133\n",
      "trainer/Q Targets Min                                   7.97017\n",
      "trainer/Log Pis Mean                                    7.84671\n",
      "trainer/Log Pis Std                                     2.12738\n",
      "trainer/Log Pis Max                                    14.5634\n",
      "trainer/Log Pis Min                                    -0.84727\n",
      "trainer/Policy mu Mean                                  0.00330565\n",
      "trainer/Policy mu Std                                   0.139112\n",
      "trainer/Policy mu Max                                   1.07434\n",
      "trainer/Policy mu Min                                  -1.2513\n",
      "trainer/Policy log std Mean                            -2.36504\n",
      "trainer/Policy log std Std                              0.20855\n",
      "trainer/Policy log std Max                             -1.67542\n",
      "trainer/Policy log std Min                             -3.51876\n",
      "trainer/Alpha                                           0.0133023\n",
      "trainer/Alpha Loss                                     -0.662141\n",
      "exploration/num steps total                         37000\n",
      "exploration/num paths total                           109\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.935308\n",
      "exploration/Rewards Std                                 0.0711262\n",
      "exploration/Rewards Max                                 1.71359\n",
      "exploration/Rewards Min                                 0.818041\n",
      "exploration/Returns Mean                              935.308\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.308\n",
      "exploration/Returns Min                               935.308\n",
      "exploration/Actions Mean                                0.0266749\n",
      "exploration/Actions Std                                 0.136757\n",
      "exploration/Actions Max                                 0.475236\n",
      "exploration/Actions Min                                -0.415419\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.308\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0140001\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0140001\n",
      "exploration/env_infos/final/reward_forward Min         -0.0140001\n",
      "exploration/env_infos/initial/reward_forward Mean       0.156335\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.156335\n",
      "exploration/env_infos/initial/reward_forward Min        0.156335\n",
      "exploration/env_infos/reward_forward Mean              -0.0114584\n",
      "exploration/env_infos/reward_forward Std                0.137272\n",
      "exploration/env_infos/reward_forward Max                0.478683\n",
      "exploration/env_infos/reward_forward Min               -0.894986\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0714024\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0714024\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0714024\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0782116\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0782116\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0782116\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0776556\n",
      "exploration/env_infos/reward_ctrl Std                   0.0286913\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00956785\n",
      "exploration/env_infos/reward_ctrl Min                  -0.181959\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0881492\n",
      "exploration/env_infos/final/torso_velocity Std          0.0794515\n",
      "exploration/env_infos/final/torso_velocity Max          0.179757\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0140001\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.115239\n",
      "exploration/env_infos/initial/torso_velocity Std        0.239203\n",
      "exploration/env_infos/initial/torso_velocity Max        0.385484\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.196101\n",
      "exploration/env_infos/torso_velocity Mean              -0.00371808\n",
      "exploration/env_infos/torso_velocity Std                0.146527\n",
      "exploration/env_infos/torso_velocity Max                0.639635\n",
      "exploration/env_infos/torso_velocity Min               -1.69624\n",
      "evaluation/num steps total                         900000\n",
      "evaluation/num paths total                            900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.949725\n",
      "evaluation/Rewards Std                                  0.0295287\n",
      "evaluation/Rewards Max                                  2.68613\n",
      "evaluation/Rewards Min                                  0.578059\n",
      "evaluation/Returns Mean                               949.725\n",
      "evaluation/Returns Std                                 11.2675\n",
      "evaluation/Returns Max                                971.146\n",
      "evaluation/Returns Min                                933.638\n",
      "evaluation/Actions Mean                                 0.0296738\n",
      "evaluation/Actions Std                                  0.109063\n",
      "evaluation/Actions Max                                  0.575414\n",
      "evaluation/Actions Min                                 -0.410142\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            949.725\n",
      "evaluation/env_infos/final/reward_forward Mean          1.08348e-06\n",
      "evaluation/env_infos/final/reward_forward Std           3.78618e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.92689e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -3.51246e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0158651\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124628\n",
      "evaluation/env_infos/initial/reward_forward Max         0.185531\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.223834\n",
      "evaluation/env_infos/reward_forward Mean               -0.00477268\n",
      "evaluation/env_infos/reward_forward Std                 0.0643625\n",
      "evaluation/env_infos/reward_forward Max                 0.603578\n",
      "evaluation/env_infos/reward_forward Min                -1.77288\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0508447\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0115206\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0296763\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0667459\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0337707\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00853017\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0210801\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0537034\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0511012\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0130333\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0210801\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.421941\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.21725e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.57177e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.31286e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.06768e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.12528\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245933\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.627621\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.293022\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00394471\n",
      "evaluation/env_infos/torso_velocity Std                 0.06244\n",
      "evaluation/env_infos/torso_velocity Max                 0.842465\n",
      "evaluation/env_infos/torso_velocity Min                -2.01061\n",
      "time/data storing (s)                                   0.0305913\n",
      "time/evaluation sampling (s)                           44.7165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/exploration sampling (s)                           2.00534\n",
      "time/logging (s)                                        0.27831\n",
      "time/saving (s)                                         0.0253395\n",
      "time/training (s)                                       4.14534\n",
      "time/epoch (s)                                         51.2014\n",
      "time/total (s)                                       1946.69\n",
      "Epoch                                                  35\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 12:28:07.772068 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 36 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  76000\n",
      "trainer/QF1 Loss                                        0.331375\n",
      "trainer/QF2 Loss                                        0.265873\n",
      "trainer/Policy Loss                                    -4.67834\n",
      "trainer/Q1 Predictions Mean                            12.9904\n",
      "trainer/Q1 Predictions Std                              1.55049\n",
      "trainer/Q1 Predictions Max                             14.8948\n",
      "trainer/Q1 Predictions Min                             -1.97824\n",
      "trainer/Q2 Predictions Mean                            12.9524\n",
      "trainer/Q2 Predictions Std                              1.46337\n",
      "trainer/Q2 Predictions Max                             14.729\n",
      "trainer/Q2 Predictions Min                              0.204397\n",
      "trainer/Q Targets Mean                                 13.0436\n",
      "trainer/Q Targets Std                                   1.47427\n",
      "trainer/Q Targets Max                                  15.4673\n",
      "trainer/Q Targets Min                                   2.07395\n",
      "trainer/Log Pis Mean                                    8.66556\n",
      "trainer/Log Pis Std                                     2.87212\n",
      "trainer/Log Pis Max                                    23.3746\n",
      "trainer/Log Pis Min                                    -1.52569\n",
      "trainer/Policy mu Mean                                 -0.0248742\n",
      "trainer/Policy mu Std                                   0.158737\n",
      "trainer/Policy mu Max                                   1.94403\n",
      "trainer/Policy mu Min                                  -1.93228\n",
      "trainer/Policy log std Mean                            -2.46184\n",
      "trainer/Policy log std Std                              0.254951\n",
      "trainer/Policy log std Max                             -1.68834\n",
      "trainer/Policy log std Min                             -4.28036\n",
      "trainer/Alpha                                           0.0129178\n",
      "trainer/Alpha Loss                                      2.89446\n",
      "exploration/num steps total                         38000\n",
      "exploration/num paths total                           110\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.941384\n",
      "exploration/Rewards Std                                 0.028771\n",
      "exploration/Rewards Max                                 1.37091\n",
      "exploration/Rewards Min                                 0.852626\n",
      "exploration/Returns Mean                              941.384\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               941.384\n",
      "exploration/Returns Min                               941.384\n",
      "exploration/Actions Mean                               -0.0133264\n",
      "exploration/Actions Std                                 0.122707\n",
      "exploration/Actions Max                                 0.398838\n",
      "exploration/Actions Min                                -0.393803\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           941.384\n",
      "exploration/env_infos/final/reward_forward Mean         0.00124352\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00124352\n",
      "exploration/env_infos/final/reward_forward Min          0.00124352\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0205405\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0205405\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0205405\n",
      "exploration/env_infos/reward_forward Mean              -0.00905844\n",
      "exploration/env_infos/reward_forward Std                0.0818564\n",
      "exploration/env_infos/reward_forward Max                0.794325\n",
      "exploration/env_infos/reward_forward Min               -0.792794\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0919122\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0919122\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0919122\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0474969\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0474969\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0474969\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.060938\n",
      "exploration/env_infos/reward_ctrl Std                   0.0233731\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00676729\n",
      "exploration/env_infos/reward_ctrl Min                  -0.147374\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0150289\n",
      "exploration/env_infos/final/torso_velocity Std          0.0166516\n",
      "exploration/env_infos/final/torso_velocity Max          0.00124352\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0379068\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.109754\n",
      "exploration/env_infos/initial/torso_velocity Std        0.177681\n",
      "exploration/env_infos/initial/torso_velocity Max        0.360975\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0205405\n",
      "exploration/env_infos/torso_velocity Mean              -2.85362e-07\n",
      "exploration/env_infos/torso_velocity Std                0.0882701\n",
      "exploration/env_infos/torso_velocity Max                0.794325\n",
      "exploration/env_infos/torso_velocity Min               -0.878637\n",
      "evaluation/num steps total                         925000\n",
      "evaluation/num paths total                            925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966769\n",
      "evaluation/Rewards Std                                  0.0146077\n",
      "evaluation/Rewards Max                                  1.91531\n",
      "evaluation/Rewards Min                                  0.844721\n",
      "evaluation/Returns Mean                               966.769\n",
      "evaluation/Returns Std                                  4.79138\n",
      "evaluation/Returns Max                                974.924\n",
      "evaluation/Returns Min                                958.932\n",
      "evaluation/Actions Mean                                -0.0105799\n",
      "evaluation/Actions Std                                  0.0913516\n",
      "evaluation/Actions Max                                  0.332857\n",
      "evaluation/Actions Min                                 -0.370255\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.769\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.03625e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.70167e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.06617e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.82065e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0564384\n",
      "evaluation/env_infos/initial/reward_forward Std         0.13947\n",
      "evaluation/env_infos/initial/reward_forward Max         0.185214\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.337071\n",
      "evaluation/env_infos/reward_forward Mean               -0.00257904\n",
      "evaluation/env_infos/reward_forward Std                 0.0490753\n",
      "evaluation/env_infos/reward_forward Max                 0.799417\n",
      "evaluation/env_infos/reward_forward Min                -1.17744\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0336606\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00490333\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0257978\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0414758\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0379271\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00825853\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0255509\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0610542\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0338282\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00562602\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00909732\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.155279\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.39146e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.52355e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.27853e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.82065e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.120857\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.27363\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.766037\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.337071\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00300395\n",
      "evaluation/env_infos/torso_velocity Std                 0.0565\n",
      "evaluation/env_infos/torso_velocity Max                 1.56656\n",
      "evaluation/env_infos/torso_velocity Min                -1.77638\n",
      "time/data storing (s)                                   0.0297169\n",
      "time/evaluation sampling (s)                           44.9677\n",
      "time/exploration sampling (s)                           1.94871\n",
      "time/logging (s)                                        0.272836\n",
      "time/saving (s)                                         0.0249982\n",
      "time/training (s)                                       4.15488\n",
      "time/epoch (s)                                         51.3988\n",
      "time/total (s)                                       1998.51\n",
      "Epoch                                                  36\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:28:59.304090 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 37 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  78000\n",
      "trainer/QF1 Loss                                        0.234566\n",
      "trainer/QF2 Loss                                        0.197544\n",
      "trainer/Policy Loss                                    -6.48488\n",
      "trainer/Q1 Predictions Mean                            13.5409\n",
      "trainer/Q1 Predictions Std                              1.25549\n",
      "trainer/Q1 Predictions Max                             15.326\n",
      "trainer/Q1 Predictions Min                              4.4504\n",
      "trainer/Q2 Predictions Mean                            13.5716\n",
      "trainer/Q2 Predictions Std                              1.27941\n",
      "trainer/Q2 Predictions Max                             15.4808\n",
      "trainer/Q2 Predictions Min                              4.49202\n",
      "trainer/Q Targets Mean                                 13.4731\n",
      "trainer/Q Targets Std                                   1.27903\n",
      "trainer/Q Targets Max                                  15.7595\n",
      "trainer/Q Targets Min                                   3.58582\n",
      "trainer/Log Pis Mean                                    7.36236\n",
      "trainer/Log Pis Std                                     2.45078\n",
      "trainer/Log Pis Max                                    14.198\n",
      "trainer/Log Pis Min                                    -4.36181\n",
      "trainer/Policy mu Mean                                  0.0261601\n",
      "trainer/Policy mu Std                                   0.140339\n",
      "trainer/Policy mu Max                                   1.06212\n",
      "trainer/Policy mu Min                                  -1.49653\n",
      "trainer/Policy log std Mean                            -2.32118\n",
      "trainer/Policy log std Std                              0.216049\n",
      "trainer/Policy log std Max                             -1.1613\n",
      "trainer/Policy log std Min                             -2.99575\n",
      "trainer/Alpha                                           0.012749\n",
      "trainer/Alpha Loss                                     -2.78138\n",
      "exploration/num steps total                         39000\n",
      "exploration/num paths total                           111\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.969046\n",
      "exploration/Rewards Std                                 0.0868756\n",
      "exploration/Rewards Max                                 1.58865\n",
      "exploration/Rewards Min                                 0.825168\n",
      "exploration/Returns Mean                              969.046\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               969.046\n",
      "exploration/Returns Min                               969.046\n",
      "exploration/Actions Mean                                0.0208938\n",
      "exploration/Actions Std                                 0.130015\n",
      "exploration/Actions Max                                 0.48035\n",
      "exploration/Actions Min                                -0.393006\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           969.046\n",
      "exploration/env_infos/final/reward_forward Mean         0.0108255\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0108255\n",
      "exploration/env_infos/final/reward_forward Min          0.0108255\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.107926\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.107926\n",
      "exploration/env_infos/initial/reward_forward Min       -0.107926\n",
      "exploration/env_infos/reward_forward Mean              -0.0147541\n",
      "exploration/env_infos/reward_forward Std                0.123021\n",
      "exploration/env_infos/reward_forward Max                0.34751\n",
      "exploration/env_infos/reward_forward Min               -0.650144\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0380848\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0380848\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0380848\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0625119\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0625119\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0625119\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0693616\n",
      "exploration/env_infos/reward_ctrl Std                   0.0293142\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00456218\n",
      "exploration/env_infos/reward_ctrl Min                  -0.240594\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00734486\n",
      "exploration/env_infos/final/torso_velocity Std          0.0025797\n",
      "exploration/env_infos/final/torso_velocity Max          0.0108255\n",
      "exploration/env_infos/final/torso_velocity Min          0.00465785\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.118366\n",
      "exploration/env_infos/initial/torso_velocity Std        0.227586\n",
      "exploration/env_infos/initial/torso_velocity Max        0.429722\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.107926\n",
      "exploration/env_infos/torso_velocity Mean              -0.0104528\n",
      "exploration/env_infos/torso_velocity Std                0.11718\n",
      "exploration/env_infos/torso_velocity Max                1.22665\n",
      "exploration/env_infos/torso_velocity Min               -1.8089\n",
      "evaluation/num steps total                         950000\n",
      "evaluation/num paths total                            950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.939588\n",
      "evaluation/Rewards Std                                  0.0278335\n",
      "evaluation/Rewards Max                                  2.42791\n",
      "evaluation/Rewards Min                                  0.531283\n",
      "evaluation/Returns Mean                               939.588\n",
      "evaluation/Returns Std                                 12.0432\n",
      "evaluation/Returns Max                                965.212\n",
      "evaluation/Returns Min                                918.791\n",
      "evaluation/Actions Mean                                 0.0424542\n",
      "evaluation/Actions Std                                  0.116067\n",
      "evaluation/Actions Max                                  0.652905\n",
      "evaluation/Actions Min                                 -0.539963\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            939.588\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.19587e-06\n",
      "evaluation/env_infos/final/reward_forward Std           6.92723e-06\n",
      "evaluation/env_infos/final/reward_forward Max           7.14832e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.51017e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0379041\n",
      "evaluation/env_infos/initial/reward_forward Std         0.12786\n",
      "evaluation/env_infos/initial/reward_forward Max         0.140662\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.391824\n",
      "evaluation/env_infos/reward_forward Mean               -0.00532654\n",
      "evaluation/env_infos/reward_forward Std                 0.0625574\n",
      "evaluation/env_infos/reward_forward Max                 0.608379\n",
      "evaluation/env_infos/reward_forward Min                -1.84732\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0606851\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0122562\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0368235\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.086392\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0555229\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.01906\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0303189\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0964214\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0610958\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0138593\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00900252\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.468717\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.79796e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           7.85754e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.31455e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.73874e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.119784\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.222723\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614807\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.391824\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00364504\n",
      "evaluation/env_infos/torso_velocity Std                 0.0605373\n",
      "evaluation/env_infos/torso_velocity Max                 1.53131\n",
      "evaluation/env_infos/torso_velocity Min                -1.84732\n",
      "time/data storing (s)                                   0.0291202\n",
      "time/evaluation sampling (s)                           44.692\n",
      "time/exploration sampling (s)                           1.9931\n",
      "time/logging (s)                                        0.287584\n",
      "time/saving (s)                                         0.0264592\n",
      "time/training (s)                                       4.11009\n",
      "time/epoch (s)                                         51.1383\n",
      "time/total (s)                                       2050.05\n",
      "Epoch                                                  37\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:29:52.784552 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 38 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  80000\n",
      "trainer/QF1 Loss                                        0.236741\n",
      "trainer/QF2 Loss                                        0.369749\n",
      "trainer/Policy Loss                                    -6.28588\n",
      "trainer/Q1 Predictions Mean                            13.965\n",
      "trainer/Q1 Predictions Std                              1.67874\n",
      "trainer/Q1 Predictions Max                             16.101\n",
      "trainer/Q1 Predictions Min                             -0.478776\n",
      "trainer/Q2 Predictions Mean                            13.958\n",
      "trainer/Q2 Predictions Std                              1.55962\n",
      "trainer/Q2 Predictions Max                             16.1502\n",
      "trainer/Q2 Predictions Min                              4.88878\n",
      "trainer/Q Targets Mean                                 13.7859\n",
      "trainer/Q Targets Std                                   1.58779\n",
      "trainer/Q Targets Max                                  15.9605\n",
      "trainer/Q Targets Min                                  -0.366532\n",
      "trainer/Log Pis Mean                                    7.96792\n",
      "trainer/Log Pis Std                                     2.85364\n",
      "trainer/Log Pis Max                                    18.1721\n",
      "trainer/Log Pis Min                                    -1.95304\n",
      "trainer/Policy mu Mean                                  0.0190268\n",
      "trainer/Policy mu Std                                   0.151528\n",
      "trainer/Policy mu Max                                   1.70924\n",
      "trainer/Policy mu Min                                  -1.37652\n",
      "trainer/Policy log std Mean                            -2.40757\n",
      "trainer/Policy log std Std                              0.275472\n",
      "trainer/Policy log std Max                             -0.996957\n",
      "trainer/Policy log std Min                             -3.77071\n",
      "trainer/Alpha                                           0.0122335\n",
      "trainer/Alpha Loss                                     -0.141228\n",
      "exploration/num steps total                         40000\n",
      "exploration/num paths total                           112\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.963244\n",
      "exploration/Rewards Std                                 0.113889\n",
      "exploration/Rewards Max                                 2.16215\n",
      "exploration/Rewards Min                                 0.780714\n",
      "exploration/Returns Mean                              963.244\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               963.244\n",
      "exploration/Returns Min                               963.244\n",
      "exploration/Actions Mean                                0.0157258\n",
      "exploration/Actions Std                                 0.123874\n",
      "exploration/Actions Max                                 0.423878\n",
      "exploration/Actions Min                                -0.538331\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           963.244\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00687824\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00687824\n",
      "exploration/env_infos/final/reward_forward Min         -0.00687824\n",
      "exploration/env_infos/initial/reward_forward Mean       0.259564\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.259564\n",
      "exploration/env_infos/initial/reward_forward Min        0.259564\n",
      "exploration/env_infos/reward_forward Mean               0.02455\n",
      "exploration/env_infos/reward_forward Std                0.200366\n",
      "exploration/env_infos/reward_forward Max                1.47836\n",
      "exploration/env_infos/reward_forward Min               -0.907043\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.062113\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.062113\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.062113\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0327577\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0327577\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0327577\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0623678\n",
      "exploration/env_infos/reward_ctrl Std                   0.0312307\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00552704\n",
      "exploration/env_infos/reward_ctrl Min                  -0.219286\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0242675\n",
      "exploration/env_infos/final/torso_velocity Std          0.0227685\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00687824\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0564317\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.190052\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254083\n",
      "exploration/env_infos/initial/torso_velocity Max        0.460605\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.150013\n",
      "exploration/env_infos/torso_velocity Mean               0.0108226\n",
      "exploration/env_infos/torso_velocity Std                0.19646\n",
      "exploration/env_infos/torso_velocity Max                1.47836\n",
      "exploration/env_infos/torso_velocity Min               -0.995614\n",
      "evaluation/num steps total                         975000\n",
      "evaluation/num paths total                            975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.970922\n",
      "evaluation/Rewards Std                                  0.0229491\n",
      "evaluation/Rewards Max                                  2.4783\n",
      "evaluation/Rewards Min                                  0.54763\n",
      "evaluation/Returns Mean                               970.922\n",
      "evaluation/Returns Std                                  5.98431\n",
      "evaluation/Returns Max                                980.715\n",
      "evaluation/Returns Min                                960.967\n",
      "evaluation/Actions Mean                                 0.0095062\n",
      "evaluation/Actions Std                                  0.085773\n",
      "evaluation/Actions Max                                  0.514559\n",
      "evaluation/Actions Min                                 -0.636418\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            970.922\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.18289e-07\n",
      "evaluation/env_infos/final/reward_forward Std           1.57187e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.82087e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -5.07651e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0226411\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124962\n",
      "evaluation/env_infos/initial/reward_forward Max         0.203237\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.228952\n",
      "evaluation/env_infos/reward_forward Mean                0.00289581\n",
      "evaluation/env_infos/reward_forward Std                 0.0622208\n",
      "evaluation/env_infos/reward_forward Max                 0.991121\n",
      "evaluation/env_infos/reward_forward Min                -0.925692\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.029006\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00557438\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0211817\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0385783\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0323844\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00586235\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0212215\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0481764\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0297895\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0109158\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0134871\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.45237\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.46449e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           5.69601e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           5.82087e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000443997\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14064\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235776\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.613447\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.228952\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00171519\n",
      "evaluation/env_infos/torso_velocity Std                 0.0747647\n",
      "evaluation/env_infos/torso_velocity Max                 1.95914\n",
      "evaluation/env_infos/torso_velocity Min                -2.11743\n",
      "time/data storing (s)                                   0.0364149\n",
      "time/evaluation sampling (s)                           45.6646\n",
      "time/exploration sampling (s)                           2.46248\n",
      "time/logging (s)                                        0.27197\n",
      "time/saving (s)                                         0.0435607\n",
      "time/training (s)                                       4.55162\n",
      "time/epoch (s)                                         53.0307\n",
      "time/total (s)                                       2103.52\n",
      "Epoch                                                  38\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:30:45.234419 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 39 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 82000\n",
      "trainer/QF1 Loss                                       0.583798\n",
      "trainer/QF2 Loss                                       0.538171\n",
      "trainer/Policy Loss                                   -6.16589\n",
      "trainer/Q1 Predictions Mean                           14.0506\n",
      "trainer/Q1 Predictions Std                             1.37552\n",
      "trainer/Q1 Predictions Max                            16.5966\n",
      "trainer/Q1 Predictions Min                             7.58125\n",
      "trainer/Q2 Predictions Mean                           14.1562\n",
      "trainer/Q2 Predictions Std                             1.39834\n",
      "trainer/Q2 Predictions Max                            16.4873\n",
      "trainer/Q2 Predictions Min                             8.22986\n",
      "trainer/Q Targets Mean                                14.0829\n",
      "trainer/Q Targets Std                                  1.64386\n",
      "trainer/Q Targets Max                                 16.0447\n",
      "trainer/Q Targets Min                                 -0.231836\n",
      "trainer/Log Pis Mean                                   8.2176\n",
      "trainer/Log Pis Std                                    2.57152\n",
      "trainer/Log Pis Max                                   16.1365\n",
      "trainer/Log Pis Min                                    0.325038\n",
      "trainer/Policy mu Mean                                 0.0153738\n",
      "trainer/Policy mu Std                                  0.160545\n",
      "trainer/Policy mu Max                                  1.4395\n",
      "trainer/Policy mu Min                                 -1.27451\n",
      "trainer/Policy log std Mean                           -2.41\n",
      "trainer/Policy log std Std                             0.233536\n",
      "trainer/Policy log std Max                            -1.51946\n",
      "trainer/Policy log std Min                            -3.71157\n",
      "trainer/Alpha                                          0.0119212\n",
      "trainer/Alpha Loss                                     0.96386\n",
      "exploration/num steps total                        41000\n",
      "exploration/num paths total                          113\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.980712\n",
      "exploration/Rewards Std                                0.0784039\n",
      "exploration/Rewards Max                                1.66022\n",
      "exploration/Rewards Min                                0.716532\n",
      "exploration/Returns Mean                             980.712\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              980.712\n",
      "exploration/Returns Min                              980.712\n",
      "exploration/Actions Mean                               0.0163958\n",
      "exploration/Actions Std                                0.102642\n",
      "exploration/Actions Max                                0.405926\n",
      "exploration/Actions Min                               -0.405225\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          980.712\n",
      "exploration/env_infos/final/reward_forward Mean       -0.056292\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.056292\n",
      "exploration/env_infos/final/reward_forward Min        -0.056292\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0665098\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0665098\n",
      "exploration/env_infos/initial/reward_forward Min       0.0665098\n",
      "exploration/env_infos/reward_forward Mean              0.0238719\n",
      "exploration/env_infos/reward_forward Std               0.195329\n",
      "exploration/env_infos/reward_forward Max               1.67561\n",
      "exploration/env_infos/reward_forward Min              -0.854987\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0601732\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0601732\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0601732\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0148973\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0148973\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0148973\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0432165\n",
      "exploration/env_infos/reward_ctrl Std                  0.0248013\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00323365\n",
      "exploration/env_infos/reward_ctrl Min                 -0.283468\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0250429\n",
      "exploration/env_infos/final/torso_velocity Std         0.0808024\n",
      "exploration/env_infos/final/torso_velocity Max         0.135223\n",
      "exploration/env_infos/final/torso_velocity Min        -0.056292\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.137403\n",
      "exploration/env_infos/initial/torso_velocity Std       0.239899\n",
      "exploration/env_infos/initial/torso_velocity Max       0.460178\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.11448\n",
      "exploration/env_infos/torso_velocity Mean              0.00815294\n",
      "exploration/env_infos/torso_velocity Std               0.173001\n",
      "exploration/env_infos/torso_velocity Max               1.67561\n",
      "exploration/env_infos/torso_velocity Min              -1.20778\n",
      "evaluation/num steps total                             1e+06\n",
      "evaluation/num paths total                          1000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.976059\n",
      "evaluation/Rewards Std                                 0.0403006\n",
      "evaluation/Rewards Max                                 2.80911\n",
      "evaluation/Rewards Min                                 0.634473\n",
      "evaluation/Returns Mean                              976.059\n",
      "evaluation/Returns Std                                11.6836\n",
      "evaluation/Returns Max                               989.146\n",
      "evaluation/Returns Min                               935.948\n",
      "evaluation/Actions Mean                                0.0144307\n",
      "evaluation/Actions Std                                 0.0787519\n",
      "evaluation/Actions Max                                 0.557283\n",
      "evaluation/Actions Min                                -0.582639\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           976.059\n",
      "evaluation/env_infos/final/reward_forward Mean         4.09891e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.37788e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.86001e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -6.45472e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0541149\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0876155\n",
      "evaluation/env_infos/initial/reward_forward Max        0.112795\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.267344\n",
      "evaluation/env_infos/reward_forward Mean              -0.000337354\n",
      "evaluation/env_infos/reward_forward Std                0.0588339\n",
      "evaluation/env_infos/reward_forward Max                1.68612\n",
      "evaluation/env_infos/reward_forward Min               -1.22154\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0242648\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0110027\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0170502\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0641511\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0189636\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00365666\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0128821\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0278964\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0256404\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0160842\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0081357\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.388381\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         9.5064e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          2.75999e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.86001e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -6.45472e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.121379\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239698\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.579512\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.283928\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00124591\n",
      "evaluation/env_infos/torso_velocity Std                0.0718173\n",
      "evaluation/env_infos/torso_velocity Max                1.68612\n",
      "evaluation/env_infos/torso_velocity Min               -1.77733\n",
      "time/data storing (s)                                  0.029548\n",
      "time/evaluation sampling (s)                          45.3251\n",
      "time/exploration sampling (s)                          2.03626\n",
      "time/logging (s)                                       0.280836\n",
      "time/saving (s)                                        0.0246682\n",
      "time/training (s)                                      4.3187\n",
      "time/epoch (s)                                        52.0151\n",
      "time/total (s)                                      2155.97\n",
      "Epoch                                                 39\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:31:37.805602 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 40 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 84000\n",
      "trainer/QF1 Loss                                       0.182996\n",
      "trainer/QF2 Loss                                       0.176018\n",
      "trainer/Policy Loss                                   -6.7163\n",
      "trainer/Q1 Predictions Mean                           14.4538\n",
      "trainer/Q1 Predictions Std                             1.37228\n",
      "trainer/Q1 Predictions Max                            16.3288\n",
      "trainer/Q1 Predictions Min                             6.40932\n",
      "trainer/Q2 Predictions Mean                           14.4811\n",
      "trainer/Q2 Predictions Std                             1.27503\n",
      "trainer/Q2 Predictions Max                            16.3095\n",
      "trainer/Q2 Predictions Min                             8.31889\n",
      "trainer/Q Targets Mean                                14.4358\n",
      "trainer/Q Targets Std                                  1.25966\n",
      "trainer/Q Targets Max                                 16.2416\n",
      "trainer/Q Targets Min                                  7.91975\n",
      "trainer/Log Pis Mean                                   8.08822\n",
      "trainer/Log Pis Std                                    2.28977\n",
      "trainer/Log Pis Max                                   15.2946\n",
      "trainer/Log Pis Min                                   -0.150938\n",
      "trainer/Policy mu Mean                                -0.00853371\n",
      "trainer/Policy mu Std                                  0.150436\n",
      "trainer/Policy mu Max                                  0.849727\n",
      "trainer/Policy mu Min                                 -1.06361\n",
      "trainer/Policy log std Mean                           -2.41891\n",
      "trainer/Policy log std Std                             0.225142\n",
      "trainer/Policy log std Max                            -1.81943\n",
      "trainer/Policy log std Min                            -3.51513\n",
      "trainer/Alpha                                          0.0119031\n",
      "trainer/Alpha Loss                                     0.390935\n",
      "exploration/num steps total                        42000\n",
      "exploration/num paths total                          114\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.953229\n",
      "exploration/Rewards Std                                0.073206\n",
      "exploration/Rewards Max                                1.81868\n",
      "exploration/Rewards Min                                0.790628\n",
      "exploration/Returns Mean                             953.229\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              953.229\n",
      "exploration/Returns Min                              953.229\n",
      "exploration/Actions Mean                              -0.0182003\n",
      "exploration/Actions Std                                0.122821\n",
      "exploration/Actions Max                                0.396745\n",
      "exploration/Actions Min                               -0.453185\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          953.229\n",
      "exploration/env_infos/final/reward_forward Mean        0.141476\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.141476\n",
      "exploration/env_infos/final/reward_forward Min         0.141476\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0247094\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0247094\n",
      "exploration/env_infos/initial/reward_forward Min       0.0247094\n",
      "exploration/env_infos/reward_forward Mean              0.00482146\n",
      "exploration/env_infos/reward_forward Std               0.177546\n",
      "exploration/env_infos/reward_forward Max               0.842129\n",
      "exploration/env_infos/reward_forward Min              -0.585998\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0290609\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0290609\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0290609\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0515945\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0515945\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0515945\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0616647\n",
      "exploration/env_infos/reward_ctrl Std                  0.0301651\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00536101\n",
      "exploration/env_infos/reward_ctrl Min                 -0.209372\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0389021\n",
      "exploration/env_infos/final/torso_velocity Std         0.0851882\n",
      "exploration/env_infos/final/torso_velocity Max         0.141476\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0671069\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.125197\n",
      "exploration/env_infos/initial/torso_velocity Std       0.248674\n",
      "exploration/env_infos/initial/torso_velocity Max       0.467305\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.116423\n",
      "exploration/env_infos/torso_velocity Mean              0.00792545\n",
      "exploration/env_infos/torso_velocity Std               0.185427\n",
      "exploration/env_infos/torso_velocity Max               1.49713\n",
      "exploration/env_infos/torso_velocity Min              -2.06493\n",
      "evaluation/num steps total                             1.025e+06\n",
      "evaluation/num paths total                          1025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.961537\n",
      "evaluation/Rewards Std                                 0.0302045\n",
      "evaluation/Rewards Max                                 2.71519\n",
      "evaluation/Rewards Min                                 0.708825\n",
      "evaluation/Returns Mean                              961.537\n",
      "evaluation/Returns Std                                 9.33003\n",
      "evaluation/Returns Max                               978.215\n",
      "evaluation/Returns Min                               942.262\n",
      "evaluation/Actions Mean                               -0.000827453\n",
      "evaluation/Actions Std                                 0.0992721\n",
      "evaluation/Actions Max                                 0.511066\n",
      "evaluation/Actions Min                                -0.524226\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           961.537\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000456772\n",
      "evaluation/env_infos/final/reward_forward Std          0.00218855\n",
      "evaluation/env_infos/final/reward_forward Max          0.0111754\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000134444\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0337885\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126614\n",
      "evaluation/env_infos/initial/reward_forward Max        0.173989\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.308186\n",
      "evaluation/env_infos/reward_forward Mean              -0.00304427\n",
      "evaluation/env_infos/reward_forward Std                0.0549368\n",
      "evaluation/env_infos/reward_forward Max                0.684339\n",
      "evaluation/env_infos/reward_forward Min               -1.55768\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0391147\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00913063\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0284147\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.0592705\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0435343\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0278716\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0192341\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.122179\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0394226\n",
      "evaluation/env_infos/reward_ctrl Std                   0.010871\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0129682\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.291175\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00017154\n",
      "evaluation/env_infos/final/torso_velocity Std          0.00128343\n",
      "evaluation/env_infos/final/torso_velocity Max          0.0111754\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000148057\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.124048\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.267326\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.636007\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.386272\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00276471\n",
      "evaluation/env_infos/torso_velocity Std                0.0686674\n",
      "evaluation/env_infos/torso_velocity Max                1.98053\n",
      "evaluation/env_infos/torso_velocity Min               -1.84228\n",
      "time/data storing (s)                                  0.0304802\n",
      "time/evaluation sampling (s)                          45.5404\n",
      "time/exploration sampling (s)                          2.00305\n",
      "time/logging (s)                                       0.275306\n",
      "time/saving (s)                                        0.0251642\n",
      "time/training (s)                                      4.21915\n",
      "time/epoch (s)                                        52.0935\n",
      "time/total (s)                                      2208.54\n",
      "Epoch                                                 40\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:32:30.041734 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 41 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 86000\n",
      "trainer/QF1 Loss                                       0.287584\n",
      "trainer/QF2 Loss                                       0.283798\n",
      "trainer/Policy Loss                                   -6.93969\n",
      "trainer/Q1 Predictions Mean                           14.8906\n",
      "trainer/Q1 Predictions Std                             1.58015\n",
      "trainer/Q1 Predictions Max                            17.3432\n",
      "trainer/Q1 Predictions Min                             1.39797\n",
      "trainer/Q2 Predictions Mean                           14.8597\n",
      "trainer/Q2 Predictions Std                             1.45453\n",
      "trainer/Q2 Predictions Max                            18.1102\n",
      "trainer/Q2 Predictions Min                             3.61117\n",
      "trainer/Q Targets Mean                                14.793\n",
      "trainer/Q Targets Std                                  1.57529\n",
      "trainer/Q Targets Max                                 20.678\n",
      "trainer/Q Targets Min                                  0.358234\n",
      "trainer/Log Pis Mean                                   8.23829\n",
      "trainer/Log Pis Std                                    2.39616\n",
      "trainer/Log Pis Max                                   17.1571\n",
      "trainer/Log Pis Min                                   -0.815845\n",
      "trainer/Policy mu Mean                                 0.0816349\n",
      "trainer/Policy mu Std                                  0.161288\n",
      "trainer/Policy mu Max                                  1.72839\n",
      "trainer/Policy mu Min                                 -1.59913\n",
      "trainer/Policy log std Mean                           -2.41506\n",
      "trainer/Policy log std Std                             0.242597\n",
      "trainer/Policy log std Max                            -1.54822\n",
      "trainer/Policy log std Min                            -3.74044\n",
      "trainer/Alpha                                          0.0118568\n",
      "trainer/Alpha Loss                                     1.05679\n",
      "exploration/num steps total                        43000\n",
      "exploration/num paths total                          115\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.914636\n",
      "exploration/Rewards Std                                0.0587893\n",
      "exploration/Rewards Max                                1.26762\n",
      "exploration/Rewards Min                                0.712188\n",
      "exploration/Returns Mean                             914.636\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              914.636\n",
      "exploration/Returns Min                              914.636\n",
      "exploration/Actions Mean                               0.106814\n",
      "exploration/Actions Std                                0.114835\n",
      "exploration/Actions Max                                0.643559\n",
      "exploration/Actions Min                               -0.317826\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          914.636\n",
      "exploration/env_infos/final/reward_forward Mean       -0.00966168\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.00966168\n",
      "exploration/env_infos/final/reward_forward Min        -0.00966168\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0989364\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0989364\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0989364\n",
      "exploration/env_infos/reward_forward Mean              0.0060439\n",
      "exploration/env_infos/reward_forward Std               0.085358\n",
      "exploration/env_infos/reward_forward Max               0.354258\n",
      "exploration/env_infos/reward_forward Min              -0.552318\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.148205\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.148205\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.148205\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0988131\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0988131\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0988131\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0983847\n",
      "exploration/env_infos/reward_ctrl Std                  0.0367735\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00868777\n",
      "exploration/env_infos/reward_ctrl Min                 -0.287812\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.000741358\n",
      "exploration/env_infos/final/torso_velocity Std         0.00730062\n",
      "exploration/env_infos/final/torso_velocity Max         0.00822096\n",
      "exploration/env_infos/final/torso_velocity Min        -0.00966168\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.135077\n",
      "exploration/env_infos/initial/torso_velocity Std       0.239158\n",
      "exploration/env_infos/initial/torso_velocity Max       0.463563\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0989364\n",
      "exploration/env_infos/torso_velocity Mean              0.00038253\n",
      "exploration/env_infos/torso_velocity Std               0.0788181\n",
      "exploration/env_infos/torso_velocity Max               0.463563\n",
      "exploration/env_infos/torso_velocity Min              -1.56526\n",
      "evaluation/num steps total                             1.05e+06\n",
      "evaluation/num paths total                          1050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.922237\n",
      "evaluation/Rewards Std                                 0.0403124\n",
      "evaluation/Rewards Max                                 2.52474\n",
      "evaluation/Rewards Min                                 0.495396\n",
      "evaluation/Returns Mean                              922.237\n",
      "evaluation/Returns Std                                33.0563\n",
      "evaluation/Returns Max                               963.65\n",
      "evaluation/Returns Min                               820.913\n",
      "evaluation/Actions Mean                                0.0986393\n",
      "evaluation/Actions Std                                 0.0999017\n",
      "evaluation/Actions Max                                 0.635527\n",
      "evaluation/Actions Min                                -0.404593\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           922.237\n",
      "evaluation/env_infos/final/reward_forward Mean         1.66142e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.72389e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.63255e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.76427e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.050296\n",
      "evaluation/env_infos/initial/reward_forward Std        0.106207\n",
      "evaluation/env_infos/initial/reward_forward Max        0.148065\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.268385\n",
      "evaluation/env_infos/reward_forward Mean              -0.00314445\n",
      "evaluation/env_infos/reward_forward Std                0.0522465\n",
      "evaluation/env_infos/reward_forward Max                1.49352\n",
      "evaluation/env_infos/reward_forward Min               -1.30937\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0777662\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0332106\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0358593\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.179204\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0609951\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0209848\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0392446\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.108603\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0788403\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0349647\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0323447\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.504604\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         7.62801e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.30923e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          5.63255e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.02383e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.123455\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.260745\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.69872\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.299361\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00314351\n",
      "evaluation/env_infos/torso_velocity Std                0.0575951\n",
      "evaluation/env_infos/torso_velocity Max                1.49352\n",
      "evaluation/env_infos/torso_velocity Min               -1.67978\n",
      "time/data storing (s)                                  0.0305938\n",
      "time/evaluation sampling (s)                          45.3309\n",
      "time/exploration sampling (s)                          2.02011\n",
      "time/logging (s)                                       0.271544\n",
      "time/saving (s)                                        0.0283957\n",
      "time/training (s)                                      4.10281\n",
      "time/epoch (s)                                        51.7844\n",
      "time/total (s)                                      2260.77\n",
      "Epoch                                                 41\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:33:21.517184 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 42 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 88000\n",
      "trainer/QF1 Loss                                       0.226757\n",
      "trainer/QF2 Loss                                       0.239823\n",
      "trainer/Policy Loss                                   -7.18497\n",
      "trainer/Q1 Predictions Mean                           14.9918\n",
      "trainer/Q1 Predictions Std                             1.28044\n",
      "trainer/Q1 Predictions Max                            16.8228\n",
      "trainer/Q1 Predictions Min                             8.79896\n",
      "trainer/Q2 Predictions Mean                           15.1369\n",
      "trainer/Q2 Predictions Std                             1.25513\n",
      "trainer/Q2 Predictions Max                            16.877\n",
      "trainer/Q2 Predictions Min                             8.61641\n",
      "trainer/Q Targets Mean                                15.2192\n",
      "trainer/Q Targets Std                                  1.23348\n",
      "trainer/Q Targets Max                                 17.0048\n",
      "trainer/Q Targets Min                                  8.97725\n",
      "trainer/Log Pis Mean                                   8.19007\n",
      "trainer/Log Pis Std                                    2.37451\n",
      "trainer/Log Pis Max                                   17.6051\n",
      "trainer/Log Pis Min                                   -0.681443\n",
      "trainer/Policy mu Mean                                 0.0325664\n",
      "trainer/Policy mu Std                                  0.144001\n",
      "trainer/Policy mu Max                                  0.822214\n",
      "trainer/Policy mu Min                                 -0.742082\n",
      "trainer/Policy log std Mean                           -2.37766\n",
      "trainer/Policy log std Std                             0.225906\n",
      "trainer/Policy log std Max                            -1.34782\n",
      "trainer/Policy log std Min                            -3.61834\n",
      "trainer/Alpha                                          0.0118709\n",
      "trainer/Alpha Loss                                     0.842706\n",
      "exploration/num steps total                        44000\n",
      "exploration/num paths total                          116\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.915637\n",
      "exploration/Rewards Std                                0.0290376\n",
      "exploration/Rewards Max                                1.03432\n",
      "exploration/Rewards Min                                0.753942\n",
      "exploration/Returns Mean                             915.637\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              915.637\n",
      "exploration/Returns Min                              915.637\n",
      "exploration/Actions Mean                               0.0363609\n",
      "exploration/Actions Std                                0.141347\n",
      "exploration/Actions Max                                0.404107\n",
      "exploration/Actions Min                               -0.459283\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          915.637\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0180135\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0180135\n",
      "exploration/env_infos/final/reward_forward Min        -0.0180135\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0868083\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0868083\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0868083\n",
      "exploration/env_infos/reward_forward Mean             -0.00229879\n",
      "exploration/env_infos/reward_forward Std               0.069692\n",
      "exploration/env_infos/reward_forward Max               0.716696\n",
      "exploration/env_infos/reward_forward Min              -0.597268\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0771978\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0771978\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0771978\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0226689\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0226689\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0226689\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0852039\n",
      "exploration/env_infos/reward_ctrl Std                  0.0287797\n",
      "exploration/env_infos/reward_ctrl Max                 -0.014657\n",
      "exploration/env_infos/reward_ctrl Min                 -0.246058\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0083863\n",
      "exploration/env_infos/final/torso_velocity Std         0.00753596\n",
      "exploration/env_infos/final/torso_velocity Max         0.000386325\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0180135\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.111128\n",
      "exploration/env_infos/initial/torso_velocity Std       0.14017\n",
      "exploration/env_infos/initial/torso_velocity Max       0.219448\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0868083\n",
      "exploration/env_infos/torso_velocity Mean             -0.00151374\n",
      "exploration/env_infos/torso_velocity Std               0.0759321\n",
      "exploration/env_infos/torso_velocity Max               0.982488\n",
      "exploration/env_infos/torso_velocity Min              -1.32112\n",
      "evaluation/num steps total                             1.075e+06\n",
      "evaluation/num paths total                          1075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.931207\n",
      "evaluation/Rewards Std                                 0.0360353\n",
      "evaluation/Rewards Max                                 2.40391\n",
      "evaluation/Rewards Min                                 0.712122\n",
      "evaluation/Returns Mean                              931.207\n",
      "evaluation/Returns Std                                17.104\n",
      "evaluation/Returns Max                               949.639\n",
      "evaluation/Returns Min                               885.288\n",
      "evaluation/Actions Mean                                0.0289902\n",
      "evaluation/Actions Std                                 0.129158\n",
      "evaluation/Actions Max                                 0.472195\n",
      "evaluation/Actions Min                                -0.551006\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           931.207\n",
      "evaluation/env_infos/final/reward_forward Mean         1.18394e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.11941e-07\n",
      "evaluation/env_infos/final/reward_forward Max          9.49235e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.72988e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.068638\n",
      "evaluation/env_infos/initial/reward_forward Std        0.152013\n",
      "evaluation/env_infos/initial/reward_forward Max        0.22588\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.280356\n",
      "evaluation/env_infos/reward_forward Mean              -0.00462161\n",
      "evaluation/env_infos/reward_forward Std                0.0607452\n",
      "evaluation/env_infos/reward_forward Max                0.910474\n",
      "evaluation/env_infos/reward_forward Min               -1.4912\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0697063\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0179888\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0496886\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.115294\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0390529\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00999212\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0261225\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0646014\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0700886\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0190739\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.02142\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.355875\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         3.18735e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.71239e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          9.49235e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -4.72988e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.137194\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.263404\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.615252\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.280356\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00478403\n",
      "evaluation/env_infos/torso_velocity Std                0.0674707\n",
      "evaluation/env_infos/torso_velocity Max                1.07253\n",
      "evaluation/env_infos/torso_velocity Min               -1.74181\n",
      "time/data storing (s)                                  0.0301421\n",
      "time/evaluation sampling (s)                          44.503\n",
      "time/exploration sampling (s)                          2.04377\n",
      "time/logging (s)                                       0.275352\n",
      "time/saving (s)                                        0.0310838\n",
      "time/training (s)                                      4.14233\n",
      "time/epoch (s)                                        51.0257\n",
      "time/total (s)                                      2312.25\n",
      "Epoch                                                 42\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:34:13.537495 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 43 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 90000\n",
      "trainer/QF1 Loss                                       0.251925\n",
      "trainer/QF2 Loss                                       0.261293\n",
      "trainer/Policy Loss                                   -8.73407\n",
      "trainer/Q1 Predictions Mean                           15.6232\n",
      "trainer/Q1 Predictions Std                             1.72755\n",
      "trainer/Q1 Predictions Max                            17.8688\n",
      "trainer/Q1 Predictions Min                            -1.4643\n",
      "trainer/Q2 Predictions Mean                           15.4252\n",
      "trainer/Q2 Predictions Std                             1.60681\n",
      "trainer/Q2 Predictions Max                            17.8788\n",
      "trainer/Q2 Predictions Min                             3.3187\n",
      "trainer/Q Targets Mean                                15.5094\n",
      "trainer/Q Targets Std                                  1.66557\n",
      "trainer/Q Targets Max                                 17.6544\n",
      "trainer/Q Targets Min                                  0.400082\n",
      "trainer/Log Pis Mean                                   7.00728\n",
      "trainer/Log Pis Std                                    2.37374\n",
      "trainer/Log Pis Max                                   17.5669\n",
      "trainer/Log Pis Min                                   -1.99447\n",
      "trainer/Policy mu Mean                                 0.0103778\n",
      "trainer/Policy mu Std                                  0.136288\n",
      "trainer/Policy mu Max                                  1.99783\n",
      "trainer/Policy mu Min                                 -1.10556\n",
      "trainer/Policy log std Mean                           -2.28601\n",
      "trainer/Policy log std Std                             0.205077\n",
      "trainer/Policy log std Max                            -0.893353\n",
      "trainer/Policy log std Min                            -3.71003\n",
      "trainer/Alpha                                          0.0118378\n",
      "trainer/Alpha Loss                                    -4.40349\n",
      "exploration/num steps total                        45000\n",
      "exploration/num paths total                          117\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.948966\n",
      "exploration/Rewards Std                                0.0625986\n",
      "exploration/Rewards Max                                1.51289\n",
      "exploration/Rewards Min                                0.743237\n",
      "exploration/Returns Mean                             948.966\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              948.966\n",
      "exploration/Returns Min                              948.966\n",
      "exploration/Actions Mean                               0.0116567\n",
      "exploration/Actions Std                                0.124554\n",
      "exploration/Actions Max                                0.506795\n",
      "exploration/Actions Min                               -0.431268\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          948.966\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0712446\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0712446\n",
      "exploration/env_infos/final/reward_forward Min        -0.0712446\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00541859\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00541859\n",
      "exploration/env_infos/initial/reward_forward Min       0.00541859\n",
      "exploration/env_infos/reward_forward Mean             -0.00852484\n",
      "exploration/env_infos/reward_forward Std               0.187113\n",
      "exploration/env_infos/reward_forward Max               1.3702\n",
      "exploration/env_infos/reward_forward Min              -0.899942\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.059269\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.059269\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.059269\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0512255\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0512255\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0512255\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0625984\n",
      "exploration/env_infos/reward_ctrl Std                  0.0299116\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00521973\n",
      "exploration/env_infos/reward_ctrl Min                 -0.256763\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.045975\n",
      "exploration/env_infos/final/torso_velocity Std         0.0851312\n",
      "exploration/env_infos/final/torso_velocity Max         0.128368\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0712446\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.112834\n",
      "exploration/env_infos/initial/torso_velocity Std       0.203858\n",
      "exploration/env_infos/initial/torso_velocity Max       0.398239\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.065155\n",
      "exploration/env_infos/torso_velocity Mean              0.00022713\n",
      "exploration/env_infos/torso_velocity Std               0.168356\n",
      "exploration/env_infos/torso_velocity Max               1.3702\n",
      "exploration/env_infos/torso_velocity Min              -0.901218\n",
      "evaluation/num steps total                             1.1e+06\n",
      "evaluation/num paths total                          1100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.973055\n",
      "evaluation/Rewards Std                                 0.0273587\n",
      "evaluation/Rewards Max                                 2.78032\n",
      "evaluation/Rewards Min                                 0.692096\n",
      "evaluation/Returns Mean                              973.055\n",
      "evaluation/Returns Std                                 7.81384\n",
      "evaluation/Returns Max                               986.019\n",
      "evaluation/Returns Min                               955.891\n",
      "evaluation/Actions Mean                               -0.0029803\n",
      "evaluation/Actions Std                                 0.0831995\n",
      "evaluation/Actions Max                                 0.605909\n",
      "evaluation/Actions Min                                -0.597685\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           973.055\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.41355e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.9766e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.23492e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.85805e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0384167\n",
      "evaluation/env_infos/initial/reward_forward Std        0.125337\n",
      "evaluation/env_infos/initial/reward_forward Max        0.204874\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.231226\n",
      "evaluation/env_infos/reward_forward Mean              -0.00110859\n",
      "evaluation/env_infos/reward_forward Std                0.0660659\n",
      "evaluation/env_infos/reward_forward Max                1.31623\n",
      "evaluation/env_infos/reward_forward Min               -1.38694\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0274169\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0077828\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0137269\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.044411\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0406069\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.010008\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0254633\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0624679\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0277242\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00950524\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00959807\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.307904\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -6.98107e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.8217e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.23492e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.85805e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.121591\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.230211\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.643378\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.246669\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00196764\n",
      "evaluation/env_infos/torso_velocity Std                0.0656102\n",
      "evaluation/env_infos/torso_velocity Max                1.55685\n",
      "evaluation/env_infos/torso_velocity Min               -1.71531\n",
      "time/data storing (s)                                  0.0295264\n",
      "time/evaluation sampling (s)                          45.1415\n",
      "time/exploration sampling (s)                          2.0042\n",
      "time/logging (s)                                       0.287763\n",
      "time/saving (s)                                        0.0271434\n",
      "time/training (s)                                      4.0753\n",
      "time/epoch (s)                                        51.5654\n",
      "time/total (s)                                      2364.28\n",
      "Epoch                                                 43\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:35:05.826846 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 44 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 92000\n",
      "trainer/QF1 Loss                                       0.732216\n",
      "trainer/QF2 Loss                                       0.774791\n",
      "trainer/Policy Loss                                   -8.9367\n",
      "trainer/Q1 Predictions Mean                           16.0297\n",
      "trainer/Q1 Predictions Std                             1.84557\n",
      "trainer/Q1 Predictions Max                            18.3099\n",
      "trainer/Q1 Predictions Min                            -1.75313\n",
      "trainer/Q2 Predictions Mean                           16.062\n",
      "trainer/Q2 Predictions Std                             1.70059\n",
      "trainer/Q2 Predictions Max                            18.171\n",
      "trainer/Q2 Predictions Min                             0.472412\n",
      "trainer/Q Targets Mean                                15.6904\n",
      "trainer/Q Targets Std                                  1.75727\n",
      "trainer/Q Targets Max                                 18.145\n",
      "trainer/Q Targets Min                                 -0.405301\n",
      "trainer/Log Pis Mean                                   7.41461\n",
      "trainer/Log Pis Std                                    2.29703\n",
      "trainer/Log Pis Max                                   16.1508\n",
      "trainer/Log Pis Min                                   -3.38178\n",
      "trainer/Policy mu Mean                                 0.00872552\n",
      "trainer/Policy mu Std                                  0.138759\n",
      "trainer/Policy mu Max                                  1.45119\n",
      "trainer/Policy mu Min                                 -1.56294\n",
      "trainer/Policy log std Mean                           -2.33113\n",
      "trainer/Policy log std Std                             0.198738\n",
      "trainer/Policy log std Max                            -1.65832\n",
      "trainer/Policy log std Min                            -3.28457\n",
      "trainer/Alpha                                          0.0113323\n",
      "trainer/Alpha Loss                                    -2.62229\n",
      "exploration/num steps total                        46000\n",
      "exploration/num paths total                          118\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.946768\n",
      "exploration/Rewards Std                                0.0750991\n",
      "exploration/Rewards Max                                1.4976\n",
      "exploration/Rewards Min                                0.745935\n",
      "exploration/Returns Mean                             946.768\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              946.768\n",
      "exploration/Returns Min                              946.768\n",
      "exploration/Actions Mean                              -0.00899526\n",
      "exploration/Actions Std                                0.132833\n",
      "exploration/Actions Max                                0.462324\n",
      "exploration/Actions Min                               -0.524405\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          946.768\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0773916\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0773916\n",
      "exploration/env_infos/final/reward_forward Min        -0.0773916\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.00524449\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.00524449\n",
      "exploration/env_infos/initial/reward_forward Min      -0.00524449\n",
      "exploration/env_infos/reward_forward Mean             -0.0163142\n",
      "exploration/env_infos/reward_forward Std               0.26721\n",
      "exploration/env_infos/reward_forward Max               0.951713\n",
      "exploration/env_infos/reward_forward Min              -1.29322\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0361242\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0361242\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0361242\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0204211\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0204211\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0204211\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0709023\n",
      "exploration/env_infos/reward_ctrl Std                  0.0346262\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00509545\n",
      "exploration/env_infos/reward_ctrl Min                 -0.254065\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0525686\n",
      "exploration/env_infos/final/torso_velocity Std         0.0520646\n",
      "exploration/env_infos/final/torso_velocity Max         0.0198759\n",
      "exploration/env_infos/final/torso_velocity Min        -0.10019\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.142968\n",
      "exploration/env_infos/initial/torso_velocity Std       0.237003\n",
      "exploration/env_infos/initial/torso_velocity Max       0.477421\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0432717\n",
      "exploration/env_infos/torso_velocity Mean             -0.00557529\n",
      "exploration/env_infos/torso_velocity Std               0.238992\n",
      "exploration/env_infos/torso_velocity Max               0.951713\n",
      "exploration/env_infos/torso_velocity Min              -1.44271\n",
      "evaluation/num steps total                             1.125e+06\n",
      "evaluation/num paths total                          1125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.966496\n",
      "evaluation/Rewards Std                                 0.0234094\n",
      "evaluation/Rewards Max                                 2.32569\n",
      "evaluation/Rewards Min                                 0.55818\n",
      "evaluation/Returns Mean                              966.496\n",
      "evaluation/Returns Std                                 4.68122\n",
      "evaluation/Returns Max                               973.007\n",
      "evaluation/Returns Min                               953.565\n",
      "evaluation/Actions Mean                               -0.00970482\n",
      "evaluation/Actions Std                                 0.0921349\n",
      "evaluation/Actions Max                                 0.549844\n",
      "evaluation/Actions Min                                -0.477738\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           966.496\n",
      "evaluation/env_infos/final/reward_forward Mean        -9.91563e-06\n",
      "evaluation/env_infos/final/reward_forward Std          4.82523e-05\n",
      "evaluation/env_infos/final/reward_forward Max          4.67236e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000246296\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00516059\n",
      "evaluation/env_infos/initial/reward_forward Std        0.142699\n",
      "evaluation/env_infos/initial/reward_forward Max        0.226\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.32525\n",
      "evaluation/env_infos/reward_forward Mean              -0.0026867\n",
      "evaluation/env_infos/reward_forward Std                0.0647772\n",
      "evaluation/env_infos/reward_forward Max                1.12461\n",
      "evaluation/env_infos/reward_forward Min               -1.49178\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0339293\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.00508292\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0263334\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.046347\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0311908\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.00615272\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0206986\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0457432\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0343321\n",
      "evaluation/env_infos/reward_ctrl Std                   0.00849208\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0111961\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.44182\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         7.44899e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          7.13752e-05\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000420183\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000246296\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.139874\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.238786\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.699355\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.32525\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00367427\n",
      "evaluation/env_infos/torso_velocity Std                0.0663073\n",
      "evaluation/env_infos/torso_velocity Max                1.12461\n",
      "evaluation/env_infos/torso_velocity Min               -1.86504\n",
      "time/data storing (s)                                  0.0306044\n",
      "time/evaluation sampling (s)                          45.3572\n",
      "time/exploration sampling (s)                          1.96144\n",
      "time/logging (s)                                       0.272468\n",
      "time/saving (s)                                        0.0324488\n",
      "time/training (s)                                      4.14011\n",
      "time/epoch (s)                                        51.7943\n",
      "time/total (s)                                      2416.55\n",
      "Epoch                                                 44\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:35:58.301303 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 45 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 94000\n",
      "trainer/QF1 Loss                                       0.268562\n",
      "trainer/QF2 Loss                                       0.254044\n",
      "trainer/Policy Loss                                   -8.03955\n",
      "trainer/Q1 Predictions Mean                           16.2108\n",
      "trainer/Q1 Predictions Std                             1.51302\n",
      "trainer/Q1 Predictions Max                            17.9447\n",
      "trainer/Q1 Predictions Min                             8.08734\n",
      "trainer/Q2 Predictions Mean                           15.9194\n",
      "trainer/Q2 Predictions Std                             1.4951\n",
      "trainer/Q2 Predictions Max                            17.6424\n",
      "trainer/Q2 Predictions Min                             7.4009\n",
      "trainer/Q Targets Mean                                16.0259\n",
      "trainer/Q Targets Std                                  1.46594\n",
      "trainer/Q Targets Max                                 17.8298\n",
      "trainer/Q Targets Min                                  8.51709\n",
      "trainer/Log Pis Mean                                   8.2464\n",
      "trainer/Log Pis Std                                    2.60123\n",
      "trainer/Log Pis Max                                   19.3268\n",
      "trainer/Log Pis Min                                   -1.37068\n",
      "trainer/Policy mu Mean                                 0.0406157\n",
      "trainer/Policy mu Std                                  0.166686\n",
      "trainer/Policy mu Max                                  1.20391\n",
      "trainer/Policy mu Min                                 -1.57135\n",
      "trainer/Policy log std Mean                           -2.41624\n",
      "trainer/Policy log std Std                             0.237572\n",
      "trainer/Policy log std Max                            -1.26445\n",
      "trainer/Policy log std Min                            -3.89244\n",
      "trainer/Alpha                                          0.0109239\n",
      "trainer/Alpha Loss                                     1.11303\n",
      "exploration/num steps total                        47000\n",
      "exploration/num paths total                          119\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.87494\n",
      "exploration/Rewards Std                                0.0635899\n",
      "exploration/Rewards Max                                1.15769\n",
      "exploration/Rewards Min                                0.536182\n",
      "exploration/Returns Mean                             874.94\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              874.94\n",
      "exploration/Returns Min                              874.94\n",
      "exploration/Actions Mean                               0.0269225\n",
      "exploration/Actions Std                                0.179388\n",
      "exploration/Actions Max                                0.569903\n",
      "exploration/Actions Min                               -0.60941\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          874.94\n",
      "exploration/env_infos/final/reward_forward Mean       -0.160697\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.160697\n",
      "exploration/env_infos/final/reward_forward Min        -0.160697\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0334897\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0334897\n",
      "exploration/env_infos/initial/reward_forward Min       0.0334897\n",
      "exploration/env_infos/reward_forward Mean             -0.0167055\n",
      "exploration/env_infos/reward_forward Std               0.214837\n",
      "exploration/env_infos/reward_forward Max               1.29013\n",
      "exploration/env_infos/reward_forward Min              -1.01216\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.136238\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.136238\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.136238\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0492972\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0492972\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0492972\n",
      "exploration/env_infos/reward_ctrl Mean                -0.13162\n",
      "exploration/env_infos/reward_ctrl Std                  0.0536601\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0260694\n",
      "exploration/env_infos/reward_ctrl Min                 -0.463818\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.174779\n",
      "exploration/env_infos/final/torso_velocity Std         0.0130608\n",
      "exploration/env_infos/final/torso_velocity Max        -0.160697\n",
      "exploration/env_infos/final/torso_velocity Min        -0.192171\n",
      "exploration/env_infos/initial/torso_velocity Mean     -0.0158018\n",
      "exploration/env_infos/initial/torso_velocity Std       0.277429\n",
      "exploration/env_infos/initial/torso_velocity Max       0.29664\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.377535\n",
      "exploration/env_infos/torso_velocity Mean             -0.0128556\n",
      "exploration/env_infos/torso_velocity Std               0.169136\n",
      "exploration/env_infos/torso_velocity Max               1.29013\n",
      "exploration/env_infos/torso_velocity Min              -1.22509\n",
      "evaluation/num steps total                             1.15e+06\n",
      "evaluation/num paths total                          1150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.893019\n",
      "evaluation/Rewards Std                                 0.0328827\n",
      "evaluation/Rewards Max                                 1.89839\n",
      "evaluation/Rewards Min                                 0.576405\n",
      "evaluation/Returns Mean                              893.019\n",
      "evaluation/Returns Std                                18.2698\n",
      "evaluation/Returns Max                               923.249\n",
      "evaluation/Returns Min                               842.788\n",
      "evaluation/Actions Mean                                0.0410466\n",
      "evaluation/Actions Std                                 0.159257\n",
      "evaluation/Actions Max                                 0.533647\n",
      "evaluation/Actions Min                                -0.59703\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           893.019\n",
      "evaluation/env_infos/final/reward_forward Mean         4.08154e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.87724e-07\n",
      "evaluation/env_infos/final/reward_forward Max          5.62159e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.14882e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0170226\n",
      "evaluation/env_infos/initial/reward_forward Std        0.129108\n",
      "evaluation/env_infos/initial/reward_forward Max        0.222625\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.253344\n",
      "evaluation/env_infos/reward_forward Mean               0.00264588\n",
      "evaluation/env_infos/reward_forward Std                0.0491721\n",
      "evaluation/env_infos/reward_forward Max                1.53228\n",
      "evaluation/env_infos/reward_forward Min               -1.19958\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.107962\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0194566\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0760683\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.157288\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.042136\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0144289\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0228295\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0671807\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.10819\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0219539\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0186267\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.423595\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.75176e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          1.87512e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          5.62159e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -5.14882e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.144468\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239567\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.576291\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.253344\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00122288\n",
      "evaluation/env_infos/torso_velocity Std                0.06831\n",
      "evaluation/env_infos/torso_velocity Max                1.53228\n",
      "evaluation/env_infos/torso_velocity Min               -1.96711\n",
      "time/data storing (s)                                  0.0621309\n",
      "time/evaluation sampling (s)                          45.355\n",
      "time/exploration sampling (s)                          2.02121\n",
      "time/logging (s)                                       0.276767\n",
      "time/saving (s)                                        0.0247297\n",
      "time/training (s)                                      4.26708\n",
      "time/epoch (s)                                        52.0069\n",
      "time/total (s)                                      2469.03\n",
      "Epoch                                                 45\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:36:51.070768 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 46 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 96000\n",
      "trainer/QF1 Loss                                       0.657867\n",
      "trainer/QF2 Loss                                       0.729976\n",
      "trainer/Policy Loss                                   -9.60747\n",
      "trainer/Q1 Predictions Mean                           16.7679\n",
      "trainer/Q1 Predictions Std                             1.47691\n",
      "trainer/Q1 Predictions Max                            18.4259\n",
      "trainer/Q1 Predictions Min                             8.41031\n",
      "trainer/Q2 Predictions Mean                           16.7749\n",
      "trainer/Q2 Predictions Std                             1.43298\n",
      "trainer/Q2 Predictions Max                            18.5246\n",
      "trainer/Q2 Predictions Min                             9.36336\n",
      "trainer/Q Targets Mean                                16.3884\n",
      "trainer/Q Targets Std                                  1.56366\n",
      "trainer/Q Targets Max                                 18.1098\n",
      "trainer/Q Targets Min                                  5.64271\n",
      "trainer/Log Pis Mean                                   7.45632\n",
      "trainer/Log Pis Std                                    2.35911\n",
      "trainer/Log Pis Max                                   13.4691\n",
      "trainer/Log Pis Min                                   -1.40276\n",
      "trainer/Policy mu Mean                                -0.0293834\n",
      "trainer/Policy mu Std                                  0.153528\n",
      "trainer/Policy mu Max                                  0.979493\n",
      "trainer/Policy mu Min                                 -1.41128\n",
      "trainer/Policy log std Mean                           -2.30206\n",
      "trainer/Policy log std Std                             0.218827\n",
      "trainer/Policy log std Max                            -1.29096\n",
      "trainer/Policy log std Min                            -3.26829\n",
      "trainer/Alpha                                          0.0106797\n",
      "trainer/Alpha Loss                                    -2.46775\n",
      "exploration/num steps total                        48000\n",
      "exploration/num paths total                          120\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.905095\n",
      "exploration/Rewards Std                                0.125731\n",
      "exploration/Rewards Max                                2.51063\n",
      "exploration/Rewards Min                                0.639249\n",
      "exploration/Returns Mean                             905.095\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              905.095\n",
      "exploration/Returns Min                              905.095\n",
      "exploration/Actions Mean                               0.000543357\n",
      "exploration/Actions Std                                0.177694\n",
      "exploration/Actions Max                                0.634429\n",
      "exploration/Actions Min                               -0.512067\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          905.095\n",
      "exploration/env_infos/final/reward_forward Mean       -0.161291\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.161291\n",
      "exploration/env_infos/final/reward_forward Min        -0.161291\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.139652\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.139652\n",
      "exploration/env_infos/initial/reward_forward Min      -0.139652\n",
      "exploration/env_infos/reward_forward Mean              0.0141034\n",
      "exploration/env_infos/reward_forward Std               0.131354\n",
      "exploration/env_infos/reward_forward Max               0.601332\n",
      "exploration/env_infos/reward_forward Min              -1.21498\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0928329\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0928329\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0928329\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0738614\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0738614\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0738614\n",
      "exploration/env_infos/reward_ctrl Mean                -0.126302\n",
      "exploration/env_infos/reward_ctrl Std                  0.0472476\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0239702\n",
      "exploration/env_infos/reward_ctrl Min                 -0.360751\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.00988845\n",
      "exploration/env_infos/final/torso_velocity Std         0.221669\n",
      "exploration/env_infos/final/torso_velocity Max         0.322918\n",
      "exploration/env_infos/final/torso_velocity Min        -0.161291\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0984738\n",
      "exploration/env_infos/initial/torso_velocity Std       0.270016\n",
      "exploration/env_infos/initial/torso_velocity Max       0.476061\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.139652\n",
      "exploration/env_infos/torso_velocity Mean              0.0109193\n",
      "exploration/env_infos/torso_velocity Std               0.132082\n",
      "exploration/env_infos/torso_velocity Max               1.32838\n",
      "exploration/env_infos/torso_velocity Min              -1.21498\n",
      "evaluation/num steps total                             1.175e+06\n",
      "evaluation/num paths total                          1175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.942008\n",
      "evaluation/Rewards Std                                 0.0304773\n",
      "evaluation/Rewards Max                                 2.40589\n",
      "evaluation/Rewards Min                                 0.678702\n",
      "evaluation/Returns Mean                              942.008\n",
      "evaluation/Returns Std                                21.9467\n",
      "evaluation/Returns Max                               968.928\n",
      "evaluation/Returns Min                               886.29\n",
      "evaluation/Actions Mean                               -0.00568715\n",
      "evaluation/Actions Std                                 0.120839\n",
      "evaluation/Actions Max                                 0.576867\n",
      "evaluation/Actions Min                                -0.507597\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           942.008\n",
      "evaluation/env_infos/final/reward_forward Mean        -1.27411e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.85743e-07\n",
      "evaluation/env_infos/final/reward_forward Max          7.59455e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -8.27696e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0395714\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0980841\n",
      "evaluation/env_infos/initial/reward_forward Max        0.136374\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.266544\n",
      "evaluation/env_infos/reward_forward Mean              -0.00461622\n",
      "evaluation/env_infos/reward_forward Std                0.0552545\n",
      "evaluation/env_infos/reward_forward Max                0.503171\n",
      "evaluation/env_infos/reward_forward Min               -1.41462\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0583122\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0225255\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0311645\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.11425\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0325064\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.010774\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0168096\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0566989\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.058538\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0235774\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0124183\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.321298\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -5.93275e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.11008e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          7.59455e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.27696e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.116807\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.24861\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.609951\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.344196\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00246196\n",
      "evaluation/env_infos/torso_velocity Std                0.0546198\n",
      "evaluation/env_infos/torso_velocity Max                1.27889\n",
      "evaluation/env_infos/torso_velocity Min               -1.76353\n",
      "time/data storing (s)                                  0.0310281\n",
      "time/evaluation sampling (s)                          44.5643\n",
      "time/exploration sampling (s)                          2.31512\n",
      "time/logging (s)                                       0.270122\n",
      "time/saving (s)                                        0.0255781\n",
      "time/training (s)                                      5.07555\n",
      "time/epoch (s)                                        52.2817\n",
      "time/total (s)                                      2521.79\n",
      "Epoch                                                 46\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:37:43.583812 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 47 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 98000\n",
      "trainer/QF1 Loss                                       0.294206\n",
      "trainer/QF2 Loss                                       0.333627\n",
      "trainer/Policy Loss                                   -8.91841\n",
      "trainer/Q1 Predictions Mean                           16.4803\n",
      "trainer/Q1 Predictions Std                             1.39909\n",
      "trainer/Q1 Predictions Max                            18.1333\n",
      "trainer/Q1 Predictions Min                             5.72263\n",
      "trainer/Q2 Predictions Mean                           16.3645\n",
      "trainer/Q2 Predictions Std                             1.37913\n",
      "trainer/Q2 Predictions Max                            17.9372\n",
      "trainer/Q2 Predictions Min                             5.2962\n",
      "trainer/Q Targets Mean                                16.7141\n",
      "trainer/Q Targets Std                                  1.38488\n",
      "trainer/Q Targets Max                                 18.4574\n",
      "trainer/Q Targets Min                                  6.94578\n",
      "trainer/Log Pis Mean                                   7.74229\n",
      "trainer/Log Pis Std                                    2.34444\n",
      "trainer/Log Pis Max                                   17.2681\n",
      "trainer/Log Pis Min                                    0.230586\n",
      "trainer/Policy mu Mean                                -0.0152048\n",
      "trainer/Policy mu Std                                  0.181154\n",
      "trainer/Policy mu Max                                  0.973308\n",
      "trainer/Policy mu Min                                 -2.07687\n",
      "trainer/Policy log std Mean                           -2.3322\n",
      "trainer/Policy log std Std                             0.213595\n",
      "trainer/Policy log std Max                            -1.51559\n",
      "trainer/Policy log std Min                            -3.68275\n",
      "trainer/Alpha                                          0.0106801\n",
      "trainer/Alpha Loss                                    -1.16983\n",
      "exploration/num steps total                        49000\n",
      "exploration/num paths total                          121\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.913802\n",
      "exploration/Rewards Std                                0.122838\n",
      "exploration/Rewards Max                                2.06977\n",
      "exploration/Rewards Min                                0.663924\n",
      "exploration/Returns Mean                             913.802\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              913.802\n",
      "exploration/Returns Min                              913.802\n",
      "exploration/Actions Mean                              -0.032865\n",
      "exploration/Actions Std                                0.166569\n",
      "exploration/Actions Max                                0.554375\n",
      "exploration/Actions Min                               -0.503796\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          913.802\n",
      "exploration/env_infos/final/reward_forward Mean        0.139233\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.139233\n",
      "exploration/env_infos/final/reward_forward Min         0.139233\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.148858\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.148858\n",
      "exploration/env_infos/initial/reward_forward Min      -0.148858\n",
      "exploration/env_infos/reward_forward Mean             -0.0143646\n",
      "exploration/env_infos/reward_forward Std               0.166921\n",
      "exploration/env_infos/reward_forward Max               1.00275\n",
      "exploration/env_infos/reward_forward Min              -0.963565\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.125861\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.125861\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.125861\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.092535\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.092535\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.092535\n",
      "exploration/env_infos/reward_ctrl Mean                -0.115302\n",
      "exploration/env_infos/reward_ctrl Std                  0.0441474\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0112824\n",
      "exploration/env_infos/reward_ctrl Min                 -0.336076\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0838443\n",
      "exploration/env_infos/final/torso_velocity Std         0.0502409\n",
      "exploration/env_infos/final/torso_velocity Max         0.139233\n",
      "exploration/env_infos/final/torso_velocity Min         0.0176105\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.182913\n",
      "exploration/env_infos/initial/torso_velocity Std       0.281368\n",
      "exploration/env_infos/initial/torso_velocity Max       0.539056\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.148858\n",
      "exploration/env_infos/torso_velocity Mean              0.000274987\n",
      "exploration/env_infos/torso_velocity Std               0.174734\n",
      "exploration/env_infos/torso_velocity Max               1.00275\n",
      "exploration/env_infos/torso_velocity Min              -1.41286\n",
      "evaluation/num steps total                             1.2e+06\n",
      "evaluation/num paths total                          1200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.909727\n",
      "evaluation/Rewards Std                                 0.0265277\n",
      "evaluation/Rewards Max                                 2.206\n",
      "evaluation/Rewards Min                                 0.644254\n",
      "evaluation/Returns Mean                              909.727\n",
      "evaluation/Returns Std                                14.6333\n",
      "evaluation/Returns Max                               925.511\n",
      "evaluation/Returns Min                               867.58\n",
      "evaluation/Actions Mean                               -0.0354118\n",
      "evaluation/Actions Std                                 0.146692\n",
      "evaluation/Actions Max                                 0.543817\n",
      "evaluation/Actions Min                                -0.576585\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           909.727\n",
      "evaluation/env_infos/final/reward_forward Mean         2.57774e-08\n",
      "evaluation/env_infos/final/reward_forward Std          2.74396e-07\n",
      "evaluation/env_infos/final/reward_forward Max          3.83144e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -4.51809e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.00858466\n",
      "evaluation/env_infos/initial/reward_forward Std        0.0854988\n",
      "evaluation/env_infos/initial/reward_forward Max        0.133244\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.215657\n",
      "evaluation/env_infos/reward_forward Mean               0.00153324\n",
      "evaluation/env_infos/reward_forward Std                0.0574573\n",
      "evaluation/env_infos/reward_forward Max                1.55113\n",
      "evaluation/env_infos/reward_forward Min               -0.793894\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0904051\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0148784\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0731219\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.1331\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0708467\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.012143\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.045315\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0920102\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0910899\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0174268\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0331353\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.355746\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.03607e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          2.49867e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          3.83144e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.07021e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.150912\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.203576\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.646168\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.215657\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00166273\n",
      "evaluation/env_infos/torso_velocity Std                0.0745957\n",
      "evaluation/env_infos/torso_velocity Max                1.55113\n",
      "evaluation/env_infos/torso_velocity Min               -1.84212\n",
      "time/data storing (s)                                  0.0322955\n",
      "time/evaluation sampling (s)                          45.3079\n",
      "time/exploration sampling (s)                          2.13281\n",
      "time/logging (s)                                       0.26055\n",
      "time/saving (s)                                        0.0600082\n",
      "time/training (s)                                      4.22569\n",
      "time/epoch (s)                                        52.0193\n",
      "time/total (s)                                      2574.29\n",
      "Epoch                                                 47\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:38:36.067829 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 48 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 100000\n",
      "trainer/QF1 Loss                                        0.173436\n",
      "trainer/QF2 Loss                                        0.286624\n",
      "trainer/Policy Loss                                   -10.2274\n",
      "trainer/Q1 Predictions Mean                            17.0302\n",
      "trainer/Q1 Predictions Std                              1.51859\n",
      "trainer/Q1 Predictions Max                             19.3369\n",
      "trainer/Q1 Predictions Min                             10.4575\n",
      "trainer/Q2 Predictions Mean                            17.195\n",
      "trainer/Q2 Predictions Std                              1.55049\n",
      "trainer/Q2 Predictions Max                             19.3114\n",
      "trainer/Q2 Predictions Min                              9.32351\n",
      "trainer/Q Targets Mean                                 16.9791\n",
      "trainer/Q Targets Std                                   1.5435\n",
      "trainer/Q Targets Max                                  18.8864\n",
      "trainer/Q Targets Min                                   9.49153\n",
      "trainer/Log Pis Mean                                    7.13635\n",
      "trainer/Log Pis Std                                     2.39033\n",
      "trainer/Log Pis Max                                    13.1926\n",
      "trainer/Log Pis Min                                    -2.41263\n",
      "trainer/Policy mu Mean                                  0.00983387\n",
      "trainer/Policy mu Std                                   0.163507\n",
      "trainer/Policy mu Max                                   0.975287\n",
      "trainer/Policy mu Min                                  -0.886102\n",
      "trainer/Policy log std Mean                            -2.29388\n",
      "trainer/Policy log std Std                              0.242824\n",
      "trainer/Policy log std Max                             -1.24078\n",
      "trainer/Policy log std Min                             -3.7987\n",
      "trainer/Alpha                                           0.010565\n",
      "trainer/Alpha Loss                                     -3.92965\n",
      "exploration/num steps total                         50000\n",
      "exploration/num paths total                           122\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.857949\n",
      "exploration/Rewards Std                                 0.0536428\n",
      "exploration/Rewards Max                                 1.20583\n",
      "exploration/Rewards Min                                 0.722508\n",
      "exploration/Returns Mean                              857.949\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               857.949\n",
      "exploration/Returns Min                               857.949\n",
      "exploration/Actions Mean                               -0.0137047\n",
      "exploration/Actions Std                                 0.192753\n",
      "exploration/Actions Max                                 0.47226\n",
      "exploration/Actions Min                                -0.59784\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           857.949\n",
      "exploration/env_infos/final/reward_forward Mean         0.0397507\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0397507\n",
      "exploration/env_infos/final/reward_forward Min          0.0397507\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0497404\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0497404\n",
      "exploration/env_infos/initial/reward_forward Min        0.0497404\n",
      "exploration/env_infos/reward_forward Mean               0.0129481\n",
      "exploration/env_infos/reward_forward Std                0.0893699\n",
      "exploration/env_infos/reward_forward Max                0.689948\n",
      "exploration/env_infos/reward_forward Min               -0.45424\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.121401\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.121401\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.121401\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.166088\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.166088\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.166088\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.149366\n",
      "exploration/env_infos/reward_ctrl Std                   0.0448812\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0349432\n",
      "exploration/env_infos/reward_ctrl Min                  -0.277492\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.119307\n",
      "exploration/env_infos/final/torso_velocity Std          0.0590817\n",
      "exploration/env_infos/final/torso_velocity Max          0.1812\n",
      "exploration/env_infos/final/torso_velocity Min          0.0397507\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.19173\n",
      "exploration/env_infos/initial/torso_velocity Std        0.237062\n",
      "exploration/env_infos/initial/torso_velocity Max        0.52574\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.000289582\n",
      "exploration/env_infos/torso_velocity Mean               0.00460966\n",
      "exploration/env_infos/torso_velocity Std                0.130767\n",
      "exploration/env_infos/torso_velocity Max                0.689948\n",
      "exploration/env_infos/torso_velocity Min               -1.30395\n",
      "evaluation/num steps total                              1.225e+06\n",
      "evaluation/num paths total                           1225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.916146\n",
      "evaluation/Rewards Std                                  0.0257015\n",
      "evaluation/Rewards Max                                  2.10434\n",
      "evaluation/Rewards Min                                  0.692335\n",
      "evaluation/Returns Mean                               916.146\n",
      "evaluation/Returns Std                                 12.3173\n",
      "evaluation/Returns Max                                934.157\n",
      "evaluation/Returns Min                                895.597\n",
      "evaluation/Actions Mean                                -0.00112063\n",
      "evaluation/Actions Std                                  0.145312\n",
      "evaluation/Actions Max                                  0.453891\n",
      "evaluation/Actions Min                                 -0.619336\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            916.146\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.21852e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.92533e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.89872e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.10583e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0104373\n",
      "evaluation/env_infos/initial/reward_forward Std         0.147772\n",
      "evaluation/env_infos/initial/reward_forward Max         0.308113\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.296381\n",
      "evaluation/env_infos/reward_forward Mean               -0.000665554\n",
      "evaluation/env_infos/reward_forward Std                 0.0415698\n",
      "evaluation/env_infos/reward_forward Max                 1.23899\n",
      "evaluation/env_infos/reward_forward Min                -1.16052\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0839067\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0127039\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0653461\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.104345\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0534188\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00973979\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0351569\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0755401\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0844676\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0147028\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0306062\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.426366\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.97861e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           3.28009e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.06718e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.99342e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141156\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.252158\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.632955\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.296381\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00233453\n",
      "evaluation/env_infos/torso_velocity Std                 0.0517832\n",
      "evaluation/env_infos/torso_velocity Max                 1.23899\n",
      "evaluation/env_infos/torso_velocity Min                -1.70195\n",
      "time/data storing (s)                                   0.0378463\n",
      "time/evaluation sampling (s)                           45.5308\n",
      "time/exploration sampling (s)                           2.04714\n",
      "time/logging (s)                                        0.269921\n",
      "time/saving (s)                                         0.026718\n",
      "time/training (s)                                       4.12593\n",
      "time/epoch (s)                                         52.0383\n",
      "time/total (s)                                       2626.79\n",
      "Epoch                                                  48\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:39:27.824251 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 49 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 102000\n",
      "trainer/QF1 Loss                                        0.197507\n",
      "trainer/QF2 Loss                                        0.31541\n",
      "trainer/Policy Loss                                   -10.0429\n",
      "trainer/Q1 Predictions Mean                            17.1754\n",
      "trainer/Q1 Predictions Std                              1.79186\n",
      "trainer/Q1 Predictions Max                             19.2286\n",
      "trainer/Q1 Predictions Min                              1.51452\n",
      "trainer/Q2 Predictions Mean                            17.1626\n",
      "trainer/Q2 Predictions Std                              1.66841\n",
      "trainer/Q2 Predictions Max                             19.0192\n",
      "trainer/Q2 Predictions Min                              5.02695\n",
      "trainer/Q Targets Mean                                 17.2825\n",
      "trainer/Q Targets Std                                   1.76339\n",
      "trainer/Q Targets Max                                  19.1733\n",
      "trainer/Q Targets Min                                  -0.137529\n",
      "trainer/Log Pis Mean                                    7.4623\n",
      "trainer/Log Pis Std                                     2.5523\n",
      "trainer/Log Pis Max                                    16.3442\n",
      "trainer/Log Pis Min                                    -0.67244\n",
      "trainer/Policy mu Mean                                  0.0496733\n",
      "trainer/Policy mu Std                                   0.164856\n",
      "trainer/Policy mu Max                                   1.46974\n",
      "trainer/Policy mu Min                                  -0.62692\n",
      "trainer/Policy log std Mean                            -2.34061\n",
      "trainer/Policy log std Std                              0.25133\n",
      "trainer/Policy log std Max                             -1.43361\n",
      "trainer/Policy log std Min                             -3.64688\n",
      "trainer/Alpha                                           0.0104615\n",
      "trainer/Alpha Loss                                     -2.45183\n",
      "exploration/num steps total                         51000\n",
      "exploration/num paths total                           123\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.916205\n",
      "exploration/Rewards Std                                 0.0940235\n",
      "exploration/Rewards Max                                 1.54702\n",
      "exploration/Rewards Min                                 0.584039\n",
      "exploration/Returns Mean                              916.205\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.205\n",
      "exploration/Returns Min                               916.205\n",
      "exploration/Actions Mean                                0.0726337\n",
      "exploration/Actions Std                                 0.149082\n",
      "exploration/Actions Max                                 0.747161\n",
      "exploration/Actions Min                                -0.500024\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.205\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0194225\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0194225\n",
      "exploration/env_infos/final/reward_forward Min         -0.0194225\n",
      "exploration/env_infos/initial/reward_forward Mean       0.197661\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.197661\n",
      "exploration/env_infos/initial/reward_forward Min        0.197661\n",
      "exploration/env_infos/reward_forward Mean              -0.000729219\n",
      "exploration/env_infos/reward_forward Std                0.129337\n",
      "exploration/env_infos/reward_forward Max                1.19568\n",
      "exploration/env_infos/reward_forward Min               -0.392556\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0375283\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0375283\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0375283\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.147615\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.147615\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.147615\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.110005\n",
      "exploration/env_infos/reward_ctrl Std                   0.0469414\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0161336\n",
      "exploration/env_infos/reward_ctrl Min                  -0.415961\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00155812\n",
      "exploration/env_infos/final/torso_velocity Std          0.0225507\n",
      "exploration/env_infos/final/torso_velocity Max          0.030253\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0194225\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.201353\n",
      "exploration/env_infos/initial/torso_velocity Std        0.28187\n",
      "exploration/env_infos/initial/torso_velocity Max        0.548403\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.142006\n",
      "exploration/env_infos/torso_velocity Mean               0.00412355\n",
      "exploration/env_infos/torso_velocity Std                0.130166\n",
      "exploration/env_infos/torso_velocity Max                1.19568\n",
      "exploration/env_infos/torso_velocity Min               -1.27637\n",
      "evaluation/num steps total                              1.25e+06\n",
      "evaluation/num paths total                           1250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.918729\n",
      "evaluation/Rewards Std                                  0.0304536\n",
      "evaluation/Rewards Max                                  1.96879\n",
      "evaluation/Rewards Min                                  0.170206\n",
      "evaluation/Returns Mean                               918.729\n",
      "evaluation/Returns Std                                 22.8808\n",
      "evaluation/Returns Max                                947.268\n",
      "evaluation/Returns Min                                858.289\n",
      "evaluation/Actions Mean                                 0.0490488\n",
      "evaluation/Actions Std                                  0.134343\n",
      "evaluation/Actions Max                                  0.76142\n",
      "evaluation/Actions Min                                 -0.64659\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            918.729\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.19482e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.75633e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.36936e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.23489e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0172702\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139863\n",
      "evaluation/env_infos/initial/reward_forward Max         0.29503\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.303678\n",
      "evaluation/env_infos/reward_forward Mean               -0.0017055\n",
      "evaluation/env_infos/reward_forward Std                 0.0531184\n",
      "evaluation/env_infos/reward_forward Max                 1.11621\n",
      "evaluation/env_infos/reward_forward Min                -1.2933\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0808676\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0230455\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.052355\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.141465\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0602891\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0224745\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0286761\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.130931\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0818151\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0268441\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0156377\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.829794\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.53902e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           4.02939e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.01132e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.90985e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134907\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.250132\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.656874\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.321807\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00284756\n",
      "evaluation/env_infos/torso_velocity Std                 0.0638944\n",
      "evaluation/env_infos/torso_velocity Max                 1.12099\n",
      "evaluation/env_infos/torso_velocity Min                -1.84901\n",
      "time/data storing (s)                                   0.0292146\n",
      "time/evaluation sampling (s)                           44.8356\n",
      "time/exploration sampling (s)                           1.98994\n",
      "time/logging (s)                                        0.273289\n",
      "time/saving (s)                                         0.0249223\n",
      "time/training (s)                                       4.06244\n",
      "time/epoch (s)                                         51.2154\n",
      "time/total (s)                                       2678.54\n",
      "Epoch                                                  49\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:40:21.312217 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 50 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 104000\n",
      "trainer/QF1 Loss                                        0.273662\n",
      "trainer/QF2 Loss                                        0.212179\n",
      "trainer/Policy Loss                                   -10.5463\n",
      "trainer/Q1 Predictions Mean                            17.4704\n",
      "trainer/Q1 Predictions Std                              1.72291\n",
      "trainer/Q1 Predictions Max                             19.3828\n",
      "trainer/Q1 Predictions Min                              1.7985\n",
      "trainer/Q2 Predictions Mean                            17.7125\n",
      "trainer/Q2 Predictions Std                              1.50802\n",
      "trainer/Q2 Predictions Max                             19.5433\n",
      "trainer/Q2 Predictions Min                              5.68209\n",
      "trainer/Q Targets Mean                                 17.6766\n",
      "trainer/Q Targets Std                                   1.54703\n",
      "trainer/Q Targets Max                                  19.3641\n",
      "trainer/Q Targets Min                                   5.8035\n",
      "trainer/Log Pis Mean                                    7.24608\n",
      "trainer/Log Pis Std                                     2.43919\n",
      "trainer/Log Pis Max                                    13.5603\n",
      "trainer/Log Pis Min                                    -2.65153\n",
      "trainer/Policy mu Mean                                  0.00989278\n",
      "trainer/Policy mu Std                                   0.175361\n",
      "trainer/Policy mu Max                                   2.26179\n",
      "trainer/Policy mu Min                                  -1.07595\n",
      "trainer/Policy log std Mean                            -2.28096\n",
      "trainer/Policy log std Std                              0.220587\n",
      "trainer/Policy log std Max                             -0.75702\n",
      "trainer/Policy log std Min                             -3.1818\n",
      "trainer/Alpha                                           0.0102743\n",
      "trainer/Alpha Loss                                     -3.45106\n",
      "exploration/num steps total                         52000\n",
      "exploration/num paths total                           124\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.904763\n",
      "exploration/Rewards Std                                 0.0605144\n",
      "exploration/Rewards Max                                 1.2983\n",
      "exploration/Rewards Min                                 0.700646\n",
      "exploration/Returns Mean                              904.763\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               904.763\n",
      "exploration/Returns Min                               904.763\n",
      "exploration/Actions Mean                               -0.00275681\n",
      "exploration/Actions Std                                 0.162059\n",
      "exploration/Actions Max                                 0.530572\n",
      "exploration/Actions Min                                -0.612118\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           904.763\n",
      "exploration/env_infos/final/reward_forward Mean         0.12112\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.12112\n",
      "exploration/env_infos/final/reward_forward Min          0.12112\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0324715\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0324715\n",
      "exploration/env_infos/initial/reward_forward Min        0.0324715\n",
      "exploration/env_infos/reward_forward Mean               0.0172777\n",
      "exploration/env_infos/reward_forward Std                0.166267\n",
      "exploration/env_infos/reward_forward Max                0.815625\n",
      "exploration/env_infos/reward_forward Min               -0.622527\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.118954\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.118954\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.118954\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.094928\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.094928\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.094928\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.105083\n",
      "exploration/env_infos/reward_ctrl Std                   0.0437647\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0124636\n",
      "exploration/env_infos/reward_ctrl Min                  -0.425202\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0161804\n",
      "exploration/env_infos/final/torso_velocity Std          0.0974116\n",
      "exploration/env_infos/final/torso_velocity Max          0.12112\n",
      "exploration/env_infos/final/torso_velocity Min         -0.094581\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.152551\n",
      "exploration/env_infos/initial/torso_velocity Std        0.209717\n",
      "exploration/env_infos/initial/torso_velocity Max        0.447447\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0222645\n",
      "exploration/env_infos/torso_velocity Mean              -0.00310406\n",
      "exploration/env_infos/torso_velocity Std                0.156415\n",
      "exploration/env_infos/torso_velocity Max                0.815625\n",
      "exploration/env_infos/torso_velocity Min               -1.61903\n",
      "evaluation/num steps total                              1.275e+06\n",
      "evaluation/num paths total                           1275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.930334\n",
      "evaluation/Rewards Std                                  0.0357446\n",
      "evaluation/Rewards Max                                  2.40365\n",
      "evaluation/Rewards Min                                  0.562589\n",
      "evaluation/Returns Mean                               930.334\n",
      "evaluation/Returns Std                                 22.2377\n",
      "evaluation/Returns Max                                957.86\n",
      "evaluation/Returns Min                                871.655\n",
      "evaluation/Actions Mean                                 0.0217622\n",
      "evaluation/Actions Std                                  0.130954\n",
      "evaluation/Actions Max                                  0.605001\n",
      "evaluation/Actions Min                                 -0.560685\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            930.334\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.88134e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.00044e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.8916e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.77675e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0347239\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132723\n",
      "evaluation/env_infos/initial/reward_forward Max         0.186872\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.307012\n",
      "evaluation/env_infos/reward_forward Mean               -0.00483013\n",
      "evaluation/env_infos/reward_forward Std                 0.0694858\n",
      "evaluation/env_infos/reward_forward Max                 0.955049\n",
      "evaluation/env_infos/reward_forward Min                -1.81804\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.071301\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0224822\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0478668\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.129998\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0684594\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0116928\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0491293\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0967883\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0704906\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0250058\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0123407\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.437411\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -8.94001e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.78319e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.8916e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.81559e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.123374\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.247159\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.707549\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.307012\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00267551\n",
      "evaluation/env_infos/torso_velocity Std                 0.0701308\n",
      "evaluation/env_infos/torso_velocity Max                 1.50425\n",
      "evaluation/env_infos/torso_velocity Min                -1.94929\n",
      "time/data storing (s)                                   0.031163\n",
      "time/evaluation sampling (s)                           46.4028\n",
      "time/exploration sampling (s)                           1.94213\n",
      "time/logging (s)                                        0.285599\n",
      "time/saving (s)                                         0.0544804\n",
      "time/training (s)                                       4.28555\n",
      "time/epoch (s)                                         53.0017\n",
      "time/total (s)                                       2732.04\n",
      "Epoch                                                  50\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:41:13.342956 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 51 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 106000\n",
      "trainer/QF1 Loss                                        0.410492\n",
      "trainer/QF2 Loss                                        0.474066\n",
      "trainer/Policy Loss                                   -10.9767\n",
      "trainer/Q1 Predictions Mean                            18.1023\n",
      "trainer/Q1 Predictions Std                              1.70157\n",
      "trainer/Q1 Predictions Max                             19.7005\n",
      "trainer/Q1 Predictions Min                             -0.848166\n",
      "trainer/Q2 Predictions Mean                            18.2013\n",
      "trainer/Q2 Predictions Std                              1.81301\n",
      "trainer/Q2 Predictions Max                             19.9084\n",
      "trainer/Q2 Predictions Min                             -3.27046\n",
      "trainer/Q Targets Mean                                 18.1068\n",
      "trainer/Q Targets Std                                   1.7038\n",
      "trainer/Q Targets Max                                  19.8944\n",
      "trainer/Q Targets Min                                   1.28237\n",
      "trainer/Log Pis Mean                                    7.33217\n",
      "trainer/Log Pis Std                                     2.30201\n",
      "trainer/Log Pis Max                                    16.8389\n",
      "trainer/Log Pis Min                                     0.231531\n",
      "trainer/Policy mu Mean                                 -0.044518\n",
      "trainer/Policy mu Std                                   0.155113\n",
      "trainer/Policy mu Max                                   2.82818\n",
      "trainer/Policy mu Min                                  -0.884313\n",
      "trainer/Policy log std Mean                            -2.28911\n",
      "trainer/Policy log std Std                              0.217009\n",
      "trainer/Policy log std Max                             -1.21452\n",
      "trainer/Policy log std Min                             -3.28758\n",
      "trainer/Alpha                                           0.00994492\n",
      "trainer/Alpha Loss                                     -3.0786\n",
      "exploration/num steps total                         53000\n",
      "exploration/num paths total                           125\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.945859\n",
      "exploration/Rewards Std                                 0.100291\n",
      "exploration/Rewards Max                                 2.01143\n",
      "exploration/Rewards Min                                 0.754554\n",
      "exploration/Returns Mean                              945.859\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               945.859\n",
      "exploration/Returns Min                               945.859\n",
      "exploration/Actions Mean                               -0.0445536\n",
      "exploration/Actions Std                                 0.131808\n",
      "exploration/Actions Max                                 0.56935\n",
      "exploration/Actions Min                                -0.592067\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           945.859\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0356907\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0356907\n",
      "exploration/env_infos/final/reward_forward Min         -0.0356907\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.175585\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.175585\n",
      "exploration/env_infos/initial/reward_forward Min       -0.175585\n",
      "exploration/env_infos/reward_forward Mean              -0.0301951\n",
      "exploration/env_infos/reward_forward Std                0.243018\n",
      "exploration/env_infos/reward_forward Max                0.831164\n",
      "exploration/env_infos/reward_forward Min               -1.428\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.105337\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.105337\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.105337\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.130941\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.130941\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.130941\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0774336\n",
      "exploration/env_infos/reward_ctrl Std                   0.0369371\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0126614\n",
      "exploration/env_infos/reward_ctrl Min                  -0.245446\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00165289\n",
      "exploration/env_infos/final/torso_velocity Std          0.0598318\n",
      "exploration/env_infos/final/torso_velocity Max          0.0824543\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0517222\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0902955\n",
      "exploration/env_infos/initial/torso_velocity Std        0.221083\n",
      "exploration/env_infos/initial/torso_velocity Max        0.365705\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.175585\n",
      "exploration/env_infos/torso_velocity Mean              -0.0089077\n",
      "exploration/env_infos/torso_velocity Std                0.25113\n",
      "exploration/env_infos/torso_velocity Max                1.1474\n",
      "exploration/env_infos/torso_velocity Min               -1.428\n",
      "evaluation/num steps total                              1.3e+06\n",
      "evaluation/num paths total                           1300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.961462\n",
      "evaluation/Rewards Std                                  0.0354176\n",
      "evaluation/Rewards Max                                  2.06354\n",
      "evaluation/Rewards Min                                  0.483529\n",
      "evaluation/Returns Mean                               961.462\n",
      "evaluation/Returns Std                                 15.3427\n",
      "evaluation/Returns Max                                986.08\n",
      "evaluation/Returns Min                                923.51\n",
      "evaluation/Actions Mean                                -0.0411851\n",
      "evaluation/Actions Std                                  0.091531\n",
      "evaluation/Actions Max                                  0.810127\n",
      "evaluation/Actions Min                                 -0.5194\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            961.462\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.00567e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.49541e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.86317e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.33852e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0093801\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118955\n",
      "evaluation/env_infos/initial/reward_forward Max         0.273958\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.178553\n",
      "evaluation/env_infos/reward_forward Mean               -0.001718\n",
      "evaluation/env_infos/reward_forward Std                 0.0708371\n",
      "evaluation/env_infos/reward_forward Max                 1.0252\n",
      "evaluation/env_infos/reward_forward Min                -1.52244\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0398039\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0149575\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0182722\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0791302\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0512825\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.017475\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0199258\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.092437\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0402965\n",
      "evaluation/env_infos/reward_ctrl Std                    0.01803\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00652911\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.516471\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.47132e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           3.23874e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.62982e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.09759e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.161381\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242005\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.629708\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.236628\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00211154\n",
      "evaluation/env_infos/torso_velocity Std                 0.0734494\n",
      "evaluation/env_infos/torso_velocity Max                 1.15921\n",
      "evaluation/env_infos/torso_velocity Min                -1.89089\n",
      "time/data storing (s)                                   0.0339389\n",
      "time/evaluation sampling (s)                           44.9727\n",
      "time/exploration sampling (s)                           1.87483\n",
      "time/logging (s)                                        0.275357\n",
      "time/saving (s)                                         0.0253433\n",
      "time/training (s)                                       4.29418\n",
      "time/epoch (s)                                         51.4763\n",
      "time/total (s)                                       2784.06\n",
      "Epoch                                                  51\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:42:06.648921 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 52 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 108000\n",
      "trainer/QF1 Loss                                        0.230048\n",
      "trainer/QF2 Loss                                        0.248498\n",
      "trainer/Policy Loss                                   -10.8228\n",
      "trainer/Q1 Predictions Mean                            18.4051\n",
      "trainer/Q1 Predictions Std                              1.48639\n",
      "trainer/Q1 Predictions Max                             20.1528\n",
      "trainer/Q1 Predictions Min                             12.8741\n",
      "trainer/Q2 Predictions Mean                            18.2842\n",
      "trainer/Q2 Predictions Std                              1.45369\n",
      "trainer/Q2 Predictions Max                             20.2212\n",
      "trainer/Q2 Predictions Min                             13.3589\n",
      "trainer/Q Targets Mean                                 18.4333\n",
      "trainer/Q Targets Std                                   1.43012\n",
      "trainer/Q Targets Max                                  20.6381\n",
      "trainer/Q Targets Min                                  12.7033\n",
      "trainer/Log Pis Mean                                    7.75122\n",
      "trainer/Log Pis Std                                     2.72856\n",
      "trainer/Log Pis Max                                    18.2678\n",
      "trainer/Log Pis Min                                    -4.64122\n",
      "trainer/Policy mu Mean                                 -0.0217182\n",
      "trainer/Policy mu Std                                   0.145674\n",
      "trainer/Policy mu Max                                   0.858984\n",
      "trainer/Policy mu Min                                  -0.606759\n",
      "trainer/Policy log std Mean                            -2.38142\n",
      "trainer/Policy log std Std                              0.223676\n",
      "trainer/Policy log std Max                             -1.41087\n",
      "trainer/Policy log std Min                             -3.94813\n",
      "trainer/Alpha                                           0.00964137\n",
      "trainer/Alpha Loss                                     -1.15485\n",
      "exploration/num steps total                         54000\n",
      "exploration/num paths total                           126\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.91208\n",
      "exploration/Rewards Std                                 0.0503604\n",
      "exploration/Rewards Max                                 1.20624\n",
      "exploration/Rewards Min                                 0.623123\n",
      "exploration/Returns Mean                              912.08\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               912.08\n",
      "exploration/Returns Min                               912.08\n",
      "exploration/Actions Mean                               -0.050705\n",
      "exploration/Actions Std                                 0.144432\n",
      "exploration/Actions Max                                 0.611281\n",
      "exploration/Actions Min                                -0.523522\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           912.08\n",
      "exploration/env_infos/final/reward_forward Mean        -0.144264\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.144264\n",
      "exploration/env_infos/final/reward_forward Min         -0.144264\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0306045\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0306045\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0306045\n",
      "exploration/env_infos/reward_forward Mean              -0.00265291\n",
      "exploration/env_infos/reward_forward Std                0.110801\n",
      "exploration/env_infos/reward_forward Max                0.568776\n",
      "exploration/env_infos/reward_forward Min               -0.374562\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.102714\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.102714\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.102714\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0401075\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0401075\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0401075\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0937263\n",
      "exploration/env_infos/reward_ctrl Std                   0.0393072\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0116615\n",
      "exploration/env_infos/reward_ctrl Min                  -0.376877\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00510518\n",
      "exploration/env_infos/final/torso_velocity Std          0.121621\n",
      "exploration/env_infos/final/torso_velocity Max          0.152017\n",
      "exploration/env_infos/final/torso_velocity Min         -0.144264\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.181055\n",
      "exploration/env_infos/initial/torso_velocity Std        0.251134\n",
      "exploration/env_infos/initial/torso_velocity Max        0.533872\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0306045\n",
      "exploration/env_infos/torso_velocity Mean               0.00301053\n",
      "exploration/env_infos/torso_velocity Std                0.13166\n",
      "exploration/env_infos/torso_velocity Max                0.568776\n",
      "exploration/env_infos/torso_velocity Min               -1.80763\n",
      "evaluation/num steps total                              1.325e+06\n",
      "evaluation/num paths total                           1325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.9409\n",
      "evaluation/Rewards Std                                  0.0241416\n",
      "evaluation/Rewards Max                                  2.20765\n",
      "evaluation/Rewards Min                                  0.374042\n",
      "evaluation/Returns Mean                               940.9\n",
      "evaluation/Returns Std                                  8.67098\n",
      "evaluation/Returns Max                                958.813\n",
      "evaluation/Returns Min                                927.482\n",
      "evaluation/Actions Mean                                -0.0345044\n",
      "evaluation/Actions Std                                  0.117193\n",
      "evaluation/Actions Max                                  0.698881\n",
      "evaluation/Actions Min                                 -0.497667\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            940.9\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.02283e-06\n",
      "evaluation/env_infos/final/reward_forward Std           4.13123e-06\n",
      "evaluation/env_infos/final/reward_forward Max           1.08819e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -2.10641e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00473902\n",
      "evaluation/env_infos/initial/reward_forward Std         0.146901\n",
      "evaluation/env_infos/initial/reward_forward Max         0.27046\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.319346\n",
      "evaluation/env_infos/reward_forward Mean               -0.00278826\n",
      "evaluation/env_infos/reward_forward Std                 0.051812\n",
      "evaluation/env_infos/reward_forward Max                 0.978244\n",
      "evaluation/env_infos/reward_forward Min                -1.69007\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.058971\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00852386\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0402703\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0726289\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0432413\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0130066\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0268702\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.078561\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0596987\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0174339\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.011493\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.625958\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.06236e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.2674e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.8497e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.10641e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.124894\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228943\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.59205\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.319346\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00287676\n",
      "evaluation/env_infos/torso_velocity Std                 0.0655356\n",
      "evaluation/env_infos/torso_velocity Max                 1.48378\n",
      "evaluation/env_infos/torso_velocity Min                -2.1573\n",
      "time/data storing (s)                                   0.0308059\n",
      "time/evaluation sampling (s)                           46.0123\n",
      "time/exploration sampling (s)                           2.068\n",
      "time/logging (s)                                        0.293449\n",
      "time/saving (s)                                         0.0287153\n",
      "time/training (s)                                       4.38226\n",
      "time/epoch (s)                                         52.8156\n",
      "time/total (s)                                       2837.39\n",
      "Epoch                                                  52\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:42:59.171071 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 53 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 110000\n",
      "trainer/QF1 Loss                                        0.376937\n",
      "trainer/QF2 Loss                                        0.357325\n",
      "trainer/Policy Loss                                   -11.4768\n",
      "trainer/Q1 Predictions Mean                            18.8371\n",
      "trainer/Q1 Predictions Std                              1.35094\n",
      "trainer/Q1 Predictions Max                             20.8996\n",
      "trainer/Q1 Predictions Min                             12.6721\n",
      "trainer/Q2 Predictions Mean                            18.6759\n",
      "trainer/Q2 Predictions Std                              1.36129\n",
      "trainer/Q2 Predictions Max                             20.6672\n",
      "trainer/Q2 Predictions Min                             12.3562\n",
      "trainer/Q Targets Mean                                 18.7004\n",
      "trainer/Q Targets Std                                   1.4872\n",
      "trainer/Q Targets Max                                  20.5047\n",
      "trainer/Q Targets Min                                  11.0981\n",
      "trainer/Log Pis Mean                                    7.40375\n",
      "trainer/Log Pis Std                                     2.60516\n",
      "trainer/Log Pis Max                                    15.2316\n",
      "trainer/Log Pis Min                                     0.601359\n",
      "trainer/Policy mu Mean                                  0.0274897\n",
      "trainer/Policy mu Std                                   0.142533\n",
      "trainer/Policy mu Max                                   0.650547\n",
      "trainer/Policy mu Min                                  -0.90586\n",
      "trainer/Policy log std Mean                            -2.32544\n",
      "trainer/Policy log std Std                              0.241024\n",
      "trainer/Policy log std Max                             -1.42012\n",
      "trainer/Policy log std Min                             -3.46448\n",
      "trainer/Alpha                                           0.00941932\n",
      "trainer/Alpha Loss                                     -2.78141\n",
      "exploration/num steps total                         55000\n",
      "exploration/num paths total                           127\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.905627\n",
      "exploration/Rewards Std                                 0.0585934\n",
      "exploration/Rewards Max                                 1.5343\n",
      "exploration/Rewards Min                                 0.691565\n",
      "exploration/Returns Mean                              905.627\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               905.627\n",
      "exploration/Returns Min                               905.627\n",
      "exploration/Actions Mean                                0.0428136\n",
      "exploration/Actions Std                                 0.152489\n",
      "exploration/Actions Max                                 0.557664\n",
      "exploration/Actions Min                                -0.534517\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           905.627\n",
      "exploration/env_infos/final/reward_forward Mean         0.118563\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.118563\n",
      "exploration/env_infos/final/reward_forward Min          0.118563\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0010873\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0010873\n",
      "exploration/env_infos/initial/reward_forward Min        0.0010873\n",
      "exploration/env_infos/reward_forward Mean              -0.0147607\n",
      "exploration/env_infos/reward_forward Std                0.139389\n",
      "exploration/env_infos/reward_forward Max                0.566539\n",
      "exploration/env_infos/reward_forward Min               -1.15091\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0937884\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0937884\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0937884\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0570374\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0570374\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0570374\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.100344\n",
      "exploration/env_infos/reward_ctrl Std                   0.0441745\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0122455\n",
      "exploration/env_infos/reward_ctrl Min                  -0.308435\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0244819\n",
      "exploration/env_infos/final/torso_velocity Std          0.113774\n",
      "exploration/env_infos/final/torso_velocity Max          0.118563\n",
      "exploration/env_infos/final/torso_velocity Min         -0.1356\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.091354\n",
      "exploration/env_infos/initial/torso_velocity Std        0.324832\n",
      "exploration/env_infos/initial/torso_velocity Max        0.526568\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.253593\n",
      "exploration/env_infos/torso_velocity Mean              -0.010256\n",
      "exploration/env_infos/torso_velocity Std                0.140331\n",
      "exploration/env_infos/torso_velocity Max                0.703951\n",
      "exploration/env_infos/torso_velocity Min               -1.15091\n",
      "evaluation/num steps total                              1.35e+06\n",
      "evaluation/num paths total                           1350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.917319\n",
      "evaluation/Rewards Std                                  0.0351085\n",
      "evaluation/Rewards Max                                  2.31169\n",
      "evaluation/Rewards Min                                  0.449304\n",
      "evaluation/Returns Mean                               917.319\n",
      "evaluation/Returns Std                                 22.7761\n",
      "evaluation/Returns Max                                958.295\n",
      "evaluation/Returns Min                                869.793\n",
      "evaluation/Actions Mean                                 0.0498424\n",
      "evaluation/Actions Std                                  0.135678\n",
      "evaluation/Actions Max                                  0.689218\n",
      "evaluation/Actions Min                                 -0.522778\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            917.319\n",
      "evaluation/env_infos/final/reward_forward Mean          2.34687e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000114773\n",
      "evaluation/env_infos/final/reward_forward Max           0.000585735\n",
      "evaluation/env_infos/final/reward_forward Min          -7.12004e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0268343\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14512\n",
      "evaluation/env_infos/initial/reward_forward Max         0.262823\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.327905\n",
      "evaluation/env_infos/reward_forward Mean               -0.00274341\n",
      "evaluation/env_infos/reward_forward Std                 0.0626129\n",
      "evaluation/env_infos/reward_forward Max                 1.2885\n",
      "evaluation/env_infos/reward_forward Min                -1.84986\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0833037\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.023348\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0461144\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.131298\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.040877\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00936182\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0242269\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0696265\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0835717\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0255924\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00807876\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.550696\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.04379e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000111379\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000766064\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.85554e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.112311\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.229444\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.599068\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.327905\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00151949\n",
      "evaluation/env_infos/torso_velocity Std                 0.0633429\n",
      "evaluation/env_infos/torso_velocity Max                 1.37382\n",
      "evaluation/env_infos/torso_velocity Min                -1.98639\n",
      "time/data storing (s)                                   0.029801\n",
      "time/evaluation sampling (s)                           45.3432\n",
      "time/exploration sampling (s)                           1.98281\n",
      "time/logging (s)                                        0.278726\n",
      "time/saving (s)                                         0.02634\n",
      "time/training (s)                                       4.2683\n",
      "time/epoch (s)                                         51.9291\n",
      "time/total (s)                                       2889.89\n",
      "Epoch                                                  53\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:43:51.863173 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 54 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 112000\n",
      "trainer/QF1 Loss                                        1.10932\n",
      "trainer/QF2 Loss                                        1.16932\n",
      "trainer/Policy Loss                                   -10.8857\n",
      "trainer/Q1 Predictions Mean                            18.8109\n",
      "trainer/Q1 Predictions Std                              2.22176\n",
      "trainer/Q1 Predictions Max                             20.7956\n",
      "trainer/Q1 Predictions Min                             -1.17641\n",
      "trainer/Q2 Predictions Mean                            18.8815\n",
      "trainer/Q2 Predictions Std                              1.97697\n",
      "trainer/Q2 Predictions Max                             20.7809\n",
      "trainer/Q2 Predictions Min                              1.47552\n",
      "trainer/Q Targets Mean                                 18.8264\n",
      "trainer/Q Targets Std                                   2.45925\n",
      "trainer/Q Targets Max                                  20.7855\n",
      "trainer/Q Targets Min                                  -0.589588\n",
      "trainer/Log Pis Mean                                    8.20603\n",
      "trainer/Log Pis Std                                     2.63641\n",
      "trainer/Log Pis Max                                    18.1658\n",
      "trainer/Log Pis Min                                     0.0701592\n",
      "trainer/Policy mu Mean                                  0.00355651\n",
      "trainer/Policy mu Std                                   0.208854\n",
      "trainer/Policy mu Max                                   1.83533\n",
      "trainer/Policy mu Min                                  -1.89728\n",
      "trainer/Policy log std Mean                            -2.38186\n",
      "trainer/Policy log std Std                              0.26919\n",
      "trainer/Policy log std Max                             -0.368779\n",
      "trainer/Policy log std Min                             -3.93153\n",
      "trainer/Alpha                                           0.00898905\n",
      "trainer/Alpha Loss                                      0.970809\n",
      "exploration/num steps total                         56000\n",
      "exploration/num paths total                           128\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.873735\n",
      "exploration/Rewards Std                                 0.0401489\n",
      "exploration/Rewards Max                                 0.961581\n",
      "exploration/Rewards Min                                 0.587407\n",
      "exploration/Returns Mean                              873.735\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               873.735\n",
      "exploration/Returns Min                               873.735\n",
      "exploration/Actions Mean                               -0.0207653\n",
      "exploration/Actions Std                                 0.176494\n",
      "exploration/Actions Max                                 0.560216\n",
      "exploration/Actions Min                                -0.471241\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           873.735\n",
      "exploration/env_infos/final/reward_forward Mean         0.0319787\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0319787\n",
      "exploration/env_infos/final/reward_forward Min          0.0319787\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.264591\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.264591\n",
      "exploration/env_infos/initial/reward_forward Min       -0.264591\n",
      "exploration/env_infos/reward_forward Mean              -0.00325844\n",
      "exploration/env_infos/reward_forward Std                0.0398099\n",
      "exploration/env_infos/reward_forward Max                0.355247\n",
      "exploration/env_infos/reward_forward Min               -0.36473\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.118613\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.118613\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.118613\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0737632\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0737632\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0737632\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.126325\n",
      "exploration/env_infos/reward_ctrl Std                   0.0401653\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0384194\n",
      "exploration/env_infos/reward_ctrl Min                  -0.412593\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0117099\n",
      "exploration/env_infos/final/torso_velocity Std          0.0182758\n",
      "exploration/env_infos/final/torso_velocity Max          0.0319787\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0123129\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0055921\n",
      "exploration/env_infos/initial/torso_velocity Std        0.295084\n",
      "exploration/env_infos/initial/torso_velocity Max        0.407283\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.264591\n",
      "exploration/env_infos/torso_velocity Mean              -0.00340254\n",
      "exploration/env_infos/torso_velocity Std                0.0538183\n",
      "exploration/env_infos/torso_velocity Max                0.407283\n",
      "exploration/env_infos/torso_velocity Min               -1.43427\n",
      "evaluation/num steps total                              1.375e+06\n",
      "evaluation/num paths total                           1375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.913536\n",
      "evaluation/Rewards Std                                  0.0264302\n",
      "evaluation/Rewards Max                                  1.57239\n",
      "evaluation/Rewards Min                                  0.386976\n",
      "evaluation/Returns Mean                               913.536\n",
      "evaluation/Returns Std                                 20.8818\n",
      "evaluation/Returns Max                                945.347\n",
      "evaluation/Returns Min                                830.251\n",
      "evaluation/Actions Mean                                 0.0136072\n",
      "evaluation/Actions Std                                  0.146754\n",
      "evaluation/Actions Max                                  0.627237\n",
      "evaluation/Actions Min                                 -0.677602\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            913.536\n",
      "evaluation/env_infos/final/reward_forward Mean          8.71839e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.37244e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.34689e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.78633e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0254815\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139873\n",
      "evaluation/env_infos/initial/reward_forward Max         0.235551\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.28687\n",
      "evaluation/env_infos/reward_forward Mean               -0.00310877\n",
      "evaluation/env_infos/reward_forward Std                 0.0502181\n",
      "evaluation/env_infos/reward_forward Max                 0.880201\n",
      "evaluation/env_infos/reward_forward Min                -1.32912\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0860963\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0217476\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0538195\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.173166\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0637026\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0137543\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0433962\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.088382\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0868876\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0241048\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0330132\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.613024\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.46836e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.55197e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.34689e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.58376e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138165\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22458\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.623322\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.28687\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00339309\n",
      "evaluation/env_infos/torso_velocity Std                 0.0592326\n",
      "evaluation/env_infos/torso_velocity Max                 1.0202\n",
      "evaluation/env_infos/torso_velocity Min                -2.03407\n",
      "time/data storing (s)                                   0.0523846\n",
      "time/evaluation sampling (s)                           44.406\n",
      "time/exploration sampling (s)                           2.29069\n",
      "time/logging (s)                                        0.286967\n",
      "time/saving (s)                                         0.0272087\n",
      "time/training (s)                                       5.10951\n",
      "time/epoch (s)                                         52.1728\n",
      "time/total (s)                                       2942.59\n",
      "Epoch                                                  54\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:44:44.014782 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 55 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 114000\n",
      "trainer/QF1 Loss                                        0.181493\n",
      "trainer/QF2 Loss                                        0.179383\n",
      "trainer/Policy Loss                                   -11.1582\n",
      "trainer/Q1 Predictions Mean                            19.246\n",
      "trainer/Q1 Predictions Std                              1.44772\n",
      "trainer/Q1 Predictions Max                             21.0878\n",
      "trainer/Q1 Predictions Min                             12.5187\n",
      "trainer/Q2 Predictions Mean                            19.3513\n",
      "trainer/Q2 Predictions Std                              1.49899\n",
      "trainer/Q2 Predictions Max                             21.2639\n",
      "trainer/Q2 Predictions Min                             12.0052\n",
      "trainer/Q Targets Mean                                 19.3039\n",
      "trainer/Q Targets Std                                   1.54989\n",
      "trainer/Q Targets Max                                  21.2549\n",
      "trainer/Q Targets Min                                  13.3409\n",
      "trainer/Log Pis Mean                                    8.34718\n",
      "trainer/Log Pis Std                                     2.67019\n",
      "trainer/Log Pis Max                                    17.1768\n",
      "trainer/Log Pis Min                                     1.25931\n",
      "trainer/Policy mu Mean                                 -0.00668983\n",
      "trainer/Policy mu Std                                   0.1658\n",
      "trainer/Policy mu Max                                   0.864403\n",
      "trainer/Policy mu Min                                  -0.90473\n",
      "trainer/Policy log std Mean                            -2.41574\n",
      "trainer/Policy log std Std                              0.249818\n",
      "trainer/Policy log std Max                             -1.67249\n",
      "trainer/Policy log std Min                             -4.08949\n",
      "trainer/Alpha                                           0.00844975\n",
      "trainer/Alpha Loss                                      1.65694\n",
      "exploration/num steps total                         57000\n",
      "exploration/num paths total                           129\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.901477\n",
      "exploration/Rewards Std                                 0.0559134\n",
      "exploration/Rewards Max                                 1.99599\n",
      "exploration/Rewards Min                                 0.728032\n",
      "exploration/Returns Mean                              901.477\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               901.477\n",
      "exploration/Returns Min                               901.477\n",
      "exploration/Actions Mean                                0.0156314\n",
      "exploration/Actions Std                                 0.158242\n",
      "exploration/Actions Max                                 0.461647\n",
      "exploration/Actions Min                                -0.572423\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           901.477\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00994782\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00994782\n",
      "exploration/env_infos/final/reward_forward Min         -0.00994782\n",
      "exploration/env_infos/initial/reward_forward Mean       0.115113\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.115113\n",
      "exploration/env_infos/initial/reward_forward Min        0.115113\n",
      "exploration/env_infos/reward_forward Mean               0.00529126\n",
      "exploration/env_infos/reward_forward Std                0.0758411\n",
      "exploration/env_infos/reward_forward Max                0.282963\n",
      "exploration/env_infos/reward_forward Min               -0.933935\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0729689\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0729689\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0729689\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.110996\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.110996\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.110996\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.101139\n",
      "exploration/env_infos/reward_ctrl Std                   0.035149\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0274119\n",
      "exploration/env_infos/reward_ctrl Min                  -0.271968\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0293197\n",
      "exploration/env_infos/final/torso_velocity Std          0.0220519\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00994782\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0601711\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.372212\n",
      "exploration/env_infos/initial/torso_velocity Std        0.214683\n",
      "exploration/env_infos/initial/torso_velocity Max        0.640613\n",
      "exploration/env_infos/initial/torso_velocity Min        0.115113\n",
      "exploration/env_infos/torso_velocity Mean               0.00179786\n",
      "exploration/env_infos/torso_velocity Std                0.0818967\n",
      "exploration/env_infos/torso_velocity Max                0.640613\n",
      "exploration/env_infos/torso_velocity Min               -1.30739\n",
      "evaluation/num steps total                              1.4e+06\n",
      "evaluation/num paths total                           1400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.900457\n",
      "evaluation/Rewards Std                                  0.0435017\n",
      "evaluation/Rewards Max                                  2.36998\n",
      "evaluation/Rewards Min                                  0.539304\n",
      "evaluation/Returns Mean                               900.457\n",
      "evaluation/Returns Std                                 36.7775\n",
      "evaluation/Returns Max                                960.301\n",
      "evaluation/Returns Min                                807.104\n",
      "evaluation/Actions Mean                                 0.00339215\n",
      "evaluation/Actions Std                                  0.158236\n",
      "evaluation/Actions Max                                  0.491779\n",
      "evaluation/Actions Min                                 -0.593754\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            900.457\n",
      "evaluation/env_infos/final/reward_forward Mean          8.10464e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.95308e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.18387e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.80501e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00837111\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122592\n",
      "evaluation/env_infos/initial/reward_forward Max         0.226908\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.251798\n",
      "evaluation/env_infos/reward_forward Mean               -0.00352184\n",
      "evaluation/env_infos/reward_forward Std                 0.0641894\n",
      "evaluation/env_infos/reward_forward Max                 0.796583\n",
      "evaluation/env_infos/reward_forward Min                -1.73678\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.100556\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0374696\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0391427\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.195918\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0772862\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0447413\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0382688\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.205414\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.100201\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0378979\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0244496\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.460696\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.36161e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.27504e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.18387e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.76255e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.155116\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219471\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.679299\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.251798\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00276417\n",
      "evaluation/env_infos/torso_velocity Std                 0.0600762\n",
      "evaluation/env_infos/torso_velocity Max                 1.3092\n",
      "evaluation/env_infos/torso_velocity Min                -1.73678\n",
      "time/data storing (s)                                   0.0304203\n",
      "time/evaluation sampling (s)                           44.8286\n",
      "time/exploration sampling (s)                           2.08593\n",
      "time/logging (s)                                        0.279401\n",
      "time/saving (s)                                         0.0252707\n",
      "time/training (s)                                       4.33677\n",
      "time/epoch (s)                                         51.5864\n",
      "time/total (s)                                       2994.73\n",
      "Epoch                                                  55\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:45:37.169939 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 56 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 116000\n",
      "trainer/QF1 Loss                                        0.320752\n",
      "trainer/QF2 Loss                                        0.304313\n",
      "trainer/Policy Loss                                   -12.0891\n",
      "trainer/Q1 Predictions Mean                            19.7511\n",
      "trainer/Q1 Predictions Std                              1.42082\n",
      "trainer/Q1 Predictions Max                             21.7869\n",
      "trainer/Q1 Predictions Min                             13.9709\n",
      "trainer/Q2 Predictions Mean                            19.7095\n",
      "trainer/Q2 Predictions Std                              1.53026\n",
      "trainer/Q2 Predictions Max                             21.7806\n",
      "trainer/Q2 Predictions Min                             13.7457\n",
      "trainer/Q Targets Mean                                 19.7045\n",
      "trainer/Q Targets Std                                   1.5788\n",
      "trainer/Q Targets Max                                  22.6731\n",
      "trainer/Q Targets Min                                  12.8179\n",
      "trainer/Log Pis Mean                                    7.81353\n",
      "trainer/Log Pis Std                                     3.0462\n",
      "trainer/Log Pis Max                                    17.3588\n",
      "trainer/Log Pis Min                                    -3.44814\n",
      "trainer/Policy mu Mean                                 -0.0015502\n",
      "trainer/Policy mu Std                                   0.159095\n",
      "trainer/Policy mu Max                                   1.12943\n",
      "trainer/Policy mu Min                                  -0.912416\n",
      "trainer/Policy log std Mean                            -2.39473\n",
      "trainer/Policy log std Std                              0.277997\n",
      "trainer/Policy log std Max                             -1.40653\n",
      "trainer/Policy log std Min                             -3.88601\n",
      "trainer/Alpha                                           0.00893253\n",
      "trainer/Alpha Loss                                     -0.879876\n",
      "exploration/num steps total                         58000\n",
      "exploration/num paths total                           130\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.842086\n",
      "exploration/Rewards Std                                 0.0717235\n",
      "exploration/Rewards Max                                 1.29089\n",
      "exploration/Rewards Min                                 0.549569\n",
      "exploration/Returns Mean                              842.086\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               842.086\n",
      "exploration/Returns Min                               842.086\n",
      "exploration/Actions Mean                                0.0172001\n",
      "exploration/Actions Std                                 0.202835\n",
      "exploration/Actions Max                                 0.647012\n",
      "exploration/Actions Min                                -0.596352\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           842.086\n",
      "exploration/env_infos/final/reward_forward Mean         0.0714487\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0714487\n",
      "exploration/env_infos/final/reward_forward Min          0.0714487\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0265896\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0265896\n",
      "exploration/env_infos/initial/reward_forward Min        0.0265896\n",
      "exploration/env_infos/reward_forward Mean               0.0150007\n",
      "exploration/env_infos/reward_forward Std                0.107357\n",
      "exploration/env_infos/reward_forward Max                0.515258\n",
      "exploration/env_infos/reward_forward Min               -0.673297\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.203286\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.203286\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.203286\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.1078\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.1078\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.1078\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.165751\n",
      "exploration/env_infos/reward_ctrl Std                   0.0589569\n",
      "exploration/env_infos/reward_ctrl Max                  -0.024909\n",
      "exploration/env_infos/reward_ctrl Min                  -0.450431\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0614206\n",
      "exploration/env_infos/final/torso_velocity Std          0.050572\n",
      "exploration/env_infos/final/torso_velocity Max          0.117732\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00491934\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.151556\n",
      "exploration/env_infos/initial/torso_velocity Std        0.201827\n",
      "exploration/env_infos/initial/torso_velocity Max        0.436276\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0081964\n",
      "exploration/env_infos/torso_velocity Mean               0.00117123\n",
      "exploration/env_infos/torso_velocity Std                0.141599\n",
      "exploration/env_infos/torso_velocity Max                0.689932\n",
      "exploration/env_infos/torso_velocity Min               -1.15999\n",
      "evaluation/num steps total                              1.425e+06\n",
      "evaluation/num paths total                           1425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.854556\n",
      "evaluation/Rewards Std                                  0.0405135\n",
      "evaluation/Rewards Max                                  2.72108\n",
      "evaluation/Rewards Min                                  0.702464\n",
      "evaluation/Returns Mean                               854.556\n",
      "evaluation/Returns Std                                 22.2738\n",
      "evaluation/Returns Max                                894.92\n",
      "evaluation/Returns Min                                807.989\n",
      "evaluation/Actions Mean                                 0.0169961\n",
      "evaluation/Actions Std                                  0.190773\n",
      "evaluation/Actions Max                                  0.4364\n",
      "evaluation/Actions Min                                 -0.517812\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            854.556\n",
      "evaluation/env_infos/final/reward_forward Mean          2.88513e-06\n",
      "evaluation/env_infos/final/reward_forward Std           7.3226e-06\n",
      "evaluation/env_infos/final/reward_forward Max           2.85435e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -8.94591e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0246085\n",
      "evaluation/env_infos/initial/reward_forward Std         0.120543\n",
      "evaluation/env_infos/initial/reward_forward Max         0.184931\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.374734\n",
      "evaluation/env_infos/reward_forward Mean                0.00387468\n",
      "evaluation/env_infos/reward_forward Std                 0.0668937\n",
      "evaluation/env_infos/reward_forward Max                 1.07097\n",
      "evaluation/env_infos/reward_forward Min                -1.1125\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.147042\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0230699\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.102373\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.194113\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0473199\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0308294\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0176379\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.123654\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.146733\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0248023\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0176379\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.297536\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.6085e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           9.95002e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           4.44517e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.94591e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127579\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227145\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.577598\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.374734\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00115685\n",
      "evaluation/env_infos/torso_velocity Std                 0.0728339\n",
      "evaluation/env_infos/torso_velocity Max                 1.53692\n",
      "evaluation/env_infos/torso_velocity Min                -1.90849\n",
      "time/data storing (s)                                   0.0306392\n",
      "time/evaluation sampling (s)                           45.9918\n",
      "time/exploration sampling (s)                           1.99748\n",
      "time/logging (s)                                        0.284418\n",
      "time/saving (s)                                         0.0281672\n",
      "time/training (s)                                       4.26986\n",
      "time/epoch (s)                                         52.6023\n",
      "time/total (s)                                       3047.89\n",
      "Epoch                                                  56\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:46:30.329989 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 57 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 118000\n",
      "trainer/QF1 Loss                                        0.387038\n",
      "trainer/QF2 Loss                                        0.358954\n",
      "trainer/Policy Loss                                   -11.8721\n",
      "trainer/Q1 Predictions Mean                            19.5076\n",
      "trainer/Q1 Predictions Std                              2.11609\n",
      "trainer/Q1 Predictions Max                             21.5586\n",
      "trainer/Q1 Predictions Min                             -4.01975\n",
      "trainer/Q2 Predictions Mean                            19.8153\n",
      "trainer/Q2 Predictions Std                              1.82332\n",
      "trainer/Q2 Predictions Max                             21.7372\n",
      "trainer/Q2 Predictions Min                              7.12064\n",
      "trainer/Q Targets Mean                                 19.7963\n",
      "trainer/Q Targets Std                                   2.02217\n",
      "trainer/Q Targets Max                                  21.7864\n",
      "trainer/Q Targets Min                                   0.49145\n",
      "trainer/Log Pis Mean                                    7.91579\n",
      "trainer/Log Pis Std                                     2.23008\n",
      "trainer/Log Pis Max                                    15.3793\n",
      "trainer/Log Pis Min                                    -0.145884\n",
      "trainer/Policy mu Mean                                 -0.01443\n",
      "trainer/Policy mu Std                                   0.161788\n",
      "trainer/Policy mu Max                                   0.511547\n",
      "trainer/Policy mu Min                                  -0.824994\n",
      "trainer/Policy log std Mean                            -2.36225\n",
      "trainer/Policy log std Std                              0.205753\n",
      "trainer/Policy log std Max                             -1.02249\n",
      "trainer/Policy log std Min                             -3.3717\n",
      "trainer/Alpha                                           0.00894172\n",
      "trainer/Alpha Loss                                     -0.397292\n",
      "exploration/num steps total                         59000\n",
      "exploration/num paths total                           131\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.731213\n",
      "exploration/Rewards Std                                 0.0665601\n",
      "exploration/Rewards Max                                 1.00474\n",
      "exploration/Rewards Min                                 0.489607\n",
      "exploration/Returns Mean                              731.213\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               731.213\n",
      "exploration/Returns Min                               731.213\n",
      "exploration/Actions Mean                               -0.0827855\n",
      "exploration/Actions Std                                 0.24967\n",
      "exploration/Actions Max                                 0.623468\n",
      "exploration/Actions Min                                -0.588652\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           731.213\n",
      "exploration/env_infos/final/reward_forward Mean         0.010772\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.010772\n",
      "exploration/env_infos/final/reward_forward Min          0.010772\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0940049\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0940049\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0940049\n",
      "exploration/env_infos/reward_forward Mean              -0.000550619\n",
      "exploration/env_infos/reward_forward Std                0.0559415\n",
      "exploration/env_infos/reward_forward Max                0.756487\n",
      "exploration/env_infos/reward_forward Min               -0.62317\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.201011\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.201011\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.201011\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0578537\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0578537\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0578537\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.276754\n",
      "exploration/env_infos/reward_ctrl Std                   0.0645166\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0578537\n",
      "exploration/env_infos/reward_ctrl Min                  -0.510393\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.017468\n",
      "exploration/env_infos/final/torso_velocity Std          0.0263058\n",
      "exploration/env_infos/final/torso_velocity Max          0.0525078\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0108758\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.083082\n",
      "exploration/env_infos/initial/torso_velocity Std        0.142117\n",
      "exploration/env_infos/initial/torso_velocity Max        0.253941\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0940049\n",
      "exploration/env_infos/torso_velocity Mean              -0.000781353\n",
      "exploration/env_infos/torso_velocity Std                0.0758585\n",
      "exploration/env_infos/torso_velocity Max                0.756487\n",
      "exploration/env_infos/torso_velocity Min               -1.12363\n",
      "evaluation/num steps total                              1.45e+06\n",
      "evaluation/num paths total                           1450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.823739\n",
      "evaluation/Rewards Std                                  0.0845967\n",
      "evaluation/Rewards Max                                  1.83998\n",
      "evaluation/Rewards Min                                  0.484216\n",
      "evaluation/Returns Mean                               823.739\n",
      "evaluation/Returns Std                                 81.6301\n",
      "evaluation/Returns Max                                946.71\n",
      "evaluation/Returns Min                                696.802\n",
      "evaluation/Actions Mean                                -0.0717849\n",
      "evaluation/Actions Std                                  0.197625\n",
      "evaluation/Actions Max                                  0.569252\n",
      "evaluation/Actions Min                                 -0.590163\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            823.739\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.01867e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.74548e-06\n",
      "evaluation/env_infos/final/reward_forward Max           2.50696e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.13679e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0434309\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121067\n",
      "evaluation/env_infos/initial/reward_forward Max         0.155841\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.316683\n",
      "evaluation/env_infos/reward_forward Mean               -0.00105432\n",
      "evaluation/env_infos/reward_forward Std                 0.0470834\n",
      "evaluation/env_infos/reward_forward Max                 1.12459\n",
      "evaluation/env_infos/reward_forward Min                -1.44914\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.177605\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0824289\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.053036\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.306493\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0295497\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0174244\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00376544\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.061137\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.176834\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0828327\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00376544\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.515784\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.49246e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.21549e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           1.47066e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.3545e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135263\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.273638\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.658726\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.316683\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00276019\n",
      "evaluation/env_infos/torso_velocity Std                 0.057643\n",
      "evaluation/env_infos/torso_velocity Max                 1.12459\n",
      "evaluation/env_infos/torso_velocity Min                -1.77172\n",
      "time/data storing (s)                                   0.0304341\n",
      "time/evaluation sampling (s)                           45.945\n",
      "time/exploration sampling (s)                           2.00387\n",
      "time/logging (s)                                        0.274119\n",
      "time/saving (s)                                         0.0248376\n",
      "time/training (s)                                       4.30133\n",
      "time/epoch (s)                                         52.5796\n",
      "time/total (s)                                       3101.04\n",
      "Epoch                                                  57\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:47:23.023457 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 58 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 120000\n",
      "trainer/QF1 Loss                                        0.242076\n",
      "trainer/QF2 Loss                                        0.422671\n",
      "trainer/Policy Loss                                   -12.6339\n",
      "trainer/Q1 Predictions Mean                            20.3675\n",
      "trainer/Q1 Predictions Std                              1.93453\n",
      "trainer/Q1 Predictions Max                             22.9581\n",
      "trainer/Q1 Predictions Min                             -0.313977\n",
      "trainer/Q2 Predictions Mean                            20.3543\n",
      "trainer/Q2 Predictions Std                              1.74439\n",
      "trainer/Q2 Predictions Max                             22.7587\n",
      "trainer/Q2 Predictions Min                              7.02694\n",
      "trainer/Q Targets Mean                                 20.1664\n",
      "trainer/Q Targets Std                                   1.94905\n",
      "trainer/Q Targets Max                                  23.0939\n",
      "trainer/Q Targets Min                                   0.205758\n",
      "trainer/Log Pis Mean                                    7.96133\n",
      "trainer/Log Pis Std                                     2.54459\n",
      "trainer/Log Pis Max                                    20.3165\n",
      "trainer/Log Pis Min                                     0.00985647\n",
      "trainer/Policy mu Mean                                  0.0058243\n",
      "trainer/Policy mu Std                                   0.157387\n",
      "trainer/Policy mu Max                                   1.35773\n",
      "trainer/Policy mu Min                                  -0.749635\n",
      "trainer/Policy log std Mean                            -2.38345\n",
      "trainer/Policy log std Std                              0.252551\n",
      "trainer/Policy log std Max                             -1.4608\n",
      "trainer/Policy log std Min                             -3.89365\n",
      "trainer/Alpha                                           0.00870764\n",
      "trainer/Alpha Loss                                     -0.183428\n",
      "exploration/num steps total                         60000\n",
      "exploration/num paths total                           132\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.905843\n",
      "exploration/Rewards Std                                 0.0843013\n",
      "exploration/Rewards Max                                 1.71034\n",
      "exploration/Rewards Min                                 0.661937\n",
      "exploration/Returns Mean                              905.843\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               905.843\n",
      "exploration/Returns Min                               905.843\n",
      "exploration/Actions Mean                                0.00806317\n",
      "exploration/Actions Std                                 0.16144\n",
      "exploration/Actions Max                                 0.531671\n",
      "exploration/Actions Min                                -0.618544\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           905.843\n",
      "exploration/env_infos/final/reward_forward Mean         0.0746653\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0746653\n",
      "exploration/env_infos/final/reward_forward Min          0.0746653\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0881818\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0881818\n",
      "exploration/env_infos/initial/reward_forward Min        0.0881818\n",
      "exploration/env_infos/reward_forward Mean               0.0035186\n",
      "exploration/env_infos/reward_forward Std                0.240689\n",
      "exploration/env_infos/reward_forward Max                0.886307\n",
      "exploration/env_infos/reward_forward Min               -0.917659\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.104099\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.104099\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.104099\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0847661\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0847661\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0847661\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.104512\n",
      "exploration/env_infos/reward_ctrl Std                   0.0520601\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00819885\n",
      "exploration/env_infos/reward_ctrl Min                  -0.338063\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00259214\n",
      "exploration/env_infos/final/torso_velocity Std          0.127107\n",
      "exploration/env_infos/final/torso_velocity Max          0.109168\n",
      "exploration/env_infos/final/torso_velocity Min         -0.176057\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.10685\n",
      "exploration/env_infos/initial/torso_velocity Std        0.204345\n",
      "exploration/env_infos/initial/torso_velocity Max        0.365932\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.133563\n",
      "exploration/env_infos/torso_velocity Mean               0.00580255\n",
      "exploration/env_infos/torso_velocity Std                0.227396\n",
      "exploration/env_infos/torso_velocity Max                0.886307\n",
      "exploration/env_infos/torso_velocity Min               -1.16545\n",
      "evaluation/num steps total                              1.475e+06\n",
      "evaluation/num paths total                           1475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.930543\n",
      "evaluation/Rewards Std                                  0.0303843\n",
      "evaluation/Rewards Max                                  2.14454\n",
      "evaluation/Rewards Min                                  0.514561\n",
      "evaluation/Returns Mean                               930.543\n",
      "evaluation/Returns Std                                 17.2383\n",
      "evaluation/Returns Max                                962.052\n",
      "evaluation/Returns Min                                899.348\n",
      "evaluation/Actions Mean                                -0.00241607\n",
      "evaluation/Actions Std                                  0.132764\n",
      "evaluation/Actions Max                                  0.569864\n",
      "evaluation/Actions Min                                 -0.511428\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            930.543\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.11298e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.58861e-05\n",
      "evaluation/env_infos/final/reward_forward Max           7.53397e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.08988e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0454177\n",
      "evaluation/env_infos/initial/reward_forward Std         0.116829\n",
      "evaluation/env_infos/initial/reward_forward Max         0.256734\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.240307\n",
      "evaluation/env_infos/reward_forward Mean               -0.00261195\n",
      "evaluation/env_infos/reward_forward Std                 0.0449691\n",
      "evaluation/env_infos/reward_forward Max                 0.770993\n",
      "evaluation/env_infos/reward_forward Min                -1.07755\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0715416\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0173257\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0392606\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.100202\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0384224\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0155632\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0197907\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0681197\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0705287\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0195094\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0140345\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.485439\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.40351e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           2.36319e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           7.0511e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000190071\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.123698\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227892\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.560246\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.265615\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00231845\n",
      "evaluation/env_infos/torso_velocity Std                 0.0574649\n",
      "evaluation/env_infos/torso_velocity Max                 1.01974\n",
      "evaluation/env_infos/torso_velocity Min                -1.81484\n",
      "time/data storing (s)                                   0.0309217\n",
      "time/evaluation sampling (s)                           45.4806\n",
      "time/exploration sampling (s)                           2.03983\n",
      "time/logging (s)                                        0.284414\n",
      "time/saving (s)                                         0.0268693\n",
      "time/training (s)                                       4.28661\n",
      "time/epoch (s)                                         52.1493\n",
      "time/total (s)                                       3153.74\n",
      "Epoch                                                  58\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:48:20.161314 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 59 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 122000\n",
      "trainer/QF1 Loss                                        0.126487\n",
      "trainer/QF2 Loss                                        0.162553\n",
      "trainer/Policy Loss                                   -11.2963\n",
      "trainer/Q1 Predictions Mean                            20.5001\n",
      "trainer/Q1 Predictions Std                              1.70112\n",
      "trainer/Q1 Predictions Max                             22.2732\n",
      "trainer/Q1 Predictions Min                              5.61738\n",
      "trainer/Q2 Predictions Mean                            20.4481\n",
      "trainer/Q2 Predictions Std                              1.75447\n",
      "trainer/Q2 Predictions Max                             22.13\n",
      "trainer/Q2 Predictions Min                              4.02188\n",
      "trainer/Q Targets Mean                                 20.5935\n",
      "trainer/Q Targets Std                                   1.70602\n",
      "trainer/Q Targets Max                                  22.6783\n",
      "trainer/Q Targets Min                                   6.03548\n",
      "trainer/Log Pis Mean                                    9.40454\n",
      "trainer/Log Pis Std                                     2.41689\n",
      "trainer/Log Pis Max                                    20.2238\n",
      "trainer/Log Pis Min                                     2.56889\n",
      "trainer/Policy mu Mean                                 -0.0536412\n",
      "trainer/Policy mu Std                                   0.170157\n",
      "trainer/Policy mu Max                                   1.51195\n",
      "trainer/Policy mu Min                                  -2.39489\n",
      "trainer/Policy log std Mean                            -2.56142\n",
      "trainer/Policy log std Std                              0.257663\n",
      "trainer/Policy log std Max                             -0.912867\n",
      "trainer/Policy log std Min                             -4.4041\n",
      "trainer/Alpha                                           0.00845057\n",
      "trainer/Alpha Loss                                      6.70565\n",
      "exploration/num steps total                         61000\n",
      "exploration/num paths total                           133\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.895628\n",
      "exploration/Rewards Std                                 0.0385232\n",
      "exploration/Rewards Max                                 1.10407\n",
      "exploration/Rewards Min                                 0.76737\n",
      "exploration/Returns Mean                              895.628\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               895.628\n",
      "exploration/Returns Min                               895.628\n",
      "exploration/Actions Mean                               -0.00490192\n",
      "exploration/Actions Std                                 0.16474\n",
      "exploration/Actions Max                                 0.493238\n",
      "exploration/Actions Min                                -0.444059\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           895.628\n",
      "exploration/env_infos/final/reward_forward Mean         0.103232\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.103232\n",
      "exploration/env_infos/final/reward_forward Min          0.103232\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.181233\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.181233\n",
      "exploration/env_infos/initial/reward_forward Min       -0.181233\n",
      "exploration/env_infos/reward_forward Mean               0.0021176\n",
      "exploration/env_infos/reward_forward Std                0.0799731\n",
      "exploration/env_infos/reward_forward Max                0.348674\n",
      "exploration/env_infos/reward_forward Min               -0.868012\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0482576\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0482576\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0482576\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0226993\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0226993\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0226993\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.108654\n",
      "exploration/env_infos/reward_ctrl Std                   0.033486\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0181411\n",
      "exploration/env_infos/reward_ctrl Min                  -0.243377\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.054856\n",
      "exploration/env_infos/final/torso_velocity Std          0.0345408\n",
      "exploration/env_infos/final/torso_velocity Max          0.103232\n",
      "exploration/env_infos/final/torso_velocity Min          0.0247989\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0254034\n",
      "exploration/env_infos/initial/torso_velocity Std        0.168358\n",
      "exploration/env_infos/initial/torso_velocity Max        0.231155\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.181233\n",
      "exploration/env_infos/torso_velocity Mean               0.00045943\n",
      "exploration/env_infos/torso_velocity Std                0.093931\n",
      "exploration/env_infos/torso_velocity Max                0.432818\n",
      "exploration/env_infos/torso_velocity Min               -1.49621\n",
      "evaluation/num steps total                              1.5e+06\n",
      "evaluation/num paths total                           1500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.918211\n",
      "evaluation/Rewards Std                                  0.0247517\n",
      "evaluation/Rewards Max                                  1.897\n",
      "evaluation/Rewards Min                                  0.5304\n",
      "evaluation/Returns Mean                               918.211\n",
      "evaluation/Returns Std                                 19.9848\n",
      "evaluation/Returns Max                                950.962\n",
      "evaluation/Returns Min                                874.786\n",
      "evaluation/Actions Mean                                -0.0165485\n",
      "evaluation/Actions Std                                  0.142369\n",
      "evaluation/Actions Max                                  0.521201\n",
      "evaluation/Actions Min                                 -0.523028\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            918.211\n",
      "evaluation/env_infos/final/reward_forward Mean          3.91143e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.75182e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.76292e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.74419e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00132014\n",
      "evaluation/env_infos/initial/reward_forward Std         0.152769\n",
      "evaluation/env_infos/initial/reward_forward Max         0.313779\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.324837\n",
      "evaluation/env_infos/reward_forward Mean               -0.00171257\n",
      "evaluation/env_infos/reward_forward Std                 0.0448249\n",
      "evaluation/env_infos/reward_forward Max                 0.970953\n",
      "evaluation/env_infos/reward_forward Min                -1.62421\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0820217\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0202828\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0483239\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.124989\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0657813\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0255019\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0309677\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.125013\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0821716\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0211462\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0286375\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.4696\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.31738e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.47579e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.76292e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.74419e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.162286\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.221755\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.682889\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.324837\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00214929\n",
      "evaluation/env_infos/torso_velocity Std                 0.0609656\n",
      "evaluation/env_infos/torso_velocity Max                 1.59712\n",
      "evaluation/env_infos/torso_velocity Min                -1.91055\n",
      "time/data storing (s)                                   0.0339308\n",
      "time/evaluation sampling (s)                           48.9403\n",
      "time/exploration sampling (s)                           2.24654\n",
      "time/logging (s)                                        0.322788\n",
      "time/saving (s)                                         0.0251905\n",
      "time/training (s)                                       4.98575\n",
      "time/epoch (s)                                         56.5545\n",
      "time/total (s)                                       3210.92\n",
      "Epoch                                                  59\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:49:13.292462 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 60 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 124000\n",
      "trainer/QF1 Loss                                        0.349412\n",
      "trainer/QF2 Loss                                        0.400532\n",
      "trainer/Policy Loss                                   -12.4262\n",
      "trainer/Q1 Predictions Mean                            20.9222\n",
      "trainer/Q1 Predictions Std                              1.44797\n",
      "trainer/Q1 Predictions Max                             22.388\n",
      "trainer/Q1 Predictions Min                             15.0386\n",
      "trainer/Q2 Predictions Mean                            21.0538\n",
      "trainer/Q2 Predictions Std                              1.53359\n",
      "trainer/Q2 Predictions Max                             22.7661\n",
      "trainer/Q2 Predictions Min                             13.4604\n",
      "trainer/Q Targets Mean                                 20.8636\n",
      "trainer/Q Targets Std                                   1.60221\n",
      "trainer/Q Targets Max                                  22.5634\n",
      "trainer/Q Targets Min                                  11.3138\n",
      "trainer/Log Pis Mean                                    8.70036\n",
      "trainer/Log Pis Std                                     2.42625\n",
      "trainer/Log Pis Max                                    17.1867\n",
      "trainer/Log Pis Min                                     1.54826\n",
      "trainer/Policy mu Mean                                 -0.0435094\n",
      "trainer/Policy mu Std                                   0.153201\n",
      "trainer/Policy mu Max                                   1.16379\n",
      "trainer/Policy mu Min                                  -1.40723\n",
      "trainer/Policy log std Mean                            -2.47393\n",
      "trainer/Policy log std Std                              0.209642\n",
      "trainer/Policy log std Max                             -1.70558\n",
      "trainer/Policy log std Min                             -3.36619\n",
      "trainer/Alpha                                           0.00890228\n",
      "trainer/Alpha Loss                                      3.30793\n",
      "exploration/num steps total                         62000\n",
      "exploration/num paths total                           134\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.892635\n",
      "exploration/Rewards Std                                 0.0624222\n",
      "exploration/Rewards Max                                 1.19324\n",
      "exploration/Rewards Min                                 0.712375\n",
      "exploration/Returns Mean                              892.635\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               892.635\n",
      "exploration/Returns Min                               892.635\n",
      "exploration/Actions Mean                               -0.0829447\n",
      "exploration/Actions Std                                 0.157365\n",
      "exploration/Actions Max                                 0.371748\n",
      "exploration/Actions Min                                -0.570589\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           892.635\n",
      "exploration/env_infos/final/reward_forward Mean         0.00524482\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00524482\n",
      "exploration/env_infos/final/reward_forward Min          0.00524482\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.137209\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.137209\n",
      "exploration/env_infos/initial/reward_forward Min       -0.137209\n",
      "exploration/env_infos/reward_forward Mean               0.00977899\n",
      "exploration/env_infos/reward_forward Std                0.124449\n",
      "exploration/env_infos/reward_forward Max                1.41521\n",
      "exploration/env_infos/reward_forward Min               -0.522001\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.159476\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.159476\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.159476\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0499924\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0499924\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0499924\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.126575\n",
      "exploration/env_infos/reward_ctrl Std                   0.0448031\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0221871\n",
      "exploration/env_infos/reward_ctrl Min                  -0.287625\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0371882\n",
      "exploration/env_infos/final/torso_velocity Std          0.0346699\n",
      "exploration/env_infos/final/torso_velocity Max          0.0853735\n",
      "exploration/env_infos/final/torso_velocity Min          0.00524482\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.011838\n",
      "exploration/env_infos/initial/torso_velocity Std        0.184282\n",
      "exploration/env_infos/initial/torso_velocity Max        0.248715\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.14702\n",
      "exploration/env_infos/torso_velocity Mean               0.00321558\n",
      "exploration/env_infos/torso_velocity Std                0.0926546\n",
      "exploration/env_infos/torso_velocity Max                1.41521\n",
      "exploration/env_infos/torso_velocity Min               -0.977427\n",
      "evaluation/num steps total                              1.525e+06\n",
      "evaluation/num paths total                           1525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.893451\n",
      "evaluation/Rewards Std                                  0.0315479\n",
      "evaluation/Rewards Max                                  2.22025\n",
      "evaluation/Rewards Min                                  0.622747\n",
      "evaluation/Returns Mean                               893.451\n",
      "evaluation/Returns Std                                 19.3931\n",
      "evaluation/Returns Max                                921.683\n",
      "evaluation/Returns Min                                853.294\n",
      "evaluation/Actions Mean                                -0.0926898\n",
      "evaluation/Actions Std                                  0.135046\n",
      "evaluation/Actions Max                                  0.399249\n",
      "evaluation/Actions Min                                 -0.46731\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            893.451\n",
      "evaluation/env_infos/final/reward_forward Mean          4.68865e-08\n",
      "evaluation/env_infos/final/reward_forward Std           5.26913e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.0682e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.19697e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00301825\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129545\n",
      "evaluation/env_infos/initial/reward_forward Max         0.300663\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.239477\n",
      "evaluation/env_infos/reward_forward Mean                0.00127911\n",
      "evaluation/env_infos/reward_forward Std                 0.045619\n",
      "evaluation/env_infos/reward_forward Max                 1.17808\n",
      "evaluation/env_infos/reward_forward Min                -1.02782\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.106967\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0210004\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0716749\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.149401\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0362657\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0223227\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0100164\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0814353\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.107315\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0225889\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0100164\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.377253\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.40087e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.52799e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.0682e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.19697e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134516\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.262553\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.688185\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.366075\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00142733\n",
      "evaluation/env_infos/torso_velocity Std                 0.0598883\n",
      "evaluation/env_infos/torso_velocity Max                 1.17808\n",
      "evaluation/env_infos/torso_velocity Min                -1.98037\n",
      "time/data storing (s)                                   0.0304811\n",
      "time/evaluation sampling (s)                           45.6511\n",
      "time/exploration sampling (s)                           2.11667\n",
      "time/logging (s)                                        0.270811\n",
      "time/saving (s)                                         0.0246507\n",
      "time/training (s)                                       4.37326\n",
      "time/epoch (s)                                         52.467\n",
      "time/total (s)                                       3263.99\n",
      "Epoch                                                  60\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:50:05.871815 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 61 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 126000\n",
      "trainer/QF1 Loss                                        0.21789\n",
      "trainer/QF2 Loss                                        0.293477\n",
      "trainer/Policy Loss                                   -13.3958\n",
      "trainer/Q1 Predictions Mean                            21.119\n",
      "trainer/Q1 Predictions Std                              1.61932\n",
      "trainer/Q1 Predictions Max                             23.0662\n",
      "trainer/Q1 Predictions Min                             10.6654\n",
      "trainer/Q2 Predictions Mean                            21.0975\n",
      "trainer/Q2 Predictions Std                              1.59494\n",
      "trainer/Q2 Predictions Max                             23.2424\n",
      "trainer/Q2 Predictions Min                             12.8591\n",
      "trainer/Q Targets Mean                                 21.1586\n",
      "trainer/Q Targets Std                                   1.64023\n",
      "trainer/Q Targets Max                                  23.1833\n",
      "trainer/Q Targets Min                                  12.1566\n",
      "trainer/Log Pis Mean                                    7.90827\n",
      "trainer/Log Pis Std                                     2.99869\n",
      "trainer/Log Pis Max                                    29.3348\n",
      "trainer/Log Pis Min                                     0.248192\n",
      "trainer/Policy mu Mean                                 -0.00679235\n",
      "trainer/Policy mu Std                                   0.19333\n",
      "trainer/Policy mu Max                                   1.73543\n",
      "trainer/Policy mu Min                                  -2.79811\n",
      "trainer/Policy log std Mean                            -2.37457\n",
      "trainer/Policy log std Std                              0.28634\n",
      "trainer/Policy log std Max                             -1.4652\n",
      "trainer/Policy log std Min                             -3.89648\n",
      "trainer/Alpha                                           0.0091308\n",
      "trainer/Alpha Loss                                     -0.430663\n",
      "exploration/num steps total                         63000\n",
      "exploration/num paths total                           135\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.802221\n",
      "exploration/Rewards Std                                 0.0612331\n",
      "exploration/Rewards Max                                 0.980115\n",
      "exploration/Rewards Min                                 0.608832\n",
      "exploration/Returns Mean                              802.221\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               802.221\n",
      "exploration/Returns Min                               802.221\n",
      "exploration/Actions Mean                               -0.0798575\n",
      "exploration/Actions Std                                 0.208236\n",
      "exploration/Actions Max                                 0.540491\n",
      "exploration/Actions Min                                -0.634242\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           802.221\n",
      "exploration/env_infos/final/reward_forward Mean         0.129952\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.129952\n",
      "exploration/env_infos/final/reward_forward Min          0.129952\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0531537\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0531537\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0531537\n",
      "exploration/env_infos/reward_forward Mean               0.00271724\n",
      "exploration/env_infos/reward_forward Std                0.0867777\n",
      "exploration/env_infos/reward_forward Max                1.15559\n",
      "exploration/env_infos/reward_forward Min               -0.449918\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.296116\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.296116\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.296116\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0508811\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0508811\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0508811\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.198958\n",
      "exploration/env_infos/reward_ctrl Std                   0.0608729\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0198847\n",
      "exploration/env_infos/reward_ctrl Min                  -0.391168\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0516207\n",
      "exploration/env_infos/final/torso_velocity Std          0.0626857\n",
      "exploration/env_infos/final/torso_velocity Max          0.129952\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0234948\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0837412\n",
      "exploration/env_infos/initial/torso_velocity Std        0.172778\n",
      "exploration/env_infos/initial/torso_velocity Max        0.327469\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0531537\n",
      "exploration/env_infos/torso_velocity Mean              -0.00428673\n",
      "exploration/env_infos/torso_velocity Std                0.0730713\n",
      "exploration/env_infos/torso_velocity Max                1.15559\n",
      "exploration/env_infos/torso_velocity Min               -1.47877\n",
      "evaluation/num steps total                              1.55e+06\n",
      "evaluation/num paths total                           1550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.862163\n",
      "evaluation/Rewards Std                                  0.0562363\n",
      "evaluation/Rewards Max                                  2.75561\n",
      "evaluation/Rewards Min                                  0.74215\n",
      "evaluation/Returns Mean                               862.163\n",
      "evaluation/Returns Std                                 46.952\n",
      "evaluation/Returns Max                                953.877\n",
      "evaluation/Returns Min                                806.038\n",
      "evaluation/Actions Mean                                -0.0513148\n",
      "evaluation/Actions Std                                  0.178946\n",
      "evaluation/Actions Max                                  0.453218\n",
      "evaluation/Actions Min                                 -0.442921\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            862.163\n",
      "evaluation/env_infos/final/reward_forward Mean          3.18572e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.04337e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.78128e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.74021e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.036424\n",
      "evaluation/env_infos/initial/reward_forward Std         0.138242\n",
      "evaluation/env_infos/initial/reward_forward Max         0.217548\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.22138\n",
      "evaluation/env_infos/reward_forward Mean                0.000802065\n",
      "evaluation/env_infos/reward_forward Std                 0.0528604\n",
      "evaluation/env_infos/reward_forward Max                 1.40726\n",
      "evaluation/env_infos/reward_forward Min                -0.914519\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.139829\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0477578\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0497684\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.197407\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0328413\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0226272\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0101953\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.105167\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.138619\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0483493\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00968221\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.25785\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.01404e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.18435e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.45014e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.03706e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.124793\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.250781\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.716445\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.278135\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00156588\n",
      "evaluation/env_infos/torso_velocity Std                 0.0585395\n",
      "evaluation/env_infos/torso_velocity Max                 1.4958\n",
      "evaluation/env_infos/torso_velocity Min                -1.94556\n",
      "time/data storing (s)                                   0.0301364\n",
      "time/evaluation sampling (s)                           45.2367\n",
      "time/exploration sampling (s)                           2.12119\n",
      "time/logging (s)                                        0.289891\n",
      "time/saving (s)                                         0.0258123\n",
      "time/training (s)                                       4.31851\n",
      "time/epoch (s)                                         52.0222\n",
      "time/total (s)                                       3316.59\n",
      "Epoch                                                  61\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:50:58.937613 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 62 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 128000\n",
      "trainer/QF1 Loss                                        0.32093\n",
      "trainer/QF2 Loss                                        0.350112\n",
      "trainer/Policy Loss                                   -13.1354\n",
      "trainer/Q1 Predictions Mean                            21.2814\n",
      "trainer/Q1 Predictions Std                              2.53307\n",
      "trainer/Q1 Predictions Max                             23.5111\n",
      "trainer/Q1 Predictions Min                             -1.55064\n",
      "trainer/Q2 Predictions Mean                            21.2745\n",
      "trainer/Q2 Predictions Std                              2.28746\n",
      "trainer/Q2 Predictions Max                             23.7235\n",
      "trainer/Q2 Predictions Min                              2.79756\n",
      "trainer/Q Targets Mean                                 21.1752\n",
      "trainer/Q Targets Std                                   2.42238\n",
      "trainer/Q Targets Max                                  23.5124\n",
      "trainer/Q Targets Min                                   0.0151227\n",
      "trainer/Log Pis Mean                                    8.39239\n",
      "trainer/Log Pis Std                                     2.85392\n",
      "trainer/Log Pis Max                                    20.1533\n",
      "trainer/Log Pis Min                                    -1.08555\n",
      "trainer/Policy mu Mean                                  0.0572709\n",
      "trainer/Policy mu Std                                   0.218847\n",
      "trainer/Policy mu Max                                   2.9358\n",
      "trainer/Policy mu Min                                  -2.41322\n",
      "trainer/Policy log std Mean                            -2.40186\n",
      "trainer/Policy log std Std                              0.254151\n",
      "trainer/Policy log std Max                             -0.977971\n",
      "trainer/Policy log std Min                             -3.77997\n",
      "trainer/Alpha                                           0.00835393\n",
      "trainer/Alpha Loss                                      1.87787\n",
      "exploration/num steps total                         64000\n",
      "exploration/num paths total                           136\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.861889\n",
      "exploration/Rewards Std                                 0.0713218\n",
      "exploration/Rewards Max                                 1.48022\n",
      "exploration/Rewards Min                                 0.6\n",
      "exploration/Returns Mean                              861.889\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               861.889\n",
      "exploration/Returns Min                               861.889\n",
      "exploration/Actions Mean                                0.0215841\n",
      "exploration/Actions Std                                 0.189954\n",
      "exploration/Actions Max                                 0.635651\n",
      "exploration/Actions Min                                -0.541933\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           861.889\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0243531\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0243531\n",
      "exploration/env_infos/final/reward_forward Min         -0.0243531\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.178521\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.178521\n",
      "exploration/env_infos/initial/reward_forward Min       -0.178521\n",
      "exploration/env_infos/reward_forward Mean               0.0270733\n",
      "exploration/env_infos/reward_forward Std                0.122331\n",
      "exploration/env_infos/reward_forward Max                0.572908\n",
      "exploration/env_infos/reward_forward Min               -0.648346\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.187199\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.187199\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.187199\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.136384\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.136384\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.136384\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.146193\n",
      "exploration/env_infos/reward_ctrl Std                   0.0541923\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0100756\n",
      "exploration/env_infos/reward_ctrl Min                  -0.4\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0341126\n",
      "exploration/env_infos/final/torso_velocity Std          0.0539683\n",
      "exploration/env_infos/final/torso_velocity Max          0.105833\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0243531\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0898032\n",
      "exploration/env_infos/initial/torso_velocity Std        0.247415\n",
      "exploration/env_infos/initial/torso_velocity Max        0.418447\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.178521\n",
      "exploration/env_infos/torso_velocity Mean               0.00288892\n",
      "exploration/env_infos/torso_velocity Std                0.133912\n",
      "exploration/env_infos/torso_velocity Max                0.572908\n",
      "exploration/env_infos/torso_velocity Min               -0.735789\n",
      "evaluation/num steps total                              1.575e+06\n",
      "evaluation/num paths total                           1575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.871668\n",
      "evaluation/Rewards Std                                  0.0539659\n",
      "evaluation/Rewards Max                                  2.43525\n",
      "evaluation/Rewards Min                                  0.479872\n",
      "evaluation/Returns Mean                               871.668\n",
      "evaluation/Returns Std                                 46.7876\n",
      "evaluation/Returns Max                                944.8\n",
      "evaluation/Returns Min                                776.27\n",
      "evaluation/Actions Mean                                 0.0346552\n",
      "evaluation/Actions Std                                  0.176279\n",
      "evaluation/Actions Max                                  0.664879\n",
      "evaluation/Actions Min                                 -0.539067\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            871.668\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.32508e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.54597e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.72078e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.35044e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0231975\n",
      "evaluation/env_infos/initial/reward_forward Std         0.149107\n",
      "evaluation/env_infos/initial/reward_forward Max         0.322072\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.26674\n",
      "evaluation/env_infos/reward_forward Mean                0.00157164\n",
      "evaluation/env_infos/reward_forward Std                 0.067472\n",
      "evaluation/env_infos/reward_forward Max                 1.62273\n",
      "evaluation/env_infos/reward_forward Min                -1.42735\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.129569\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0472259\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0545779\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.228239\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.112664\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0560577\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0512032\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.231865\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.129101\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0485482\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0195628\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.520128\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.49444e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.83247e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.23788e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.05372e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134036\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.241317\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.588857\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.271169\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00148032\n",
      "evaluation/env_infos/torso_velocity Std                 0.063866\n",
      "evaluation/env_infos/torso_velocity Max                 1.62273\n",
      "evaluation/env_infos/torso_velocity Min                -1.69585\n",
      "time/data storing (s)                                   0.0376159\n",
      "time/evaluation sampling (s)                           45.6953\n",
      "time/exploration sampling (s)                           2.04707\n",
      "time/logging (s)                                        0.274064\n",
      "time/saving (s)                                         0.0257354\n",
      "time/training (s)                                       4.35119\n",
      "time/epoch (s)                                         52.4309\n",
      "time/total (s)                                       3369.64\n",
      "Epoch                                                  62\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:51:51.642064 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 63 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 130000\n",
      "trainer/QF1 Loss                                        0.268391\n",
      "trainer/QF2 Loss                                        0.913654\n",
      "trainer/Policy Loss                                   -12.9451\n",
      "trainer/Q1 Predictions Mean                            21.7173\n",
      "trainer/Q1 Predictions Std                              1.83593\n",
      "trainer/Q1 Predictions Max                             23.8837\n",
      "trainer/Q1 Predictions Min                              4.86475\n",
      "trainer/Q2 Predictions Mean                            21.7993\n",
      "trainer/Q2 Predictions Std                              1.64775\n",
      "trainer/Q2 Predictions Max                             23.7625\n",
      "trainer/Q2 Predictions Min                              6.83056\n",
      "trainer/Q Targets Mean                                 21.8196\n",
      "trainer/Q Targets Std                                   2.04725\n",
      "trainer/Q Targets Max                                  24.6334\n",
      "trainer/Q Targets Min                                  -0.164691\n",
      "trainer/Log Pis Mean                                    9.01548\n",
      "trainer/Log Pis Std                                     3.09239\n",
      "trainer/Log Pis Max                                    26.6839\n",
      "trainer/Log Pis Min                                     0.877698\n",
      "trainer/Policy mu Mean                                 -0.0296001\n",
      "trainer/Policy mu Std                                   0.161385\n",
      "trainer/Policy mu Max                                   0.809216\n",
      "trainer/Policy mu Min                                  -1.85069\n",
      "trainer/Policy log std Mean                            -2.5013\n",
      "trainer/Policy log std Std                              0.332628\n",
      "trainer/Policy log std Max                             -1.8274\n",
      "trainer/Policy log std Min                             -5.22548\n",
      "trainer/Alpha                                           0.00806108\n",
      "trainer/Alpha Loss                                      4.89571\n",
      "exploration/num steps total                         65000\n",
      "exploration/num paths total                           137\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.886684\n",
      "exploration/Rewards Std                                 0.0370924\n",
      "exploration/Rewards Max                                 1.00781\n",
      "exploration/Rewards Min                                 0.759893\n",
      "exploration/Returns Mean                              886.684\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               886.684\n",
      "exploration/Returns Min                               886.684\n",
      "exploration/Actions Mean                               -0.0496634\n",
      "exploration/Actions Std                                 0.1612\n",
      "exploration/Actions Max                                 0.515884\n",
      "exploration/Actions Min                                -0.420835\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           886.684\n",
      "exploration/env_infos/final/reward_forward Mean         0.280829\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.280829\n",
      "exploration/env_infos/final/reward_forward Min          0.280829\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0788786\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0788786\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0788786\n",
      "exploration/env_infos/reward_forward Mean              -0.017282\n",
      "exploration/env_infos/reward_forward Std                0.131904\n",
      "exploration/env_infos/reward_forward Max                0.504296\n",
      "exploration/env_infos/reward_forward Min               -0.780154\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0843567\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0843567\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0843567\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.062757\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.062757\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.062757\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.113808\n",
      "exploration/env_infos/reward_ctrl Std                   0.0366058\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0209269\n",
      "exploration/env_infos/reward_ctrl Min                  -0.240107\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0377405\n",
      "exploration/env_infos/final/torso_velocity Std          0.212154\n",
      "exploration/env_infos/final/torso_velocity Max          0.280829\n",
      "exploration/env_infos/final/torso_velocity Min         -0.236104\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.113315\n",
      "exploration/env_infos/initial/torso_velocity Std        0.196595\n",
      "exploration/env_infos/initial/torso_velocity Max        0.383396\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0788786\n",
      "exploration/env_infos/torso_velocity Mean              -0.00617424\n",
      "exploration/env_infos/torso_velocity Std                0.128727\n",
      "exploration/env_infos/torso_velocity Max                0.780948\n",
      "exploration/env_infos/torso_velocity Min               -1.40045\n",
      "evaluation/num steps total                              1.6e+06\n",
      "evaluation/num paths total                           1600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.907295\n",
      "evaluation/Rewards Std                                  0.0351093\n",
      "evaluation/Rewards Max                                  2.10919\n",
      "evaluation/Rewards Min                                  0.58563\n",
      "evaluation/Returns Mean                               907.295\n",
      "evaluation/Returns Std                                 27.8333\n",
      "evaluation/Returns Max                                959.614\n",
      "evaluation/Returns Min                                851.582\n",
      "evaluation/Actions Mean                                -0.0418397\n",
      "evaluation/Actions Std                                  0.147101\n",
      "evaluation/Actions Max                                  0.429423\n",
      "evaluation/Actions Min                                 -0.670878\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            907.295\n",
      "evaluation/env_infos/final/reward_forward Mean         -9.30652e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.17937e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.25393e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.17766e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0197903\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0812091\n",
      "evaluation/env_infos/initial/reward_forward Max         0.15109\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.172082\n",
      "evaluation/env_infos/reward_forward Mean               -0.000881329\n",
      "evaluation/env_infos/reward_forward Std                 0.0525461\n",
      "evaluation/env_infos/reward_forward Max                 1.59178\n",
      "evaluation/env_infos/reward_forward Min                -1.6086\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0935991\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0278275\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0428505\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.149206\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0548075\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0179853\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0323035\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0993514\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0935576\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0286411\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0213835\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.41437\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.00615e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.86489e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.96505e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.39789e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.132347\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.237444\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.65951\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.361219\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00160723\n",
      "evaluation/env_infos/torso_velocity Std                 0.0611757\n",
      "evaluation/env_infos/torso_velocity Max                 1.59178\n",
      "evaluation/env_infos/torso_velocity Min                -1.81314\n",
      "time/data storing (s)                                   0.0309512\n",
      "time/evaluation sampling (s)                           44.5777\n",
      "time/exploration sampling (s)                           2.11137\n",
      "time/logging (s)                                        0.282629\n",
      "time/saving (s)                                         0.0283078\n",
      "time/training (s)                                       5.09582\n",
      "time/epoch (s)                                         52.1268\n",
      "time/total (s)                                       3422.35\n",
      "Epoch                                                  63\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:52:44.133658 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 64 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 132000\n",
      "trainer/QF1 Loss                                        0.14521\n",
      "trainer/QF2 Loss                                        0.300102\n",
      "trainer/Policy Loss                                   -14.2208\n",
      "trainer/Q1 Predictions Mean                            21.8451\n",
      "trainer/Q1 Predictions Std                              1.70978\n",
      "trainer/Q1 Predictions Max                             23.8464\n",
      "trainer/Q1 Predictions Min                             14.0961\n",
      "trainer/Q2 Predictions Mean                            21.9078\n",
      "trainer/Q2 Predictions Std                              1.69948\n",
      "trainer/Q2 Predictions Max                             23.8261\n",
      "trainer/Q2 Predictions Min                             13.3392\n",
      "trainer/Q Targets Mean                                 21.89\n",
      "trainer/Q Targets Std                                   1.69766\n",
      "trainer/Q Targets Max                                  23.7853\n",
      "trainer/Q Targets Min                                  12.742\n",
      "trainer/Log Pis Mean                                    7.86703\n",
      "trainer/Log Pis Std                                     2.76752\n",
      "trainer/Log Pis Max                                    18.2144\n",
      "trainer/Log Pis Min                                    -3.09746\n",
      "trainer/Policy mu Mean                                  0.0178463\n",
      "trainer/Policy mu Std                                   0.148988\n",
      "trainer/Policy mu Max                                   0.935084\n",
      "trainer/Policy mu Min                                  -0.943271\n",
      "trainer/Policy log std Mean                            -2.375\n",
      "trainer/Policy log std Std                              0.266488\n",
      "trainer/Policy log std Max                             -0.587295\n",
      "trainer/Policy log std Min                             -3.91637\n",
      "trainer/Alpha                                           0.0083718\n",
      "trainer/Alpha Loss                                     -0.636153\n",
      "exploration/num steps total                         66000\n",
      "exploration/num paths total                           138\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.951799\n",
      "exploration/Rewards Std                                 0.128465\n",
      "exploration/Rewards Max                                 2.10474\n",
      "exploration/Rewards Min                                 0.739591\n",
      "exploration/Returns Mean                              951.799\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               951.799\n",
      "exploration/Returns Min                               951.799\n",
      "exploration/Actions Mean                               -0.00549493\n",
      "exploration/Actions Std                                 0.144382\n",
      "exploration/Actions Max                                 0.491657\n",
      "exploration/Actions Min                                -0.535203\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           951.799\n",
      "exploration/env_infos/final/reward_forward Mean        -0.545461\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.545461\n",
      "exploration/env_infos/final/reward_forward Min         -0.545461\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0498655\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0498655\n",
      "exploration/env_infos/initial/reward_forward Min        0.0498655\n",
      "exploration/env_infos/reward_forward Mean              -0.0106312\n",
      "exploration/env_infos/reward_forward Std                0.170848\n",
      "exploration/env_infos/reward_forward Max                0.76935\n",
      "exploration/env_infos/reward_forward Min               -0.712605\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.132793\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.132793\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.132793\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.131121\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.131121\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.131121\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0835059\n",
      "exploration/env_infos/reward_ctrl Std                   0.0381637\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0103992\n",
      "exploration/env_infos/reward_ctrl Min                  -0.260409\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.579936\n",
      "exploration/env_infos/final/torso_velocity Std          0.107907\n",
      "exploration/env_infos/final/torso_velocity Max         -0.468432\n",
      "exploration/env_infos/final/torso_velocity Min         -0.725916\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.111255\n",
      "exploration/env_infos/initial/torso_velocity Std        0.313015\n",
      "exploration/env_infos/initial/torso_velocity Max        0.521608\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.237709\n",
      "exploration/env_infos/torso_velocity Mean              -0.0141205\n",
      "exploration/env_infos/torso_velocity Std                0.216609\n",
      "exploration/env_infos/torso_velocity Max                0.845142\n",
      "exploration/env_infos/torso_velocity Min               -1.42612\n",
      "evaluation/num steps total                              1.625e+06\n",
      "evaluation/num paths total                           1625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.925497\n",
      "evaluation/Rewards Std                                  0.0356118\n",
      "evaluation/Rewards Max                                  2.38191\n",
      "evaluation/Rewards Min                                  0.711018\n",
      "evaluation/Returns Mean                               925.497\n",
      "evaluation/Returns Std                                 26.9692\n",
      "evaluation/Returns Max                                976.21\n",
      "evaluation/Returns Min                                858.224\n",
      "evaluation/Actions Mean                                -0.00400303\n",
      "evaluation/Actions Std                                  0.137078\n",
      "evaluation/Actions Max                                  0.505131\n",
      "evaluation/Actions Min                                 -0.394844\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            925.497\n",
      "evaluation/env_infos/final/reward_forward Mean         -7.49371e-09\n",
      "evaluation/env_infos/final/reward_forward Std           1.98129e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.63219e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.95576e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00575834\n",
      "evaluation/env_infos/initial/reward_forward Std         0.141973\n",
      "evaluation/env_infos/initial/reward_forward Max         0.34122\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.239753\n",
      "evaluation/env_infos/reward_forward Mean                5.38864e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0507064\n",
      "evaluation/env_infos/reward_forward Max                 1.22859\n",
      "evaluation/env_infos/reward_forward Min                -1.73487\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0750917\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0274978\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0232248\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.142662\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0568594\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0115896\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0349032\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0752894\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0752252\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0276405\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.014692\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.288982\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.00702e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.99155e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.10299e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.95576e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.121633\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.220848\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.629189\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.298569\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0031506\n",
      "evaluation/env_infos/torso_velocity Std                 0.0641851\n",
      "evaluation/env_infos/torso_velocity Max                 1.22859\n",
      "evaluation/env_infos/torso_velocity Min                -1.89367\n",
      "time/data storing (s)                                   0.0297633\n",
      "time/evaluation sampling (s)                           45.2696\n",
      "time/exploration sampling (s)                           1.98788\n",
      "time/logging (s)                                        0.28298\n",
      "time/saving (s)                                         0.0264698\n",
      "time/training (s)                                       4.27605\n",
      "time/epoch (s)                                         51.8728\n",
      "time/total (s)                                       3474.84\n",
      "Epoch                                                  64\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:53:36.817537 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 65 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 134000\n",
      "trainer/QF1 Loss                                        0.301939\n",
      "trainer/QF2 Loss                                        0.398761\n",
      "trainer/Policy Loss                                   -15.2575\n",
      "trainer/Q1 Predictions Mean                            21.9863\n",
      "trainer/Q1 Predictions Std                              2.21898\n",
      "trainer/Q1 Predictions Max                             25.0472\n",
      "trainer/Q1 Predictions Min                              4.02014\n",
      "trainer/Q2 Predictions Mean                            21.9077\n",
      "trainer/Q2 Predictions Std                              2.28845\n",
      "trainer/Q2 Predictions Max                             25.0167\n",
      "trainer/Q2 Predictions Min                              4.14228\n",
      "trainer/Q Targets Mean                                 22.0166\n",
      "trainer/Q Targets Std                                   2.37983\n",
      "trainer/Q Targets Max                                  26.4755\n",
      "trainer/Q Targets Min                                  -0.81889\n",
      "trainer/Log Pis Mean                                    6.9026\n",
      "trainer/Log Pis Std                                     2.85267\n",
      "trainer/Log Pis Max                                    19.8763\n",
      "trainer/Log Pis Min                                    -3.55157\n",
      "trainer/Policy mu Mean                                 -0.00649476\n",
      "trainer/Policy mu Std                                   0.195228\n",
      "trainer/Policy mu Max                                   3.0829\n",
      "trainer/Policy mu Min                                  -1.41017\n",
      "trainer/Policy log std Mean                            -2.25583\n",
      "trainer/Policy log std Std                              0.261392\n",
      "trainer/Policy log std Max                             -1.04215\n",
      "trainer/Policy log std Min                             -3.97769\n",
      "trainer/Alpha                                           0.00804231\n",
      "trainer/Alpha Loss                                     -5.29137\n",
      "exploration/num steps total                         67000\n",
      "exploration/num paths total                           139\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.885281\n",
      "exploration/Rewards Std                                 0.134196\n",
      "exploration/Rewards Max                                 1.88174\n",
      "exploration/Rewards Min                                 0.440731\n",
      "exploration/Returns Mean                              885.281\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               885.281\n",
      "exploration/Returns Min                               885.281\n",
      "exploration/Actions Mean                                0.0211816\n",
      "exploration/Actions Std                                 0.18603\n",
      "exploration/Actions Max                                 0.73014\n",
      "exploration/Actions Min                                -0.670623\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           885.281\n",
      "exploration/env_infos/final/reward_forward Mean         0.106522\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.106522\n",
      "exploration/env_infos/final/reward_forward Min          0.106522\n",
      "exploration/env_infos/initial/reward_forward Mean       0.034479\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.034479\n",
      "exploration/env_infos/initial/reward_forward Min        0.034479\n",
      "exploration/env_infos/reward_forward Mean               0.0200512\n",
      "exploration/env_infos/reward_forward Std                0.222085\n",
      "exploration/env_infos/reward_forward Max                1.00349\n",
      "exploration/env_infos/reward_forward Min               -1.27356\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.186394\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.186394\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.186394\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.104708\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.104708\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.104708\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.140224\n",
      "exploration/env_infos/reward_ctrl Std                   0.0630079\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0155457\n",
      "exploration/env_infos/reward_ctrl Min                  -0.559269\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.215029\n",
      "exploration/env_infos/final/torso_velocity Std          0.0896408\n",
      "exploration/env_infos/final/torso_velocity Max          0.326053\n",
      "exploration/env_infos/final/torso_velocity Min          0.106522\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.137219\n",
      "exploration/env_infos/initial/torso_velocity Std        0.165164\n",
      "exploration/env_infos/initial/torso_velocity Max        0.370254\n",
      "exploration/env_infos/initial/torso_velocity Min        0.00692407\n",
      "exploration/env_infos/torso_velocity Mean               9.89357e-05\n",
      "exploration/env_infos/torso_velocity Std                0.227725\n",
      "exploration/env_infos/torso_velocity Max                1.00349\n",
      "exploration/env_infos/torso_velocity Min               -1.69135\n",
      "evaluation/num steps total                              1.65e+06\n",
      "evaluation/num paths total                           1650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.919301\n",
      "evaluation/Rewards Std                                  0.049954\n",
      "evaluation/Rewards Max                                  2.30103\n",
      "evaluation/Rewards Min                                  0.13099\n",
      "evaluation/Returns Mean                               919.301\n",
      "evaluation/Returns Std                                 38.1864\n",
      "evaluation/Returns Max                                964.527\n",
      "evaluation/Returns Min                                778.517\n",
      "evaluation/Actions Mean                                -0.0473515\n",
      "evaluation/Actions Std                                  0.135097\n",
      "evaluation/Actions Max                                  0.529714\n",
      "evaluation/Actions Min                                 -0.821039\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            919.301\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.36512e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.98081e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.42652e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.75222e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00759499\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128561\n",
      "evaluation/env_infos/initial/reward_forward Max         0.27349\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.285885\n",
      "evaluation/env_infos/reward_forward Mean                0.00251339\n",
      "evaluation/env_infos/reward_forward Std                 0.0598962\n",
      "evaluation/env_infos/reward_forward Max                 1.4055\n",
      "evaluation/env_infos/reward_forward Min                -1.59157\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0823468\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0385922\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0350286\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.22477\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0512163\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0296511\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0113047\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.100298\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0819731\n",
      "evaluation/env_infos/reward_ctrl Std                    0.039131\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0113047\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.86901\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -7.56407e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.6187e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.77669e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.75222e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134148\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.236537\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.576986\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.285885\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00270428\n",
      "evaluation/env_infos/torso_velocity Std                 0.0647766\n",
      "evaluation/env_infos/torso_velocity Max                 1.4055\n",
      "evaluation/env_infos/torso_velocity Min                -1.80143\n",
      "time/data storing (s)                                   0.0305083\n",
      "time/evaluation sampling (s)                           45.4974\n",
      "time/exploration sampling (s)                           1.95913\n",
      "time/logging (s)                                        0.273523\n",
      "time/saving (s)                                         0.0256765\n",
      "time/training (s)                                       4.26424\n",
      "time/epoch (s)                                         52.0505\n",
      "time/total (s)                                       3527.51\n",
      "Epoch                                                  65\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:54:29.450987 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 66 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 136000\n",
      "trainer/QF1 Loss                                        2.64195\n",
      "trainer/QF2 Loss                                        2.20541\n",
      "trainer/Policy Loss                                   -14.8678\n",
      "trainer/Q1 Predictions Mean                            22.9169\n",
      "trainer/Q1 Predictions Std                              1.45601\n",
      "trainer/Q1 Predictions Max                             24.537\n",
      "trainer/Q1 Predictions Min                             15.9232\n",
      "trainer/Q2 Predictions Mean                            22.892\n",
      "trainer/Q2 Predictions Std                              1.50626\n",
      "trainer/Q2 Predictions Max                             24.4754\n",
      "trainer/Q2 Predictions Min                              9.9676\n",
      "trainer/Q Targets Mean                                 22.7246\n",
      "trainer/Q Targets Std                                   2.44131\n",
      "trainer/Q Targets Max                                  24.5298\n",
      "trainer/Q Targets Min                                   0.475999\n",
      "trainer/Log Pis Mean                                    8.1925\n",
      "trainer/Log Pis Std                                     2.9049\n",
      "trainer/Log Pis Max                                    20.6634\n",
      "trainer/Log Pis Min                                    -0.331267\n",
      "trainer/Policy mu Mean                                 -0.00148756\n",
      "trainer/Policy mu Std                                   0.158664\n",
      "trainer/Policy mu Max                                   1.10681\n",
      "trainer/Policy mu Min                                  -0.706813\n",
      "trainer/Policy log std Mean                            -2.42231\n",
      "trainer/Policy log std Std                              0.262082\n",
      "trainer/Policy log std Max                             -1.19355\n",
      "trainer/Policy log std Min                             -4.28975\n",
      "trainer/Alpha                                           0.00812081\n",
      "trainer/Alpha Loss                                      0.92671\n",
      "exploration/num steps total                         68000\n",
      "exploration/num paths total                           140\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.907012\n",
      "exploration/Rewards Std                                 0.0705094\n",
      "exploration/Rewards Max                                 1.32589\n",
      "exploration/Rewards Min                                 0.704282\n",
      "exploration/Returns Mean                              907.012\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               907.012\n",
      "exploration/Returns Min                               907.012\n",
      "exploration/Actions Mean                               -0.0205478\n",
      "exploration/Actions Std                                 0.162989\n",
      "exploration/Actions Max                                 0.540786\n",
      "exploration/Actions Min                                -0.565935\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           907.012\n",
      "exploration/env_infos/final/reward_forward Mean         0.00357604\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00357604\n",
      "exploration/env_infos/final/reward_forward Min          0.00357604\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0155303\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0155303\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0155303\n",
      "exploration/env_infos/reward_forward Mean              -0.000936314\n",
      "exploration/env_infos/reward_forward Std                0.0931577\n",
      "exploration/env_infos/reward_forward Max                0.554851\n",
      "exploration/env_infos/reward_forward Min               -0.787528\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.086026\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.086026\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.086026\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0687479\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0687479\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0687479\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.10795\n",
      "exploration/env_infos/reward_ctrl Std                   0.0465418\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0161605\n",
      "exploration/env_infos/reward_ctrl Min                  -0.295718\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00105484\n",
      "exploration/env_infos/final/torso_velocity Std          0.00339949\n",
      "exploration/env_infos/final/torso_velocity Max          0.00357604\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00448878\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.188447\n",
      "exploration/env_infos/initial/torso_velocity Std        0.268035\n",
      "exploration/env_infos/initial/torso_velocity Max        0.567129\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0155303\n",
      "exploration/env_infos/torso_velocity Mean              -0.00340886\n",
      "exploration/env_infos/torso_velocity Std                0.120016\n",
      "exploration/env_infos/torso_velocity Max                0.567129\n",
      "exploration/env_infos/torso_velocity Min               -1.47396\n",
      "evaluation/num steps total                              1.675e+06\n",
      "evaluation/num paths total                           1675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.916539\n",
      "evaluation/Rewards Std                                  0.0380688\n",
      "evaluation/Rewards Max                                  1.96561\n",
      "evaluation/Rewards Min                                  0.53992\n",
      "evaluation/Returns Mean                               916.539\n",
      "evaluation/Returns Std                                 33.9146\n",
      "evaluation/Returns Max                                969.402\n",
      "evaluation/Returns Min                                828.176\n",
      "evaluation/Actions Mean                                -0.00718103\n",
      "evaluation/Actions Std                                  0.14477\n",
      "evaluation/Actions Max                                  0.515311\n",
      "evaluation/Actions Min                                 -0.439066\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            916.539\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.47935e-06\n",
      "evaluation/env_infos/final/reward_forward Std           6.71281e-06\n",
      "evaluation/env_infos/final/reward_forward Max           8.37305e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.43048e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0896477\n",
      "evaluation/env_infos/initial/reward_forward Std         0.152937\n",
      "evaluation/env_infos/initial/reward_forward Max         0.210048\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.420631\n",
      "evaluation/env_infos/reward_forward Mean               -0.00172362\n",
      "evaluation/env_infos/reward_forward Std                 0.0500755\n",
      "evaluation/env_infos/reward_forward Max                 1.38767\n",
      "evaluation/env_infos/reward_forward Min                -1.6656\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0833371\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0344799\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0333646\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.172662\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.069105\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0447322\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0207689\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.172575\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0840402\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0344858\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0186889\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.46008\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.1505e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           2.86388e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00019903\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.43048e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.101023\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.265268\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.585647\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.420631\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00192519\n",
      "evaluation/env_infos/torso_velocity Std                 0.0505657\n",
      "evaluation/env_infos/torso_velocity Max                 1.38767\n",
      "evaluation/env_infos/torso_velocity Min                -1.75492\n",
      "time/data storing (s)                                   0.0358863\n",
      "time/evaluation sampling (s)                           45.432\n",
      "time/exploration sampling (s)                           1.97252\n",
      "time/logging (s)                                        0.28352\n",
      "time/saving (s)                                         0.0263177\n",
      "time/training (s)                                       4.28952\n",
      "time/epoch (s)                                         52.0397\n",
      "time/total (s)                                       3580.16\n",
      "Epoch                                                  66\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 12:55:22.666142 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 67 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 138000\n",
      "trainer/QF1 Loss                                        0.622993\n",
      "trainer/QF2 Loss                                        0.750176\n",
      "trainer/Policy Loss                                   -14.1645\n",
      "trainer/Q1 Predictions Mean                            22.9082\n",
      "trainer/Q1 Predictions Std                              1.71672\n",
      "trainer/Q1 Predictions Max                             24.7766\n",
      "trainer/Q1 Predictions Min                              8.64797\n",
      "trainer/Q2 Predictions Mean                            23.0464\n",
      "trainer/Q2 Predictions Std                              1.49999\n",
      "trainer/Q2 Predictions Max                             24.9283\n",
      "trainer/Q2 Predictions Min                             15.8634\n",
      "trainer/Q Targets Mean                                 22.8902\n",
      "trainer/Q Targets Std                                   1.90165\n",
      "trainer/Q Targets Max                                  24.6852\n",
      "trainer/Q Targets Min                                   5.39837\n",
      "trainer/Log Pis Mean                                    8.94488\n",
      "trainer/Log Pis Std                                     2.63831\n",
      "trainer/Log Pis Max                                    21.4393\n",
      "trainer/Log Pis Min                                    -0.823017\n",
      "trainer/Policy mu Mean                                 -0.0486609\n",
      "trainer/Policy mu Std                                   0.185851\n",
      "trainer/Policy mu Max                                   0.935595\n",
      "trainer/Policy mu Min                                  -0.752351\n",
      "trainer/Policy log std Mean                            -2.49783\n",
      "trainer/Policy log std Std                              0.22961\n",
      "trainer/Policy log std Max                             -1.82421\n",
      "trainer/Policy log std Min                             -4.46174\n",
      "trainer/Alpha                                           0.00795655\n",
      "trainer/Alpha Loss                                      4.5695\n",
      "exploration/num steps total                         69000\n",
      "exploration/num paths total                           141\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.765726\n",
      "exploration/Rewards Std                                 0.0481374\n",
      "exploration/Rewards Max                                 0.918888\n",
      "exploration/Rewards Min                                 0.603661\n",
      "exploration/Returns Mean                              765.726\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               765.726\n",
      "exploration/Returns Min                               765.726\n",
      "exploration/Actions Mean                               -0.0538823\n",
      "exploration/Actions Std                                 0.236152\n",
      "exploration/Actions Max                                 0.476931\n",
      "exploration/Actions Min                                -0.57331\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           765.726\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00184566\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00184566\n",
      "exploration/env_infos/final/reward_forward Min         -0.00184566\n",
      "exploration/env_infos/initial/reward_forward Mean       0.074184\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.074184\n",
      "exploration/env_infos/initial/reward_forward Min        0.074184\n",
      "exploration/env_infos/reward_forward Mean              -0.00653433\n",
      "exploration/env_infos/reward_forward Std                0.0760137\n",
      "exploration/env_infos/reward_forward Max                0.074184\n",
      "exploration/env_infos/reward_forward Min               -1.60241\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.206882\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.206882\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.206882\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.161211\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.161211\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.161211\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.234685\n",
      "exploration/env_infos/reward_ctrl Std                   0.0478987\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0811124\n",
      "exploration/env_infos/reward_ctrl Min                  -0.396339\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.000120595\n",
      "exploration/env_infos/final/torso_velocity Std          0.00136677\n",
      "exploration/env_infos/final/torso_velocity Max          0.00149705\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00184566\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0873455\n",
      "exploration/env_infos/initial/torso_velocity Std        0.107194\n",
      "exploration/env_infos/initial/torso_velocity Max        0.224716\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0368636\n",
      "exploration/env_infos/torso_velocity Mean              -0.00180142\n",
      "exploration/env_infos/torso_velocity Std                0.0660332\n",
      "exploration/env_infos/torso_velocity Max                0.754745\n",
      "exploration/env_infos/torso_velocity Min               -1.60241\n",
      "evaluation/num steps total                              1.7e+06\n",
      "evaluation/num paths total                           1700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.851518\n",
      "evaluation/Rewards Std                                  0.0469077\n",
      "evaluation/Rewards Max                                  1.89686\n",
      "evaluation/Rewards Min                                  0.616835\n",
      "evaluation/Returns Mean                               851.518\n",
      "evaluation/Returns Std                                 43.248\n",
      "evaluation/Returns Max                                922.993\n",
      "evaluation/Returns Min                                770.047\n",
      "evaluation/Actions Mean                                -0.0593913\n",
      "evaluation/Actions Std                                  0.183597\n",
      "evaluation/Actions Max                                  0.475575\n",
      "evaluation/Actions Min                                 -0.573695\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            851.518\n",
      "evaluation/env_infos/final/reward_forward Mean          1.67779e-07\n",
      "evaluation/env_infos/final/reward_forward Std           1.52769e-06\n",
      "evaluation/env_infos/final/reward_forward Max           7.51042e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -7.8263e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0599086\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128551\n",
      "evaluation/env_infos/initial/reward_forward Max         0.160069\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.337176\n",
      "evaluation/env_infos/reward_forward Mean                0.000799955\n",
      "evaluation/env_infos/reward_forward Std                 0.050669\n",
      "evaluation/env_infos/reward_forward Max                 1.3532\n",
      "evaluation/env_infos/reward_forward Min                -1.37721\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.149345\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0439101\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0764899\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.231477\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.1403\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0496534\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0454102\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.209022\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.14894\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0444038\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0272887\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.383165\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.12122e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           7.56405e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           6.53136e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.8263e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.108961\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246803\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.560031\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.337176\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00226257\n",
      "evaluation/env_infos/torso_velocity Std                 0.0556991\n",
      "evaluation/env_infos/torso_velocity Max                 1.3532\n",
      "evaluation/env_infos/torso_velocity Min                -1.87841\n",
      "time/data storing (s)                                   0.0299523\n",
      "time/evaluation sampling (s)                           45.5374\n",
      "time/exploration sampling (s)                           2.15036\n",
      "time/logging (s)                                        0.276277\n",
      "time/saving (s)                                         0.0266212\n",
      "time/training (s)                                       4.52776\n",
      "time/epoch (s)                                         52.5484\n",
      "time/total (s)                                       3633.36\n",
      "Epoch                                                  67\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:56:15.887615 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 68 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 140000\n",
      "trainer/QF1 Loss                                        0.396831\n",
      "trainer/QF2 Loss                                        0.222841\n",
      "trainer/Policy Loss                                   -16.137\n",
      "trainer/Q1 Predictions Mean                            23.4645\n",
      "trainer/Q1 Predictions Std                              1.813\n",
      "trainer/Q1 Predictions Max                             25.3752\n",
      "trainer/Q1 Predictions Min                             13.1581\n",
      "trainer/Q2 Predictions Mean                            23.2566\n",
      "trainer/Q2 Predictions Std                              1.9025\n",
      "trainer/Q2 Predictions Max                             25.0781\n",
      "trainer/Q2 Predictions Min                             10.6711\n",
      "trainer/Q Targets Mean                                 23.1272\n",
      "trainer/Q Targets Std                                   1.85548\n",
      "trainer/Q Targets Max                                  24.8555\n",
      "trainer/Q Targets Min                                   9.89147\n",
      "trainer/Log Pis Mean                                    7.35585\n",
      "trainer/Log Pis Std                                     2.28384\n",
      "trainer/Log Pis Max                                    16.9959\n",
      "trainer/Log Pis Min                                     1.6925\n",
      "trainer/Policy mu Mean                                 -0.0416255\n",
      "trainer/Policy mu Std                                   0.202949\n",
      "trainer/Policy mu Max                                   2.43258\n",
      "trainer/Policy mu Min                                  -1.91469\n",
      "trainer/Policy log std Mean                            -2.28434\n",
      "trainer/Policy log std Std                              0.202262\n",
      "trainer/Policy log std Max                             -0.818896\n",
      "trainer/Policy log std Min                             -3.69319\n",
      "trainer/Alpha                                           0.00787034\n",
      "trainer/Alpha Loss                                     -3.11982\n",
      "exploration/num steps total                         70000\n",
      "exploration/num paths total                           142\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.912936\n",
      "exploration/Rewards Std                                 0.112883\n",
      "exploration/Rewards Max                                 1.82909\n",
      "exploration/Rewards Min                                 0.652818\n",
      "exploration/Returns Mean                              912.936\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               912.936\n",
      "exploration/Returns Min                               912.936\n",
      "exploration/Actions Mean                               -0.0188083\n",
      "exploration/Actions Std                                 0.168147\n",
      "exploration/Actions Max                                 0.559367\n",
      "exploration/Actions Min                                -0.520766\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           912.936\n",
      "exploration/env_infos/final/reward_forward Mean         0.0228315\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0228315\n",
      "exploration/env_infos/final/reward_forward Min          0.0228315\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0767966\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0767966\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0767966\n",
      "exploration/env_infos/reward_forward Mean              -0.016163\n",
      "exploration/env_infos/reward_forward Std                0.165862\n",
      "exploration/env_infos/reward_forward Max                0.834103\n",
      "exploration/env_infos/reward_forward Min               -0.618133\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.101394\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.101394\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.101394\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.236413\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.236413\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.236413\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.114508\n",
      "exploration/env_infos/reward_ctrl Std                   0.0477255\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00710951\n",
      "exploration/env_infos/reward_ctrl Min                  -0.388849\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00397608\n",
      "exploration/env_infos/final/torso_velocity Std          0.021106\n",
      "exploration/env_infos/final/torso_velocity Max          0.0228315\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0287469\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.090383\n",
      "exploration/env_infos/initial/torso_velocity Std        0.169424\n",
      "exploration/env_infos/initial/torso_velocity Max        0.322617\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0767966\n",
      "exploration/env_infos/torso_velocity Mean              -0.00736057\n",
      "exploration/env_infos/torso_velocity Std                0.205677\n",
      "exploration/env_infos/torso_velocity Max                1.32252\n",
      "exploration/env_infos/torso_velocity Min               -1.20812\n",
      "evaluation/num steps total                              1.725e+06\n",
      "evaluation/num paths total                           1725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.893817\n",
      "evaluation/Rewards Std                                  0.0698857\n",
      "evaluation/Rewards Max                                  2.5813\n",
      "evaluation/Rewards Min                                  0.635594\n",
      "evaluation/Returns Mean                               893.817\n",
      "evaluation/Returns Std                                 60.1254\n",
      "evaluation/Returns Max                                971.635\n",
      "evaluation/Returns Min                                748.673\n",
      "evaluation/Actions Mean                                -0.0435649\n",
      "evaluation/Actions Std                                  0.158164\n",
      "evaluation/Actions Max                                  0.559255\n",
      "evaluation/Actions Min                                 -0.543607\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            893.817\n",
      "evaluation/env_infos/final/reward_forward Mean          1.14164e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.524e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.00578e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.37702e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00279631\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0990542\n",
      "evaluation/env_infos/initial/reward_forward Max         0.184801\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.19731\n",
      "evaluation/env_infos/reward_forward Mean               -0.00243629\n",
      "evaluation/env_infos/reward_forward Std                 0.0596602\n",
      "evaluation/env_infos/reward_forward Max                 1.31102\n",
      "evaluation/env_infos/reward_forward Min                -1.72107\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.107662\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0608641\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0372654\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.26035\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0735555\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0381888\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0322702\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.151159\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.107655\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0606283\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0320057\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.364406\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.85588e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.11808e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.29776e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.00347e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.102773\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.243315\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.589279\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.346214\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00179405\n",
      "evaluation/env_infos/torso_velocity Std                 0.0646043\n",
      "evaluation/env_infos/torso_velocity Max                 1.41704\n",
      "evaluation/env_infos/torso_velocity Min                -1.97874\n",
      "time/data storing (s)                                   0.031311\n",
      "time/evaluation sampling (s)                           45.8839\n",
      "time/exploration sampling (s)                           2.08332\n",
      "time/logging (s)                                        0.269799\n",
      "time/saving (s)                                         0.0259944\n",
      "time/training (s)                                       4.29289\n",
      "time/epoch (s)                                         52.5873\n",
      "time/total (s)                                       3686.58\n",
      "Epoch                                                  68\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:57:08.153647 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 69 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 142000\n",
      "trainer/QF1 Loss                                        0.18579\n",
      "trainer/QF2 Loss                                        0.243487\n",
      "trainer/Policy Loss                                   -14.6307\n",
      "trainer/Q1 Predictions Mean                            23.3796\n",
      "trainer/Q1 Predictions Std                              2.00069\n",
      "trainer/Q1 Predictions Max                             25.6241\n",
      "trainer/Q1 Predictions Min                              9.12042\n",
      "trainer/Q2 Predictions Mean                            23.588\n",
      "trainer/Q2 Predictions Std                              2.08487\n",
      "trainer/Q2 Predictions Max                             25.966\n",
      "trainer/Q2 Predictions Min                              9.63564\n",
      "trainer/Q Targets Mean                                 23.3825\n",
      "trainer/Q Targets Std                                   1.97984\n",
      "trainer/Q Targets Max                                  25.5774\n",
      "trainer/Q Targets Min                                  10.5812\n",
      "trainer/Log Pis Mean                                    8.97525\n",
      "trainer/Log Pis Std                                     2.69029\n",
      "trainer/Log Pis Max                                    18.1127\n",
      "trainer/Log Pis Min                                    -0.896834\n",
      "trainer/Policy mu Mean                                  0.00355369\n",
      "trainer/Policy mu Std                                   0.238529\n",
      "trainer/Policy mu Max                                   1.19152\n",
      "trainer/Policy mu Min                                  -2.20221\n",
      "trainer/Policy log std Mean                            -2.47462\n",
      "trainer/Policy log std Std                              0.24892\n",
      "trainer/Policy log std Max                             -0.899402\n",
      "trainer/Policy log std Min                             -3.67509\n",
      "trainer/Alpha                                           0.00714915\n",
      "trainer/Alpha Loss                                      4.81908\n",
      "exploration/num steps total                         71000\n",
      "exploration/num paths total                           143\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.73033\n",
      "exploration/Rewards Std                                 0.0667574\n",
      "exploration/Rewards Max                                 1.05919\n",
      "exploration/Rewards Min                                 0.477404\n",
      "exploration/Returns Mean                              730.33\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               730.33\n",
      "exploration/Returns Min                               730.33\n",
      "exploration/Actions Mean                                0.00388207\n",
      "exploration/Actions Std                                 0.264013\n",
      "exploration/Actions Max                                 0.617109\n",
      "exploration/Actions Min                                -0.661963\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           730.33\n",
      "exploration/env_infos/final/reward_forward Mean         0.00484941\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00484941\n",
      "exploration/env_infos/final/reward_forward Min          0.00484941\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0089861\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0089861\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0089861\n",
      "exploration/env_infos/reward_forward Mean               0.00950301\n",
      "exploration/env_infos/reward_forward Std                0.0606363\n",
      "exploration/env_infos/reward_forward Max                0.39032\n",
      "exploration/env_infos/reward_forward Min               -0.163734\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.261184\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.261184\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.261184\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0602016\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0602016\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0602016\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.278872\n",
      "exploration/env_infos/reward_ctrl Std                   0.0608186\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0602016\n",
      "exploration/env_infos/reward_ctrl Min                  -0.522596\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00629412\n",
      "exploration/env_infos/final/torso_velocity Std          0.0257993\n",
      "exploration/env_infos/final/torso_velocity Max          0.0385892\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0245563\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.223292\n",
      "exploration/env_infos/initial/torso_velocity Std        0.287139\n",
      "exploration/env_infos/initial/torso_velocity Max        0.62789\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0089861\n",
      "exploration/env_infos/torso_velocity Mean               0.00131625\n",
      "exploration/env_infos/torso_velocity Std                0.0685288\n",
      "exploration/env_infos/torso_velocity Max                0.62789\n",
      "exploration/env_infos/torso_velocity Min               -0.827876\n",
      "evaluation/num steps total                              1.75e+06\n",
      "evaluation/num paths total                           1750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.813661\n",
      "evaluation/Rewards Std                                  0.0680369\n",
      "evaluation/Rewards Max                                  2.39518\n",
      "evaluation/Rewards Min                                 -0.0322592\n",
      "evaluation/Returns Mean                               813.661\n",
      "evaluation/Returns Std                                 62.14\n",
      "evaluation/Returns Max                                882.233\n",
      "evaluation/Returns Min                                677.512\n",
      "evaluation/Actions Mean                                 0.0134849\n",
      "evaluation/Actions Std                                  0.215827\n",
      "evaluation/Actions Max                                  0.679596\n",
      "evaluation/Actions Min                                 -0.840017\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            813.661\n",
      "evaluation/env_infos/final/reward_forward Mean          4.68881e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.49975e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.77576e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.26812e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00256015\n",
      "evaluation/env_infos/initial/reward_forward Std         0.133017\n",
      "evaluation/env_infos/initial/reward_forward Max         0.218398\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.280257\n",
      "evaluation/env_infos/reward_forward Mean                0.000499734\n",
      "evaluation/env_infos/reward_forward Std                 0.0462139\n",
      "evaluation/env_infos/reward_forward Max                 1.64089\n",
      "evaluation/env_infos/reward_forward Min                -1.26532\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.186587\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0624226\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.118526\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.324488\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0660126\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0127585\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0497468\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.102127\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.187053\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0644624\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0491199\n",
      "evaluation/env_infos/reward_ctrl Min                   -1.03226\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.25918e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.62015e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.52771e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.91971e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.12485\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.232798\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.572317\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.280257\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0011509\n",
      "evaluation/env_infos/torso_velocity Std                 0.0546574\n",
      "evaluation/env_infos/torso_velocity Max                 1.64089\n",
      "evaluation/env_infos/torso_velocity Min                -1.62594\n",
      "time/data storing (s)                                   0.0315679\n",
      "time/evaluation sampling (s)                           44.9255\n",
      "time/exploration sampling (s)                           2.07785\n",
      "time/logging (s)                                        0.279901\n",
      "time/saving (s)                                         0.0259227\n",
      "time/training (s)                                       4.30657\n",
      "time/epoch (s)                                         51.6473\n",
      "time/total (s)                                       3738.85\n",
      "Epoch                                                  69\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:58:01.069080 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 70 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 144000\n",
      "trainer/QF1 Loss                                        0.232288\n",
      "trainer/QF2 Loss                                        0.313903\n",
      "trainer/Policy Loss                                   -15.6247\n",
      "trainer/Q1 Predictions Mean                            23.492\n",
      "trainer/Q1 Predictions Std                              1.91833\n",
      "trainer/Q1 Predictions Max                             25.6721\n",
      "trainer/Q1 Predictions Min                              9.7327\n",
      "trainer/Q2 Predictions Mean                            23.7356\n",
      "trainer/Q2 Predictions Std                              1.80927\n",
      "trainer/Q2 Predictions Max                             25.6876\n",
      "trainer/Q2 Predictions Min                             12.6468\n",
      "trainer/Q Targets Mean                                 23.7102\n",
      "trainer/Q Targets Std                                   1.92765\n",
      "trainer/Q Targets Max                                  26.6311\n",
      "trainer/Q Targets Min                                  10.0843\n",
      "trainer/Log Pis Mean                                    8.12514\n",
      "trainer/Log Pis Std                                     2.46143\n",
      "trainer/Log Pis Max                                    19.2134\n",
      "trainer/Log Pis Min                                     0.946965\n",
      "trainer/Policy mu Mean                                 -0.0293579\n",
      "trainer/Policy mu Std                                   0.188084\n",
      "trainer/Policy mu Max                                   2.27912\n",
      "trainer/Policy mu Min                                  -1.75823\n",
      "trainer/Policy log std Mean                            -2.38467\n",
      "trainer/Policy log std Std                              0.254076\n",
      "trainer/Policy log std Max                             -1.73002\n",
      "trainer/Policy log std Min                             -4.03976\n",
      "trainer/Alpha                                           0.00718328\n",
      "trainer/Alpha Loss                                      0.617712\n",
      "exploration/num steps total                         72000\n",
      "exploration/num paths total                           144\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.897814\n",
      "exploration/Rewards Std                                 0.0636015\n",
      "exploration/Rewards Max                                 1.38074\n",
      "exploration/Rewards Min                                 0.700822\n",
      "exploration/Returns Mean                              897.814\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               897.814\n",
      "exploration/Returns Min                               897.814\n",
      "exploration/Actions Mean                               -0.0271991\n",
      "exploration/Actions Std                                 0.165021\n",
      "exploration/Actions Max                                 0.482226\n",
      "exploration/Actions Min                                -0.538277\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           897.814\n",
      "exploration/env_infos/final/reward_forward Mean         0.195427\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.195427\n",
      "exploration/env_infos/final/reward_forward Min          0.195427\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0244098\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0244098\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0244098\n",
      "exploration/env_infos/reward_forward Mean               0.0292425\n",
      "exploration/env_infos/reward_forward Std                0.131828\n",
      "exploration/env_infos/reward_forward Max                0.67151\n",
      "exploration/env_infos/reward_forward Min               -0.502417\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0966951\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0966951\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0966951\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.116701\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.116701\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.116701\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.111887\n",
      "exploration/env_infos/reward_ctrl Std                   0.0474809\n",
      "exploration/env_infos/reward_ctrl Max                  -0.019853\n",
      "exploration/env_infos/reward_ctrl Min                  -0.299178\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.188063\n",
      "exploration/env_infos/final/torso_velocity Std          0.228049\n",
      "exploration/env_infos/final/torso_velocity Max          0.463611\n",
      "exploration/env_infos/final/torso_velocity Min         -0.094848\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.156584\n",
      "exploration/env_infos/initial/torso_velocity Std        0.324832\n",
      "exploration/env_infos/initial/torso_velocity Max        0.612738\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.118576\n",
      "exploration/env_infos/torso_velocity Mean               0.010953\n",
      "exploration/env_infos/torso_velocity Std                0.182555\n",
      "exploration/env_infos/torso_velocity Max                0.763245\n",
      "exploration/env_infos/torso_velocity Min               -1.55416\n",
      "evaluation/num steps total                              1.775e+06\n",
      "evaluation/num paths total                           1775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.870269\n",
      "evaluation/Rewards Std                                  0.0580479\n",
      "evaluation/Rewards Max                                  1.87047\n",
      "evaluation/Rewards Min                                  0.624033\n",
      "evaluation/Returns Mean                               870.269\n",
      "evaluation/Returns Std                                 52.8905\n",
      "evaluation/Returns Max                                944.837\n",
      "evaluation/Returns Min                                734.95\n",
      "evaluation/Actions Mean                                -0.03646\n",
      "evaluation/Actions Std                                  0.176874\n",
      "evaluation/Actions Max                                  0.499671\n",
      "evaluation/Actions Min                                 -0.556566\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            870.269\n",
      "evaluation/env_infos/final/reward_forward Mean          4.36846e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.37142e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.76285e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.51127e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0247608\n",
      "evaluation/env_infos/initial/reward_forward Std         0.117281\n",
      "evaluation/env_infos/initial/reward_forward Max         0.261206\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.182455\n",
      "evaluation/env_infos/reward_forward Mean                0.00137058\n",
      "evaluation/env_infos/reward_forward Std                 0.0525759\n",
      "evaluation/env_infos/reward_forward Max                 1.12688\n",
      "evaluation/env_infos/reward_forward Min                -1.44674\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.131887\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0539096\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0574112\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.273168\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.096351\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0701533\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.013984\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.224989\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.130455\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0540059\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0138124\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.375967\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.43607e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.973e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.29861e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.51127e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144992\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246879\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.676872\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.342158\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00171632\n",
      "evaluation/env_infos/torso_velocity Std                 0.0597287\n",
      "evaluation/env_infos/torso_velocity Max                 1.18613\n",
      "evaluation/env_infos/torso_velocity Min                -1.69327\n",
      "time/data storing (s)                                   0.0344783\n",
      "time/evaluation sampling (s)                           45.6265\n",
      "time/exploration sampling (s)                           1.98431\n",
      "time/logging (s)                                        0.271269\n",
      "time/saving (s)                                         0.0254237\n",
      "time/training (s)                                       4.31062\n",
      "time/epoch (s)                                         52.2526\n",
      "time/total (s)                                       3791.75\n",
      "Epoch                                                  70\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:58:54.415594 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 71 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 146000\n",
      "trainer/QF1 Loss                                        0.332845\n",
      "trainer/QF2 Loss                                        0.636521\n",
      "trainer/Policy Loss                                   -15.7857\n",
      "trainer/Q1 Predictions Mean                            23.8571\n",
      "trainer/Q1 Predictions Std                              2.27668\n",
      "trainer/Q1 Predictions Max                             26.1005\n",
      "trainer/Q1 Predictions Min                              2.01096\n",
      "trainer/Q2 Predictions Mean                            23.8934\n",
      "trainer/Q2 Predictions Std                              2.0196\n",
      "trainer/Q2 Predictions Max                             26.0606\n",
      "trainer/Q2 Predictions Min                              9.37643\n",
      "trainer/Q Targets Mean                                 23.8571\n",
      "trainer/Q Targets Std                                   2.23913\n",
      "trainer/Q Targets Max                                  25.9455\n",
      "trainer/Q Targets Min                                  -0.43273\n",
      "trainer/Log Pis Mean                                    8.30417\n",
      "trainer/Log Pis Std                                     2.63749\n",
      "trainer/Log Pis Max                                    20.7839\n",
      "trainer/Log Pis Min                                    -0.535115\n",
      "trainer/Policy mu Mean                                 -0.00624553\n",
      "trainer/Policy mu Std                                   0.18731\n",
      "trainer/Policy mu Max                                   1.15847\n",
      "trainer/Policy mu Min                                  -1.41474\n",
      "trainer/Policy log std Mean                            -2.37252\n",
      "trainer/Policy log std Std                              0.27291\n",
      "trainer/Policy log std Max                             -0.536592\n",
      "trainer/Policy log std Min                             -4.13454\n",
      "trainer/Alpha                                           0.00724769\n",
      "trainer/Alpha Loss                                      1.49879\n",
      "exploration/num steps total                         73000\n",
      "exploration/num paths total                           145\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.895147\n",
      "exploration/Rewards Std                                 0.0679639\n",
      "exploration/Rewards Max                                 1.55355\n",
      "exploration/Rewards Min                                 0.69791\n",
      "exploration/Returns Mean                              895.147\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               895.147\n",
      "exploration/Returns Min                               895.147\n",
      "exploration/Actions Mean                                0.0437198\n",
      "exploration/Actions Std                                 0.160997\n",
      "exploration/Actions Max                                 0.502992\n",
      "exploration/Actions Min                                -0.475735\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           895.147\n",
      "exploration/env_infos/final/reward_forward Mean         0.197629\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.197629\n",
      "exploration/env_infos/final/reward_forward Min          0.197629\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0848726\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0848726\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0848726\n",
      "exploration/env_infos/reward_forward Mean              -0.0165719\n",
      "exploration/env_infos/reward_forward Std                0.188175\n",
      "exploration/env_infos/reward_forward Max                0.841672\n",
      "exploration/env_infos/reward_forward Min               -0.94062\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0911302\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0911302\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0911302\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0477411\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0477411\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0477411\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.111326\n",
      "exploration/env_infos/reward_ctrl Std                   0.0445964\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0166134\n",
      "exploration/env_infos/reward_ctrl Min                  -0.30209\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.156676\n",
      "exploration/env_infos/final/torso_velocity Std          0.186435\n",
      "exploration/env_infos/final/torso_velocity Max          0.361763\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0893642\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.167659\n",
      "exploration/env_infos/initial/torso_velocity Std        0.357389\n",
      "exploration/env_infos/initial/torso_velocity Max        0.673084\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.085234\n",
      "exploration/env_infos/torso_velocity Mean              -0.00984736\n",
      "exploration/env_infos/torso_velocity Std                0.197414\n",
      "exploration/env_infos/torso_velocity Max                0.995937\n",
      "exploration/env_infos/torso_velocity Min               -1.39926\n",
      "evaluation/num steps total                              1.8e+06\n",
      "evaluation/num paths total                           1800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.93513\n",
      "evaluation/Rewards Std                                  0.0426751\n",
      "evaluation/Rewards Max                                  2.76519\n",
      "evaluation/Rewards Min                                  0.579418\n",
      "evaluation/Returns Mean                               935.13\n",
      "evaluation/Returns Std                                 33.7608\n",
      "evaluation/Returns Max                                980.755\n",
      "evaluation/Returns Min                                856.686\n",
      "evaluation/Actions Mean                                 0.0229264\n",
      "evaluation/Actions Std                                  0.126049\n",
      "evaluation/Actions Max                                  0.549076\n",
      "evaluation/Actions Min                                 -0.50792\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            935.13\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.12256e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.84728e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.96486e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.26538e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0682355\n",
      "evaluation/env_infos/initial/reward_forward Std         0.108418\n",
      "evaluation/env_infos/initial/reward_forward Max         0.132111\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.287382\n",
      "evaluation/env_infos/reward_forward Mean               -0.00170814\n",
      "evaluation/env_infos/reward_forward Std                 0.0531325\n",
      "evaluation/env_infos/reward_forward Max                 0.923452\n",
      "evaluation/env_infos/reward_forward Min                -1.82394\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0655427\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0344236\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0187714\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.145372\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0566279\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0257859\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0164626\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0996673\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0656558\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0351457\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00823752\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.420582\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.68705e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.50687e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.96486e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.09014e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.120488\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.25343\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.646654\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.287382\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00201034\n",
      "evaluation/env_infos/torso_velocity Std                 0.063272\n",
      "evaluation/env_infos/torso_velocity Max                 1.22802\n",
      "evaluation/env_infos/torso_velocity Min                -1.99415\n",
      "time/data storing (s)                                   0.0301026\n",
      "time/evaluation sampling (s)                           45.7849\n",
      "time/exploration sampling (s)                           2.25898\n",
      "time/logging (s)                                        0.267358\n",
      "time/saving (s)                                         0.0290526\n",
      "time/training (s)                                       4.33521\n",
      "time/epoch (s)                                         52.7056\n",
      "time/total (s)                                       3845.1\n",
      "Epoch                                                  71\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 12:59:47.008707 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 72 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 148000\n",
      "trainer/QF1 Loss                                        0.463969\n",
      "trainer/QF2 Loss                                        0.499806\n",
      "trainer/Policy Loss                                   -15.4543\n",
      "trainer/Q1 Predictions Mean                            23.9376\n",
      "trainer/Q1 Predictions Std                              2.40078\n",
      "trainer/Q1 Predictions Max                             25.6939\n",
      "trainer/Q1 Predictions Min                             -2.81556\n",
      "trainer/Q2 Predictions Mean                            23.928\n",
      "trainer/Q2 Predictions Std                              2.47861\n",
      "trainer/Q2 Predictions Max                             25.7205\n",
      "trainer/Q2 Predictions Min                             -4.70506\n",
      "trainer/Q Targets Mean                                 24.2308\n",
      "trainer/Q Targets Std                                   2.2182\n",
      "trainer/Q Targets Max                                  28.0983\n",
      "trainer/Q Targets Min                                   0.400082\n",
      "trainer/Log Pis Mean                                    8.65848\n",
      "trainer/Log Pis Std                                     2.82322\n",
      "trainer/Log Pis Max                                    21.6393\n",
      "trainer/Log Pis Min                                    -0.472578\n",
      "trainer/Policy mu Mean                                  0.0031351\n",
      "trainer/Policy mu Std                                   0.197062\n",
      "trainer/Policy mu Max                                   2.17223\n",
      "trainer/Policy mu Min                                  -1.56352\n",
      "trainer/Policy log std Mean                            -2.43557\n",
      "trainer/Policy log std Std                              0.3088\n",
      "trainer/Policy log std Max                             -0.643501\n",
      "trainer/Policy log std Min                             -4.57506\n",
      "trainer/Alpha                                           0.00770741\n",
      "trainer/Alpha Loss                                      3.20413\n",
      "exploration/num steps total                         74000\n",
      "exploration/num paths total                           146\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.879316\n",
      "exploration/Rewards Std                                 0.0589004\n",
      "exploration/Rewards Max                                 1.95515\n",
      "exploration/Rewards Min                                 0.693135\n",
      "exploration/Returns Mean                              879.316\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               879.316\n",
      "exploration/Returns Min                               879.316\n",
      "exploration/Actions Mean                               -0.0523759\n",
      "exploration/Actions Std                                 0.168669\n",
      "exploration/Actions Max                                 0.552818\n",
      "exploration/Actions Min                                -0.479892\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           879.316\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0501204\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0501204\n",
      "exploration/env_infos/final/reward_forward Min         -0.0501204\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.125824\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.125824\n",
      "exploration/env_infos/initial/reward_forward Min       -0.125824\n",
      "exploration/env_infos/reward_forward Mean              -0.00547463\n",
      "exploration/env_infos/reward_forward Std                0.0922282\n",
      "exploration/env_infos/reward_forward Max                0.313668\n",
      "exploration/env_infos/reward_forward Min               -0.95275\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.214015\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.214015\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.214015\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.16067\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.16067\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.16067\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.124769\n",
      "exploration/env_infos/reward_ctrl Std                   0.0429476\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0181393\n",
      "exploration/env_infos/reward_ctrl Min                  -0.306865\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00160542\n",
      "exploration/env_infos/final/torso_velocity Std          0.0429204\n",
      "exploration/env_infos/final/torso_velocity Max          0.0542426\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0501204\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.120726\n",
      "exploration/env_infos/initial/torso_velocity Std        0.250863\n",
      "exploration/env_infos/initial/torso_velocity Max        0.464926\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.125824\n",
      "exploration/env_infos/torso_velocity Mean              -0.000262337\n",
      "exploration/env_infos/torso_velocity Std                0.103757\n",
      "exploration/env_infos/torso_velocity Max                0.542\n",
      "exploration/env_infos/torso_velocity Min               -1.49821\n",
      "evaluation/num steps total                              1.825e+06\n",
      "evaluation/num paths total                           1825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.79045\n",
      "evaluation/Rewards Std                                  0.0822039\n",
      "evaluation/Rewards Max                                  1.63339\n",
      "evaluation/Rewards Min                                  0.601214\n",
      "evaluation/Returns Mean                               790.45\n",
      "evaluation/Returns Std                                 80.7533\n",
      "evaluation/Returns Max                                914.37\n",
      "evaluation/Returns Min                                683.65\n",
      "evaluation/Actions Mean                                -0.0312973\n",
      "evaluation/Actions Std                                  0.226875\n",
      "evaluation/Actions Max                                  0.481563\n",
      "evaluation/Actions Min                                 -0.673007\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            790.45\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.68749e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.27365e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.42508e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.50602e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0256927\n",
      "evaluation/env_infos/initial/reward_forward Std         0.150695\n",
      "evaluation/env_infos/initial/reward_forward Max         0.3386\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.24104\n",
      "evaluation/env_infos/reward_forward Mean               -0.00188432\n",
      "evaluation/env_infos/reward_forward Std                 0.049218\n",
      "evaluation/env_infos/reward_forward Max                 1.07101\n",
      "evaluation/env_infos/reward_forward Min                -1.82051\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.210444\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0813598\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0859404\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.318068\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0756162\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0343529\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0432597\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.148916\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.209808\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0814234\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0187618\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.398786\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.43091e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.23758e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           8.23239e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.50602e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.109604\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.25828\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.673671\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.24104\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0031614\n",
      "evaluation/env_infos/torso_velocity Std                 0.0575698\n",
      "evaluation/env_infos/torso_velocity Max                 1.07101\n",
      "evaluation/env_infos/torso_velocity Min                -1.82051\n",
      "time/data storing (s)                                   0.0303991\n",
      "time/evaluation sampling (s)                           45.0119\n",
      "time/exploration sampling (s)                           2.02316\n",
      "time/logging (s)                                        0.295419\n",
      "time/saving (s)                                         0.0310531\n",
      "time/training (s)                                       4.52821\n",
      "time/epoch (s)                                         51.9201\n",
      "time/total (s)                                       3897.72\n",
      "Epoch                                                  72\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:00:41.510232 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 73 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 150000\n",
      "trainer/QF1 Loss                                        0.189292\n",
      "trainer/QF2 Loss                                        0.167514\n",
      "trainer/Policy Loss                                   -15.05\n",
      "trainer/Q1 Predictions Mean                            24.5422\n",
      "trainer/Q1 Predictions Std                              1.8689\n",
      "trainer/Q1 Predictions Max                             26.9762\n",
      "trainer/Q1 Predictions Min                             15.2639\n",
      "trainer/Q2 Predictions Mean                            24.6176\n",
      "trainer/Q2 Predictions Std                              1.75862\n",
      "trainer/Q2 Predictions Max                             26.7212\n",
      "trainer/Q2 Predictions Min                             15.6055\n",
      "trainer/Q Targets Mean                                 24.5545\n",
      "trainer/Q Targets Std                                   1.72678\n",
      "trainer/Q Targets Max                                  26.917\n",
      "trainer/Q Targets Min                                  15.3386\n",
      "trainer/Log Pis Mean                                    9.7808\n",
      "trainer/Log Pis Std                                     2.6851\n",
      "trainer/Log Pis Max                                    16.8777\n",
      "trainer/Log Pis Min                                     2.58518\n",
      "trainer/Policy mu Mean                                  0.128127\n",
      "trainer/Policy mu Std                                   0.160019\n",
      "trainer/Policy mu Max                                   1.44537\n",
      "trainer/Policy mu Min                                  -0.753352\n",
      "trainer/Policy log std Mean                            -2.58324\n",
      "trainer/Policy log std Std                              0.274465\n",
      "trainer/Policy log std Max                             -1.97969\n",
      "trainer/Policy log std Min                             -3.94167\n",
      "trainer/Alpha                                           0.00873324\n",
      "trainer/Alpha Loss                                      8.44937\n",
      "exploration/num steps total                         75000\n",
      "exploration/num paths total                           147\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.718876\n",
      "exploration/Rewards Std                                 0.104739\n",
      "exploration/Rewards Max                                 1.40143\n",
      "exploration/Rewards Min                                 0.503384\n",
      "exploration/Returns Mean                              718.876\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               718.876\n",
      "exploration/Returns Min                               718.876\n",
      "exploration/Actions Mean                                0.177269\n",
      "exploration/Actions Std                                 0.204881\n",
      "exploration/Actions Max                                 0.597372\n",
      "exploration/Actions Min                                -0.425705\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           718.876\n",
      "exploration/env_infos/final/reward_forward Mean         0.211861\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.211861\n",
      "exploration/env_infos/final/reward_forward Min          0.211861\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0758129\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0758129\n",
      "exploration/env_infos/initial/reward_forward Min        0.0758129\n",
      "exploration/env_infos/reward_forward Mean               0.0227975\n",
      "exploration/env_infos/reward_forward Std                0.112945\n",
      "exploration/env_infos/reward_forward Max                0.83427\n",
      "exploration/env_infos/reward_forward Min               -0.611862\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.18226\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.18226\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.18226\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0953521\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0953521\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0953521\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.293603\n",
      "exploration/env_infos/reward_ctrl Std                   0.0757211\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0519289\n",
      "exploration/env_infos/reward_ctrl Min                  -0.496616\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0151378\n",
      "exploration/env_infos/final/torso_velocity Std          0.151862\n",
      "exploration/env_infos/final/torso_velocity Max          0.211861\n",
      "exploration/env_infos/final/torso_velocity Min         -0.157844\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.206708\n",
      "exploration/env_infos/initial/torso_velocity Std        0.16101\n",
      "exploration/env_infos/initial/torso_velocity Max        0.433512\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0758129\n",
      "exploration/env_infos/torso_velocity Mean               0.00322098\n",
      "exploration/env_infos/torso_velocity Std                0.139519\n",
      "exploration/env_infos/torso_velocity Max                0.83427\n",
      "exploration/env_infos/torso_velocity Min               -1.60461\n",
      "evaluation/num steps total                              1.85e+06\n",
      "evaluation/num paths total                           1850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.683326\n",
      "evaluation/Rewards Std                                  0.0597331\n",
      "evaluation/Rewards Max                                  1.93699\n",
      "evaluation/Rewards Min                                  0.35982\n",
      "evaluation/Returns Mean                               683.326\n",
      "evaluation/Returns Std                                 50.9971\n",
      "evaluation/Returns Max                                812.991\n",
      "evaluation/Returns Min                                576.446\n",
      "evaluation/Actions Mean                                 0.199013\n",
      "evaluation/Actions Std                                  0.199182\n",
      "evaluation/Actions Max                                  0.727941\n",
      "evaluation/Actions Min                                 -0.438874\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            683.326\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.85762e-08\n",
      "evaluation/env_infos/final/reward_forward Std           1.80624e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.18397e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -3.31266e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0141058\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136379\n",
      "evaluation/env_infos/initial/reward_forward Max         0.24615\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.255331\n",
      "evaluation/env_infos/reward_forward Mean                6.50212e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0501625\n",
      "evaluation/env_infos/reward_forward Max                 1.60634\n",
      "evaluation/env_infos/reward_forward Min                -1.43047\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.31525\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0589898\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.120918\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.424566\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.104724\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0368212\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0402623\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.163866\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.317118\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0571303\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0402623\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.66856\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.29311e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.4346e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           3.83426e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.89945e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135417\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.250397\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.664862\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.322471\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0027443\n",
      "evaluation/env_infos/torso_velocity Std                 0.0647096\n",
      "evaluation/env_infos/torso_velocity Max                 1.60634\n",
      "evaluation/env_infos/torso_velocity Min                -2.16429\n",
      "time/data storing (s)                                   0.0303669\n",
      "time/evaluation sampling (s)                           46.7121\n",
      "time/exploration sampling (s)                           2.15261\n",
      "time/logging (s)                                        0.28475\n",
      "time/saving (s)                                         0.0268608\n",
      "time/training (s)                                       4.57666\n",
      "time/epoch (s)                                         53.7834\n",
      "time/total (s)                                       3952.2\n",
      "Epoch                                                  73\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:01:35.827308 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 74 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 152000\n",
      "trainer/QF1 Loss                                        0.250886\n",
      "trainer/QF2 Loss                                        0.339604\n",
      "trainer/Policy Loss                                   -17.4323\n",
      "trainer/Q1 Predictions Mean                            24.632\n",
      "trainer/Q1 Predictions Std                              1.98485\n",
      "trainer/Q1 Predictions Max                             26.669\n",
      "trainer/Q1 Predictions Min                              8.72412\n",
      "trainer/Q2 Predictions Mean                            24.8814\n",
      "trainer/Q2 Predictions Std                              2.39474\n",
      "trainer/Q2 Predictions Max                             26.8683\n",
      "trainer/Q2 Predictions Min                             -0.0916165\n",
      "trainer/Q Targets Mean                                 24.8468\n",
      "trainer/Q Targets Std                                   2.01251\n",
      "trainer/Q Targets Max                                  26.9236\n",
      "trainer/Q Targets Min                                   5.71002\n",
      "trainer/Log Pis Mean                                    7.51365\n",
      "trainer/Log Pis Std                                     2.68935\n",
      "trainer/Log Pis Max                                    17.8716\n",
      "trainer/Log Pis Min                                    -1.73945\n",
      "trainer/Policy mu Mean                                  0.0108965\n",
      "trainer/Policy mu Std                                   0.213818\n",
      "trainer/Policy mu Max                                   3.82819\n",
      "trainer/Policy mu Min                                  -2.67157\n",
      "trainer/Policy log std Mean                            -2.31883\n",
      "trainer/Policy log std Std                              0.260876\n",
      "trainer/Policy log std Max                              0.412305\n",
      "trainer/Policy log std Min                             -3.63067\n",
      "trainer/Alpha                                           0.0084412\n",
      "trainer/Alpha Loss                                     -2.32156\n",
      "exploration/num steps total                         76000\n",
      "exploration/num paths total                           148\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.883679\n",
      "exploration/Rewards Std                                 0.148742\n",
      "exploration/Rewards Max                                 2.2104\n",
      "exploration/Rewards Min                                 0.619637\n",
      "exploration/Returns Mean                              883.679\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               883.679\n",
      "exploration/Returns Min                               883.679\n",
      "exploration/Actions Mean                                0.0144316\n",
      "exploration/Actions Std                                 0.198424\n",
      "exploration/Actions Max                                 0.520083\n",
      "exploration/Actions Min                                -0.715149\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           883.679\n",
      "exploration/env_infos/final/reward_forward Mean         0.235362\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.235362\n",
      "exploration/env_infos/final/reward_forward Min          0.235362\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0354563\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0354563\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0354563\n",
      "exploration/env_infos/reward_forward Mean               0.00332225\n",
      "exploration/env_infos/reward_forward Std                0.21302\n",
      "exploration/env_infos/reward_forward Max                1.11007\n",
      "exploration/env_infos/reward_forward Min               -0.703061\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.184296\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.184296\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.184296\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.100956\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.100956\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.100956\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.158322\n",
      "exploration/env_infos/reward_ctrl Std                   0.0562154\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0198743\n",
      "exploration/env_infos/reward_ctrl Min                  -0.417284\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.164184\n",
      "exploration/env_infos/final/torso_velocity Std          0.0512104\n",
      "exploration/env_infos/final/torso_velocity Max          0.235362\n",
      "exploration/env_infos/final/torso_velocity Min          0.117017\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.14212\n",
      "exploration/env_infos/initial/torso_velocity Std        0.223236\n",
      "exploration/env_infos/initial/torso_velocity Max        0.456965\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0354563\n",
      "exploration/env_infos/torso_velocity Mean              -0.000386072\n",
      "exploration/env_infos/torso_velocity Std                0.156832\n",
      "exploration/env_infos/torso_velocity Max                1.11007\n",
      "exploration/env_infos/torso_velocity Min               -1.41931\n",
      "evaluation/num steps total                              1.875e+06\n",
      "evaluation/num paths total                           1875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.83496\n",
      "evaluation/Rewards Std                                  0.0602323\n",
      "evaluation/Rewards Max                                  2.05686\n",
      "evaluation/Rewards Min                                  0.477826\n",
      "evaluation/Returns Mean                               834.96\n",
      "evaluation/Returns Std                                 55.3962\n",
      "evaluation/Returns Max                                940.698\n",
      "evaluation/Returns Min                                735.365\n",
      "evaluation/Actions Mean                                 0.021085\n",
      "evaluation/Actions Std                                  0.202439\n",
      "evaluation/Actions Max                                  0.680873\n",
      "evaluation/Actions Min                                 -0.753299\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            834.96\n",
      "evaluation/env_infos/final/reward_forward Mean          6.66133e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.69828e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.31225e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.18354e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0568296\n",
      "evaluation/env_infos/initial/reward_forward Std         0.145068\n",
      "evaluation/env_infos/initial/reward_forward Max         0.179498\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.385235\n",
      "evaluation/env_infos/reward_forward Mean               -0.00363717\n",
      "evaluation/env_infos/reward_forward Std                 0.0564223\n",
      "evaluation/env_infos/reward_forward Max                 0.768024\n",
      "evaluation/env_infos/reward_forward Min                -1.22662\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.166304\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0561083\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0621166\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.270219\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0801377\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0188908\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0399243\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.115498\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.165704\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0568189\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0139346\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.522174\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.66668e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.55787e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.31225e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.18354e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.117345\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239933\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.591077\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.385235\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00394646\n",
      "evaluation/env_infos/torso_velocity Std                 0.0614778\n",
      "evaluation/env_infos/torso_velocity Max                 0.858646\n",
      "evaluation/env_infos/torso_velocity Min                -1.67624\n",
      "time/data storing (s)                                   0.0329008\n",
      "time/evaluation sampling (s)                           46.8916\n",
      "time/exploration sampling (s)                           2.08722\n",
      "time/logging (s)                                        0.280785\n",
      "time/saving (s)                                         0.0262758\n",
      "time/training (s)                                       4.29408\n",
      "time/epoch (s)                                         53.6129\n",
      "time/total (s)                                       4006.52\n",
      "Epoch                                                  74\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:02:30.227203 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 75 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 154000\n",
      "trainer/QF1 Loss                                        1.93889\n",
      "trainer/QF2 Loss                                        2.02926\n",
      "trainer/Policy Loss                                   -17.6617\n",
      "trainer/Q1 Predictions Mean                            25.1585\n",
      "trainer/Q1 Predictions Std                              1.59181\n",
      "trainer/Q1 Predictions Max                             26.8774\n",
      "trainer/Q1 Predictions Min                             16.6654\n",
      "trainer/Q2 Predictions Mean                            25.4743\n",
      "trainer/Q2 Predictions Std                              1.63784\n",
      "trainer/Q2 Predictions Max                             27.13\n",
      "trainer/Q2 Predictions Min                             16.385\n",
      "trainer/Q Targets Mean                                 25.1392\n",
      "trainer/Q Targets Std                                   2.29769\n",
      "trainer/Q Targets Max                                  27.0601\n",
      "trainer/Q Targets Min                                  -0.31103\n",
      "trainer/Log Pis Mean                                    7.70111\n",
      "trainer/Log Pis Std                                     2.27757\n",
      "trainer/Log Pis Max                                    17.9384\n",
      "trainer/Log Pis Min                                    -0.205908\n",
      "trainer/Policy mu Mean                                  0.0272249\n",
      "trainer/Policy mu Std                                   0.13834\n",
      "trainer/Policy mu Max                                   0.539389\n",
      "trainer/Policy mu Min                                  -1.1032\n",
      "trainer/Policy log std Mean                            -2.35989\n",
      "trainer/Policy log std Std                              0.23544\n",
      "trainer/Policy log std Max                             -1.75687\n",
      "trainer/Policy log std Min                             -4.13773\n",
      "trainer/Alpha                                           0.00814035\n",
      "trainer/Alpha Loss                                     -1.4377\n",
      "exploration/num steps total                         77000\n",
      "exploration/num paths total                           149\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.905268\n",
      "exploration/Rewards Std                                 0.0768645\n",
      "exploration/Rewards Max                                 1.51068\n",
      "exploration/Rewards Min                                 0.730775\n",
      "exploration/Returns Mean                              905.268\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               905.268\n",
      "exploration/Returns Min                               905.268\n",
      "exploration/Actions Mean                                0.0535159\n",
      "exploration/Actions Std                                 0.155868\n",
      "exploration/Actions Max                                 0.537725\n",
      "exploration/Actions Min                                -0.528273\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           905.268\n",
      "exploration/env_infos/final/reward_forward Mean         0.465131\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.465131\n",
      "exploration/env_infos/final/reward_forward Min          0.465131\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0420676\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0420676\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0420676\n",
      "exploration/env_infos/reward_forward Mean               0.0152199\n",
      "exploration/env_infos/reward_forward Std                0.152025\n",
      "exploration/env_infos/reward_forward Max                0.670576\n",
      "exploration/env_infos/reward_forward Min               -0.662263\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0868171\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0868171\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0868171\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0751403\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0751403\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0751403\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.108635\n",
      "exploration/env_infos/reward_ctrl Std                   0.0449373\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0161318\n",
      "exploration/env_infos/reward_ctrl Min                  -0.269225\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.220859\n",
      "exploration/env_infos/final/torso_velocity Std          0.208829\n",
      "exploration/env_infos/final/torso_velocity Max          0.465131\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0450224\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.121762\n",
      "exploration/env_infos/initial/torso_velocity Std        0.161461\n",
      "exploration/env_infos/initial/torso_velocity Max        0.341424\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0420676\n",
      "exploration/env_infos/torso_velocity Mean               0.00212849\n",
      "exploration/env_infos/torso_velocity Std                0.163506\n",
      "exploration/env_infos/torso_velocity Max                0.674334\n",
      "exploration/env_infos/torso_velocity Min               -1.00363\n",
      "evaluation/num steps total                              1.9e+06\n",
      "evaluation/num paths total                           1900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.924284\n",
      "evaluation/Rewards Std                                  0.0382891\n",
      "evaluation/Rewards Max                                  2.60301\n",
      "evaluation/Rewards Min                                  0.670361\n",
      "evaluation/Returns Mean                               924.284\n",
      "evaluation/Returns Std                                 17.175\n",
      "evaluation/Returns Max                                946.634\n",
      "evaluation/Returns Min                                873.234\n",
      "evaluation/Actions Mean                                 0.0503692\n",
      "evaluation/Actions Std                                  0.129223\n",
      "evaluation/Actions Max                                  0.482528\n",
      "evaluation/Actions Min                                 -0.453004\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            924.284\n",
      "evaluation/env_infos/final/reward_forward Mean          2.93783e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.42863e-05\n",
      "evaluation/env_infos/final/reward_forward Max           7.29159e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -6.76286e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0225426\n",
      "evaluation/env_infos/initial/reward_forward Std         0.153813\n",
      "evaluation/env_infos/initial/reward_forward Max         0.299312\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.324662\n",
      "evaluation/env_infos/reward_forward Mean                0.00179874\n",
      "evaluation/env_infos/reward_forward Std                 0.078274\n",
      "evaluation/env_infos/reward_forward Max                 1.5913\n",
      "evaluation/env_infos/reward_forward Min                -1.7853\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0771863\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0177823\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0554375\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.129382\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0730621\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0255098\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0372292\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.127245\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0769427\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0187206\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0128778\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.329639\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.0128e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.53136e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           7.29159e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000110847\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.131835\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.257955\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.645588\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.324662\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000292918\n",
      "evaluation/env_infos/torso_velocity Std                 0.0723294\n",
      "evaluation/env_infos/torso_velocity Max                 1.5913\n",
      "evaluation/env_infos/torso_velocity Min                -1.81454\n",
      "time/data storing (s)                                   0.029467\n",
      "time/evaluation sampling (s)                           46.9475\n",
      "time/exploration sampling (s)                           2.03816\n",
      "time/logging (s)                                        0.28293\n",
      "time/saving (s)                                         0.0269111\n",
      "time/training (s)                                       4.40275\n",
      "time/epoch (s)                                         53.7278\n",
      "time/total (s)                                       4060.91\n",
      "Epoch                                                  75\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:03:23.860009 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 76 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 156000\n",
      "trainer/QF1 Loss                                        0.284481\n",
      "trainer/QF2 Loss                                        0.197369\n",
      "trainer/Policy Loss                                   -18.306\n",
      "trainer/Q1 Predictions Mean                            25.5582\n",
      "trainer/Q1 Predictions Std                              1.69788\n",
      "trainer/Q1 Predictions Max                             27.5932\n",
      "trainer/Q1 Predictions Min                             13.6197\n",
      "trainer/Q2 Predictions Mean                            25.4578\n",
      "trainer/Q2 Predictions Std                              1.74697\n",
      "trainer/Q2 Predictions Max                             27.6637\n",
      "trainer/Q2 Predictions Min                             15.0014\n",
      "trainer/Q Targets Mean                                 25.4869\n",
      "trainer/Q Targets Std                                   1.66621\n",
      "trainer/Q Targets Max                                  27.2352\n",
      "trainer/Q Targets Min                                  13.8439\n",
      "trainer/Log Pis Mean                                    7.37958\n",
      "trainer/Log Pis Std                                     2.66745\n",
      "trainer/Log Pis Max                                    14.9925\n",
      "trainer/Log Pis Min                                    -2.68365\n",
      "trainer/Policy mu Mean                                  0.00848698\n",
      "trainer/Policy mu Std                                   0.177048\n",
      "trainer/Policy mu Max                                   1.67037\n",
      "trainer/Policy mu Min                                  -0.891112\n",
      "trainer/Policy log std Mean                            -2.3109\n",
      "trainer/Policy log std Std                              0.254534\n",
      "trainer/Policy log std Max                             -1.6496\n",
      "trainer/Policy log std Min                             -3.61399\n",
      "trainer/Alpha                                           0.00737839\n",
      "trainer/Alpha Loss                                     -3.04441\n",
      "exploration/num steps total                         78000\n",
      "exploration/num paths total                           150\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.903472\n",
      "exploration/Rewards Std                                 0.0869204\n",
      "exploration/Rewards Max                                 1.61621\n",
      "exploration/Rewards Min                                 0.359927\n",
      "exploration/Returns Mean                              903.472\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               903.472\n",
      "exploration/Returns Min                               903.472\n",
      "exploration/Actions Mean                                0.00566021\n",
      "exploration/Actions Std                                 0.162417\n",
      "exploration/Actions Max                                 0.673532\n",
      "exploration/Actions Min                                -0.752502\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           903.472\n",
      "exploration/env_infos/final/reward_forward Mean         0.234895\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.234895\n",
      "exploration/env_infos/final/reward_forward Min          0.234895\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.180396\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.180396\n",
      "exploration/env_infos/initial/reward_forward Min       -0.180396\n",
      "exploration/env_infos/reward_forward Mean               0.0301807\n",
      "exploration/env_infos/reward_forward Std                0.303709\n",
      "exploration/env_infos/reward_forward Max                1.0893\n",
      "exploration/env_infos/reward_forward Min               -0.958353\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.095227\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.095227\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.095227\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0763365\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0763365\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0763365\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.105646\n",
      "exploration/env_infos/reward_ctrl Std                   0.0652984\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0141776\n",
      "exploration/env_infos/reward_ctrl Min                  -0.640073\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.105139\n",
      "exploration/env_infos/final/torso_velocity Std          0.240552\n",
      "exploration/env_infos/final/torso_velocity Max          0.234895\n",
      "exploration/env_infos/final/torso_velocity Min         -0.284144\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0288056\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254238\n",
      "exploration/env_infos/initial/torso_velocity Max        0.386648\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.180396\n",
      "exploration/env_infos/torso_velocity Mean               0.0236082\n",
      "exploration/env_infos/torso_velocity Std                0.255719\n",
      "exploration/env_infos/torso_velocity Max                1.1481\n",
      "exploration/env_infos/torso_velocity Min               -1.48898\n",
      "evaluation/num steps total                              1.925e+06\n",
      "evaluation/num paths total                           1925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.951752\n",
      "evaluation/Rewards Std                                  0.0231518\n",
      "evaluation/Rewards Max                                  2.07231\n",
      "evaluation/Rewards Min                                  0.458872\n",
      "evaluation/Returns Mean                               951.752\n",
      "evaluation/Returns Std                                 10.2935\n",
      "evaluation/Returns Max                                971.665\n",
      "evaluation/Returns Min                                928.286\n",
      "evaluation/Actions Mean                                 0.000854739\n",
      "evaluation/Actions Std                                  0.110793\n",
      "evaluation/Actions Max                                  0.676451\n",
      "evaluation/Actions Min                                 -0.73704\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            951.752\n",
      "evaluation/env_infos/final/reward_forward Mean          1.20309e-07\n",
      "evaluation/env_infos/final/reward_forward Std           5.01161e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.0581e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -8.86712e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0247249\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14062\n",
      "evaluation/env_infos/initial/reward_forward Max         0.2281\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.348088\n",
      "evaluation/env_infos/reward_forward Mean                0.00258821\n",
      "evaluation/env_infos/reward_forward Std                 0.0617377\n",
      "evaluation/env_infos/reward_forward Max                 1.17042\n",
      "evaluation/env_infos/reward_forward Min                -1.10825\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0485461\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.010465\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0283426\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0719966\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0834009\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0391813\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0297908\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.189165\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0491032\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0140458\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00597544\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.541128\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.61146e-10\n",
      "evaluation/env_infos/final/torso_velocity Std           3.60661e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.0581e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.4956e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.137738\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258482\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.701666\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.348088\n",
      "evaluation/env_infos/torso_velocity Mean                0.000116374\n",
      "evaluation/env_infos/torso_velocity Std                 0.0658954\n",
      "evaluation/env_infos/torso_velocity Max                 1.40165\n",
      "evaluation/env_infos/torso_velocity Min                -1.89116\n",
      "time/data storing (s)                                   0.0355916\n",
      "time/evaluation sampling (s)                           46.1147\n",
      "time/exploration sampling (s)                           2.10642\n",
      "time/logging (s)                                        0.276043\n",
      "time/saving (s)                                         0.0253569\n",
      "time/training (s)                                       4.33869\n",
      "time/epoch (s)                                         52.8968\n",
      "time/total (s)                                       4114.54\n",
      "Epoch                                                  76\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:04:17.334147 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 77 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 158000\n",
      "trainer/QF1 Loss                                        0.149636\n",
      "trainer/QF2 Loss                                        0.385948\n",
      "trainer/Policy Loss                                   -18.4381\n",
      "trainer/Q1 Predictions Mean                            25.7075\n",
      "trainer/Q1 Predictions Std                              2.2554\n",
      "trainer/Q1 Predictions Max                             27.7194\n",
      "trainer/Q1 Predictions Min                              2.04362\n",
      "trainer/Q2 Predictions Mean                            25.737\n",
      "trainer/Q2 Predictions Std                              2.0831\n",
      "trainer/Q2 Predictions Max                             27.7331\n",
      "trainer/Q2 Predictions Min                              7.19694\n",
      "trainer/Q Targets Mean                                 25.6994\n",
      "trainer/Q Targets Std                                   2.29185\n",
      "trainer/Q Targets Max                                  27.5565\n",
      "trainer/Q Targets Min                                  -0.149175\n",
      "trainer/Log Pis Mean                                    7.41926\n",
      "trainer/Log Pis Std                                     2.5259\n",
      "trainer/Log Pis Max                                    16.2428\n",
      "trainer/Log Pis Min                                    -1.40287\n",
      "trainer/Policy mu Mean                                  0.0487592\n",
      "trainer/Policy mu Std                                   0.164432\n",
      "trainer/Policy mu Max                                   1.03075\n",
      "trainer/Policy mu Min                                  -1.50787\n",
      "trainer/Policy log std Mean                            -2.31255\n",
      "trainer/Policy log std Std                              0.269099\n",
      "trainer/Policy log std Max                             -1.38\n",
      "trainer/Policy log std Min                             -3.79543\n",
      "trainer/Alpha                                           0.00743068\n",
      "trainer/Alpha Loss                                     -2.84667\n",
      "exploration/num steps total                         79000\n",
      "exploration/num paths total                           151\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.909447\n",
      "exploration/Rewards Std                                 0.0833255\n",
      "exploration/Rewards Max                                 1.46351\n",
      "exploration/Rewards Min                                 0.631903\n",
      "exploration/Returns Mean                              909.447\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               909.447\n",
      "exploration/Returns Min                               909.447\n",
      "exploration/Actions Mean                                0.03969\n",
      "exploration/Actions Std                                 0.160468\n",
      "exploration/Actions Max                                 0.654594\n",
      "exploration/Actions Min                                -0.689477\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           909.447\n",
      "exploration/env_infos/final/reward_forward Mean         0.00966281\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00966281\n",
      "exploration/env_infos/final/reward_forward Min          0.00966281\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0906126\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0906126\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0906126\n",
      "exploration/env_infos/reward_forward Mean              -0.0102266\n",
      "exploration/env_infos/reward_forward Std                0.147999\n",
      "exploration/env_infos/reward_forward Max                0.698609\n",
      "exploration/env_infos/reward_forward Min               -1.17811\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.27957\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.27957\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.27957\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0708183\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0708183\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0708183\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109301\n",
      "exploration/env_infos/reward_ctrl Std                   0.0522942\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0102484\n",
      "exploration/env_infos/reward_ctrl Min                  -0.368097\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0937786\n",
      "exploration/env_infos/final/torso_velocity Std          0.068565\n",
      "exploration/env_infos/final/torso_velocity Max          0.177611\n",
      "exploration/env_infos/final/torso_velocity Min          0.00966281\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0293279\n",
      "exploration/env_infos/initial/torso_velocity Std        0.296425\n",
      "exploration/env_infos/initial/torso_velocity Max        0.360459\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.357831\n",
      "exploration/env_infos/torso_velocity Mean               0.00416392\n",
      "exploration/env_infos/torso_velocity Std                0.153322\n",
      "exploration/env_infos/torso_velocity Max                0.843146\n",
      "exploration/env_infos/torso_velocity Min               -1.44811\n",
      "evaluation/num steps total                              1.95e+06\n",
      "evaluation/num paths total                           1950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.908258\n",
      "evaluation/Rewards Std                                  0.0417538\n",
      "evaluation/Rewards Max                                  2.2829\n",
      "evaluation/Rewards Min                                  0.566097\n",
      "evaluation/Returns Mean                               908.258\n",
      "evaluation/Returns Std                                 28.8405\n",
      "evaluation/Returns Max                                954.639\n",
      "evaluation/Returns Min                                869.61\n",
      "evaluation/Actions Mean                                 0.0460403\n",
      "evaluation/Actions Std                                  0.145655\n",
      "evaluation/Actions Max                                  0.684619\n",
      "evaluation/Actions Min                                 -0.512113\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            908.258\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00477235\n",
      "evaluation/env_infos/final/reward_forward Std           0.0233801\n",
      "evaluation/env_infos/final/reward_forward Max           0.119311\n",
      "evaluation/env_infos/final/reward_forward Min          -6.00003e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0335373\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122805\n",
      "evaluation/env_infos/initial/reward_forward Max         0.311405\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.158203\n",
      "evaluation/env_infos/reward_forward Mean               -0.000938461\n",
      "evaluation/env_infos/reward_forward Std                 0.132282\n",
      "evaluation/env_infos/reward_forward Max                 1.4645\n",
      "evaluation/env_infos/reward_forward Min                -1.37097\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0937499\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0296705\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0472574\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.131197\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0884194\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.026794\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0359797\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.135053\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0933409\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0303854\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.021915\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.433903\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00355479\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0183881\n",
      "evaluation/env_infos/final/torso_velocity Max           0.119311\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.43344e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.149181\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238517\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.669327\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.260519\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00193095\n",
      "evaluation/env_infos/torso_velocity Std                 0.102466\n",
      "evaluation/env_infos/torso_velocity Max                 1.4645\n",
      "evaluation/env_infos/torso_velocity Min                -1.96111\n",
      "time/data storing (s)                                   0.0310993\n",
      "time/evaluation sampling (s)                           46.0989\n",
      "time/exploration sampling (s)                           2.00513\n",
      "time/logging (s)                                        0.2856\n",
      "time/saving (s)                                         0.0291134\n",
      "time/training (s)                                       4.30161\n",
      "time/epoch (s)                                         52.7515\n",
      "time/total (s)                                       4168.02\n",
      "Epoch                                                  77\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:05:15.377496 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 78 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 160000\n",
      "trainer/QF1 Loss                                        0.344737\n",
      "trainer/QF2 Loss                                        0.349286\n",
      "trainer/Policy Loss                                   -16.9565\n",
      "trainer/Q1 Predictions Mean                            26.069\n",
      "trainer/Q1 Predictions Std                              1.75779\n",
      "trainer/Q1 Predictions Max                             28.0808\n",
      "trainer/Q1 Predictions Min                             16.9972\n",
      "trainer/Q2 Predictions Mean                            25.8105\n",
      "trainer/Q2 Predictions Std                              1.72016\n",
      "trainer/Q2 Predictions Max                             27.9468\n",
      "trainer/Q2 Predictions Min                             16.1676\n",
      "trainer/Q Targets Mean                                 26.096\n",
      "trainer/Q Targets Std                                   1.83424\n",
      "trainer/Q Targets Max                                  27.9952\n",
      "trainer/Q Targets Min                                  10.8989\n",
      "trainer/Log Pis Mean                                    9.0134\n",
      "trainer/Log Pis Std                                     3.07273\n",
      "trainer/Log Pis Max                                    25.8742\n",
      "trainer/Log Pis Min                                     2.0326\n",
      "trainer/Policy mu Mean                                  0.030189\n",
      "trainer/Policy mu Std                                   0.18116\n",
      "trainer/Policy mu Max                                   1.36337\n",
      "trainer/Policy mu Min                                  -1.62347\n",
      "trainer/Policy log std Mean                            -2.47937\n",
      "trainer/Policy log std Std                              0.305816\n",
      "trainer/Policy log std Max                             -1.82246\n",
      "trainer/Policy log std Min                             -4.82248\n",
      "trainer/Alpha                                           0.00741952\n",
      "trainer/Alpha Loss                                      4.97175\n",
      "exploration/num steps total                         80000\n",
      "exploration/num paths total                           152\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.9145\n",
      "exploration/Rewards Std                                 0.0439661\n",
      "exploration/Rewards Max                                 1.19727\n",
      "exploration/Rewards Min                                 0.563369\n",
      "exploration/Returns Mean                              914.5\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               914.5\n",
      "exploration/Returns Min                               914.5\n",
      "exploration/Actions Mean                                0.035398\n",
      "exploration/Actions Std                                 0.14581\n",
      "exploration/Actions Max                                 0.79403\n",
      "exploration/Actions Min                                -0.378467\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           914.5\n",
      "exploration/env_infos/final/reward_forward Mean         0.00548494\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00548494\n",
      "exploration/env_infos/final/reward_forward Min          0.00548494\n",
      "exploration/env_infos/initial/reward_forward Mean       0.118345\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.118345\n",
      "exploration/env_infos/initial/reward_forward Min        0.118345\n",
      "exploration/env_infos/reward_forward Mean              -0.0285993\n",
      "exploration/env_infos/reward_forward Std                0.120824\n",
      "exploration/env_infos/reward_forward Max                0.3343\n",
      "exploration/env_infos/reward_forward Min               -0.878412\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0720905\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0720905\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0720905\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.111373\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.111373\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.111373\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.090054\n",
      "exploration/env_infos/reward_ctrl Std                   0.0377726\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0114586\n",
      "exploration/env_infos/reward_ctrl Min                  -0.436631\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00464272\n",
      "exploration/env_infos/final/torso_velocity Std          0.00157974\n",
      "exploration/env_infos/final/torso_velocity Max          0.00601363\n",
      "exploration/env_infos/final/torso_velocity Min          0.00242958\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.123167\n",
      "exploration/env_infos/initial/torso_velocity Std        0.280633\n",
      "exploration/env_infos/initial/torso_velocity Max        0.469257\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.218099\n",
      "exploration/env_infos/torso_velocity Mean              -0.0111943\n",
      "exploration/env_infos/torso_velocity Std                0.121145\n",
      "exploration/env_infos/torso_velocity Max                0.710055\n",
      "exploration/env_infos/torso_velocity Min               -0.878412\n",
      "evaluation/num steps total                              1.975e+06\n",
      "evaluation/num paths total                           1975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.940538\n",
      "evaluation/Rewards Std                                  0.0224206\n",
      "evaluation/Rewards Max                                  2.06373\n",
      "evaluation/Rewards Min                                  0.482774\n",
      "evaluation/Returns Mean                               940.538\n",
      "evaluation/Returns Std                                  9.74\n",
      "evaluation/Returns Max                                961.718\n",
      "evaluation/Returns Min                                922.263\n",
      "evaluation/Actions Mean                                 0.0406244\n",
      "evaluation/Actions Std                                  0.115452\n",
      "evaluation/Actions Max                                  0.858005\n",
      "evaluation/Actions Min                                 -0.423376\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            940.538\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.90181e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.30482e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.69096e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.02574e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0784324\n",
      "evaluation/env_infos/initial/reward_forward Std         0.116796\n",
      "evaluation/env_infos/initial/reward_forward Max         0.19252\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.274927\n",
      "evaluation/env_infos/reward_forward Mean               -0.00334605\n",
      "evaluation/env_infos/reward_forward Std                 0.0590472\n",
      "evaluation/env_infos/reward_forward Max                 0.935384\n",
      "evaluation/env_infos/reward_forward Min                -1.62564\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0590709\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00986095\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0374407\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0779193\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0981866\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0307567\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.042007\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.172863\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0599182\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0181789\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00778419\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.517226\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.1969e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.10079e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           3.69096e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.02574e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126771\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.263332\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.64339\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.274927\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00252624\n",
      "evaluation/env_infos/torso_velocity Std                 0.0636344\n",
      "evaluation/env_infos/torso_velocity Max                 1.41927\n",
      "evaluation/env_infos/torso_velocity Min                -1.88711\n",
      "time/data storing (s)                                   0.0310911\n",
      "time/evaluation sampling (s)                           50.4148\n",
      "time/exploration sampling (s)                           2.0847\n",
      "time/logging (s)                                        0.284699\n",
      "time/saving (s)                                         0.0278775\n",
      "time/training (s)                                       4.49233\n",
      "time/epoch (s)                                         57.3355\n",
      "time/total (s)                                       4226.06\n",
      "Epoch                                                  78\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:06:10.676754 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 79 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 162000\n",
      "trainer/QF1 Loss                                        0.161981\n",
      "trainer/QF2 Loss                                        0.16978\n",
      "trainer/Policy Loss                                   -18.6458\n",
      "trainer/Q1 Predictions Mean                            26.4553\n",
      "trainer/Q1 Predictions Std                              1.59685\n",
      "trainer/Q1 Predictions Max                             28.3619\n",
      "trainer/Q1 Predictions Min                             18.1562\n",
      "trainer/Q2 Predictions Mean                            26.698\n",
      "trainer/Q2 Predictions Std                              1.58853\n",
      "trainer/Q2 Predictions Max                             28.5379\n",
      "trainer/Q2 Predictions Min                             18.6569\n",
      "trainer/Q Targets Mean                                 26.4979\n",
      "trainer/Q Targets Std                                   1.50803\n",
      "trainer/Q Targets Max                                  28.1726\n",
      "trainer/Q Targets Min                                  18.3793\n",
      "trainer/Log Pis Mean                                    7.96386\n",
      "trainer/Log Pis Std                                     2.50957\n",
      "trainer/Log Pis Max                                    25.7685\n",
      "trainer/Log Pis Min                                     1.44056\n",
      "trainer/Policy mu Mean                                 -0.00700429\n",
      "trainer/Policy mu Std                                   0.167078\n",
      "trainer/Policy mu Max                                   2.82939\n",
      "trainer/Policy mu Min                                  -1.97966\n",
      "trainer/Policy log std Mean                            -2.35667\n",
      "trainer/Policy log std Std                              0.253082\n",
      "trainer/Policy log std Max                             -0.242103\n",
      "trainer/Policy log std Min                             -4.84731\n",
      "trainer/Alpha                                           0.00718151\n",
      "trainer/Alpha Loss                                     -0.178351\n",
      "exploration/num steps total                         81000\n",
      "exploration/num paths total                           153\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.933073\n",
      "exploration/Rewards Std                                 0.0611895\n",
      "exploration/Rewards Max                                 1.4698\n",
      "exploration/Rewards Min                                 0.730205\n",
      "exploration/Returns Mean                              933.073\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               933.073\n",
      "exploration/Returns Min                               933.073\n",
      "exploration/Actions Mean                               -0.0220461\n",
      "exploration/Actions Std                                 0.137022\n",
      "exploration/Actions Max                                 0.510273\n",
      "exploration/Actions Min                                -0.579451\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           933.073\n",
      "exploration/env_infos/final/reward_forward Mean        -0.01851\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.01851\n",
      "exploration/env_infos/final/reward_forward Min         -0.01851\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.052339\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.052339\n",
      "exploration/env_infos/initial/reward_forward Min       -0.052339\n",
      "exploration/env_infos/reward_forward Mean               0.00158972\n",
      "exploration/env_infos/reward_forward Std                0.14112\n",
      "exploration/env_infos/reward_forward Max                1.28673\n",
      "exploration/env_infos/reward_forward Min               -0.747486\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.140627\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.140627\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.140627\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.060496\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.060496\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.060496\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0770445\n",
      "exploration/env_infos/reward_ctrl Std                   0.0377889\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00750957\n",
      "exploration/env_infos/reward_ctrl Min                  -0.269795\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0341475\n",
      "exploration/env_infos/final/torso_velocity Std          0.0387709\n",
      "exploration/env_infos/final/torso_velocity Max          0.00354613\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0874786\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.14335\n",
      "exploration/env_infos/initial/torso_velocity Std        0.233\n",
      "exploration/env_infos/initial/torso_velocity Max        0.470787\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.052339\n",
      "exploration/env_infos/torso_velocity Mean               0.00450963\n",
      "exploration/env_infos/torso_velocity Std                0.150215\n",
      "exploration/env_infos/torso_velocity Max                1.28673\n",
      "exploration/env_infos/torso_velocity Min               -1.28062\n",
      "evaluation/num steps total                              2e+06\n",
      "evaluation/num paths total                           2000\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.955764\n",
      "evaluation/Rewards Std                                  0.0383499\n",
      "evaluation/Rewards Max                                  2.43701\n",
      "evaluation/Rewards Min                                  0.590848\n",
      "evaluation/Returns Mean                               955.764\n",
      "evaluation/Returns Std                                 25.1662\n",
      "evaluation/Returns Max                                979.773\n",
      "evaluation/Returns Min                                888.621\n",
      "evaluation/Actions Mean                                -0.0149233\n",
      "evaluation/Actions Std                                  0.105658\n",
      "evaluation/Actions Max                                  0.708778\n",
      "evaluation/Actions Min                                 -0.728465\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            955.764\n",
      "evaluation/env_infos/final/reward_forward Mean          2.60285e-07\n",
      "evaluation/env_infos/final/reward_forward Std           1.86208e-06\n",
      "evaluation/env_infos/final/reward_forward Max           9.18073e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -8.61494e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0257339\n",
      "evaluation/env_infos/initial/reward_forward Std         0.142131\n",
      "evaluation/env_infos/initial/reward_forward Max         0.256607\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.357158\n",
      "evaluation/env_infos/reward_forward Mean                0.0021987\n",
      "evaluation/env_infos/reward_forward Std                 0.0586987\n",
      "evaluation/env_infos/reward_forward Max                 1.5877\n",
      "evaluation/env_infos/reward_forward Min                -1.68423\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0448557\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0249013\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0214208\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.111322\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0510612\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0223709\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0200357\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.092811\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0455455\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0262667\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00648674\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.723027\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.14013e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           1.00888e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           8.70363e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.02124e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135264\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238429\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.759051\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.357158\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000659249\n",
      "evaluation/env_infos/torso_velocity Std                 0.0654179\n",
      "evaluation/env_infos/torso_velocity Max                 1.5877\n",
      "evaluation/env_infos/torso_velocity Min                -1.7803\n",
      "time/data storing (s)                                   0.0316433\n",
      "time/evaluation sampling (s)                           47.6037\n",
      "time/exploration sampling (s)                           2.08569\n",
      "time/logging (s)                                        0.286152\n",
      "time/saving (s)                                         0.0276382\n",
      "time/training (s)                                       4.53562\n",
      "time/epoch (s)                                         54.5705\n",
      "time/total (s)                                       4281.36\n",
      "Epoch                                                  79\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:07:05.093906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 80 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 164000\n",
      "trainer/QF1 Loss                                        0.287703\n",
      "trainer/QF2 Loss                                        0.272469\n",
      "trainer/Policy Loss                                   -18.5376\n",
      "trainer/Q1 Predictions Mean                            26.7925\n",
      "trainer/Q1 Predictions Std                              1.75994\n",
      "trainer/Q1 Predictions Max                             28.4151\n",
      "trainer/Q1 Predictions Min                             14.2888\n",
      "trainer/Q2 Predictions Mean                            26.6178\n",
      "trainer/Q2 Predictions Std                              1.69462\n",
      "trainer/Q2 Predictions Max                             28.3639\n",
      "trainer/Q2 Predictions Min                             14.5499\n",
      "trainer/Q Targets Mean                                 26.7737\n",
      "trainer/Q Targets Std                                   1.88987\n",
      "trainer/Q Targets Max                                  28.9083\n",
      "trainer/Q Targets Min                                   7.91103\n",
      "trainer/Log Pis Mean                                    8.2348\n",
      "trainer/Log Pis Std                                     2.41789\n",
      "trainer/Log Pis Max                                    15.1526\n",
      "trainer/Log Pis Min                                     0.671478\n",
      "trainer/Policy mu Mean                                  0.0456073\n",
      "trainer/Policy mu Std                                   0.169341\n",
      "trainer/Policy mu Max                                   0.94953\n",
      "trainer/Policy mu Min                                  -1.08893\n",
      "trainer/Policy log std Mean                            -2.40344\n",
      "trainer/Policy log std Std                              0.255487\n",
      "trainer/Policy log std Max                             -1.50831\n",
      "trainer/Policy log std Min                             -3.47253\n",
      "trainer/Alpha                                           0.00746959\n",
      "trainer/Alpha Loss                                      1.15025\n",
      "exploration/num steps total                         82000\n",
      "exploration/num paths total                           154\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.809406\n",
      "exploration/Rewards Std                                 0.061851\n",
      "exploration/Rewards Max                                 0.984858\n",
      "exploration/Rewards Min                                 0.561815\n",
      "exploration/Returns Mean                              809.406\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               809.406\n",
      "exploration/Returns Min                               809.406\n",
      "exploration/Actions Mean                                0.0220092\n",
      "exploration/Actions Std                                 0.217433\n",
      "exploration/Actions Max                                 0.618738\n",
      "exploration/Actions Min                                -0.470108\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           809.406\n",
      "exploration/env_infos/final/reward_forward Mean         0.00593157\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00593157\n",
      "exploration/env_infos/final/reward_forward Min          0.00593157\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0718479\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0718479\n",
      "exploration/env_infos/initial/reward_forward Min        0.0718479\n",
      "exploration/env_infos/reward_forward Mean              -0.0062817\n",
      "exploration/env_infos/reward_forward Std                0.0755497\n",
      "exploration/env_infos/reward_forward Max                0.335575\n",
      "exploration/env_infos/reward_forward Min               -0.989631\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.127549\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.127549\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.127549\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0956011\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0956011\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0956011\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.191047\n",
      "exploration/env_infos/reward_ctrl Std                   0.0619952\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0151421\n",
      "exploration/env_infos/reward_ctrl Min                  -0.438185\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.000267249\n",
      "exploration/env_infos/final/torso_velocity Std          0.004012\n",
      "exploration/env_infos/final/torso_velocity Max          0.00593157\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00284905\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.132014\n",
      "exploration/env_infos/initial/torso_velocity Std        0.191022\n",
      "exploration/env_infos/initial/torso_velocity Max        0.390173\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0659801\n",
      "exploration/env_infos/torso_velocity Mean              -0.00240509\n",
      "exploration/env_infos/torso_velocity Std                0.0606604\n",
      "exploration/env_infos/torso_velocity Max                0.510521\n",
      "exploration/env_infos/torso_velocity Min               -0.989631\n",
      "evaluation/num steps total                              2.025e+06\n",
      "evaluation/num paths total                           2025\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.89358\n",
      "evaluation/Rewards Std                                  0.05619\n",
      "evaluation/Rewards Max                                  2.13851\n",
      "evaluation/Rewards Min                                  0.424741\n",
      "evaluation/Returns Mean                               893.58\n",
      "evaluation/Returns Std                                 39.0391\n",
      "evaluation/Returns Max                                963.175\n",
      "evaluation/Returns Min                                811.608\n",
      "evaluation/Actions Mean                                 0.0440203\n",
      "evaluation/Actions Std                                  0.159033\n",
      "evaluation/Actions Max                                  0.696012\n",
      "evaluation/Actions Min                                 -0.578128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            893.58\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0151572\n",
      "evaluation/env_infos/final/reward_forward Std           0.0496763\n",
      "evaluation/env_infos/final/reward_forward Max           0.00764946\n",
      "evaluation/env_infos/final/reward_forward Min          -0.227883\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0181066\n",
      "evaluation/env_infos/initial/reward_forward Std         0.115281\n",
      "evaluation/env_infos/initial/reward_forward Max         0.259041\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.174188\n",
      "evaluation/env_infos/reward_forward Mean               -0.00233693\n",
      "evaluation/env_infos/reward_forward Std                 0.104893\n",
      "evaluation/env_infos/reward_forward Max                 1.48211\n",
      "evaluation/env_infos/reward_forward Min                -1.05453\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.110353\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0425324\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0359375\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.189136\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0771854\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0254015\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0441538\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.166317\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.108917\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0447272\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0161769\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.575259\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0166297\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0686867\n",
      "evaluation/env_infos/final/torso_velocity Max           0.043729\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.405895\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.133239\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242543\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.669912\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.271482\n",
      "evaluation/env_infos/torso_velocity Mean                0.00108496\n",
      "evaluation/env_infos/torso_velocity Std                 0.125718\n",
      "evaluation/env_infos/torso_velocity Max                 1.48211\n",
      "evaluation/env_infos/torso_velocity Min                -1.86208\n",
      "time/data storing (s)                                   0.032031\n",
      "time/evaluation sampling (s)                           46.582\n",
      "time/exploration sampling (s)                           2.19617\n",
      "time/logging (s)                                        0.281642\n",
      "time/saving (s)                                         0.0267608\n",
      "time/training (s)                                       4.50543\n",
      "time/epoch (s)                                         53.624\n",
      "time/total (s)                                       4335.77\n",
      "Epoch                                                  80\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:07:58.534906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 81 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 166000\n",
      "trainer/QF1 Loss                                        0.387321\n",
      "trainer/QF2 Loss                                        0.363528\n",
      "trainer/Policy Loss                                   -18.8025\n",
      "trainer/Q1 Predictions Mean                            26.5512\n",
      "trainer/Q1 Predictions Std                              1.69299\n",
      "trainer/Q1 Predictions Max                             28.2056\n",
      "trainer/Q1 Predictions Min                             17.3477\n",
      "trainer/Q2 Predictions Mean                            26.7156\n",
      "trainer/Q2 Predictions Std                              1.59721\n",
      "trainer/Q2 Predictions Max                             29.4462\n",
      "trainer/Q2 Predictions Min                             18.3697\n",
      "trainer/Q Targets Mean                                 26.8977\n",
      "trainer/Q Targets Std                                   1.5634\n",
      "trainer/Q Targets Max                                  28.6982\n",
      "trainer/Q Targets Min                                  18.5525\n",
      "trainer/Log Pis Mean                                    7.94485\n",
      "trainer/Log Pis Std                                     2.51037\n",
      "trainer/Log Pis Max                                    17.2516\n",
      "trainer/Log Pis Min                                    -0.982885\n",
      "trainer/Policy mu Mean                                  0.027453\n",
      "trainer/Policy mu Std                                   0.134052\n",
      "trainer/Policy mu Max                                   0.708176\n",
      "trainer/Policy mu Min                                  -0.787505\n",
      "trainer/Policy log std Mean                            -2.38323\n",
      "trainer/Policy log std Std                              0.245475\n",
      "trainer/Policy log std Max                             -1.77676\n",
      "trainer/Policy log std Min                             -3.64826\n",
      "trainer/Alpha                                           0.00827475\n",
      "trainer/Alpha Loss                                     -0.264505\n",
      "exploration/num steps total                         83000\n",
      "exploration/num paths total                           155\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.92772\n",
      "exploration/Rewards Std                                 0.0586512\n",
      "exploration/Rewards Max                                 1.42998\n",
      "exploration/Rewards Min                                 0.749738\n",
      "exploration/Returns Mean                              927.72\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               927.72\n",
      "exploration/Returns Min                               927.72\n",
      "exploration/Actions Mean                                0.0182946\n",
      "exploration/Actions Std                                 0.140876\n",
      "exploration/Actions Max                                 0.48442\n",
      "exploration/Actions Min                                -0.494246\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           927.72\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0459283\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0459283\n",
      "exploration/env_infos/final/reward_forward Min         -0.0459283\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0975492\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0975492\n",
      "exploration/env_infos/initial/reward_forward Min        0.0975492\n",
      "exploration/env_infos/reward_forward Mean               0.00967063\n",
      "exploration/env_infos/reward_forward Std                0.183398\n",
      "exploration/env_infos/reward_forward Max                0.708768\n",
      "exploration/env_infos/reward_forward Min               -1.13219\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.077803\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.077803\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.077803\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.160709\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.160709\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.160709\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0807228\n",
      "exploration/env_infos/reward_ctrl Std                   0.0376077\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0091688\n",
      "exploration/env_infos/reward_ctrl Min                  -0.255472\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0420815\n",
      "exploration/env_infos/final/torso_velocity Std          0.0429318\n",
      "exploration/env_infos/final/torso_velocity Max          0.0123167\n",
      "exploration/env_infos/final/torso_velocity Min         -0.092633\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.147629\n",
      "exploration/env_infos/initial/torso_velocity Std        0.194505\n",
      "exploration/env_infos/initial/torso_velocity Max        0.406907\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0615696\n",
      "exploration/env_infos/torso_velocity Mean              -0.0101804\n",
      "exploration/env_infos/torso_velocity Std                0.210435\n",
      "exploration/env_infos/torso_velocity Max                1.14872\n",
      "exploration/env_infos/torso_velocity Min               -1.40742\n",
      "evaluation/num steps total                              2.05e+06\n",
      "evaluation/num paths total                           2050\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.95645\n",
      "evaluation/Rewards Std                                  0.0274176\n",
      "evaluation/Rewards Max                                  1.88947\n",
      "evaluation/Rewards Min                                  0.278482\n",
      "evaluation/Returns Mean                               956.45\n",
      "evaluation/Returns Std                                 13.59\n",
      "evaluation/Returns Max                                975.401\n",
      "evaluation/Returns Min                                923.352\n",
      "evaluation/Actions Mean                                 0.0434046\n",
      "evaluation/Actions Std                                  0.0960765\n",
      "evaluation/Actions Max                                  0.762851\n",
      "evaluation/Actions Min                                 -0.494894\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            956.45\n",
      "evaluation/env_infos/final/reward_forward Mean          2.7797e-07\n",
      "evaluation/env_infos/final/reward_forward Std           9.87954e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.79797e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -8.10849e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0773643\n",
      "evaluation/env_infos/initial/reward_forward Std         0.142917\n",
      "evaluation/env_infos/initial/reward_forward Max         0.254489\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.313417\n",
      "evaluation/env_infos/reward_forward Mean               -0.000448854\n",
      "evaluation/env_infos/reward_forward Std                 0.0546781\n",
      "evaluation/env_infos/reward_forward Max                 0.962163\n",
      "evaluation/env_infos/reward_forward Min                -1.39902\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0436251\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0136901\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0238922\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0778511\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0884333\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0303293\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0391301\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.146439\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0444586\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0187791\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00610049\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.721518\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.20559e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           6.37361e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.79797e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.07162e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.102509\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227682\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.519362\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.313417\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00109114\n",
      "evaluation/env_infos/torso_velocity Std                 0.0574504\n",
      "evaluation/env_infos/torso_velocity Max                 0.962163\n",
      "evaluation/env_infos/torso_velocity Min                -1.82895\n",
      "time/data storing (s)                                   0.0320558\n",
      "time/evaluation sampling (s)                           45.9703\n",
      "time/exploration sampling (s)                           2.03387\n",
      "time/logging (s)                                        0.281074\n",
      "time/saving (s)                                         0.0261034\n",
      "time/training (s)                                       4.36692\n",
      "time/epoch (s)                                         52.7104\n",
      "time/total (s)                                       4389.21\n",
      "Epoch                                                  81\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:08:56.889584 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 82 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 168000\n",
      "trainer/QF1 Loss                                        0.304197\n",
      "trainer/QF2 Loss                                        0.165637\n",
      "trainer/Policy Loss                                   -19.7264\n",
      "trainer/Q1 Predictions Mean                            26.8566\n",
      "trainer/Q1 Predictions Std                              1.65625\n",
      "trainer/Q1 Predictions Max                             29.0814\n",
      "trainer/Q1 Predictions Min                             15.8251\n",
      "trainer/Q2 Predictions Mean                            26.9792\n",
      "trainer/Q2 Predictions Std                              1.61884\n",
      "trainer/Q2 Predictions Max                             28.8411\n",
      "trainer/Q2 Predictions Min                             17.252\n",
      "trainer/Q Targets Mean                                 27.2263\n",
      "trainer/Q Targets Std                                   1.65564\n",
      "trainer/Q Targets Max                                  29.4542\n",
      "trainer/Q Targets Min                                  18.4945\n",
      "trainer/Log Pis Mean                                    7.30157\n",
      "trainer/Log Pis Std                                     2.2329\n",
      "trainer/Log Pis Max                                    22.7287\n",
      "trainer/Log Pis Min                                     0.687732\n",
      "trainer/Policy mu Mean                                 -0.0602318\n",
      "trainer/Policy mu Std                                   0.19381\n",
      "trainer/Policy mu Max                                   1.28392\n",
      "trainer/Policy mu Min                                  -2.99059\n",
      "trainer/Policy log std Mean                            -2.27051\n",
      "trainer/Policy log std Std                              0.200022\n",
      "trainer/Policy log std Max                             -1.72131\n",
      "trainer/Policy log std Min                             -3.3382\n",
      "trainer/Alpha                                           0.00791225\n",
      "trainer/Alpha Loss                                     -3.3791\n",
      "exploration/num steps total                         84000\n",
      "exploration/num paths total                           156\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.927251\n",
      "exploration/Rewards Std                                 0.102158\n",
      "exploration/Rewards Max                                 1.59745\n",
      "exploration/Rewards Min                                 0.721255\n",
      "exploration/Returns Mean                              927.251\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               927.251\n",
      "exploration/Returns Min                               927.251\n",
      "exploration/Actions Mean                               -0.023028\n",
      "exploration/Actions Std                                 0.156681\n",
      "exploration/Actions Max                                 0.504215\n",
      "exploration/Actions Min                                -0.519542\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           927.251\n",
      "exploration/env_infos/final/reward_forward Mean         0.00564464\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00564464\n",
      "exploration/env_infos/final/reward_forward Min          0.00564464\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.117304\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.117304\n",
      "exploration/env_infos/initial/reward_forward Min       -0.117304\n",
      "exploration/env_infos/reward_forward Mean               0.000437736\n",
      "exploration/env_infos/reward_forward Std                0.148837\n",
      "exploration/env_infos/reward_forward Max                0.678379\n",
      "exploration/env_infos/reward_forward Min               -0.527466\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0385659\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0385659\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0385659\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.116048\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.116048\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.116048\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.100317\n",
      "exploration/env_infos/reward_ctrl Std                   0.0432478\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0125522\n",
      "exploration/env_infos/reward_ctrl Min                  -0.278745\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00421726\n",
      "exploration/env_infos/final/torso_velocity Std          0.0687701\n",
      "exploration/env_infos/final/torso_velocity Max          0.0746435\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0929399\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.113509\n",
      "exploration/env_infos/initial/torso_velocity Std        0.203766\n",
      "exploration/env_infos/initial/torso_velocity Max        0.378331\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.117304\n",
      "exploration/env_infos/torso_velocity Mean               0.000277304\n",
      "exploration/env_infos/torso_velocity Std                0.122807\n",
      "exploration/env_infos/torso_velocity Max                0.822449\n",
      "exploration/env_infos/torso_velocity Min               -0.903384\n",
      "evaluation/num steps total                              2.075e+06\n",
      "evaluation/num paths total                           2075\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.920536\n",
      "evaluation/Rewards Std                                  0.0507843\n",
      "evaluation/Rewards Max                                  2.22366\n",
      "evaluation/Rewards Min                                  0.660143\n",
      "evaluation/Returns Mean                               920.536\n",
      "evaluation/Returns Std                                 41.2207\n",
      "evaluation/Returns Max                                983.76\n",
      "evaluation/Returns Min                                830.084\n",
      "evaluation/Actions Mean                                -0.0386423\n",
      "evaluation/Actions Std                                  0.13657\n",
      "evaluation/Actions Max                                  0.436889\n",
      "evaluation/Actions Min                                 -0.525415\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            920.536\n",
      "evaluation/env_infos/final/reward_forward Mean          3.80033e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.7486e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.81023e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.41332e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0699909\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121252\n",
      "evaluation/env_infos/initial/reward_forward Max         0.197747\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.253145\n",
      "evaluation/env_infos/reward_forward Mean               -0.000224038\n",
      "evaluation/env_infos/reward_forward Std                 0.0553524\n",
      "evaluation/env_infos/reward_forward Max                 1.46921\n",
      "evaluation/env_infos/reward_forward Min                -1.14461\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0802634\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0413236\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0148203\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.169791\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.145471\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0550838\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0528345\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.27296\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0805778\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0418159\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00458583\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.339857\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.06879e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.19821e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.81023e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.41332e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.0950743\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.254612\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.65035\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.253145\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00168826\n",
      "evaluation/env_infos/torso_velocity Std                 0.0573865\n",
      "evaluation/env_infos/torso_velocity Max                 1.46921\n",
      "evaluation/env_infos/torso_velocity Min                -1.94153\n",
      "time/data storing (s)                                   0.0327471\n",
      "time/evaluation sampling (s)                           48.5493\n",
      "time/exploration sampling (s)                           3.07147\n",
      "time/logging (s)                                        0.372831\n",
      "time/saving (s)                                         0.050355\n",
      "time/training (s)                                       5.63964\n",
      "time/epoch (s)                                         57.7163\n",
      "time/total (s)                                       4447.65\n",
      "Epoch                                                  82\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:50.630307 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 83 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 170000\n",
      "trainer/QF1 Loss                                        0.352925\n",
      "trainer/QF2 Loss                                        0.320712\n",
      "trainer/Policy Loss                                   -20.6388\n",
      "trainer/Q1 Predictions Mean                            27.759\n",
      "trainer/Q1 Predictions Std                              1.71093\n",
      "trainer/Q1 Predictions Max                             30.7266\n",
      "trainer/Q1 Predictions Min                             18.5328\n",
      "trainer/Q2 Predictions Mean                            27.5684\n",
      "trainer/Q2 Predictions Std                              1.67499\n",
      "trainer/Q2 Predictions Max                             29.5213\n",
      "trainer/Q2 Predictions Min                             18.9318\n",
      "trainer/Q Targets Mean                                 27.4112\n",
      "trainer/Q Targets Std                                   1.76115\n",
      "trainer/Q Targets Max                                  30.4037\n",
      "trainer/Q Targets Min                                  18.9978\n",
      "trainer/Log Pis Mean                                    7.14047\n",
      "trainer/Log Pis Std                                     2.58787\n",
      "trainer/Log Pis Max                                    15.5004\n",
      "trainer/Log Pis Min                                    -1.06762\n",
      "trainer/Policy mu Mean                                 -0.0291949\n",
      "trainer/Policy mu Std                                   0.181178\n",
      "trainer/Policy mu Max                                   0.872859\n",
      "trainer/Policy mu Min                                  -1.21744\n",
      "trainer/Policy log std Mean                            -2.28261\n",
      "trainer/Policy log std Std                              0.264026\n",
      "trainer/Policy log std Max                             -1.59232\n",
      "trainer/Policy log std Min                             -3.37706\n",
      "trainer/Alpha                                           0.00831722\n",
      "trainer/Alpha Loss                                     -4.1141\n",
      "exploration/num steps total                         85000\n",
      "exploration/num paths total                           157\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.935978\n",
      "exploration/Rewards Std                                 0.145075\n",
      "exploration/Rewards Max                                 2.05525\n",
      "exploration/Rewards Min                                 0.702977\n",
      "exploration/Returns Mean                              935.978\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.978\n",
      "exploration/Returns Min                               935.978\n",
      "exploration/Actions Mean                                0.0118434\n",
      "exploration/Actions Std                                 0.157725\n",
      "exploration/Actions Max                                 0.542269\n",
      "exploration/Actions Min                                -0.589994\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.978\n",
      "exploration/env_infos/final/reward_forward Mean         0.0226498\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0226498\n",
      "exploration/env_infos/final/reward_forward Min          0.0226498\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.153476\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.153476\n",
      "exploration/env_infos/initial/reward_forward Min       -0.153476\n",
      "exploration/env_infos/reward_forward Mean              -0.0139557\n",
      "exploration/env_infos/reward_forward Std                0.295814\n",
      "exploration/env_infos/reward_forward Max                1.00816\n",
      "exploration/env_infos/reward_forward Min               -1.30198\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0562135\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0562135\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0562135\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.170024\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.170024\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.170024\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.100069\n",
      "exploration/env_infos/reward_ctrl Std                   0.0440572\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0160042\n",
      "exploration/env_infos/reward_ctrl Min                  -0.340597\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0309043\n",
      "exploration/env_infos/final/torso_velocity Std          0.0365028\n",
      "exploration/env_infos/final/torso_velocity Max          0.079163\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00909974\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.11106\n",
      "exploration/env_infos/initial/torso_velocity Std        0.244211\n",
      "exploration/env_infos/initial/torso_velocity Max        0.435614\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.153476\n",
      "exploration/env_infos/torso_velocity Mean              -0.0138219\n",
      "exploration/env_infos/torso_velocity Std                0.26574\n",
      "exploration/env_infos/torso_velocity Max                1.11808\n",
      "exploration/env_infos/torso_velocity Min               -1.30198\n",
      "evaluation/num steps total                              2.1e+06\n",
      "evaluation/num paths total                           2100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.920982\n",
      "evaluation/Rewards Std                                  0.0563597\n",
      "evaluation/Rewards Max                                  2.4399\n",
      "evaluation/Rewards Min                                  0.583108\n",
      "evaluation/Returns Mean                               920.982\n",
      "evaluation/Returns Std                                 39.5417\n",
      "evaluation/Returns Max                                966.571\n",
      "evaluation/Returns Min                                793.813\n",
      "evaluation/Actions Mean                                -0.00914212\n",
      "evaluation/Actions Std                                  0.141894\n",
      "evaluation/Actions Max                                  0.495327\n",
      "evaluation/Actions Min                                 -0.488123\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            920.982\n",
      "evaluation/env_infos/final/reward_forward Mean          1.23799e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.33568e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.3395e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.11953e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00923346\n",
      "evaluation/env_infos/initial/reward_forward Std         0.144042\n",
      "evaluation/env_infos/initial/reward_forward Max         0.249156\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.285543\n",
      "evaluation/env_infos/reward_forward Mean                0.0026096\n",
      "evaluation/env_infos/reward_forward Std                 0.0656335\n",
      "evaluation/env_infos/reward_forward Max                 1.55565\n",
      "evaluation/env_infos/reward_forward Min                -1.46323\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0808982\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0401147\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0320049\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.208869\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0879423\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0363534\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0312853\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.183428\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0808697\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0401708\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0231057\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.416892\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          4.88993e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           1.95275e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.3395e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.41557e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144842\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.219818\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.605904\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.285543\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000927701\n",
      "evaluation/env_infos/torso_velocity Std                 0.0791705\n",
      "evaluation/env_infos/torso_velocity Max                 1.55565\n",
      "evaluation/env_infos/torso_velocity Min                -1.76627\n",
      "time/data storing (s)                                   0.0313948\n",
      "time/evaluation sampling (s)                           45.4066\n",
      "time/exploration sampling (s)                           1.85076\n",
      "time/logging (s)                                        0.2686\n",
      "time/saving (s)                                         0.0258277\n",
      "time/training (s)                                       5.07957\n",
      "time/epoch (s)                                         52.6628\n",
      "time/total (s)                                       4501.28\n",
      "Epoch                                                  83\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:10:42.867927 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 84 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 172000\n",
      "trainer/QF1 Loss                                        0.251202\n",
      "trainer/QF2 Loss                                        0.189312\n",
      "trainer/Policy Loss                                   -20.1435\n",
      "trainer/Q1 Predictions Mean                            27.4663\n",
      "trainer/Q1 Predictions Std                              2.10194\n",
      "trainer/Q1 Predictions Max                             29.423\n",
      "trainer/Q1 Predictions Min                             16.2329\n",
      "trainer/Q2 Predictions Mean                            27.4742\n",
      "trainer/Q2 Predictions Std                              2.32023\n",
      "trainer/Q2 Predictions Max                             29.6013\n",
      "trainer/Q2 Predictions Min                              7.73908\n",
      "trainer/Q Targets Mean                                 27.5463\n",
      "trainer/Q Targets Std                                   2.19279\n",
      "trainer/Q Targets Max                                  29.5316\n",
      "trainer/Q Targets Min                                  11.049\n",
      "trainer/Log Pis Mean                                    7.42689\n",
      "trainer/Log Pis Std                                     2.38648\n",
      "trainer/Log Pis Max                                    13.4676\n",
      "trainer/Log Pis Min                                    -7.18186\n",
      "trainer/Policy mu Mean                                  0.00321994\n",
      "trainer/Policy mu Std                                   0.149072\n",
      "trainer/Policy mu Max                                   1.24833\n",
      "trainer/Policy mu Min                                  -1.02395\n",
      "trainer/Policy log std Mean                            -2.30228\n",
      "trainer/Policy log std Std                              0.207484\n",
      "trainer/Policy log std Max                             -1.18041\n",
      "trainer/Policy log std Min                             -3.73709\n",
      "trainer/Alpha                                           0.0080693\n",
      "trainer/Alpha Loss                                     -2.76164\n",
      "exploration/num steps total                         86000\n",
      "exploration/num paths total                           158\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.90094\n",
      "exploration/Rewards Std                                 0.0640439\n",
      "exploration/Rewards Max                                 1.35441\n",
      "exploration/Rewards Min                                 0.724332\n",
      "exploration/Returns Mean                              900.94\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               900.94\n",
      "exploration/Returns Min                               900.94\n",
      "exploration/Actions Mean                                0.0121476\n",
      "exploration/Actions Std                                 0.165049\n",
      "exploration/Actions Max                                 0.513426\n",
      "exploration/Actions Min                                -0.558017\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           900.94\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0463032\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0463032\n",
      "exploration/env_infos/final/reward_forward Min         -0.0463032\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0866296\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0866296\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0866296\n",
      "exploration/env_infos/reward_forward Mean              -0.0058823\n",
      "exploration/env_infos/reward_forward Std                0.158497\n",
      "exploration/env_infos/reward_forward Max                1.09551\n",
      "exploration/env_infos/reward_forward Min               -0.736822\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.108893\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.108893\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.108893\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.185985\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.185985\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.185985\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109555\n",
      "exploration/env_infos/reward_ctrl Std                   0.0433081\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0190922\n",
      "exploration/env_infos/reward_ctrl Min                  -0.275668\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00296\n",
      "exploration/env_infos/final/torso_velocity Std          0.0383092\n",
      "exploration/env_infos/final/torso_velocity Max          0.0471171\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0463032\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.147697\n",
      "exploration/env_infos/initial/torso_velocity Std        0.291304\n",
      "exploration/env_infos/initial/torso_velocity Max        0.558298\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0866296\n",
      "exploration/env_infos/torso_velocity Mean              -0.0030941\n",
      "exploration/env_infos/torso_velocity Std                0.222029\n",
      "exploration/env_infos/torso_velocity Max                1.09551\n",
      "exploration/env_infos/torso_velocity Min               -1.57258\n",
      "evaluation/num steps total                              2.125e+06\n",
      "evaluation/num paths total                           2125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.906297\n",
      "evaluation/Rewards Std                                  0.029748\n",
      "evaluation/Rewards Max                                  2.38474\n",
      "evaluation/Rewards Min                                  0.610759\n",
      "evaluation/Returns Mean                               906.297\n",
      "evaluation/Returns Std                                 18.5613\n",
      "evaluation/Returns Max                                934.194\n",
      "evaluation/Returns Min                                873.136\n",
      "evaluation/Actions Mean                                -0.0272682\n",
      "evaluation/Actions Std                                  0.15106\n",
      "evaluation/Actions Max                                  0.486072\n",
      "evaluation/Actions Min                                 -0.543624\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            906.297\n",
      "evaluation/env_infos/final/reward_forward Mean          1.43781e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.30414e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.11015e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.32635e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00279984\n",
      "evaluation/env_infos/initial/reward_forward Std         0.138192\n",
      "evaluation/env_infos/initial/reward_forward Max         0.274527\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.233018\n",
      "evaluation/env_infos/reward_forward Mean               -0.00136533\n",
      "evaluation/env_infos/reward_forward Std                 0.0517623\n",
      "evaluation/env_infos/reward_forward Max                 1.11043\n",
      "evaluation/env_infos/reward_forward Min                -1.58909\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0940764\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0188161\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0701431\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.130267\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.136894\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0380588\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0533312\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.198206\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0942503\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0198574\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0196062\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.389241\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.3708e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.22875e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.08355e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.91868e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.124019\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239184\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.635508\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.32782\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00501434\n",
      "evaluation/env_infos/torso_velocity Std                 0.0757882\n",
      "evaluation/env_infos/torso_velocity Max                 1.11043\n",
      "evaluation/env_infos/torso_velocity Min                -1.89735\n",
      "time/data storing (s)                                   0.0299794\n",
      "time/evaluation sampling (s)                           45.0674\n",
      "time/exploration sampling (s)                           1.95139\n",
      "time/logging (s)                                        0.268387\n",
      "time/saving (s)                                         0.0259727\n",
      "time/training (s)                                       4.17179\n",
      "time/epoch (s)                                         51.5149\n",
      "time/total (s)                                       4553.52\n",
      "Epoch                                                  84\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:11:35.514957 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 85 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 174000\n",
      "trainer/QF1 Loss                                        0.251056\n",
      "trainer/QF2 Loss                                        0.228038\n",
      "trainer/Policy Loss                                   -20.3342\n",
      "trainer/Q1 Predictions Mean                            27.9478\n",
      "trainer/Q1 Predictions Std                              1.97267\n",
      "trainer/Q1 Predictions Max                             30.8403\n",
      "trainer/Q1 Predictions Min                             17.5996\n",
      "trainer/Q2 Predictions Mean                            27.8864\n",
      "trainer/Q2 Predictions Std                              1.94017\n",
      "trainer/Q2 Predictions Max                             30.6814\n",
      "trainer/Q2 Predictions Min                             18.7867\n",
      "trainer/Q Targets Mean                                 27.9588\n",
      "trainer/Q Targets Std                                   1.97577\n",
      "trainer/Q Targets Max                                  31.1388\n",
      "trainer/Q Targets Min                                  18.3752\n",
      "trainer/Log Pis Mean                                    7.71098\n",
      "trainer/Log Pis Std                                     2.17542\n",
      "trainer/Log Pis Max                                    13.8623\n",
      "trainer/Log Pis Min                                    -1.61091\n",
      "trainer/Policy mu Mean                                  0.0351902\n",
      "trainer/Policy mu Std                                   0.138096\n",
      "trainer/Policy mu Max                                   1.28366\n",
      "trainer/Policy mu Min                                  -0.775762\n",
      "trainer/Policy log std Mean                            -2.34241\n",
      "trainer/Policy log std Std                              0.200248\n",
      "trainer/Policy log std Max                             -1.52012\n",
      "trainer/Policy log std Min                             -3.23797\n",
      "trainer/Alpha                                           0.00774718\n",
      "trainer/Alpha Loss                                     -1.40431\n",
      "exploration/num steps total                         87000\n",
      "exploration/num paths total                           159\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.911518\n",
      "exploration/Rewards Std                                 0.0670591\n",
      "exploration/Rewards Max                                 1.80529\n",
      "exploration/Rewards Min                                 0.625922\n",
      "exploration/Returns Mean                              911.518\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               911.518\n",
      "exploration/Returns Min                               911.518\n",
      "exploration/Actions Mean                                0.0413938\n",
      "exploration/Actions Std                                 0.148779\n",
      "exploration/Actions Max                                 0.57614\n",
      "exploration/Actions Min                                -0.417229\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           911.518\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0196579\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0196579\n",
      "exploration/env_infos/final/reward_forward Min         -0.0196579\n",
      "exploration/env_infos/initial/reward_forward Mean       0.197183\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.197183\n",
      "exploration/env_infos/initial/reward_forward Min        0.197183\n",
      "exploration/env_infos/reward_forward Mean               0.00903603\n",
      "exploration/env_infos/reward_forward Std                0.185024\n",
      "exploration/env_infos/reward_forward Max                0.917238\n",
      "exploration/env_infos/reward_forward Min               -0.782545\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0728883\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0728883\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0728883\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0499051\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0499051\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0499051\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0953943\n",
      "exploration/env_infos/reward_ctrl Std                   0.0405561\n",
      "exploration/env_infos/reward_ctrl Max                  -0.01915\n",
      "exploration/env_infos/reward_ctrl Min                  -0.374078\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0401494\n",
      "exploration/env_infos/final/torso_velocity Std          0.0465697\n",
      "exploration/env_infos/final/torso_velocity Max          0.0939365\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0196579\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.124056\n",
      "exploration/env_infos/initial/torso_velocity Std        0.144786\n",
      "exploration/env_infos/initial/torso_velocity Max        0.253124\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0781397\n",
      "exploration/env_infos/torso_velocity Mean              -0.0037899\n",
      "exploration/env_infos/torso_velocity Std                0.172479\n",
      "exploration/env_infos/torso_velocity Max                1.00802\n",
      "exploration/env_infos/torso_velocity Min               -1.30793\n",
      "evaluation/num steps total                              2.15e+06\n",
      "evaluation/num paths total                           2150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.938988\n",
      "evaluation/Rewards Std                                  0.0545422\n",
      "evaluation/Rewards Max                                  2.24503\n",
      "evaluation/Rewards Min                                  0.468757\n",
      "evaluation/Returns Mean                               938.988\n",
      "evaluation/Returns Std                                 49.7373\n",
      "evaluation/Returns Max                                987.646\n",
      "evaluation/Returns Min                                823.386\n",
      "evaluation/Actions Mean                                 0.037446\n",
      "evaluation/Actions Std                                  0.1185\n",
      "evaluation/Actions Max                                  0.705424\n",
      "evaluation/Actions Min                                 -0.531525\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            938.988\n",
      "evaluation/env_infos/final/reward_forward Mean          9.42727e-06\n",
      "evaluation/env_infos/final/reward_forward Std           7.92886e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000371234\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000146508\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0019296\n",
      "evaluation/env_infos/initial/reward_forward Std         0.139442\n",
      "evaluation/env_infos/initial/reward_forward Max         0.249851\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.330477\n",
      "evaluation/env_infos/reward_forward Mean               -0.00428959\n",
      "evaluation/env_infos/reward_forward Std                 0.0753476\n",
      "evaluation/env_infos/reward_forward Max                 0.610147\n",
      "evaluation/env_infos/reward_forward Min                -1.8682\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.061248\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.050497\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0120187\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.182565\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0311111\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0136595\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00946796\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0580141\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0617774\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0514135\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00820491\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.531243\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.16727e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           5.97331e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000371234\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000146508\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.124873\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248728\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.625377\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.330477\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00359792\n",
      "evaluation/env_infos/torso_velocity Std                 0.0710997\n",
      "evaluation/env_infos/torso_velocity Max                 1.12751\n",
      "evaluation/env_infos/torso_velocity Min                -1.928\n",
      "time/data storing (s)                                   0.0306132\n",
      "time/evaluation sampling (s)                           45.2855\n",
      "time/exploration sampling (s)                           2.04428\n",
      "time/logging (s)                                        0.275722\n",
      "time/saving (s)                                         0.0249147\n",
      "time/training (s)                                       4.24208\n",
      "time/epoch (s)                                         51.9031\n",
      "time/total (s)                                       4606.17\n",
      "Epoch                                                  85\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:12:26.513487 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 86 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 176000\n",
      "trainer/QF1 Loss                                        0.339432\n",
      "trainer/QF2 Loss                                        0.37237\n",
      "trainer/Policy Loss                                   -20.9317\n",
      "trainer/Q1 Predictions Mean                            28.1008\n",
      "trainer/Q1 Predictions Std                              2.46679\n",
      "trainer/Q1 Predictions Max                             30.5088\n",
      "trainer/Q1 Predictions Min                             16.928\n",
      "trainer/Q2 Predictions Mean                            27.8864\n",
      "trainer/Q2 Predictions Std                              2.41589\n",
      "trainer/Q2 Predictions Max                             30.3735\n",
      "trainer/Q2 Predictions Min                             16.6751\n",
      "trainer/Q Targets Mean                                 28.0531\n",
      "trainer/Q Targets Std                                   2.34353\n",
      "trainer/Q Targets Max                                  33.9277\n",
      "trainer/Q Targets Min                                  18.5345\n",
      "trainer/Log Pis Mean                                    7.16283\n",
      "trainer/Log Pis Std                                     2.40937\n",
      "trainer/Log Pis Max                                    22.3878\n",
      "trainer/Log Pis Min                                    -0.614695\n",
      "trainer/Policy mu Mean                                 -0.0327133\n",
      "trainer/Policy mu Std                                   0.208268\n",
      "trainer/Policy mu Max                                   2.7146\n",
      "trainer/Policy mu Min                                  -3.75554\n",
      "trainer/Policy log std Mean                            -2.25528\n",
      "trainer/Policy log std Std                              0.246147\n",
      "trainer/Policy log std Max                             -0.223902\n",
      "trainer/Policy log std Min                             -3.29099\n",
      "trainer/Alpha                                           0.00826997\n",
      "trainer/Alpha Loss                                     -4.01075\n",
      "exploration/num steps total                         88000\n",
      "exploration/num paths total                           160\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.896457\n",
      "exploration/Rewards Std                                 0.0795547\n",
      "exploration/Rewards Max                                 1.82417\n",
      "exploration/Rewards Min                                 0.630085\n",
      "exploration/Returns Mean                              896.457\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               896.457\n",
      "exploration/Returns Min                               896.457\n",
      "exploration/Actions Mean                               -0.000328055\n",
      "exploration/Actions Std                                 0.167698\n",
      "exploration/Actions Max                                 0.531894\n",
      "exploration/Actions Min                                -0.574672\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           896.457\n",
      "exploration/env_infos/final/reward_forward Mean         0.119977\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.119977\n",
      "exploration/env_infos/final/reward_forward Min          0.119977\n",
      "exploration/env_infos/initial/reward_forward Mean       0.275012\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.275012\n",
      "exploration/env_infos/initial/reward_forward Min        0.275012\n",
      "exploration/env_infos/reward_forward Mean              -0.0273218\n",
      "exploration/env_infos/reward_forward Std                0.173503\n",
      "exploration/env_infos/reward_forward Max                0.749653\n",
      "exploration/env_infos/reward_forward Min               -0.867659\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.049557\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.049557\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.049557\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.143034\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.143034\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.143034\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.112491\n",
      "exploration/env_infos/reward_ctrl Std                   0.0482024\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0195549\n",
      "exploration/env_infos/reward_ctrl Min                  -0.369915\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0148071\n",
      "exploration/env_infos/final/torso_velocity Std          0.106373\n",
      "exploration/env_infos/final/torso_velocity Max          0.119977\n",
      "exploration/env_infos/final/torso_velocity Min         -0.130929\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.248627\n",
      "exploration/env_infos/initial/torso_velocity Std        0.135695\n",
      "exploration/env_infos/initial/torso_velocity Max        0.400047\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0708209\n",
      "exploration/env_infos/torso_velocity Mean              -0.011549\n",
      "exploration/env_infos/torso_velocity Std                0.186083\n",
      "exploration/env_infos/torso_velocity Max                0.791494\n",
      "exploration/env_infos/torso_velocity Min               -1.08391\n",
      "evaluation/num steps total                              2.175e+06\n",
      "evaluation/num paths total                           2175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.940813\n",
      "evaluation/Rewards Std                                  0.0239712\n",
      "evaluation/Rewards Max                                  2.08293\n",
      "evaluation/Rewards Min                                  0.79804\n",
      "evaluation/Returns Mean                               940.813\n",
      "evaluation/Returns Std                                 16.8654\n",
      "evaluation/Returns Max                                971.617\n",
      "evaluation/Returns Min                                907.826\n",
      "evaluation/Actions Mean                                -0.0148731\n",
      "evaluation/Actions Std                                  0.121186\n",
      "evaluation/Actions Max                                  0.344341\n",
      "evaluation/Actions Min                                 -0.401186\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            940.813\n",
      "evaluation/env_infos/final/reward_forward Mean          8.92881e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.76101e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.92044e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.97596e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0259504\n",
      "evaluation/env_infos/initial/reward_forward Std         0.145125\n",
      "evaluation/env_infos/initial/reward_forward Max         0.257674\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.277342\n",
      "evaluation/env_infos/reward_forward Mean                0.00340713\n",
      "evaluation/env_infos/reward_forward Std                 0.0618015\n",
      "evaluation/env_infos/reward_forward Max                 2.08506\n",
      "evaluation/env_infos/reward_forward Min                -1.13625\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0597308\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0173995\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0281147\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0954282\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.102699\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0239798\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0521432\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.148192\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0596295\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0175946\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0152831\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.20196\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.62064e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.33438e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.92044e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.97596e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.139227\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258052\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.646178\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.309747\n",
      "evaluation/env_infos/torso_velocity Mean                0.000731771\n",
      "evaluation/env_infos/torso_velocity Std                 0.0688647\n",
      "evaluation/env_infos/torso_velocity Max                 2.08506\n",
      "evaluation/env_infos/torso_velocity Min                -1.73203\n",
      "time/data storing (s)                                   0.0300216\n",
      "time/evaluation sampling (s)                           43.9212\n",
      "time/exploration sampling (s)                           1.96408\n",
      "time/logging (s)                                        0.246993\n",
      "time/saving (s)                                         0.0249069\n",
      "time/training (s)                                       4.27454\n",
      "time/epoch (s)                                         50.4618\n",
      "time/total (s)                                       4657.14\n",
      "Epoch                                                  86\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:13:18.854251 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 87 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 178000\n",
      "trainer/QF1 Loss                                        0.137317\n",
      "trainer/QF2 Loss                                        0.12562\n",
      "trainer/Policy Loss                                   -20.7673\n",
      "trainer/Q1 Predictions Mean                            28.5819\n",
      "trainer/Q1 Predictions Std                              1.70244\n",
      "trainer/Q1 Predictions Max                             30.8556\n",
      "trainer/Q1 Predictions Min                             17.9\n",
      "trainer/Q2 Predictions Mean                            28.587\n",
      "trainer/Q2 Predictions Std                              1.66802\n",
      "trainer/Q2 Predictions Max                             30.4903\n",
      "trainer/Q2 Predictions Min                             17.6865\n",
      "trainer/Q Targets Mean                                 28.6346\n",
      "trainer/Q Targets Std                                   1.64309\n",
      "trainer/Q Targets Max                                  31.1404\n",
      "trainer/Q Targets Min                                  18.525\n",
      "trainer/Log Pis Mean                                    7.92033\n",
      "trainer/Log Pis Std                                     2.21832\n",
      "trainer/Log Pis Max                                    14.0596\n",
      "trainer/Log Pis Min                                    -0.647973\n",
      "trainer/Policy mu Mean                                 -0.0156801\n",
      "trainer/Policy mu Std                                   0.15175\n",
      "trainer/Policy mu Max                                   0.837916\n",
      "trainer/Policy mu Min                                  -1.40168\n",
      "trainer/Policy log std Mean                            -2.37955\n",
      "trainer/Policy log std Std                              0.210239\n",
      "trainer/Policy log std Max                             -1.22407\n",
      "trainer/Policy log std Min                             -3.63663\n",
      "trainer/Alpha                                           0.00767521\n",
      "trainer/Alpha Loss                                     -0.387924\n",
      "exploration/num steps total                         89000\n",
      "exploration/num paths total                           161\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.88906\n",
      "exploration/Rewards Std                                 0.0494266\n",
      "exploration/Rewards Max                                 1.48778\n",
      "exploration/Rewards Min                                 0.739066\n",
      "exploration/Returns Mean                              889.06\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               889.06\n",
      "exploration/Returns Min                               889.06\n",
      "exploration/Actions Mean                               -0.0563541\n",
      "exploration/Actions Std                                 0.158783\n",
      "exploration/Actions Max                                 0.393245\n",
      "exploration/Actions Min                                -0.515809\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           889.06\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0541107\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0541107\n",
      "exploration/env_infos/final/reward_forward Min         -0.0541107\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0174922\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0174922\n",
      "exploration/env_infos/initial/reward_forward Min        0.0174922\n",
      "exploration/env_infos/reward_forward Mean              -0.00106157\n",
      "exploration/env_infos/reward_forward Std                0.146275\n",
      "exploration/env_infos/reward_forward Max                0.478901\n",
      "exploration/env_infos/reward_forward Min               -0.814189\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0871845\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0871845\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0871845\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.142567\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.142567\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.142567\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.113551\n",
      "exploration/env_infos/reward_ctrl Std                   0.0410369\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0151793\n",
      "exploration/env_infos/reward_ctrl Min                  -0.260934\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0250012\n",
      "exploration/env_infos/final/torso_velocity Std          0.020608\n",
      "exploration/env_infos/final/torso_velocity Max         -0.00921641\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0541107\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.127944\n",
      "exploration/env_infos/initial/torso_velocity Std        0.25004\n",
      "exploration/env_infos/initial/torso_velocity Max        0.474082\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.107743\n",
      "exploration/env_infos/torso_velocity Mean               0.00548164\n",
      "exploration/env_infos/torso_velocity Std                0.153559\n",
      "exploration/env_infos/torso_velocity Max                0.803674\n",
      "exploration/env_infos/torso_velocity Min               -1.64684\n",
      "evaluation/num steps total                              2.2e+06\n",
      "evaluation/num paths total                           2200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.915554\n",
      "evaluation/Rewards Std                                  0.0421244\n",
      "evaluation/Rewards Max                                  2.93039\n",
      "evaluation/Rewards Min                                  0.661072\n",
      "evaluation/Returns Mean                               915.554\n",
      "evaluation/Returns Std                                 27.3718\n",
      "evaluation/Returns Max                                967.959\n",
      "evaluation/Returns Min                                886.997\n",
      "evaluation/Actions Mean                                -0.0420826\n",
      "evaluation/Actions Std                                  0.139803\n",
      "evaluation/Actions Max                                  0.575209\n",
      "evaluation/Actions Min                                 -0.568787\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            915.554\n",
      "evaluation/env_infos/final/reward_forward Mean         -6.57985e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.67813e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.51592e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.84021e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0032722\n",
      "evaluation/env_infos/initial/reward_forward Std         0.144002\n",
      "evaluation/env_infos/initial/reward_forward Max         0.356632\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.218363\n",
      "evaluation/env_infos/reward_forward Mean               -0.004382\n",
      "evaluation/env_infos/reward_forward Std                 0.0787877\n",
      "evaluation/env_infos/reward_forward Max                 0.907565\n",
      "evaluation/env_infos/reward_forward Min                -1.84991\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0851386\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.02807\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0310701\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.113987\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.118833\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0218523\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0789602\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.160496\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0852631\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0287998\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0249083\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.338928\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.7584e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.14939e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.94782e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.84021e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.114665\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.247715\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.56983\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.43436\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00465182\n",
      "evaluation/env_infos/torso_velocity Std                 0.0795798\n",
      "evaluation/env_infos/torso_velocity Max                 0.907565\n",
      "evaluation/env_infos/torso_velocity Min                -2.10633\n",
      "time/data storing (s)                                   0.0312717\n",
      "time/evaluation sampling (s)                           44.9907\n",
      "time/exploration sampling (s)                           2.04694\n",
      "time/logging (s)                                        0.278358\n",
      "time/saving (s)                                         0.0264029\n",
      "time/training (s)                                       4.42015\n",
      "time/epoch (s)                                         51.7938\n",
      "time/total (s)                                       4709.51\n",
      "Epoch                                                  87\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:14:10.394334 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 88 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 180000\n",
      "trainer/QF1 Loss                                        0.204905\n",
      "trainer/QF2 Loss                                        0.183335\n",
      "trainer/Policy Loss                                   -21.2175\n",
      "trainer/Q1 Predictions Mean                            28.8415\n",
      "trainer/Q1 Predictions Std                              1.95844\n",
      "trainer/Q1 Predictions Max                             31.1424\n",
      "trainer/Q1 Predictions Min                             19.2877\n",
      "trainer/Q2 Predictions Mean                            28.7619\n",
      "trainer/Q2 Predictions Std                              2.05593\n",
      "trainer/Q2 Predictions Max                             30.8212\n",
      "trainer/Q2 Predictions Min                             18.6994\n",
      "trainer/Q Targets Mean                                 28.7191\n",
      "trainer/Q Targets Std                                   1.95358\n",
      "trainer/Q Targets Max                                  30.6639\n",
      "trainer/Q Targets Min                                  19.2531\n",
      "trainer/Log Pis Mean                                    7.68551\n",
      "trainer/Log Pis Std                                     2.33581\n",
      "trainer/Log Pis Max                                    12.6458\n",
      "trainer/Log Pis Min                                    -4.82846\n",
      "trainer/Policy mu Mean                                  0.0464127\n",
      "trainer/Policy mu Std                                   0.131007\n",
      "trainer/Policy mu Max                                   1.03051\n",
      "trainer/Policy mu Min                                  -0.853925\n",
      "trainer/Policy log std Mean                            -2.33995\n",
      "trainer/Policy log std Std                              0.236116\n",
      "trainer/Policy log std Max                             -1.38864\n",
      "trainer/Policy log std Min                             -3.22101\n",
      "trainer/Alpha                                           0.00835803\n",
      "trainer/Alpha Loss                                     -1.50438\n",
      "exploration/num steps total                         90000\n",
      "exploration/num paths total                           162\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.912134\n",
      "exploration/Rewards Std                                 0.0768476\n",
      "exploration/Rewards Max                                 1.5384\n",
      "exploration/Rewards Min                                 0.718301\n",
      "exploration/Returns Mean                              912.134\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               912.134\n",
      "exploration/Returns Min                               912.134\n",
      "exploration/Actions Mean                                0.0579786\n",
      "exploration/Actions Std                                 0.148328\n",
      "exploration/Actions Max                                 0.506444\n",
      "exploration/Actions Min                                -0.421127\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           912.134\n",
      "exploration/env_infos/final/reward_forward Mean         0.0599026\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0599026\n",
      "exploration/env_infos/final/reward_forward Min          0.0599026\n",
      "exploration/env_infos/initial/reward_forward Mean       0.14182\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.14182\n",
      "exploration/env_infos/initial/reward_forward Min        0.14182\n",
      "exploration/env_infos/reward_forward Mean              -0.038956\n",
      "exploration/env_infos/reward_forward Std                0.15149\n",
      "exploration/env_infos/reward_forward Max                0.49327\n",
      "exploration/env_infos/reward_forward Min               -0.957566\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0422517\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0422517\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0422517\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0861186\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0861186\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0861186\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.101451\n",
      "exploration/env_infos/reward_ctrl Std                   0.0423667\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0143666\n",
      "exploration/env_infos/reward_ctrl Min                  -0.281699\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0294067\n",
      "exploration/env_infos/final/torso_velocity Std          0.0685052\n",
      "exploration/env_infos/final/torso_velocity Max          0.0599026\n",
      "exploration/env_infos/final/torso_velocity Min         -0.106578\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.266797\n",
      "exploration/env_infos/initial/torso_velocity Std        0.217325\n",
      "exploration/env_infos/initial/torso_velocity Max        0.572453\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0861164\n",
      "exploration/env_infos/torso_velocity Mean              -0.0146429\n",
      "exploration/env_infos/torso_velocity Std                0.177153\n",
      "exploration/env_infos/torso_velocity Max                0.915626\n",
      "exploration/env_infos/torso_velocity Min               -1.13813\n",
      "evaluation/num steps total                              2.225e+06\n",
      "evaluation/num paths total                           2225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.939402\n",
      "evaluation/Rewards Std                                  0.039677\n",
      "evaluation/Rewards Max                                  2.30319\n",
      "evaluation/Rewards Min                                  0.61309\n",
      "evaluation/Returns Mean                               939.402\n",
      "evaluation/Returns Std                                 27.1371\n",
      "evaluation/Returns Max                                967.836\n",
      "evaluation/Returns Min                                890.136\n",
      "evaluation/Actions Mean                                 0.0473973\n",
      "evaluation/Actions Std                                  0.114878\n",
      "evaluation/Actions Max                                  0.546946\n",
      "evaluation/Actions Min                                 -0.535792\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            939.402\n",
      "evaluation/env_infos/final/reward_forward Mean          3.60659e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.79267e-05\n",
      "evaluation/env_infos/final/reward_forward Max           9.14031e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -9.50423e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0192014\n",
      "evaluation/env_infos/initial/reward_forward Std         0.110797\n",
      "evaluation/env_infos/initial/reward_forward Max         0.205571\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.217945\n",
      "evaluation/env_infos/reward_forward Mean                0.00166547\n",
      "evaluation/env_infos/reward_forward Std                 0.053162\n",
      "evaluation/env_infos/reward_forward Max                 0.914642\n",
      "evaluation/env_infos/reward_forward Min                -1.28423\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0623422\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0281036\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0316691\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.112332\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0822384\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0372778\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0219125\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.147222\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0617741\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0295721\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00904318\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.38691\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.69721e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.67558e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           9.14031e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000112271\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130362\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.266673\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.737252\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.371227\n",
      "evaluation/env_infos/torso_velocity Mean                0.000382262\n",
      "evaluation/env_infos/torso_velocity Std                 0.0611978\n",
      "evaluation/env_infos/torso_velocity Max                 1.36214\n",
      "evaluation/env_infos/torso_velocity Min                -1.80533\n",
      "time/data storing (s)                                   0.0306609\n",
      "time/evaluation sampling (s)                           44.2659\n",
      "time/exploration sampling (s)                           2.02391\n",
      "time/logging (s)                                        0.269436\n",
      "time/saving (s)                                         0.0252703\n",
      "time/training (s)                                       4.06958\n",
      "time/epoch (s)                                         50.6848\n",
      "time/total (s)                                       4761.04\n",
      "Epoch                                                  88\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:02.017674 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 89 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 182000\n",
      "trainer/QF1 Loss                                        0.237123\n",
      "trainer/QF2 Loss                                        0.2291\n",
      "trainer/Policy Loss                                   -20.3378\n",
      "trainer/Q1 Predictions Mean                            28.9738\n",
      "trainer/Q1 Predictions Std                              1.77215\n",
      "trainer/Q1 Predictions Max                             30.8234\n",
      "trainer/Q1 Predictions Min                             19.4546\n",
      "trainer/Q2 Predictions Mean                            28.9275\n",
      "trainer/Q2 Predictions Std                              1.78496\n",
      "trainer/Q2 Predictions Max                             30.8317\n",
      "trainer/Q2 Predictions Min                             18.6597\n",
      "trainer/Q Targets Mean                                 29.155\n",
      "trainer/Q Targets Std                                   1.6615\n",
      "trainer/Q Targets Max                                  31.2342\n",
      "trainer/Q Targets Min                                  18.6335\n",
      "trainer/Log Pis Mean                                    8.77052\n",
      "trainer/Log Pis Std                                     2.4963\n",
      "trainer/Log Pis Max                                    15.1232\n",
      "trainer/Log Pis Min                                    -0.446946\n",
      "trainer/Policy mu Mean                                 -0.000542261\n",
      "trainer/Policy mu Std                                   0.180806\n",
      "trainer/Policy mu Max                                   0.856029\n",
      "trainer/Policy mu Min                                  -1.19686\n",
      "trainer/Policy log std Mean                            -2.46954\n",
      "trainer/Policy log std Std                              0.243476\n",
      "trainer/Policy log std Max                             -1.37486\n",
      "trainer/Policy log std Min                             -3.74278\n",
      "trainer/Alpha                                           0.00848375\n",
      "trainer/Alpha Loss                                      3.67729\n",
      "exploration/num steps total                         91000\n",
      "exploration/num paths total                           163\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.865423\n",
      "exploration/Rewards Std                                 0.0822214\n",
      "exploration/Rewards Max                                 1.30892\n",
      "exploration/Rewards Min                                 0.491882\n",
      "exploration/Returns Mean                              865.423\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               865.423\n",
      "exploration/Returns Min                               865.423\n",
      "exploration/Actions Mean                                0.0129186\n",
      "exploration/Actions Std                                 0.193954\n",
      "exploration/Actions Max                                 0.651187\n",
      "exploration/Actions Min                                -0.613374\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           865.423\n",
      "exploration/env_infos/final/reward_forward Mean         0.00441141\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00441141\n",
      "exploration/env_infos/final/reward_forward Min          0.00441141\n",
      "exploration/env_infos/initial/reward_forward Mean       0.18137\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.18137\n",
      "exploration/env_infos/initial/reward_forward Min        0.18137\n",
      "exploration/env_infos/reward_forward Mean              -0.0135445\n",
      "exploration/env_infos/reward_forward Std                0.177902\n",
      "exploration/env_infos/reward_forward Max                0.615453\n",
      "exploration/env_infos/reward_forward Min               -0.872514\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.172167\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.172167\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.172167\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0912594\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0912594\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0912594\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.151141\n",
      "exploration/env_infos/reward_ctrl Std                   0.0623689\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0258401\n",
      "exploration/env_infos/reward_ctrl Min                  -0.508118\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00534818\n",
      "exploration/env_infos/final/torso_velocity Std          0.00235337\n",
      "exploration/env_infos/final/torso_velocity Max          0.00858231\n",
      "exploration/env_infos/final/torso_velocity Min          0.00305082\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.223861\n",
      "exploration/env_infos/initial/torso_velocity Std        0.203537\n",
      "exploration/env_infos/initial/torso_velocity Max        0.491656\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00144373\n",
      "exploration/env_infos/torso_velocity Mean              -0.0113093\n",
      "exploration/env_infos/torso_velocity Std                0.20554\n",
      "exploration/env_infos/torso_velocity Max                0.890006\n",
      "exploration/env_infos/torso_velocity Min               -1.28149\n",
      "evaluation/num steps total                              2.25e+06\n",
      "evaluation/num paths total                           2250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.898915\n",
      "evaluation/Rewards Std                                  0.0415375\n",
      "evaluation/Rewards Max                                  2.5101\n",
      "evaluation/Rewards Min                                  0.563648\n",
      "evaluation/Returns Mean                               898.915\n",
      "evaluation/Returns Std                                 30.1354\n",
      "evaluation/Returns Max                                937.421\n",
      "evaluation/Returns Min                                832.006\n",
      "evaluation/Actions Mean                                 0.000250007\n",
      "evaluation/Actions Std                                  0.159955\n",
      "evaluation/Actions Max                                  0.609838\n",
      "evaluation/Actions Min                                 -0.575169\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            898.915\n",
      "evaluation/env_infos/final/reward_forward Mean          1.25591e-05\n",
      "evaluation/env_infos/final/reward_forward Std           4.74918e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.00023165\n",
      "evaluation/env_infos/final/reward_forward Min          -7.18174e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0471731\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0897294\n",
      "evaluation/env_infos/initial/reward_forward Max         0.194187\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.219996\n",
      "evaluation/env_infos/reward_forward Mean               -0.00093061\n",
      "evaluation/env_infos/reward_forward Std                 0.0548379\n",
      "evaluation/env_infos/reward_forward Max                 1.06483\n",
      "evaluation/env_infos/reward_forward Min                -1.61604\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.102664\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0309131\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0623229\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.170432\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.126028\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0340205\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0569819\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.183829\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.102342\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0316982\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.022428\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.436352\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.48853e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           3.20568e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00023165\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000123061\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.0938034\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.25496\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.643135\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.32792\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00252058\n",
      "evaluation/env_infos/torso_velocity Std                 0.0697258\n",
      "evaluation/env_infos/torso_velocity Max                 1.06483\n",
      "evaluation/env_infos/torso_velocity Min                -2.23696\n",
      "time/data storing (s)                                   0.0303251\n",
      "time/evaluation sampling (s)                           44.4813\n",
      "time/exploration sampling (s)                           1.87664\n",
      "time/logging (s)                                        0.266598\n",
      "time/saving (s)                                         0.0254149\n",
      "time/training (s)                                       4.18856\n",
      "time/epoch (s)                                         50.8688\n",
      "time/total (s)                                       4812.66\n",
      "Epoch                                                  89\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:15:53.972521 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 90 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 184000\n",
      "trainer/QF1 Loss                                        0.240666\n",
      "trainer/QF2 Loss                                        0.319529\n",
      "trainer/Policy Loss                                   -22.0895\n",
      "trainer/Q1 Predictions Mean                            29.2663\n",
      "trainer/Q1 Predictions Std                              1.87899\n",
      "trainer/Q1 Predictions Max                             31.2987\n",
      "trainer/Q1 Predictions Min                             18.4379\n",
      "trainer/Q2 Predictions Mean                            29.4865\n",
      "trainer/Q2 Predictions Std                              1.83414\n",
      "trainer/Q2 Predictions Max                             32.4154\n",
      "trainer/Q2 Predictions Min                             19.8218\n",
      "trainer/Q Targets Mean                                 29.2621\n",
      "trainer/Q Targets Std                                   2.00373\n",
      "trainer/Q Targets Max                                  32.8512\n",
      "trainer/Q Targets Min                                  17.7463\n",
      "trainer/Log Pis Mean                                    7.33734\n",
      "trainer/Log Pis Std                                     2.1624\n",
      "trainer/Log Pis Max                                    12.7544\n",
      "trainer/Log Pis Min                                    -1.37539\n",
      "trainer/Policy mu Mean                                  0.0245978\n",
      "trainer/Policy mu Std                                   0.148436\n",
      "trainer/Policy mu Max                                   1.33654\n",
      "trainer/Policy mu Min                                  -0.878009\n",
      "trainer/Policy log std Mean                            -2.34886\n",
      "trainer/Policy log std Std                              0.233499\n",
      "trainer/Policy log std Max                             -1.55298\n",
      "trainer/Policy log std Min                             -3.34949\n",
      "trainer/Alpha                                           0.00881307\n",
      "trainer/Alpha Loss                                     -3.1342\n",
      "exploration/num steps total                         92000\n",
      "exploration/num paths total                           164\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.895994\n",
      "exploration/Rewards Std                                 0.051012\n",
      "exploration/Rewards Max                                 1.2339\n",
      "exploration/Rewards Min                                 0.727404\n",
      "exploration/Returns Mean                              895.994\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               895.994\n",
      "exploration/Returns Min                               895.994\n",
      "exploration/Actions Mean                                0.0346729\n",
      "exploration/Actions Std                                 0.162857\n",
      "exploration/Actions Max                                 0.510258\n",
      "exploration/Actions Min                                -0.545475\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           895.994\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0365987\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0365987\n",
      "exploration/env_infos/final/reward_forward Min         -0.0365987\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0724446\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0724446\n",
      "exploration/env_infos/initial/reward_forward Min        0.0724446\n",
      "exploration/env_infos/reward_forward Mean              -0.0566643\n",
      "exploration/env_infos/reward_forward Std                0.192063\n",
      "exploration/env_infos/reward_forward Max                0.59822\n",
      "exploration/env_infos/reward_forward Min               -0.981138\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0915775\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0915775\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0915775\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.148962\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.148962\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.148962\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.110899\n",
      "exploration/env_infos/reward_ctrl Std                   0.0420162\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0180368\n",
      "exploration/env_infos/reward_ctrl Min                  -0.272596\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0535776\n",
      "exploration/env_infos/final/torso_velocity Std          0.0125498\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0365987\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0665427\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0658146\n",
      "exploration/env_infos/initial/torso_velocity Std        0.273931\n",
      "exploration/env_infos/initial/torso_velocity Max        0.397945\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.272946\n",
      "exploration/env_infos/torso_velocity Mean              -0.0171561\n",
      "exploration/env_infos/torso_velocity Std                0.25634\n",
      "exploration/env_infos/torso_velocity Max                1.3899\n",
      "exploration/env_infos/torso_velocity Min               -1.08755\n",
      "evaluation/num steps total                              2.275e+06\n",
      "evaluation/num paths total                           2275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.915018\n",
      "evaluation/Rewards Std                                  0.0439144\n",
      "evaluation/Rewards Max                                  2.58389\n",
      "evaluation/Rewards Min                                  0.657304\n",
      "evaluation/Returns Mean                               915.018\n",
      "evaluation/Returns Std                                 27.1833\n",
      "evaluation/Returns Max                                962.345\n",
      "evaluation/Returns Min                                856.535\n",
      "evaluation/Actions Mean                                 0.00728715\n",
      "evaluation/Actions Std                                  0.14686\n",
      "evaluation/Actions Max                                  0.431837\n",
      "evaluation/Actions Min                                 -0.530867\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            915.018\n",
      "evaluation/env_infos/final/reward_forward Mean          5.68346e-08\n",
      "evaluation/env_infos/final/reward_forward Std           2.92806e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.72168e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.94746e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0698164\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121626\n",
      "evaluation/env_infos/initial/reward_forward Max         0.179054\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.24566\n",
      "evaluation/env_infos/reward_forward Mean                0.00148086\n",
      "evaluation/env_infos/reward_forward Std                 0.0636138\n",
      "evaluation/env_infos/reward_forward Max                 1.42745\n",
      "evaluation/env_infos/reward_forward Min                -1.63549\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0867985\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0274158\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0371528\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.144998\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.133805\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0740254\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0255652\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.250351\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0864835\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0281349\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0131895\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.342696\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          2.53982e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.15766e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.72168e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.94746e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129128\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.266945\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.73053\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.303103\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00044095\n",
      "evaluation/env_infos/torso_velocity Std                 0.0756321\n",
      "evaluation/env_infos/torso_velocity Max                 1.72647\n",
      "evaluation/env_infos/torso_velocity Min                -1.75837\n",
      "time/data storing (s)                                   0.0361129\n",
      "time/evaluation sampling (s)                           44.4352\n",
      "time/exploration sampling (s)                           2.25932\n",
      "time/logging (s)                                        0.28303\n",
      "time/saving (s)                                         0.0295286\n",
      "time/training (s)                                       4.16974\n",
      "time/epoch (s)                                         51.2129\n",
      "time/total (s)                                       4864.63\n",
      "Epoch                                                  90\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:16:47.008178 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 91 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 186000\n",
      "trainer/QF1 Loss                                        0.706186\n",
      "trainer/QF2 Loss                                        0.276451\n",
      "trainer/Policy Loss                                   -21.3702\n",
      "trainer/Q1 Predictions Mean                            29.186\n",
      "trainer/Q1 Predictions Std                              2.74179\n",
      "trainer/Q1 Predictions Max                             31.4063\n",
      "trainer/Q1 Predictions Min                              8.78586\n",
      "trainer/Q2 Predictions Mean                            29.1501\n",
      "trainer/Q2 Predictions Std                              3.07559\n",
      "trainer/Q2 Predictions Max                             31.7802\n",
      "trainer/Q2 Predictions Min                             -2.5782\n",
      "trainer/Q Targets Mean                                 29.4095\n",
      "trainer/Q Targets Std                                   2.92088\n",
      "trainer/Q Targets Max                                  31.9796\n",
      "trainer/Q Targets Min                                  -1.11722\n",
      "trainer/Log Pis Mean                                    7.93918\n",
      "trainer/Log Pis Std                                     2.48715\n",
      "trainer/Log Pis Max                                    23.7828\n",
      "trainer/Log Pis Min                                    -0.318207\n",
      "trainer/Policy mu Mean                                  0.00220873\n",
      "trainer/Policy mu Std                                   0.237088\n",
      "trainer/Policy mu Max                                   3.27157\n",
      "trainer/Policy mu Min                                  -3.74698\n",
      "trainer/Policy log std Mean                            -2.3511\n",
      "trainer/Policy log std Std                              0.211748\n",
      "trainer/Policy log std Max                             -0.718347\n",
      "trainer/Policy log std Min                             -3.75085\n",
      "trainer/Alpha                                           0.00753153\n",
      "trainer/Alpha Loss                                     -0.297201\n",
      "exploration/num steps total                         93000\n",
      "exploration/num paths total                           165\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.882106\n",
      "exploration/Rewards Std                                 0.0489262\n",
      "exploration/Rewards Max                                 1.14683\n",
      "exploration/Rewards Min                                 0.675342\n",
      "exploration/Returns Mean                              882.106\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               882.106\n",
      "exploration/Returns Min                               882.106\n",
      "exploration/Actions Mean                               -0.00214872\n",
      "exploration/Actions Std                                 0.173393\n",
      "exploration/Actions Max                                 0.442003\n",
      "exploration/Actions Min                                -0.576359\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           882.106\n",
      "exploration/env_infos/final/reward_forward Mean         0.1109\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.1109\n",
      "exploration/env_infos/final/reward_forward Min          0.1109\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.206303\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.206303\n",
      "exploration/env_infos/initial/reward_forward Min       -0.206303\n",
      "exploration/env_infos/reward_forward Mean              -0.00598834\n",
      "exploration/env_infos/reward_forward Std                0.077056\n",
      "exploration/env_infos/reward_forward Max                0.301552\n",
      "exploration/env_infos/reward_forward Min               -0.880484\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.148592\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.148592\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.148592\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.11781\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.11781\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.11781\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.120279\n",
      "exploration/env_infos/reward_ctrl Std                   0.046737\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0164573\n",
      "exploration/env_infos/reward_ctrl Min                  -0.331582\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0650436\n",
      "exploration/env_infos/final/torso_velocity Std          0.176641\n",
      "exploration/env_infos/final/torso_velocity Max          0.254779\n",
      "exploration/env_infos/final/torso_velocity Min         -0.170549\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0151235\n",
      "exploration/env_infos/initial/torso_velocity Std        0.342257\n",
      "exploration/env_infos/initial/torso_velocity Max        0.49858\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.246907\n",
      "exploration/env_infos/torso_velocity Mean               0.00557627\n",
      "exploration/env_infos/torso_velocity Std                0.113553\n",
      "exploration/env_infos/torso_velocity Max                1.22907\n",
      "exploration/env_infos/torso_velocity Min               -1.22548\n",
      "evaluation/num steps total                              2.3e+06\n",
      "evaluation/num paths total                           2300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.945103\n",
      "evaluation/Rewards Std                                  0.0399603\n",
      "evaluation/Rewards Max                                  2.16967\n",
      "evaluation/Rewards Min                                  0.716485\n",
      "evaluation/Returns Mean                               945.103\n",
      "evaluation/Returns Std                                 16.4364\n",
      "evaluation/Returns Max                                976.507\n",
      "evaluation/Returns Min                                901.743\n",
      "evaluation/Actions Mean                                -0.0133855\n",
      "evaluation/Actions Std                                  0.118159\n",
      "evaluation/Actions Max                                  0.414152\n",
      "evaluation/Actions Min                                 -0.430393\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            945.103\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000203073\n",
      "evaluation/env_infos/final/reward_forward Std           0.00099036\n",
      "evaluation/env_infos/final/reward_forward Max           0.00505479\n",
      "evaluation/env_infos/final/reward_forward Min          -9.93518e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0163969\n",
      "evaluation/env_infos/initial/reward_forward Std         0.138997\n",
      "evaluation/env_infos/initial/reward_forward Max         0.252631\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.254156\n",
      "evaluation/env_infos/reward_forward Mean                0.00480041\n",
      "evaluation/env_infos/reward_forward Std                 0.0702268\n",
      "evaluation/env_infos/reward_forward Max                 1.57702\n",
      "evaluation/env_infos/reward_forward Min                -1.2715\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0568691\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0162447\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0248725\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0990862\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.108596\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0379847\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0369159\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.183105\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0565628\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0170748\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0130357\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.283515\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000170205\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000983379\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00697625\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.93518e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135055\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.266348\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.697666\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.269149\n",
      "evaluation/env_infos/torso_velocity Mean                0.00132552\n",
      "evaluation/env_infos/torso_velocity Std                 0.0744332\n",
      "evaluation/env_infos/torso_velocity Max                 1.57702\n",
      "evaluation/env_infos/torso_velocity Min                -1.72236\n",
      "time/data storing (s)                                   0.0303273\n",
      "time/evaluation sampling (s)                           45.3558\n",
      "time/exploration sampling (s)                           2.06184\n",
      "time/logging (s)                                        0.274343\n",
      "time/saving (s)                                         0.0268696\n",
      "time/training (s)                                       4.4406\n",
      "time/epoch (s)                                         52.1898\n",
      "time/total (s)                                       4917.65\n",
      "Epoch                                                  91\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:17:39.031726 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 92 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 188000\n",
      "trainer/QF1 Loss                                        0.285569\n",
      "trainer/QF2 Loss                                        0.304524\n",
      "trainer/Policy Loss                                   -21.4568\n",
      "trainer/Q1 Predictions Mean                            29.6923\n",
      "trainer/Q1 Predictions Std                              2.4569\n",
      "trainer/Q1 Predictions Max                             31.6596\n",
      "trainer/Q1 Predictions Min                              1.08499\n",
      "trainer/Q2 Predictions Mean                            29.8355\n",
      "trainer/Q2 Predictions Std                              2.28954\n",
      "trainer/Q2 Predictions Max                             31.7934\n",
      "trainer/Q2 Predictions Min                              5.53143\n",
      "trainer/Q Targets Mean                                 29.8328\n",
      "trainer/Q Targets Std                                   2.5741\n",
      "trainer/Q Targets Max                                  31.71\n",
      "trainer/Q Targets Min                                  -0.603525\n",
      "trainer/Log Pis Mean                                    8.35604\n",
      "trainer/Log Pis Std                                     2.37555\n",
      "trainer/Log Pis Max                                    17.3288\n",
      "trainer/Log Pis Min                                     0.28136\n",
      "trainer/Policy mu Mean                                  0.000225165\n",
      "trainer/Policy mu Std                                   0.159995\n",
      "trainer/Policy mu Max                                   1.46136\n",
      "trainer/Policy mu Min                                  -1.10627\n",
      "trainer/Policy log std Mean                            -2.43652\n",
      "trainer/Policy log std Std                              0.19774\n",
      "trainer/Policy log std Max                             -1.89658\n",
      "trainer/Policy log std Min                             -3.71144\n",
      "trainer/Alpha                                           0.00694675\n",
      "trainer/Alpha Loss                                      1.76992\n",
      "exploration/num steps total                         94000\n",
      "exploration/num paths total                           166\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.920691\n",
      "exploration/Rewards Std                                 0.0680805\n",
      "exploration/Rewards Max                                 1.5266\n",
      "exploration/Rewards Min                                 0.691451\n",
      "exploration/Returns Mean                              920.691\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               920.691\n",
      "exploration/Returns Min                               920.691\n",
      "exploration/Actions Mean                               -0.0229595\n",
      "exploration/Actions Std                                 0.146779\n",
      "exploration/Actions Max                                 0.514544\n",
      "exploration/Actions Min                                -0.52299\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           920.691\n",
      "exploration/env_infos/final/reward_forward Mean         0.327793\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.327793\n",
      "exploration/env_infos/final/reward_forward Min          0.327793\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00879955\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00879955\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00879955\n",
      "exploration/env_infos/reward_forward Mean              -0.00288093\n",
      "exploration/env_infos/reward_forward Std                0.166277\n",
      "exploration/env_infos/reward_forward Max                0.722334\n",
      "exploration/env_infos/reward_forward Min               -0.731491\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0532378\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0532378\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0532378\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.196176\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.196176\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.196176\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0882848\n",
      "exploration/env_infos/reward_ctrl Std                   0.0420891\n",
      "exploration/env_infos/reward_ctrl Max                  -0.007405\n",
      "exploration/env_infos/reward_ctrl Min                  -0.308549\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.179949\n",
      "exploration/env_infos/final/torso_velocity Std          0.460002\n",
      "exploration/env_infos/final/torso_velocity Max          0.65467\n",
      "exploration/env_infos/final/torso_velocity Min         -0.442617\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0720599\n",
      "exploration/env_infos/initial/torso_velocity Std        0.189665\n",
      "exploration/env_infos/initial/torso_velocity Max        0.333974\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.108995\n",
      "exploration/env_infos/torso_velocity Mean               0.0022885\n",
      "exploration/env_infos/torso_velocity Std                0.177052\n",
      "exploration/env_infos/torso_velocity Max                0.88284\n",
      "exploration/env_infos/torso_velocity Min               -1.27186\n",
      "evaluation/num steps total                              2.325e+06\n",
      "evaluation/num paths total                           2325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.958171\n",
      "evaluation/Rewards Std                                  0.0412973\n",
      "evaluation/Rewards Max                                  2.41062\n",
      "evaluation/Rewards Min                                  0.582628\n",
      "evaluation/Returns Mean                               958.171\n",
      "evaluation/Returns Std                                 12.9104\n",
      "evaluation/Returns Max                                977.731\n",
      "evaluation/Returns Min                                936.771\n",
      "evaluation/Actions Mean                                -0.0270339\n",
      "evaluation/Actions Std                                  0.101958\n",
      "evaluation/Actions Max                                  0.552294\n",
      "evaluation/Actions Min                                 -0.528355\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            958.171\n",
      "evaluation/env_infos/final/reward_forward Mean          2.02493e-05\n",
      "evaluation/env_infos/final/reward_forward Std           9.8223e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000500305\n",
      "evaluation/env_infos/final/reward_forward Min          -2.14528e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0681373\n",
      "evaluation/env_infos/initial/reward_forward Std         0.119634\n",
      "evaluation/env_infos/initial/reward_forward Max         0.301969\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.200868\n",
      "evaluation/env_infos/reward_forward Mean                0.00124712\n",
      "evaluation/env_infos/reward_forward Std                 0.0957996\n",
      "evaluation/env_infos/reward_forward Max                 1.27042\n",
      "evaluation/env_infos/reward_forward Min                -1.35659\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0436053\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0137532\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0231127\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0735231\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.246749\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0774418\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.104276\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.373916\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0445049\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0181373\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0119562\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.417372\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.03443e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000272083\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00225968\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000512778\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.12783\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.265462\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.61849\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.200868\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00230902\n",
      "evaluation/env_infos/torso_velocity Std                 0.0863933\n",
      "evaluation/env_infos/torso_velocity Max                 1.27042\n",
      "evaluation/env_infos/torso_velocity Min                -1.71569\n",
      "time/data storing (s)                                   0.0304325\n",
      "time/evaluation sampling (s)                           44.6018\n",
      "time/exploration sampling (s)                           1.981\n",
      "time/logging (s)                                        0.277885\n",
      "time/saving (s)                                         0.0271553\n",
      "time/training (s)                                       4.31456\n",
      "time/epoch (s)                                         51.2328\n",
      "time/total (s)                                       4969.67\n",
      "Epoch                                                  92\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:18:31.064801 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 93 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 190000\n",
      "trainer/QF1 Loss                                        0.341565\n",
      "trainer/QF2 Loss                                        0.376254\n",
      "trainer/Policy Loss                                   -21.2928\n",
      "trainer/Q1 Predictions Mean                            30.1824\n",
      "trainer/Q1 Predictions Std                              2.19377\n",
      "trainer/Q1 Predictions Max                             32.465\n",
      "trainer/Q1 Predictions Min                             17.8431\n",
      "trainer/Q2 Predictions Mean                            30.1496\n",
      "trainer/Q2 Predictions Std                              2.15848\n",
      "trainer/Q2 Predictions Max                             32.2749\n",
      "trainer/Q2 Predictions Min                             17.1377\n",
      "trainer/Q Targets Mean                                 29.9576\n",
      "trainer/Q Targets Std                                   2.31724\n",
      "trainer/Q Targets Max                                  32.4532\n",
      "trainer/Q Targets Min                                  14.0972\n",
      "trainer/Log Pis Mean                                    9.04243\n",
      "trainer/Log Pis Std                                     2.60044\n",
      "trainer/Log Pis Max                                    18.0092\n",
      "trainer/Log Pis Min                                    -0.738763\n",
      "trainer/Policy mu Mean                                  0.0381131\n",
      "trainer/Policy mu Std                                   0.236835\n",
      "trainer/Policy mu Max                                   1.58264\n",
      "trainer/Policy mu Min                                  -1.42408\n",
      "trainer/Policy log std Mean                            -2.47977\n",
      "trainer/Policy log std Std                              0.219618\n",
      "trainer/Policy log std Max                             -1.44494\n",
      "trainer/Policy log std Min                             -3.68289\n",
      "trainer/Alpha                                           0.00785899\n",
      "trainer/Alpha Loss                                      5.0571\n",
      "exploration/num steps total                         95000\n",
      "exploration/num paths total                           167\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.82935\n",
      "exploration/Rewards Std                                 0.0727593\n",
      "exploration/Rewards Max                                 1.3283\n",
      "exploration/Rewards Min                                 0.613317\n",
      "exploration/Returns Mean                              829.35\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               829.35\n",
      "exploration/Returns Min                               829.35\n",
      "exploration/Actions Mean                               -0.0186134\n",
      "exploration/Actions Std                                 0.21357\n",
      "exploration/Actions Max                                 0.550742\n",
      "exploration/Actions Min                                -0.515393\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           829.35\n",
      "exploration/env_infos/final/reward_forward Mean        -0.000633484\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.000633484\n",
      "exploration/env_infos/final/reward_forward Min         -0.000633484\n",
      "exploration/env_infos/initial/reward_forward Mean       0.193775\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.193775\n",
      "exploration/env_infos/initial/reward_forward Min        0.193775\n",
      "exploration/env_infos/reward_forward Mean              -0.00445759\n",
      "exploration/env_infos/reward_forward Std                0.102656\n",
      "exploration/env_infos/reward_forward Max                0.574624\n",
      "exploration/env_infos/reward_forward Min               -0.584855\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.24623\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.24623\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.24623\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.317046\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.317046\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.317046\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.183835\n",
      "exploration/env_infos/reward_ctrl Std                   0.0490141\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0583352\n",
      "exploration/env_infos/reward_ctrl Min                  -0.386683\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.000620075\n",
      "exploration/env_infos/final/torso_velocity Std          0.000802952\n",
      "exploration/env_infos/final/torso_velocity Max          0.000369973\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00159671\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.206159\n",
      "exploration/env_infos/initial/torso_velocity Std        0.193201\n",
      "exploration/env_infos/initial/torso_velocity Max        0.44873\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0240281\n",
      "exploration/env_infos/torso_velocity Mean               0.0051553\n",
      "exploration/env_infos/torso_velocity Std                0.111765\n",
      "exploration/env_infos/torso_velocity Max                0.574624\n",
      "exploration/env_infos/torso_velocity Min               -0.935585\n",
      "evaluation/num steps total                              2.35e+06\n",
      "evaluation/num paths total                           2350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.823039\n",
      "evaluation/Rewards Std                                  0.111628\n",
      "evaluation/Rewards Max                                  2.04055\n",
      "evaluation/Rewards Min                                  0.211611\n",
      "evaluation/Returns Mean                               823.039\n",
      "evaluation/Returns Std                                108.284\n",
      "evaluation/Returns Max                                924.395\n",
      "evaluation/Returns Min                                594.004\n",
      "evaluation/Actions Mean                                 0.0232763\n",
      "evaluation/Actions Std                                  0.209382\n",
      "evaluation/Actions Max                                  0.729466\n",
      "evaluation/Actions Min                                 -0.52791\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            823.039\n",
      "evaluation/env_infos/final/reward_forward Mean         -4.07556e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.02702e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.91313e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.70093e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.029924\n",
      "evaluation/env_infos/initial/reward_forward Std         0.152395\n",
      "evaluation/env_infos/initial/reward_forward Max         0.399891\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.310247\n",
      "evaluation/env_infos/reward_forward Mean                0.0033489\n",
      "evaluation/env_infos/reward_forward Std                 0.0593616\n",
      "evaluation/env_infos/reward_forward Max                 1.53887\n",
      "evaluation/env_infos/reward_forward Min                -1.12488\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.176668\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.109241\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0745126\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.407418\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.217811\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0714775\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.102534\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.37674\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.17753\n",
      "evaluation/env_infos/reward_ctrl Std                    0.109812\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0217533\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.788389\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.87429e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.68945e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.56749e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.70093e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.140032\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242641\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.62682\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310247\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00201893\n",
      "evaluation/env_infos/torso_velocity Std                 0.0705599\n",
      "evaluation/env_infos/torso_velocity Max                 1.86319\n",
      "evaluation/env_infos/torso_velocity Min                -2.22691\n",
      "time/data storing (s)                                   0.0306351\n",
      "time/evaluation sampling (s)                           44.6831\n",
      "time/exploration sampling (s)                           2.04413\n",
      "time/logging (s)                                        0.271028\n",
      "time/saving (s)                                         0.0262343\n",
      "time/training (s)                                       4.20119\n",
      "time/epoch (s)                                         51.2563\n",
      "time/total (s)                                       5021.7\n",
      "Epoch                                                  93\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:19:24.307457 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 94 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 192000\n",
      "trainer/QF1 Loss                                        0.160273\n",
      "trainer/QF2 Loss                                        0.179531\n",
      "trainer/Policy Loss                                   -23.1593\n",
      "trainer/Q1 Predictions Mean                            30.3226\n",
      "trainer/Q1 Predictions Std                              1.57798\n",
      "trainer/Q1 Predictions Max                             32.105\n",
      "trainer/Q1 Predictions Min                             18.0975\n",
      "trainer/Q2 Predictions Mean                            30.3263\n",
      "trainer/Q2 Predictions Std                              1.60547\n",
      "trainer/Q2 Predictions Max                             32.0972\n",
      "trainer/Q2 Predictions Min                             18.2598\n",
      "trainer/Q Targets Mean                                 30.4842\n",
      "trainer/Q Targets Std                                   1.53819\n",
      "trainer/Q Targets Max                                  32.4359\n",
      "trainer/Q Targets Min                                  19.5967\n",
      "trainer/Log Pis Mean                                    7.2161\n",
      "trainer/Log Pis Std                                     2.09589\n",
      "trainer/Log Pis Max                                    12.6417\n",
      "trainer/Log Pis Min                                     0.14021\n",
      "trainer/Policy mu Mean                                  0.0219886\n",
      "trainer/Policy mu Std                                   0.144603\n",
      "trainer/Policy mu Max                                   1.09266\n",
      "trainer/Policy mu Min                                  -0.509808\n",
      "trainer/Policy log std Mean                            -2.28101\n",
      "trainer/Policy log std Std                              0.162755\n",
      "trainer/Policy log std Max                             -1.78932\n",
      "trainer/Policy log std Min                             -2.97303\n",
      "trainer/Alpha                                           0.0076772\n",
      "trainer/Alpha Loss                                     -3.81512\n",
      "exploration/num steps total                         96000\n",
      "exploration/num paths total                           168\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.908421\n",
      "exploration/Rewards Std                                 0.0728155\n",
      "exploration/Rewards Max                                 1.43952\n",
      "exploration/Rewards Min                                 0.63757\n",
      "exploration/Returns Mean                              908.421\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               908.421\n",
      "exploration/Returns Min                               908.421\n",
      "exploration/Actions Mean                                0.00336581\n",
      "exploration/Actions Std                                 0.158808\n",
      "exploration/Actions Max                                 0.720066\n",
      "exploration/Actions Min                                -0.516147\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           908.421\n",
      "exploration/env_infos/final/reward_forward Mean         0.179386\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.179386\n",
      "exploration/env_infos/final/reward_forward Min          0.179386\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.02881\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.02881\n",
      "exploration/env_infos/initial/reward_forward Min       -0.02881\n",
      "exploration/env_infos/reward_forward Mean               0.00907671\n",
      "exploration/env_infos/reward_forward Std                0.21125\n",
      "exploration/env_infos/reward_forward Max                0.858851\n",
      "exploration/env_infos/reward_forward Min               -0.841116\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.114375\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.114375\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.114375\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.228283\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.228283\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.228283\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.100925\n",
      "exploration/env_infos/reward_ctrl Std                   0.0476605\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0124144\n",
      "exploration/env_infos/reward_ctrl Min                  -0.36243\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.249851\n",
      "exploration/env_infos/final/torso_velocity Std          0.207441\n",
      "exploration/env_infos/final/torso_velocity Max          0.531707\n",
      "exploration/env_infos/final/torso_velocity Min          0.0384583\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.100852\n",
      "exploration/env_infos/initial/torso_velocity Std        0.261363\n",
      "exploration/env_infos/initial/torso_velocity Max        0.465444\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.134077\n",
      "exploration/env_infos/torso_velocity Mean              -0.00590723\n",
      "exploration/env_infos/torso_velocity Std                0.292792\n",
      "exploration/env_infos/torso_velocity Max                1.19646\n",
      "exploration/env_infos/torso_velocity Min               -1.55179\n",
      "evaluation/num steps total                              2.375e+06\n",
      "evaluation/num paths total                           2375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.951325\n",
      "evaluation/Rewards Std                                  0.0281912\n",
      "evaluation/Rewards Max                                  2.06314\n",
      "evaluation/Rewards Min                                  0.545923\n",
      "evaluation/Returns Mean                               951.325\n",
      "evaluation/Returns Std                                 11.6762\n",
      "evaluation/Returns Max                                970.155\n",
      "evaluation/Returns Min                                919.725\n",
      "evaluation/Actions Mean                                 0.0237608\n",
      "evaluation/Actions Std                                  0.108895\n",
      "evaluation/Actions Max                                  0.523478\n",
      "evaluation/Actions Min                                 -0.48573\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            951.325\n",
      "evaluation/env_infos/final/reward_forward Mean          7.6841e-09\n",
      "evaluation/env_infos/final/reward_forward Std           1.58721e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.59131e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.36701e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.062099\n",
      "evaluation/env_infos/initial/reward_forward Std         0.113405\n",
      "evaluation/env_infos/initial/reward_forward Max         0.123209\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.275804\n",
      "evaluation/env_infos/reward_forward Mean                5.41133e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0581693\n",
      "evaluation/env_infos/reward_forward Max                 1.44904\n",
      "evaluation/env_infos/reward_forward Min                -1.3072\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0491968\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.01172\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0299818\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0830126\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.138801\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0424127\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0702015\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.242577\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0496908\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0156415\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0160814\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.454077\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.89114e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           1.47373e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.77402e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.0757e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.125114\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.260638\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.638845\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.28025\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0024188\n",
      "evaluation/env_infos/torso_velocity Std                 0.073049\n",
      "evaluation/env_infos/torso_velocity Max                 1.44904\n",
      "evaluation/env_infos/torso_velocity Min                -2.09224\n",
      "time/data storing (s)                                   0.0302995\n",
      "time/evaluation sampling (s)                           45.914\n",
      "time/exploration sampling (s)                           2.06678\n",
      "time/logging (s)                                        0.280593\n",
      "time/saving (s)                                         0.0256119\n",
      "time/training (s)                                       4.15395\n",
      "time/epoch (s)                                         52.4712\n",
      "time/total (s)                                       5074.95\n",
      "Epoch                                                  94\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:20:18.741780 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 95 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 194000\n",
      "trainer/QF1 Loss                                        0.178622\n",
      "trainer/QF2 Loss                                        0.17815\n",
      "trainer/Policy Loss                                   -21.8666\n",
      "trainer/Q1 Predictions Mean                            30.6279\n",
      "trainer/Q1 Predictions Std                              2.04679\n",
      "trainer/Q1 Predictions Max                             32.5291\n",
      "trainer/Q1 Predictions Min                             18.2689\n",
      "trainer/Q2 Predictions Mean                            30.5676\n",
      "trainer/Q2 Predictions Std                              2.06484\n",
      "trainer/Q2 Predictions Max                             32.6985\n",
      "trainer/Q2 Predictions Min                             19.0197\n",
      "trainer/Q Targets Mean                                 30.602\n",
      "trainer/Q Targets Std                                   2.02618\n",
      "trainer/Q Targets Max                                  32.7743\n",
      "trainer/Q Targets Min                                  18.559\n",
      "trainer/Log Pis Mean                                    8.80223\n",
      "trainer/Log Pis Std                                     2.41379\n",
      "trainer/Log Pis Max                                    16.9597\n",
      "trainer/Log Pis Min                                    -1.39447\n",
      "trainer/Policy mu Mean                                  0.0810329\n",
      "trainer/Policy mu Std                                   0.170242\n",
      "trainer/Policy mu Max                                   1.22197\n",
      "trainer/Policy mu Min                                  -1.95064\n",
      "trainer/Policy log std Mean                            -2.47876\n",
      "trainer/Policy log std Std                              0.221068\n",
      "trainer/Policy log std Max                             -1.66799\n",
      "trainer/Policy log std Min                             -3.91305\n",
      "trainer/Alpha                                           0.00754198\n",
      "trainer/Alpha Loss                                      3.92327\n",
      "exploration/num steps total                         97000\n",
      "exploration/num paths total                           169\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.895896\n",
      "exploration/Rewards Std                                 0.096012\n",
      "exploration/Rewards Max                                 1.48084\n",
      "exploration/Rewards Min                                 0.493304\n",
      "exploration/Returns Mean                              895.896\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               895.896\n",
      "exploration/Returns Min                               895.896\n",
      "exploration/Actions Mean                                0.0179046\n",
      "exploration/Actions Std                                 0.177703\n",
      "exploration/Actions Max                                 0.77632\n",
      "exploration/Actions Min                                -0.539062\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           895.896\n",
      "exploration/env_infos/final/reward_forward Mean         0.191891\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.191891\n",
      "exploration/env_infos/final/reward_forward Min          0.191891\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0278083\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0278083\n",
      "exploration/env_infos/initial/reward_forward Min        0.0278083\n",
      "exploration/env_infos/reward_forward Mean              -0.0127838\n",
      "exploration/env_infos/reward_forward Std                0.128722\n",
      "exploration/env_infos/reward_forward Max                1.18656\n",
      "exploration/env_infos/reward_forward Min               -0.556228\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.154909\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.154909\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.154909\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.409119\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.409119\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.409119\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.127595\n",
      "exploration/env_infos/reward_ctrl Std                   0.056054\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00883357\n",
      "exploration/env_infos/reward_ctrl Min                  -0.506696\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0204968\n",
      "exploration/env_infos/final/torso_velocity Std          0.221\n",
      "exploration/env_infos/final/torso_velocity Max          0.191891\n",
      "exploration/env_infos/final/torso_velocity Min         -0.325261\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.290208\n",
      "exploration/env_infos/initial/torso_velocity Std        0.239557\n",
      "exploration/env_infos/initial/torso_velocity Max        0.606991\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0278083\n",
      "exploration/env_infos/torso_velocity Mean              -0.00805898\n",
      "exploration/env_infos/torso_velocity Std                0.161762\n",
      "exploration/env_infos/torso_velocity Max                1.18656\n",
      "exploration/env_infos/torso_velocity Min               -1.12925\n",
      "evaluation/num steps total                              2.4e+06\n",
      "evaluation/num paths total                           2400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.922727\n",
      "evaluation/Rewards Std                                  0.0473396\n",
      "evaluation/Rewards Max                                  2.72742\n",
      "evaluation/Rewards Min                                  0.46228\n",
      "evaluation/Returns Mean                               922.727\n",
      "evaluation/Returns Std                                 27.9458\n",
      "evaluation/Returns Max                                976.981\n",
      "evaluation/Returns Min                                864.965\n",
      "evaluation/Actions Mean                                 0.0681194\n",
      "evaluation/Actions Std                                  0.12285\n",
      "evaluation/Actions Max                                  0.570984\n",
      "evaluation/Actions Min                                 -0.555086\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            922.727\n",
      "evaluation/env_infos/final/reward_forward Mean          2.26667e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.98457e-05\n",
      "evaluation/env_infos/final/reward_forward Max           9.55066e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -2.81702e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0253574\n",
      "evaluation/env_infos/initial/reward_forward Std         0.145931\n",
      "evaluation/env_infos/initial/reward_forward Max         0.232892\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.350259\n",
      "evaluation/env_infos/reward_forward Mean                0.00343058\n",
      "evaluation/env_infos/reward_forward Std                 0.0607146\n",
      "evaluation/env_infos/reward_forward Max                 1.31082\n",
      "evaluation/env_infos/reward_forward Min                -1.19572\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0771458\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0277503\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0269714\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.135223\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.248679\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0758678\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0864527\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.34344\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0789292\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0311153\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0229444\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.563621\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.81447e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           2.20944e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0001429\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.81702e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138175\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.276335\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.724181\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.350259\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00152854\n",
      "evaluation/env_infos/torso_velocity Std                 0.0667817\n",
      "evaluation/env_infos/torso_velocity Max                 1.31082\n",
      "evaluation/env_infos/torso_velocity Min                -1.75329\n",
      "time/data storing (s)                                   0.0307684\n",
      "time/evaluation sampling (s)                           46.868\n",
      "time/exploration sampling (s)                           2.05704\n",
      "time/logging (s)                                        0.282642\n",
      "time/saving (s)                                         0.0265576\n",
      "time/training (s)                                       4.37296\n",
      "time/epoch (s)                                         53.638\n",
      "time/total (s)                                       5129.38\n",
      "Epoch                                                  95\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:21:15.417307 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 96 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 196000\n",
      "trainer/QF1 Loss                                        0.209811\n",
      "trainer/QF2 Loss                                        0.235021\n",
      "trainer/Policy Loss                                   -23.7482\n",
      "trainer/Q1 Predictions Mean                            30.7159\n",
      "trainer/Q1 Predictions Std                              2.1693\n",
      "trainer/Q1 Predictions Max                             33.1766\n",
      "trainer/Q1 Predictions Min                             18.7303\n",
      "trainer/Q2 Predictions Mean                            30.859\n",
      "trainer/Q2 Predictions Std                              2.19916\n",
      "trainer/Q2 Predictions Max                             33.2208\n",
      "trainer/Q2 Predictions Min                             18.9293\n",
      "trainer/Q Targets Mean                                 30.7203\n",
      "trainer/Q Targets Std                                   2.19134\n",
      "trainer/Q Targets Max                                  32.7637\n",
      "trainer/Q Targets Min                                  18.652\n",
      "trainer/Log Pis Mean                                    7.14422\n",
      "trainer/Log Pis Std                                     2.0486\n",
      "trainer/Log Pis Max                                    14.6098\n",
      "trainer/Log Pis Min                                     0.165176\n",
      "trainer/Policy mu Mean                                 -4.17226e-05\n",
      "trainer/Policy mu Std                                   0.154835\n",
      "trainer/Policy mu Max                                   2.20484\n",
      "trainer/Policy mu Min                                  -1.86618\n",
      "trainer/Policy log std Mean                            -2.27684\n",
      "trainer/Policy log std Std                              0.175565\n",
      "trainer/Policy log std Max                             -1.25498\n",
      "trainer/Policy log std Min                             -3.04047\n",
      "trainer/Alpha                                           0.00819382\n",
      "trainer/Alpha Loss                                     -4.10913\n",
      "exploration/num steps total                         98000\n",
      "exploration/num paths total                           170\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.935039\n",
      "exploration/Rewards Std                                 0.0826665\n",
      "exploration/Rewards Max                                 1.53977\n",
      "exploration/Rewards Min                                 0.717938\n",
      "exploration/Returns Mean                              935.039\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               935.039\n",
      "exploration/Returns Min                               935.039\n",
      "exploration/Actions Mean                               -0.0103201\n",
      "exploration/Actions Std                                 0.140959\n",
      "exploration/Actions Max                                 0.497932\n",
      "exploration/Actions Min                                -0.536059\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           935.039\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0339565\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0339565\n",
      "exploration/env_infos/final/reward_forward Min         -0.0339565\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0655123\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0655123\n",
      "exploration/env_infos/initial/reward_forward Min        0.0655123\n",
      "exploration/env_infos/reward_forward Mean               0.0369827\n",
      "exploration/env_infos/reward_forward Std                0.362331\n",
      "exploration/env_infos/reward_forward Max                1.41852\n",
      "exploration/env_infos/reward_forward Min               -1.1885\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.104908\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.104908\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.104908\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.25888\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.25888\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.25888\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0799041\n",
      "exploration/env_infos/reward_ctrl Std                   0.0432906\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00666365\n",
      "exploration/env_infos/reward_ctrl Min                  -0.282062\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00760753\n",
      "exploration/env_infos/final/torso_velocity Std          0.0432747\n",
      "exploration/env_infos/final/torso_velocity Max          0.0672916\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0339565\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.199931\n",
      "exploration/env_infos/initial/torso_velocity Std        0.13127\n",
      "exploration/env_infos/initial/torso_velocity Max        0.378029\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0655123\n",
      "exploration/env_infos/torso_velocity Mean               0.0151559\n",
      "exploration/env_infos/torso_velocity Std                0.303151\n",
      "exploration/env_infos/torso_velocity Max                1.41852\n",
      "exploration/env_infos/torso_velocity Min               -1.32265\n",
      "evaluation/num steps total                              2.425e+06\n",
      "evaluation/num paths total                           2425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.965598\n",
      "evaluation/Rewards Std                                  0.034467\n",
      "evaluation/Rewards Max                                  2.24223\n",
      "evaluation/Rewards Min                                  0.442606\n",
      "evaluation/Returns Mean                               965.598\n",
      "evaluation/Returns Std                                 19.0537\n",
      "evaluation/Returns Max                                989.444\n",
      "evaluation/Returns Min                                922.142\n",
      "evaluation/Actions Mean                                -0.0213297\n",
      "evaluation/Actions Std                                  0.0917247\n",
      "evaluation/Actions Max                                  0.692263\n",
      "evaluation/Actions Min                                 -0.589564\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            965.598\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.75148e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.04078e-06\n",
      "evaluation/env_infos/final/reward_forward Max           3.30618e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.04083e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0416909\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0987853\n",
      "evaluation/env_infos/initial/reward_forward Max         0.167312\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.164453\n",
      "evaluation/env_infos/reward_forward Mean                0.000981144\n",
      "evaluation/env_infos/reward_forward Std                 0.0476503\n",
      "evaluation/env_infos/reward_forward Max                 1.2979\n",
      "evaluation/env_infos/reward_forward Min                -0.676862\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0347599\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0192836\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00981392\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0781361\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.137112\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0587664\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0166501\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.225374\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0354735\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0213966\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0093469\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.557394\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.16938e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.44647e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           3.99472e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.04083e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130644\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.259157\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.694505\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.277202\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00305485\n",
      "evaluation/env_infos/torso_velocity Std                 0.0683168\n",
      "evaluation/env_infos/torso_velocity Max                 1.2979\n",
      "evaluation/env_infos/torso_velocity Min                -2.12967\n",
      "time/data storing (s)                                   0.0299017\n",
      "time/evaluation sampling (s)                           49.0924\n",
      "time/exploration sampling (s)                           1.90222\n",
      "time/logging (s)                                        0.291029\n",
      "time/saving (s)                                         0.0267491\n",
      "time/training (s)                                       4.42783\n",
      "time/epoch (s)                                         55.7702\n",
      "time/total (s)                                       5186.06\n",
      "Epoch                                                  96\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:22:15.180175 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 97 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 198000\n",
      "trainer/QF1 Loss                                        0.24339\n",
      "trainer/QF2 Loss                                        0.252041\n",
      "trainer/Policy Loss                                   -23.1974\n",
      "trainer/Q1 Predictions Mean                            30.5922\n",
      "trainer/Q1 Predictions Std                              2.35814\n",
      "trainer/Q1 Predictions Max                             32.4883\n",
      "trainer/Q1 Predictions Min                             18.2809\n",
      "trainer/Q2 Predictions Mean                            30.684\n",
      "trainer/Q2 Predictions Std                              2.51527\n",
      "trainer/Q2 Predictions Max                             32.9183\n",
      "trainer/Q2 Predictions Min                             18.3351\n",
      "trainer/Q Targets Mean                                 30.9129\n",
      "trainer/Q Targets Std                                   2.41746\n",
      "trainer/Q Targets Max                                  33.1501\n",
      "trainer/Q Targets Min                                  18.3164\n",
      "trainer/Log Pis Mean                                    7.52622\n",
      "trainer/Log Pis Std                                     2.28241\n",
      "trainer/Log Pis Max                                    15.3923\n",
      "trainer/Log Pis Min                                     0.585272\n",
      "trainer/Policy mu Mean                                  0.00951225\n",
      "trainer/Policy mu Std                                   0.142994\n",
      "trainer/Policy mu Max                                   1.18174\n",
      "trainer/Policy mu Min                                  -1.11577\n",
      "trainer/Policy log std Mean                            -2.33498\n",
      "trainer/Policy log std Std                              0.187371\n",
      "trainer/Policy log std Max                             -1.71494\n",
      "trainer/Policy log std Min                             -3.5578\n",
      "trainer/Alpha                                           0.00642131\n",
      "trainer/Alpha Loss                                     -2.39068\n",
      "exploration/num steps total                         99000\n",
      "exploration/num paths total                           171\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.953152\n",
      "exploration/Rewards Std                                 0.0677808\n",
      "exploration/Rewards Max                                 1.60983\n",
      "exploration/Rewards Min                                 0.659612\n",
      "exploration/Returns Mean                              953.152\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               953.152\n",
      "exploration/Returns Min                               953.152\n",
      "exploration/Actions Mean                                0.0267919\n",
      "exploration/Actions Std                                 0.117989\n",
      "exploration/Actions Max                                 0.565245\n",
      "exploration/Actions Min                                -0.723212\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           953.152\n",
      "exploration/env_infos/final/reward_forward Mean         0.0362523\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0362523\n",
      "exploration/env_infos/final/reward_forward Min          0.0362523\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00683206\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00683206\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00683206\n",
      "exploration/env_infos/reward_forward Mean               0.00616368\n",
      "exploration/env_infos/reward_forward Std                0.212745\n",
      "exploration/env_infos/reward_forward Max                0.984895\n",
      "exploration/env_infos/reward_forward Min               -0.864776\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0383606\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0383606\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0383606\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.268887\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.268887\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.268887\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.058557\n",
      "exploration/env_infos/reward_ctrl Std                   0.0349258\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00839789\n",
      "exploration/env_infos/reward_ctrl Min                  -0.669679\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0965138\n",
      "exploration/env_infos/final/torso_velocity Std          0.0787301\n",
      "exploration/env_infos/final/torso_velocity Max          0.207725\n",
      "exploration/env_infos/final/torso_velocity Min          0.0362523\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0790099\n",
      "exploration/env_infos/initial/torso_velocity Std        0.250799\n",
      "exploration/env_infos/initial/torso_velocity Max        0.419964\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.176102\n",
      "exploration/env_infos/torso_velocity Mean               0.00739854\n",
      "exploration/env_infos/torso_velocity Std                0.208735\n",
      "exploration/env_infos/torso_velocity Max                0.984895\n",
      "exploration/env_infos/torso_velocity Min               -0.950749\n",
      "evaluation/num steps total                              2.45e+06\n",
      "evaluation/num paths total                           2450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.979449\n",
      "evaluation/Rewards Std                                  0.053991\n",
      "evaluation/Rewards Max                                  2.45958\n",
      "evaluation/Rewards Min                                  0.0241646\n",
      "evaluation/Returns Mean                               979.449\n",
      "evaluation/Returns Std                                 10.7673\n",
      "evaluation/Returns Max                                996.664\n",
      "evaluation/Returns Min                                960.243\n",
      "evaluation/Actions Mean                                -0.01779\n",
      "evaluation/Actions Std                                  0.0794103\n",
      "evaluation/Actions Max                                  0.649504\n",
      "evaluation/Actions Min                                 -0.841996\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            979.449\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00686875\n",
      "evaluation/env_infos/final/reward_forward Std           0.0238337\n",
      "evaluation/env_infos/final/reward_forward Max           0.000898454\n",
      "evaluation/env_infos/final/reward_forward Min          -0.101611\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0185595\n",
      "evaluation/env_infos/initial/reward_forward Std         0.147485\n",
      "evaluation/env_infos/initial/reward_forward Max         0.300787\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.265217\n",
      "evaluation/env_infos/reward_forward Mean                0.00593676\n",
      "evaluation/env_infos/reward_forward Std                 0.0905492\n",
      "evaluation/env_infos/reward_forward Max                 1.19836\n",
      "evaluation/env_infos/reward_forward Min                -1.51204\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0239706\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0109774\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00991676\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0408758\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.273582\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0596604\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.17468\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.378484\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0264899\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0273967\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00327635\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.975835\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00104174\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0224468\n",
      "evaluation/env_infos/final/torso_velocity Max           0.105884\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.101611\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138311\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238492\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.570561\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.265217\n",
      "evaluation/env_infos/torso_velocity Mean                0.00231712\n",
      "evaluation/env_infos/torso_velocity Std                 0.0872879\n",
      "evaluation/env_infos/torso_velocity Max                 1.2484\n",
      "evaluation/env_infos/torso_velocity Min                -1.86131\n",
      "time/data storing (s)                                   0.03096\n",
      "time/evaluation sampling (s)                           51.694\n",
      "time/exploration sampling (s)                           2.11884\n",
      "time/logging (s)                                        0.268473\n",
      "time/saving (s)                                         0.0248219\n",
      "time/training (s)                                       4.58121\n",
      "time/epoch (s)                                         58.7183\n",
      "time/total (s)                                       5245.8\n",
      "Epoch                                                  97\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:23:09.441821 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 98 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 200000\n",
      "trainer/QF1 Loss                                        0.191689\n",
      "trainer/QF2 Loss                                        0.338027\n",
      "trainer/Policy Loss                                   -23.3373\n",
      "trainer/Q1 Predictions Mean                            31.2207\n",
      "trainer/Q1 Predictions Std                              2.31316\n",
      "trainer/Q1 Predictions Max                             33.443\n",
      "trainer/Q1 Predictions Min                             19.5494\n",
      "trainer/Q2 Predictions Mean                            31.3133\n",
      "trainer/Q2 Predictions Std                              2.17286\n",
      "trainer/Q2 Predictions Max                             34.4004\n",
      "trainer/Q2 Predictions Min                             19.7321\n",
      "trainer/Q Targets Mean                                 31.1735\n",
      "trainer/Q Targets Std                                   2.21823\n",
      "trainer/Q Targets Max                                  32.9731\n",
      "trainer/Q Targets Min                                  18.649\n",
      "trainer/Log Pis Mean                                    8.05079\n",
      "trainer/Log Pis Std                                     2.09227\n",
      "trainer/Log Pis Max                                    13.0045\n",
      "trainer/Log Pis Min                                     1.2903\n",
      "trainer/Policy mu Mean                                 -0.0153156\n",
      "trainer/Policy mu Std                                   0.217311\n",
      "trainer/Policy mu Max                                   1.29082\n",
      "trainer/Policy mu Min                                  -0.805385\n",
      "trainer/Policy log std Mean                            -2.39302\n",
      "trainer/Policy log std Std                              0.175654\n",
      "trainer/Policy log std Max                             -1.92618\n",
      "trainer/Policy log std Min                             -3.40052\n",
      "trainer/Alpha                                           0.00783627\n",
      "trainer/Alpha Loss                                      0.246351\n",
      "exploration/num steps total                        100000\n",
      "exploration/num paths total                           172\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.832877\n",
      "exploration/Rewards Std                                 0.0822703\n",
      "exploration/Rewards Max                                 2.01482\n",
      "exploration/Rewards Min                                 0.554158\n",
      "exploration/Returns Mean                              832.877\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               832.877\n",
      "exploration/Returns Min                               832.877\n",
      "exploration/Actions Mean                               -0.0600072\n",
      "exploration/Actions Std                                 0.199444\n",
      "exploration/Actions Max                                 0.580065\n",
      "exploration/Actions Min                                -0.662108\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           832.877\n",
      "exploration/env_infos/final/reward_forward Mean         0.0446813\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0446813\n",
      "exploration/env_infos/final/reward_forward Min          0.0446813\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0230845\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0230845\n",
      "exploration/env_infos/initial/reward_forward Min        0.0230845\n",
      "exploration/env_infos/reward_forward Mean               0.0116177\n",
      "exploration/env_infos/reward_forward Std                0.121898\n",
      "exploration/env_infos/reward_forward Max                0.414223\n",
      "exploration/env_infos/reward_forward Min               -1.11899\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.115132\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.115132\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.115132\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.309941\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.309941\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.309941\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.173515\n",
      "exploration/env_infos/reward_ctrl Std                   0.0577874\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0453913\n",
      "exploration/env_infos/reward_ctrl Min                  -0.445842\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0810477\n",
      "exploration/env_infos/final/torso_velocity Std          0.0796093\n",
      "exploration/env_infos/final/torso_velocity Max          0.191505\n",
      "exploration/env_infos/final/torso_velocity Min          0.00695643\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.13151\n",
      "exploration/env_infos/initial/torso_velocity Std        0.357755\n",
      "exploration/env_infos/initial/torso_velocity Max        0.613702\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.242255\n",
      "exploration/env_infos/torso_velocity Mean              -0.000866279\n",
      "exploration/env_infos/torso_velocity Std                0.138511\n",
      "exploration/env_infos/torso_velocity Max                0.739789\n",
      "exploration/env_infos/torso_velocity Min               -1.44601\n",
      "evaluation/num steps total                              2.475e+06\n",
      "evaluation/num paths total                           2475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.826695\n",
      "evaluation/Rewards Std                                  0.0503292\n",
      "evaluation/Rewards Max                                  2.01864\n",
      "evaluation/Rewards Min                                 -0.107205\n",
      "evaluation/Returns Mean                               826.695\n",
      "evaluation/Returns Std                                 42.4606\n",
      "evaluation/Returns Max                                888.086\n",
      "evaluation/Returns Min                                754.696\n",
      "evaluation/Actions Mean                                -0.0537121\n",
      "evaluation/Actions Std                                  0.201337\n",
      "evaluation/Actions Max                                  0.76739\n",
      "evaluation/Actions Min                                 -0.792847\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            826.695\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.56603e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.197e-07\n",
      "evaluation/env_infos/final/reward_forward Max           2.74576e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.8637e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00422919\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118976\n",
      "evaluation/env_infos/initial/reward_forward Max         0.252864\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.221962\n",
      "evaluation/env_infos/reward_forward Mean                0.00144422\n",
      "evaluation/env_infos/reward_forward Std                 0.0493334\n",
      "evaluation/env_infos/reward_forward Max                 1.52267\n",
      "evaluation/env_infos/reward_forward Min                -1.60222\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.171464\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0415035\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.111044\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.238282\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.274968\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0652241\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0983726\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.376381\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.173687\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0491385\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0532422\n",
      "evaluation/env_infos/reward_ctrl Min                   -1.1072\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.56107e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.36993e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           3.73515e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.8637e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.103929\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.260395\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.635569\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.298704\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00340322\n",
      "evaluation/env_infos/torso_velocity Std                 0.0638009\n",
      "evaluation/env_infos/torso_velocity Max                 1.52267\n",
      "evaluation/env_infos/torso_velocity Min                -1.83464\n",
      "time/data storing (s)                                   0.0304493\n",
      "time/evaluation sampling (s)                           46.8246\n",
      "time/exploration sampling (s)                           1.99839\n",
      "time/logging (s)                                        0.299161\n",
      "time/saving (s)                                         0.0290214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/training (s)                                       4.2983\n",
      "time/epoch (s)                                         53.4799\n",
      "time/total (s)                                       5300.09\n",
      "Epoch                                                  98\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:24:01.676418 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_11_54_52_0000--s-10] Epoch 99 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 202000\n",
      "trainer/QF1 Loss                                        0.257093\n",
      "trainer/QF2 Loss                                        0.749104\n",
      "trainer/Policy Loss                                   -23.4516\n",
      "trainer/Q1 Predictions Mean                            31.1777\n",
      "trainer/Q1 Predictions Std                              3.11543\n",
      "trainer/Q1 Predictions Max                             33.7172\n",
      "trainer/Q1 Predictions Min                              0.887906\n",
      "trainer/Q2 Predictions Mean                            31.1727\n",
      "trainer/Q2 Predictions Std                              2.90445\n",
      "trainer/Q2 Predictions Max                             33.9241\n",
      "trainer/Q2 Predictions Min                             10.2995\n",
      "trainer/Q Targets Mean                                 31.2738\n",
      "trainer/Q Targets Std                                   3.21145\n",
      "trainer/Q Targets Max                                  33.6297\n",
      "trainer/Q Targets Min                                  -1.05216\n",
      "trainer/Log Pis Mean                                    7.87004\n",
      "trainer/Log Pis Std                                     2.33775\n",
      "trainer/Log Pis Max                                    16.9244\n",
      "trainer/Log Pis Min                                     0.0229264\n",
      "trainer/Policy mu Mean                                  0.022075\n",
      "trainer/Policy mu Std                                   0.168354\n",
      "trainer/Policy mu Max                                   1.634\n",
      "trainer/Policy mu Min                                  -1.56356\n",
      "trainer/Policy log std Mean                            -2.34969\n",
      "trainer/Policy log std Std                              0.214388\n",
      "trainer/Policy log std Max                             -1.52651\n",
      "trainer/Policy log std Min                             -3.56499\n",
      "trainer/Alpha                                           0.00754366\n",
      "trainer/Alpha Loss                                     -0.634878\n",
      "exploration/num steps total                        101000\n",
      "exploration/num paths total                           173\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.946797\n",
      "exploration/Rewards Std                                 0.0918418\n",
      "exploration/Rewards Max                                 1.72036\n",
      "exploration/Rewards Min                                 0.661494\n",
      "exploration/Returns Mean                              946.797\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               946.797\n",
      "exploration/Returns Min                               946.797\n",
      "exploration/Actions Mean                               -0.00156435\n",
      "exploration/Actions Std                                 0.134549\n",
      "exploration/Actions Max                                 0.531576\n",
      "exploration/Actions Min                                -0.622274\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           946.797\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0558211\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0558211\n",
      "exploration/env_infos/final/reward_forward Min         -0.0558211\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00967086\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.00967086\n",
      "exploration/env_infos/initial/reward_forward Min        0.00967086\n",
      "exploration/env_infos/reward_forward Mean               0.0394631\n",
      "exploration/env_infos/reward_forward Std                0.201707\n",
      "exploration/env_infos/reward_forward Max                0.742581\n",
      "exploration/env_infos/reward_forward Min               -0.600857\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0979984\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0979984\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0979984\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.338506\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.338506\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.338506\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0724234\n",
      "exploration/env_infos/reward_ctrl Std                   0.0385539\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00779849\n",
      "exploration/env_infos/reward_ctrl Min                  -0.338506\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.165329\n",
      "exploration/env_infos/final/torso_velocity Std          0.107801\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0558211\n",
      "exploration/env_infos/final/torso_velocity Min         -0.311939\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.00649108\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0946732\n",
      "exploration/env_infos/initial/torso_velocity Max        0.120819\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.111017\n",
      "exploration/env_infos/torso_velocity Mean               0.0149929\n",
      "exploration/env_infos/torso_velocity Std                0.27845\n",
      "exploration/env_infos/torso_velocity Max                1.05345\n",
      "exploration/env_infos/torso_velocity Min               -1.504\n",
      "evaluation/num steps total                              2.5e+06\n",
      "evaluation/num paths total                           2500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.956725\n",
      "evaluation/Rewards Std                                  0.052888\n",
      "evaluation/Rewards Max                                  2.49828\n",
      "evaluation/Rewards Min                                  0.469315\n",
      "evaluation/Returns Mean                               956.725\n",
      "evaluation/Returns Std                                 44.3664\n",
      "evaluation/Returns Max                                986.726\n",
      "evaluation/Returns Min                                831.058\n",
      "evaluation/Actions Mean                                 0.0119235\n",
      "evaluation/Actions Std                                  0.104296\n",
      "evaluation/Actions Max                                  0.623498\n",
      "evaluation/Actions Min                                 -0.667821\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            956.725\n",
      "evaluation/env_infos/final/reward_forward Mean          2.96887e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.63405e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.43063e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.10538e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0257868\n",
      "evaluation/env_infos/initial/reward_forward Std         0.133999\n",
      "evaluation/env_infos/initial/reward_forward Max         0.172523\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.370778\n",
      "evaluation/env_infos/reward_forward Mean               -5.73709e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0456329\n",
      "evaluation/env_infos/reward_forward Max                 1.37931\n",
      "evaluation/env_infos/reward_forward Min                -1.01152\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0429033\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0448407\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0147104\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.168719\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.311546\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0584748\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.181175\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.439256\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0440789\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0467104\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00646603\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.530685\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.79003e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.92321e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.43063e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.10538e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126418\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.263107\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.673037\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.370778\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00343545\n",
      "evaluation/env_infos/torso_velocity Std                 0.0622724\n",
      "evaluation/env_infos/torso_velocity Max                 1.37931\n",
      "evaluation/env_infos/torso_velocity Min                -1.97834\n",
      "time/data storing (s)                                   0.0307661\n",
      "time/evaluation sampling (s)                           44.8775\n",
      "time/exploration sampling (s)                           1.92262\n",
      "time/logging (s)                                        0.270401\n",
      "time/saving (s)                                         0.0285409\n",
      "time/training (s)                                       4.22244\n",
      "time/epoch (s)                                         51.3523\n",
      "time/total (s)                                       5352.29\n",
      "Epoch                                                  99\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodad not detected\n",
      "objc[21496]: Class GLFWApplicationDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1e7cb778) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1e84c740). One of the two will be used. Which one is undefined.\n",
      "objc[21496]: Class GLFWWindowDelegate is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1e7cb700) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1e84c768). One of the two will be used. Which one is undefined.\n",
      "objc[21496]: Class GLFWContentView is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1e7cb7a0) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1e84c7b8). One of the two will be used. Which one is undefined.\n",
      "objc[21496]: Class GLFWWindow is implemented in both /Users/shanliyu/.mujoco/mujoco200/bin/libglfw.3.dylib (0x1a1e7cb818) and /Users/shanliyu/anaconda3/lib/python3.6/site-packages/glfw/libglfw.3.dylib (0x1a1e84c830). One of the two will be used. Which one is undefined.\n",
      "2021-05-25 13:24:09.247843 PDT | Variant:\n",
      "2021-05-25 13:24:09.248612 PDT | {\n",
      "  \"seed\": 10,\n",
      "  \"algorithm\": \"SAC\",\n",
      "  \"env_name\": \"antdirectionnewsparse\",\n",
      "  \"algo_kwargs\": {\n",
      "    \"batch_size\": 256,\n",
      "    \"num_epochs\": 100,\n",
      "    \"num_eval_steps_per_epoch\": 25000,\n",
      "    \"num_expl_steps_per_train_loop\": 1000,\n",
      "    \"num_trains_per_train_loop\": 100,\n",
      "    \"min_num_steps_before_training\": 1000,\n",
      "    \"max_path_length\": 1000,\n",
      "    \"num_train_loops_per_epoch\": 1\n",
      "  },\n",
      "  \"trainer_kwargs\": {\n",
      "    \"discount\": 0.99,\n",
      "    \"soft_target_tau\": 0.005,\n",
      "    \"target_update_period\": 1,\n",
      "    \"policy_lr\": 0.003,\n",
      "    \"qf_lr\": 0.003,\n",
      "    \"reward_scale\": 1,\n",
      "    \"use_automatic_entropy_tuning\": true\n",
      "  },\n",
      "  \"replay_buffer_kwargs\": {\n",
      "    \"max_replay_buffer_size\": 1000000,\n",
      "    \"latent_dim\": 1,\n",
      "    \"approx_irl\": false,\n",
      "    \"plot\": false\n",
      "  },\n",
      "  \"relabeler_kwargs\": {\n",
      "    \"relabel\": true,\n",
      "    \"use_adv\": true,\n",
      "    \"n_sampled_latents\": 100,\n",
      "    \"n_to_take\": 1,\n",
      "    \"cache\": false,\n",
      "    \"type\": \"360\"\n",
      "  },\n",
      "  \"qf_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"policy_kwargs\": {\n",
      "    \"hidden_sizes\": [\n",
      "      256,\n",
      "      256\n",
      "    ],\n",
      "    \"latent_shape_multiplier\": 1,\n",
      "    \"latent_to_all_layers\": false\n",
      "  },\n",
      "  \"path_collector_kwargs\": {\n",
      "    \"save_videos\": false\n",
      "  },\n",
      "  \"use_advantages\": true,\n",
      "  \"proper_advantages\": true,\n",
      "  \"plot\": false,\n",
      "  \"test\": false,\n",
      "  \"gpu\": 0,\n",
      "  \"mode\": \"here_no_doodad\",\n",
      "  \"local_docker\": false,\n",
      "  \"insert_time\": false,\n",
      "  \"latent_shape_multiplier\": 1,\n",
      "  \"env_kwargs\": {\n",
      "    \"use_xy\": false,\n",
      "    \"contact_forces\": false\n",
      "  }\n",
      "}\n",
      "antdirectionnewsparse\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2021-05-25 13:25:04.257155 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 0 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  4000\n",
      "trainer/QF1 Loss                                      22.7482\n",
      "trainer/QF2 Loss                                      22.7273\n",
      "trainer/Policy Loss                                   -5.32946\n",
      "trainer/Q1 Predictions Mean                            0.0019236\n",
      "trainer/Q1 Predictions Std                             0.00375833\n",
      "trainer/Q1 Predictions Max                             0.0119071\n",
      "trainer/Q1 Predictions Min                            -0.0102538\n",
      "trainer/Q2 Predictions Mean                            0.00409954\n",
      "trainer/Q2 Predictions Std                             0.00334705\n",
      "trainer/Q2 Predictions Max                             0.0141614\n",
      "trainer/Q2 Predictions Min                            -0.00413745\n",
      "trainer/Q Targets Mean                                 4.68065\n",
      "trainer/Q Targets Std                                  0.926052\n",
      "trainer/Q Targets Max                                  7.5882\n",
      "trainer/Q Targets Min                                 -1.40656\n",
      "trainer/Log Pis Mean                                  -5.32867\n",
      "trainer/Log Pis Std                                    0.609286\n",
      "trainer/Log Pis Max                                   -3.46773\n",
      "trainer/Log Pis Min                                   -7.64343\n",
      "trainer/Policy mu Mean                                -0.000240829\n",
      "trainer/Policy mu Std                                  0.00206236\n",
      "trainer/Policy mu Max                                  0.00593952\n",
      "trainer/Policy mu Min                                 -0.00733663\n",
      "trainer/Policy log std Mean                            1.03237e-05\n",
      "trainer/Policy log std Std                             0.00221965\n",
      "trainer/Policy log std Max                             0.00797385\n",
      "trainer/Policy log std Min                            -0.0072714\n",
      "trainer/Alpha                                          0.997005\n",
      "trainer/Alpha Loss                                    -0\n",
      "exploration/num steps total                         2000\n",
      "exploration/num paths total                           15\n",
      "exploration/path length Mean                         142.857\n",
      "exploration/path length Std                          207.429\n",
      "exploration/path length Max                          641\n",
      "exploration/path length Min                           16\n",
      "exploration/Rewards Mean                              -0.510054\n",
      "exploration/Rewards Std                                0.469113\n",
      "exploration/Rewards Max                                1.39148\n",
      "exploration/Rewards Min                               -1.86377\n",
      "exploration/Returns Mean                             -72.8648\n",
      "exploration/Returns Std                              103.793\n",
      "exploration/Returns Max                               -8.09713\n",
      "exploration/Returns Min                             -320.333\n",
      "exploration/Actions Mean                               0.00462201\n",
      "exploration/Actions Std                                0.622424\n",
      "exploration/Actions Max                                0.999663\n",
      "exploration/Actions Min                               -0.999062\n",
      "exploration/Num Paths                                  7\n",
      "exploration/Average Returns                          -72.8648\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0385522\n",
      "exploration/env_infos/final/reward_forward Std         0.977755\n",
      "exploration/env_infos/final/reward_forward Max         1.36971\n",
      "exploration/env_infos/final/reward_forward Min        -1.74981\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0580186\n",
      "exploration/env_infos/initial/reward_forward Std       0.229437\n",
      "exploration/env_infos/initial/reward_forward Max       0.399949\n",
      "exploration/env_infos/initial/reward_forward Min      -0.315119\n",
      "exploration/env_infos/reward_forward Mean              0.134292\n",
      "exploration/env_infos/reward_forward Std               0.59203\n",
      "exploration/env_infos/reward_forward Max               2.71644\n",
      "exploration/env_infos/reward_forward Min              -2.29091\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.52531\n",
      "exploration/env_infos/final/reward_ctrl Std            0.368686\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.904569\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.02138\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.58427\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.529483\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.778912\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.26015\n",
      "exploration/env_infos/reward_ctrl Mean                -1.54973\n",
      "exploration/env_infos/reward_ctrl Std                  0.438054\n",
      "exploration/env_infos/reward_ctrl Max                 -0.477419\n",
      "exploration/env_infos/reward_ctrl Min                 -2.89975\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.380911\n",
      "exploration/env_infos/final/torso_velocity Std         1.04755\n",
      "exploration/env_infos/final/torso_velocity Max         2.34465\n",
      "exploration/env_infos/final/torso_velocity Min        -1.74981\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.155478\n",
      "exploration/env_infos/initial/torso_velocity Std       0.286971\n",
      "exploration/env_infos/initial/torso_velocity Max       0.542151\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.45473\n",
      "exploration/env_infos/torso_velocity Mean              0.0828184\n",
      "exploration/env_infos/torso_velocity Std               0.588215\n",
      "exploration/env_infos/torso_velocity Max               3.6035\n",
      "exploration/env_infos/torso_velocity Min              -3.36356\n",
      "evaluation/num steps total                         25000\n",
      "evaluation/num paths total                            25\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                1.00067\n",
      "evaluation/Rewards Std                                 0.012298\n",
      "evaluation/Rewards Max                                 2.29812\n",
      "evaluation/Rewards Min                                 0.999985\n",
      "evaluation/Returns Mean                             1000.67\n",
      "evaluation/Returns Std                                 0.772689\n",
      "evaluation/Returns Max                              1002.79\n",
      "evaluation/Returns Min                               999.994\n",
      "evaluation/Actions Mean                               -0.000143181\n",
      "evaluation/Actions Std                                 0.00116459\n",
      "evaluation/Actions Max                                 0.00354216\n",
      "evaluation/Actions Min                                -0.00361978\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                          1000.67\n",
      "evaluation/env_infos/final/reward_forward Mean         0.000317323\n",
      "evaluation/env_infos/final/reward_forward Std          0.00035511\n",
      "evaluation/env_infos/final/reward_forward Max          0.00125821\n",
      "evaluation/env_infos/final/reward_forward Min         -1.79198e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0210831\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126274\n",
      "evaluation/env_infos/initial/reward_forward Max        0.17449\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.289082\n",
      "evaluation/env_infos/reward_forward Mean               0.00181115\n",
      "evaluation/env_infos/reward_forward Std                0.0456941\n",
      "evaluation/env_infos/reward_forward Max                1.33381\n",
      "evaluation/env_infos/reward_forward Min               -1.19598\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -5.47657e-06\n",
      "evaluation/env_infos/final/reward_ctrl Std             5.7904e-07\n",
      "evaluation/env_infos/final/reward_ctrl Max            -4.56639e-06\n",
      "evaluation/env_infos/final/reward_ctrl Min            -6.75503e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -6.32111e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Std           5.37251e-07\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -5.29296e-06\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -7.14374e-06\n",
      "evaluation/env_infos/reward_ctrl Mean                 -5.5071e-06\n",
      "evaluation/env_infos/reward_ctrl Std                   7.11531e-07\n",
      "evaluation/env_infos/reward_ctrl Max                  -4.24987e-06\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.4685e-05\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.000170902\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000385032\n",
      "evaluation/env_infos/final/torso_velocity Max          0.00190907\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000141515\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.126406\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.22264\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.666486\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.289082\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0007811\n",
      "evaluation/env_infos/torso_velocity Std                0.0532046\n",
      "evaluation/env_infos/torso_velocity Max                1.33381\n",
      "evaluation/env_infos/torso_velocity Min               -1.89354\n",
      "time/data storing (s)                                  0.489717\n",
      "time/evaluation sampling (s)                          46.1387\n",
      "time/exploration sampling (s)                          1.9851\n",
      "time/logging (s)                                       0.306885\n",
      "time/saving (s)                                        0.0905976\n",
      "time/training (s)                                      3.8598\n",
      "time/epoch (s)                                        52.8708\n",
      "time/total (s)                                        58.9404\n",
      "Epoch                                                  0\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:25:58.903240 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 1 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  6000\n",
      "trainer/QF1 Loss                                       0.751119\n",
      "trainer/QF2 Loss                                       0.759062\n",
      "trainer/Policy Loss                                   -9.49165\n",
      "trainer/Q1 Predictions Mean                            4.04208\n",
      "trainer/Q1 Predictions Std                             0.381656\n",
      "trainer/Q1 Predictions Max                             5.16306\n",
      "trainer/Q1 Predictions Min                             2.36018\n",
      "trainer/Q2 Predictions Mean                            4.04435\n",
      "trainer/Q2 Predictions Std                             0.387959\n",
      "trainer/Q2 Predictions Max                             5.24569\n",
      "trainer/Q2 Predictions Min                             2.37601\n",
      "trainer/Q Targets Mean                                 4.11364\n",
      "trainer/Q Targets Std                                  0.822543\n",
      "trainer/Q Targets Max                                  6.76762\n",
      "trainer/Q Targets Min                                 -1.02138\n",
      "trainer/Log Pis Mean                                  -5.47144\n",
      "trainer/Log Pis Std                                    0.435839\n",
      "trainer/Log Pis Max                                   -4.6188\n",
      "trainer/Log Pis Min                                   -9.39445\n",
      "trainer/Policy mu Mean                                -0.000351974\n",
      "trainer/Policy mu Std                                  0.024486\n",
      "trainer/Policy mu Max                                  0.0758825\n",
      "trainer/Policy mu Min                                 -0.069534\n",
      "trainer/Policy log std Mean                           -0.109386\n",
      "trainer/Policy log std Std                             0.021066\n",
      "trainer/Policy log std Max                            -0.0538591\n",
      "trainer/Policy log std Min                            -0.192239\n",
      "trainer/Alpha                                          0.738529\n",
      "trainer/Alpha Loss                                    -4.04268\n",
      "exploration/num steps total                         3000\n",
      "exploration/num paths total                           29\n",
      "exploration/path length Mean                          71.4286\n",
      "exploration/path length Std                           48.1986\n",
      "exploration/path length Max                          219\n",
      "exploration/path length Min                           23\n",
      "exploration/Rewards Mean                              -0.334919\n",
      "exploration/Rewards Std                                0.536849\n",
      "exploration/Rewards Max                                2.27315\n",
      "exploration/Rewards Min                               -1.78804\n",
      "exploration/Returns Mean                             -23.9228\n",
      "exploration/Returns Std                               12.7756\n",
      "exploration/Returns Max                               -8.4262\n",
      "exploration/Returns Min                              -56.6008\n",
      "exploration/Actions Mean                              -0.00782511\n",
      "exploration/Actions Std                                0.599035\n",
      "exploration/Actions Max                                0.998483\n",
      "exploration/Actions Min                               -0.998896\n",
      "exploration/Num Paths                                 14\n",
      "exploration/Average Returns                          -23.9228\n",
      "exploration/env_infos/final/reward_forward Mean        0.140629\n",
      "exploration/env_infos/final/reward_forward Std         0.855715\n",
      "exploration/env_infos/final/reward_forward Max         1.77925\n",
      "exploration/env_infos/final/reward_forward Min        -1.28896\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0701936\n",
      "exploration/env_infos/initial/reward_forward Std       0.180257\n",
      "exploration/env_infos/initial/reward_forward Max       0.335551\n",
      "exploration/env_infos/initial/reward_forward Min      -0.229443\n",
      "exploration/env_infos/reward_forward Mean              0.0199627\n",
      "exploration/env_infos/reward_forward Std               0.808287\n",
      "exploration/env_infos/reward_forward Max               2.68553\n",
      "exploration/env_infos/reward_forward Min              -2.95289\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.45409\n",
      "exploration/env_infos/final/reward_ctrl Std            0.341202\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.91542\n",
      "exploration/env_infos/final/reward_ctrl Min           -2.08791\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.38653\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.397978\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.522889\n",
      "exploration/env_infos/initial/reward_ctrl Min         -2.04767\n",
      "exploration/env_infos/reward_ctrl Mean                -1.43562\n",
      "exploration/env_infos/reward_ctrl Std                  0.411664\n",
      "exploration/env_infos/reward_ctrl Max                 -0.393384\n",
      "exploration/env_infos/reward_ctrl Min                 -2.78804\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.376024\n",
      "exploration/env_infos/final/torso_velocity Std         1.18673\n",
      "exploration/env_infos/final/torso_velocity Max         2.56863\n",
      "exploration/env_infos/final/torso_velocity Min        -2.3864\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.163309\n",
      "exploration/env_infos/initial/torso_velocity Std       0.252097\n",
      "exploration/env_infos/initial/torso_velocity Max       0.549671\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.279595\n",
      "exploration/env_infos/torso_velocity Mean              0.0490452\n",
      "exploration/env_infos/torso_velocity Std               0.847667\n",
      "exploration/env_infos/torso_velocity Max               3.68803\n",
      "exploration/env_infos/torso_velocity Min              -2.95289\n",
      "evaluation/num steps total                         50000\n",
      "evaluation/num paths total                            50\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.999478\n",
      "evaluation/Rewards Std                                 0.023998\n",
      "evaluation/Rewards Max                                 2.25986\n",
      "evaluation/Rewards Min                                 0.997517\n",
      "evaluation/Returns Mean                              999.478\n",
      "evaluation/Returns Std                                 1.34578\n",
      "evaluation/Returns Max                              1003.91\n",
      "evaluation/Returns Min                               998.163\n",
      "evaluation/Actions Mean                               -0.00251796\n",
      "evaluation/Actions Std                                 0.0191565\n",
      "evaluation/Actions Max                                 0.0389735\n",
      "evaluation/Actions Min                                -0.0517382\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           999.478\n",
      "evaluation/env_infos/final/reward_forward Mean        -6.99003e-05\n",
      "evaluation/env_infos/final/reward_forward Std          0.000269564\n",
      "evaluation/env_infos/final/reward_forward Max          0.000613423\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000489845\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00485402\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126227\n",
      "evaluation/env_infos/initial/reward_forward Max        0.211573\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.271993\n",
      "evaluation/env_infos/reward_forward Mean               0.00469314\n",
      "evaluation/env_infos/reward_forward Std                0.0599226\n",
      "evaluation/env_infos/reward_forward Max                1.563\n",
      "evaluation/env_infos/reward_forward Min               -1.22773\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0014879\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000236727\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00107758\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00186136\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00116316\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000273978\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.000805995\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00163924\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00149324\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000240918\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.000606166\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00248317\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -7.77536e-05\n",
      "evaluation/env_infos/final/torso_velocity Std          0.000210463\n",
      "evaluation/env_infos/final/torso_velocity Max          0.000613423\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.000897805\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.144758\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.234735\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.703255\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.271993\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00104364\n",
      "evaluation/env_infos/torso_velocity Std                0.0624997\n",
      "evaluation/env_infos/torso_velocity Max                1.563\n",
      "evaluation/env_infos/torso_velocity Min               -1.71689\n",
      "time/data storing (s)                                  0.571684\n",
      "time/evaluation sampling (s)                          48.1367\n",
      "time/exploration sampling (s)                          1.9129\n",
      "time/logging (s)                                       0.281894\n",
      "time/saving (s)                                        0.0272685\n",
      "time/training (s)                                      3.35926\n",
      "time/epoch (s)                                        54.2897\n",
      "time/total (s)                                       113.561\n",
      "Epoch                                                  1\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:26:51.302834 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 2 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  8000\n",
      "trainer/QF1 Loss                                       0.457568\n",
      "trainer/QF2 Loss                                       0.44929\n",
      "trainer/Policy Loss                                   -9.13384\n",
      "trainer/Q1 Predictions Mean                            3.69772\n",
      "trainer/Q1 Predictions Std                             0.314461\n",
      "trainer/Q1 Predictions Max                             4.46478\n",
      "trainer/Q1 Predictions Min                             2.68682\n",
      "trainer/Q2 Predictions Mean                            3.68993\n",
      "trainer/Q2 Predictions Std                             0.318783\n",
      "trainer/Q2 Predictions Max                             4.59058\n",
      "trainer/Q2 Predictions Min                             2.66067\n",
      "trainer/Q Targets Mean                                 3.89269\n",
      "trainer/Q Targets Std                                  0.659794\n",
      "trainer/Q Targets Max                                  7.05411\n",
      "trainer/Q Targets Min                                  1.45055\n",
      "trainer/Log Pis Mean                                  -5.47156\n",
      "trainer/Log Pis Std                                    0.340807\n",
      "trainer/Log Pis Max                                   -4.71296\n",
      "trainer/Log Pis Min                                   -8.01047\n",
      "trainer/Policy mu Mean                                -0.00826333\n",
      "trainer/Policy mu Std                                  0.0246421\n",
      "trainer/Policy mu Max                                  0.0604169\n",
      "trainer/Policy mu Min                                 -0.0740911\n",
      "trainer/Policy log std Mean                           -0.130711\n",
      "trainer/Policy log std Std                             0.0134658\n",
      "trainer/Policy log std Max                            -0.0931783\n",
      "trainer/Policy log std Min                            -0.193403\n",
      "trainer/Alpha                                          0.547059\n",
      "trainer/Alpha Loss                                    -8.08558\n",
      "exploration/num steps total                         4000\n",
      "exploration/num paths total                           41\n",
      "exploration/path length Mean                          83.3333\n",
      "exploration/path length Std                           62.3569\n",
      "exploration/path length Max                          215\n",
      "exploration/path length Min                           13\n",
      "exploration/Rewards Mean                              -0.351427\n",
      "exploration/Rewards Std                                0.442825\n",
      "exploration/Rewards Max                                1.45055\n",
      "exploration/Rewards Min                               -1.71604\n",
      "exploration/Returns Mean                             -29.2856\n",
      "exploration/Returns Std                               21.8592\n",
      "exploration/Returns Max                               -4.0456\n",
      "exploration/Returns Min                              -83.2396\n",
      "exploration/Actions Mean                              -0.00510798\n",
      "exploration/Actions Std                                0.590049\n",
      "exploration/Actions Max                                0.997883\n",
      "exploration/Actions Min                               -0.996073\n",
      "exploration/Num Paths                                 12\n",
      "exploration/Average Returns                          -29.2856\n",
      "exploration/env_infos/final/reward_forward Mean       -0.518175\n",
      "exploration/env_infos/final/reward_forward Std         0.865961\n",
      "exploration/env_infos/final/reward_forward Max         0.96807\n",
      "exploration/env_infos/final/reward_forward Min        -2.22082\n",
      "exploration/env_infos/initial/reward_forward Mean      0.085771\n",
      "exploration/env_infos/initial/reward_forward Std       0.189345\n",
      "exploration/env_infos/initial/reward_forward Max       0.333662\n",
      "exploration/env_infos/initial/reward_forward Min      -0.298633\n",
      "exploration/env_infos/reward_forward Mean             -0.16757\n",
      "exploration/env_infos/reward_forward Std               0.752939\n",
      "exploration/env_infos/reward_forward Max               2.41282\n",
      "exploration/env_infos/reward_forward Min              -2.70297\n",
      "exploration/env_infos/final/reward_ctrl Mean          -1.36457\n",
      "exploration/env_infos/final/reward_ctrl Std            0.366234\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.773194\n",
      "exploration/env_infos/final/reward_ctrl Min           -1.90991\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -1.32347\n",
      "exploration/env_infos/initial/reward_ctrl Std          0.417688\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.779076\n",
      "exploration/env_infos/initial/reward_ctrl Min         -1.91262\n",
      "exploration/env_infos/reward_ctrl Mean                -1.39274\n",
      "exploration/env_infos/reward_ctrl Std                  0.420612\n",
      "exploration/env_infos/reward_ctrl Max                 -0.185555\n",
      "exploration/env_infos/reward_ctrl Min                 -2.71604\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.264288\n",
      "exploration/env_infos/final/torso_velocity Std         1.19052\n",
      "exploration/env_infos/final/torso_velocity Max         3.75853\n",
      "exploration/env_infos/final/torso_velocity Min        -2.22082\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.184737\n",
      "exploration/env_infos/initial/torso_velocity Std       0.276074\n",
      "exploration/env_infos/initial/torso_velocity Max       0.629125\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.385658\n",
      "exploration/env_infos/torso_velocity Mean             -0.0303811\n",
      "exploration/env_infos/torso_velocity Std               0.771651\n",
      "exploration/env_infos/torso_velocity Max               4.45928\n",
      "exploration/env_infos/torso_velocity Min              -2.73764\n",
      "evaluation/num steps total                         75000\n",
      "evaluation/num paths total                            75\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.997745\n",
      "evaluation/Rewards Std                                 0.0202121\n",
      "evaluation/Rewards Max                                 2.14309\n",
      "evaluation/Rewards Min                                 0.99553\n",
      "evaluation/Returns Mean                              997.745\n",
      "evaluation/Returns Std                                 0.880876\n",
      "evaluation/Returns Max                               999.728\n",
      "evaluation/Returns Min                               996.735\n",
      "evaluation/Actions Mean                               -0.0127268\n",
      "evaluation/Actions Std                                 0.0245035\n",
      "evaluation/Actions Max                                 0.0423217\n",
      "evaluation/Actions Min                                -0.0619272\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           997.745\n",
      "evaluation/env_infos/final/reward_forward Mean         2.42836e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.7497e-07\n",
      "evaluation/env_infos/final/reward_forward Max          6.88586e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.14877e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.00182703\n",
      "evaluation/env_infos/initial/reward_forward Std        0.136472\n",
      "evaluation/env_infos/initial/reward_forward Max        0.217911\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.335114\n",
      "evaluation/env_infos/reward_forward Mean               0.000921814\n",
      "evaluation/env_infos/reward_forward Std                0.0510027\n",
      "evaluation/env_infos/reward_forward Max                1.28983\n",
      "evaluation/env_infos/reward_forward Min               -1.00038\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.00304715\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.000236544\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.00248989\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.00338036\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.00183275\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.000210281\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00156037\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.00226188\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.00304958\n",
      "evaluation/env_infos/reward_ctrl Std                   0.000244083\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00144889\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.00447009\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.94467e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          3.26266e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          6.88586e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.01599e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.126192\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.235752\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.63473\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.335114\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00224415\n",
      "evaluation/env_infos/torso_velocity Std                0.0559114\n",
      "evaluation/env_infos/torso_velocity Max                1.3398\n",
      "evaluation/env_infos/torso_velocity Min               -1.75222\n",
      "time/data storing (s)                                  0.601818\n",
      "time/evaluation sampling (s)                          45.433\n",
      "time/exploration sampling (s)                          1.81723\n",
      "time/logging (s)                                       0.275816\n",
      "time/saving (s)                                        0.0267484\n",
      "time/training (s)                                      4.05323\n",
      "time/epoch (s)                                        52.2079\n",
      "time/total (s)                                       165.954\n",
      "Epoch                                                  2\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:27:42.446755 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 3 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  10000\n",
      "trainer/QF1 Loss                                        0.461864\n",
      "trainer/QF2 Loss                                        0.447585\n",
      "trainer/Policy Loss                                    -9.22181\n",
      "trainer/Q1 Predictions Mean                             3.77236\n",
      "trainer/Q1 Predictions Std                              0.382311\n",
      "trainer/Q1 Predictions Max                              4.78246\n",
      "trainer/Q1 Predictions Min                              2.91526\n",
      "trainer/Q2 Predictions Mean                             3.76972\n",
      "trainer/Q2 Predictions Std                              0.389648\n",
      "trainer/Q2 Predictions Max                              4.9279\n",
      "trainer/Q2 Predictions Min                              2.9241\n",
      "trainer/Q Targets Mean                                  3.85738\n",
      "trainer/Q Targets Std                                   0.727222\n",
      "trainer/Q Targets Max                                   6.89497\n",
      "trainer/Q Targets Min                                  -0.183696\n",
      "trainer/Log Pis Mean                                   -5.47697\n",
      "trainer/Log Pis Std                                     0.423083\n",
      "trainer/Log Pis Max                                    -4.70207\n",
      "trainer/Log Pis Min                                    -9.50406\n",
      "trainer/Policy mu Mean                                 -0.00745476\n",
      "trainer/Policy mu Std                                   0.0154815\n",
      "trainer/Policy mu Max                                   0.0633337\n",
      "trainer/Policy mu Min                                  -0.0661697\n",
      "trainer/Policy log std Mean                            -0.130664\n",
      "trainer/Policy log std Std                              0.0146778\n",
      "trainer/Policy log std Max                             -0.0896393\n",
      "trainer/Policy log std Min                             -0.201032\n",
      "trainer/Alpha                                           0.405255\n",
      "trainer/Alpha Loss                                    -12.1325\n",
      "exploration/num steps total                          5000\n",
      "exploration/num paths total                            43\n",
      "exploration/path length Mean                          500\n",
      "exploration/path length Std                           268\n",
      "exploration/path length Max                           768\n",
      "exploration/path length Min                           232\n",
      "exploration/Rewards Mean                               -0.343092\n",
      "exploration/Rewards Std                                 0.444001\n",
      "exploration/Rewards Max                                 1.54176\n",
      "exploration/Rewards Min                                -1.71301\n",
      "exploration/Returns Mean                             -171.546\n",
      "exploration/Returns Std                                87.7114\n",
      "exploration/Returns Max                               -83.8345\n",
      "exploration/Returns Min                              -259.257\n",
      "exploration/Actions Mean                               -0.00155053\n",
      "exploration/Actions Std                                 0.588854\n",
      "exploration/Actions Max                                 0.997244\n",
      "exploration/Actions Min                                -0.994682\n",
      "exploration/Num Paths                                   2\n",
      "exploration/Average Returns                          -171.546\n",
      "exploration/env_infos/final/reward_forward Mean         0.764006\n",
      "exploration/env_infos/final/reward_forward Std          0.627005\n",
      "exploration/env_infos/final/reward_forward Max          1.39101\n",
      "exploration/env_infos/final/reward_forward Min          0.137001\n",
      "exploration/env_infos/initial/reward_forward Mean       0.195237\n",
      "exploration/env_infos/initial/reward_forward Std        0.0478992\n",
      "exploration/env_infos/initial/reward_forward Max        0.243136\n",
      "exploration/env_infos/initial/reward_forward Min        0.147338\n",
      "exploration/env_infos/reward_forward Mean              -0.0293284\n",
      "exploration/env_infos/reward_forward Std                0.434241\n",
      "exploration/env_infos/reward_forward Max                1.88123\n",
      "exploration/env_infos/reward_forward Min               -1.36804\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.03885\n",
      "exploration/env_infos/final/reward_ctrl Std             0.227143\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.81171\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.266\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.11179\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.260025\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.851765\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.37181\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.387\n",
      "exploration/env_infos/reward_ctrl Std                   0.406921\n",
      "exploration/env_infos/reward_ctrl Max                  -0.323681\n",
      "exploration/env_infos/reward_ctrl Min                  -2.71301\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.164225\n",
      "exploration/env_infos/final/torso_velocity Std          1.37636\n",
      "exploration/env_infos/final/torso_velocity Max          1.53437\n",
      "exploration/env_infos/final/torso_velocity Min         -2.65502\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.172873\n",
      "exploration/env_infos/initial/torso_velocity Std        0.180478\n",
      "exploration/env_infos/initial/torso_velocity Max        0.408219\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.072337\n",
      "exploration/env_infos/torso_velocity Mean              -0.0289848\n",
      "exploration/env_infos/torso_velocity Std                0.46631\n",
      "exploration/env_infos/torso_velocity Max                2.66501\n",
      "exploration/env_infos/torso_velocity Min               -3.02957\n",
      "evaluation/num steps total                         100000\n",
      "evaluation/num paths total                            100\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.999287\n",
      "evaluation/Rewards Std                                  0.0212861\n",
      "evaluation/Rewards Max                                  2.36979\n",
      "evaluation/Rewards Min                                  0.997129\n",
      "evaluation/Returns Mean                               999.287\n",
      "evaluation/Returns Std                                  1.09277\n",
      "evaluation/Returns Max                               1001.89\n",
      "evaluation/Returns Min                                998.113\n",
      "evaluation/Actions Mean                                -0.0109094\n",
      "evaluation/Actions Std                                  0.0160559\n",
      "evaluation/Actions Max                                  0.0251842\n",
      "evaluation/Actions Min                                 -0.0512814\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            999.287\n",
      "evaluation/env_infos/final/reward_forward Mean          1.44946e-06\n",
      "evaluation/env_infos/final/reward_forward Std           1.11505e-05\n",
      "evaluation/env_infos/final/reward_forward Max           5.16818e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -2.20028e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00403481\n",
      "evaluation/env_infos/initial/reward_forward Std         0.136108\n",
      "evaluation/env_infos/initial/reward_forward Max         0.35524\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.260607\n",
      "evaluation/env_infos/reward_forward Mean               -0.00121329\n",
      "evaluation/env_infos/reward_forward Std                 0.0432928\n",
      "evaluation/env_infos/reward_forward Max                 0.980974\n",
      "evaluation/env_infos/reward_forward Min                -1.3671\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.00150878\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.000234764\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00107818\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.00192139\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00103313\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.000413009\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.000525773\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.00192262\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00150723\n",
      "evaluation/env_infos/reward_ctrl Std                    0.000248708\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.000183925\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.00287104\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.24845e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           3.23019e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000215029\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000157214\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152231\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.232604\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.671699\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.260607\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00217378\n",
      "evaluation/env_infos/torso_velocity Std                 0.0505475\n",
      "evaluation/env_infos/torso_velocity Max                 1.2799\n",
      "evaluation/env_infos/torso_velocity Min                -1.70498\n",
      "time/data storing (s)                                   0.344063\n",
      "time/evaluation sampling (s)                           44.9389\n",
      "time/exploration sampling (s)                           1.84032\n",
      "time/logging (s)                                        0.295631\n",
      "time/saving (s)                                         0.027796\n",
      "time/training (s)                                       3.5181\n",
      "time/epoch (s)                                         50.9648\n",
      "time/total (s)                                        217.118\n",
      "Epoch                                                   3\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:28:34.098457 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 4 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  12000\n",
      "trainer/QF1 Loss                                        0.536381\n",
      "trainer/QF2 Loss                                        0.521797\n",
      "trainer/Policy Loss                                    -9.26176\n",
      "trainer/Q1 Predictions Mean                             3.87193\n",
      "trainer/Q1 Predictions Std                              0.391135\n",
      "trainer/Q1 Predictions Max                              5.50689\n",
      "trainer/Q1 Predictions Min                              2.79703\n",
      "trainer/Q2 Predictions Mean                             3.86027\n",
      "trainer/Q2 Predictions Std                              0.394919\n",
      "trainer/Q2 Predictions Max                              5.5225\n",
      "trainer/Q2 Predictions Min                              2.60047\n",
      "trainer/Q Targets Mean                                  3.78343\n",
      "trainer/Q Targets Std                                   0.78104\n",
      "trainer/Q Targets Max                                   7.21243\n",
      "trainer/Q Targets Min                                  -0.914411\n",
      "trainer/Log Pis Mean                                   -5.42232\n",
      "trainer/Log Pis Std                                     0.383828\n",
      "trainer/Log Pis Max                                    -4.43954\n",
      "trainer/Log Pis Min                                    -7.05377\n",
      "trainer/Policy mu Mean                                 -0.0187286\n",
      "trainer/Policy mu Std                                   0.0488117\n",
      "trainer/Policy mu Max                                   0.268195\n",
      "trainer/Policy mu Min                                  -0.227781\n",
      "trainer/Policy log std Mean                            -0.129319\n",
      "trainer/Policy log std Std                              0.0173634\n",
      "trainer/Policy log std Max                             -0.0927731\n",
      "trainer/Policy log std Min                             -0.233804\n",
      "trainer/Alpha                                           0.300197\n",
      "trainer/Alpha Loss                                    -16.111\n",
      "exploration/num steps total                          6000\n",
      "exploration/num paths total                            52\n",
      "exploration/path length Mean                          111.111\n",
      "exploration/path length Std                           152.9\n",
      "exploration/path length Max                           536\n",
      "exploration/path length Min                            12\n",
      "exploration/Rewards Mean                               -0.319144\n",
      "exploration/Rewards Std                                 0.485519\n",
      "exploration/Rewards Max                                 2.46923\n",
      "exploration/Rewards Min                                -1.84179\n",
      "exploration/Returns Mean                              -35.4604\n",
      "exploration/Returns Std                                54.7572\n",
      "exploration/Returns Max                                 0.8672\n",
      "exploration/Returns Min                              -187.534\n",
      "exploration/Actions Mean                               -0.0231141\n",
      "exploration/Actions Std                                 0.587721\n",
      "exploration/Actions Max                                 0.996351\n",
      "exploration/Actions Min                                -0.999015\n",
      "exploration/Num Paths                                   9\n",
      "exploration/Average Returns                           -35.4604\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0871587\n",
      "exploration/env_infos/final/reward_forward Std          0.737417\n",
      "exploration/env_infos/final/reward_forward Max          1.05619\n",
      "exploration/env_infos/final/reward_forward Min         -1.28924\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0536347\n",
      "exploration/env_infos/initial/reward_forward Std        0.102891\n",
      "exploration/env_infos/initial/reward_forward Max        0.219891\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0703041\n",
      "exploration/env_infos/reward_forward Mean              -0.134641\n",
      "exploration/env_infos/reward_forward Std                0.621749\n",
      "exploration/env_infos/reward_forward Max                1.62069\n",
      "exploration/env_infos/reward_forward Min               -3.18091\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.31446\n",
      "exploration/env_infos/final/reward_ctrl Std             0.411556\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.723449\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.94007\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.13887\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.28359\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.653412\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.59261\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.3838\n",
      "exploration/env_infos/reward_ctrl Std                   0.404836\n",
      "exploration/env_infos/reward_ctrl Max                  -0.280604\n",
      "exploration/env_infos/reward_ctrl Min                  -2.84179\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.37444\n",
      "exploration/env_infos/final/torso_velocity Std          0.775751\n",
      "exploration/env_infos/final/torso_velocity Max          1.71833\n",
      "exploration/env_infos/final/torso_velocity Min         -1.28924\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.166876\n",
      "exploration/env_infos/initial/torso_velocity Std        0.215567\n",
      "exploration/env_infos/initial/torso_velocity Max        0.555385\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.260072\n",
      "exploration/env_infos/torso_velocity Mean              -0.00776452\n",
      "exploration/env_infos/torso_velocity Std                0.658688\n",
      "exploration/env_infos/torso_velocity Max                3.43619\n",
      "exploration/env_infos/torso_velocity Min               -3.18091\n",
      "evaluation/num steps total                         125000\n",
      "evaluation/num paths total                            125\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.991839\n",
      "evaluation/Rewards Std                                  0.0265419\n",
      "evaluation/Rewards Max                                  2.37722\n",
      "evaluation/Rewards Min                                  0.976594\n",
      "evaluation/Returns Mean                               991.839\n",
      "evaluation/Returns Std                                  2.6582\n",
      "evaluation/Returns Max                               1000.49\n",
      "evaluation/Returns Min                                986.279\n",
      "evaluation/Actions Mean                                -0.0239131\n",
      "evaluation/Actions Std                                  0.0411895\n",
      "evaluation/Actions Max                                  0.110532\n",
      "evaluation/Actions Min                                 -0.118829\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            991.839\n",
      "evaluation/env_infos/final/reward_forward Mean          4.80464e-07\n",
      "evaluation/env_infos/final/reward_forward Std           1.65277e-06\n",
      "evaluation/env_infos/final/reward_forward Max           8.34285e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -6.02792e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0216926\n",
      "evaluation/env_infos/initial/reward_forward Std         0.114297\n",
      "evaluation/env_infos/initial/reward_forward Max         0.211365\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.160451\n",
      "evaluation/env_infos/reward_forward Mean                0.000845571\n",
      "evaluation/env_infos/reward_forward Std                 0.0427367\n",
      "evaluation/env_infos/reward_forward Max                 1.15783\n",
      "evaluation/env_infos/reward_forward Min                -1.10926\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.009082\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00225616\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00636448\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0136843\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00880978\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00368525\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00549599\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.017359\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.00907365\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00226417\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00417636\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0234055\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          5.63441e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.4894e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.92193e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.18127e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.139572\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.250524\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.637569\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.3187\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000711669\n",
      "evaluation/env_infos/torso_velocity Std                 0.0508088\n",
      "evaluation/env_infos/torso_velocity Max                 1.36016\n",
      "evaluation/env_infos/torso_velocity Min                -1.99439\n",
      "time/data storing (s)                                   0.467031\n",
      "time/evaluation sampling (s)                           45.3568\n",
      "time/exploration sampling (s)                           1.87353\n",
      "time/logging (s)                                        0.288412\n",
      "time/saving (s)                                         0.0264401\n",
      "time/training (s)                                       3.41469\n",
      "time/epoch (s)                                         51.4269\n",
      "time/total (s)                                        268.762\n",
      "Epoch                                                   4\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:29:25.149181 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 5 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  14000\n",
      "trainer/QF1 Loss                                        0.421919\n",
      "trainer/QF2 Loss                                        0.413746\n",
      "trainer/Policy Loss                                    -9.33083\n",
      "trainer/Q1 Predictions Mean                             3.98494\n",
      "trainer/Q1 Predictions Std                              0.438652\n",
      "trainer/Q1 Predictions Max                              5.73677\n",
      "trainer/Q1 Predictions Min                              2.96404\n",
      "trainer/Q2 Predictions Mean                             3.92696\n",
      "trainer/Q2 Predictions Std                              0.451645\n",
      "trainer/Q2 Predictions Max                              6.07422\n",
      "trainer/Q2 Predictions Min                              2.8992\n",
      "trainer/Q Targets Mean                                  3.94173\n",
      "trainer/Q Targets Std                                   0.705307\n",
      "trainer/Q Targets Max                                   7.43982\n",
      "trainer/Q Targets Min                                   2.41648\n",
      "trainer/Log Pis Mean                                   -5.41958\n",
      "trainer/Log Pis Std                                     0.597096\n",
      "trainer/Log Pis Max                                    -3.82263\n",
      "trainer/Log Pis Min                                    -9.5437\n",
      "trainer/Policy mu Mean                                 -0.0424191\n",
      "trainer/Policy mu Std                                   0.0956571\n",
      "trainer/Policy mu Max                                   0.302117\n",
      "trainer/Policy mu Min                                  -0.467101\n",
      "trainer/Policy log std Mean                            -0.131503\n",
      "trainer/Policy log std Std                              0.0221261\n",
      "trainer/Policy log std Max                             -0.0836691\n",
      "trainer/Policy log std Min                             -0.273898\n",
      "trainer/Alpha                                           0.22244\n",
      "trainer/Alpha Loss                                    -20.1308\n",
      "exploration/num steps total                          7000\n",
      "exploration/num paths total                            64\n",
      "exploration/path length Mean                           83.3333\n",
      "exploration/path length Std                            77.546\n",
      "exploration/path length Max                           293\n",
      "exploration/path length Min                            18\n",
      "exploration/Rewards Mean                               -0.321189\n",
      "exploration/Rewards Std                                 0.547623\n",
      "exploration/Rewards Max                                 2.59803\n",
      "exploration/Rewards Min                                -2.03928\n",
      "exploration/Returns Mean                              -26.7657\n",
      "exploration/Returns Std                                26.9648\n",
      "exploration/Returns Max                                -5.76191\n",
      "exploration/Returns Min                               -99.1655\n",
      "exploration/Actions Mean                               -0.0282867\n",
      "exploration/Actions Std                                 0.594506\n",
      "exploration/Actions Max                                 0.996016\n",
      "exploration/Actions Min                                -0.997708\n",
      "exploration/Num Paths                                  12\n",
      "exploration/Average Returns                           -26.7657\n",
      "exploration/env_infos/final/reward_forward Mean         0.192156\n",
      "exploration/env_infos/final/reward_forward Std          0.946484\n",
      "exploration/env_infos/final/reward_forward Max          1.88947\n",
      "exploration/env_infos/final/reward_forward Min         -1.85191\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0599319\n",
      "exploration/env_infos/initial/reward_forward Std        0.197783\n",
      "exploration/env_infos/initial/reward_forward Max        0.346584\n",
      "exploration/env_infos/initial/reward_forward Min       -0.369443\n",
      "exploration/env_infos/reward_forward Mean              -0.0312359\n",
      "exploration/env_infos/reward_forward Std                0.783318\n",
      "exploration/env_infos/reward_forward Max                2.23707\n",
      "exploration/env_infos/reward_forward Min               -2.13588\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.1344\n",
      "exploration/env_infos/final/reward_ctrl Std             0.416281\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.603133\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.90114\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.49291\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.405611\n",
      "exploration/env_infos/initial/reward_ctrl Max          -1.00653\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.22446\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.41695\n",
      "exploration/env_infos/reward_ctrl Std                   0.424711\n",
      "exploration/env_infos/reward_ctrl Max                  -0.259321\n",
      "exploration/env_infos/reward_ctrl Min                  -3.03928\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.400486\n",
      "exploration/env_infos/final/torso_velocity Std          0.987178\n",
      "exploration/env_infos/final/torso_velocity Max          2.57357\n",
      "exploration/env_infos/final/torso_velocity Min         -1.85191\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.14016\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27025\n",
      "exploration/env_infos/initial/torso_velocity Max        0.642694\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.369443\n",
      "exploration/env_infos/torso_velocity Mean               0.0213186\n",
      "exploration/env_infos/torso_velocity Std                0.844186\n",
      "exploration/env_infos/torso_velocity Max                3.75917\n",
      "exploration/env_infos/torso_velocity Min               -2.42712\n",
      "evaluation/num steps total                         150000\n",
      "evaluation/num paths total                            150\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.982897\n",
      "evaluation/Rewards Std                                  0.0221113\n",
      "evaluation/Rewards Max                                  1.87343\n",
      "evaluation/Rewards Min                                  0.935123\n",
      "evaluation/Returns Mean                               982.897\n",
      "evaluation/Returns Std                                  5.13682\n",
      "evaluation/Returns Max                                990.439\n",
      "evaluation/Returns Min                                971.724\n",
      "evaluation/Actions Mean                                -0.031216\n",
      "evaluation/Actions Std                                  0.0595552\n",
      "evaluation/Actions Max                                  0.185394\n",
      "evaluation/Actions Min                                 -0.198401\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            982.897\n",
      "evaluation/env_infos/final/reward_forward Mean          4.44533e-05\n",
      "evaluation/env_infos/final/reward_forward Std           9.2434e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000288837\n",
      "evaluation/env_infos/final/reward_forward Min          -2.89289e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0131562\n",
      "evaluation/env_infos/initial/reward_forward Std         0.110984\n",
      "evaluation/env_infos/initial/reward_forward Max         0.243608\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.259026\n",
      "evaluation/env_infos/reward_forward Mean                0.000514107\n",
      "evaluation/env_infos/reward_forward Std                 0.0525997\n",
      "evaluation/env_infos/reward_forward Max                 1.02157\n",
      "evaluation/env_infos/reward_forward Min                -1.38941\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0180347\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0045229\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0114748\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0284913\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0198766\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0120518\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00839469\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0503235\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.018085\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00471352\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00777799\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0648775\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.645e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           7.53355e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000288837\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000248656\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126126\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.254\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.655296\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.392014\n",
      "evaluation/env_infos/torso_velocity Mean               -2.29281e-05\n",
      "evaluation/env_infos/torso_velocity Std                 0.053411\n",
      "evaluation/env_infos/torso_velocity Max                 1.10479\n",
      "evaluation/env_infos/torso_velocity Min                -1.92618\n",
      "time/data storing (s)                                   0.52211\n",
      "time/evaluation sampling (s)                           44.8123\n",
      "time/exploration sampling (s)                           1.80011\n",
      "time/logging (s)                                        0.284777\n",
      "time/saving (s)                                         0.02664\n",
      "time/training (s)                                       3.38062\n",
      "time/epoch (s)                                         50.8265\n",
      "time/total (s)                                        319.809\n",
      "Epoch                                                   5\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:30:17.356485 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 6 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  16000\n",
      "trainer/QF1 Loss                                        0.401252\n",
      "trainer/QF2 Loss                                        0.398817\n",
      "trainer/Policy Loss                                    -9.3032\n",
      "trainer/Q1 Predictions Mean                             3.92657\n",
      "trainer/Q1 Predictions Std                              0.46717\n",
      "trainer/Q1 Predictions Max                              5.40058\n",
      "trainer/Q1 Predictions Min                              2.64671\n",
      "trainer/Q2 Predictions Mean                             3.92611\n",
      "trainer/Q2 Predictions Std                              0.47681\n",
      "trainer/Q2 Predictions Max                              5.5726\n",
      "trainer/Q2 Predictions Min                              2.68109\n",
      "trainer/Q Targets Mean                                  3.89899\n",
      "trainer/Q Targets Std                                   0.702904\n",
      "trainer/Q Targets Max                                   6.67834\n",
      "trainer/Q Targets Min                                  -0.288285\n",
      "trainer/Log Pis Mean                                   -5.42507\n",
      "trainer/Log Pis Std                                     0.558161\n",
      "trainer/Log Pis Max                                    -3.63218\n",
      "trainer/Log Pis Min                                   -10.1786\n",
      "trainer/Policy mu Mean                                  0.0200106\n",
      "trainer/Policy mu Std                                   0.0959003\n",
      "trainer/Policy mu Max                                   0.511283\n",
      "trainer/Policy mu Min                                  -0.262941\n",
      "trainer/Policy log std Mean                            -0.153658\n",
      "trainer/Policy log std Std                              0.0192949\n",
      "trainer/Policy log std Max                             -0.106421\n",
      "trainer/Policy log std Min                             -0.230352\n",
      "trainer/Alpha                                           0.164857\n",
      "trainer/Alpha Loss                                    -24.1609\n",
      "exploration/num steps total                          8000\n",
      "exploration/num paths total                            84\n",
      "exploration/path length Mean                           50\n",
      "exploration/path length Std                            32.7857\n",
      "exploration/path length Max                           125\n",
      "exploration/path length Min                            11\n",
      "exploration/Rewards Mean                               -0.301342\n",
      "exploration/Rewards Std                                 0.477518\n",
      "exploration/Rewards Max                                 1.74245\n",
      "exploration/Rewards Min                                -1.5948\n",
      "exploration/Returns Mean                              -15.0671\n",
      "exploration/Returns Std                                11.0498\n",
      "exploration/Returns Max                                 0.44532\n",
      "exploration/Returns Min                               -40.2214\n",
      "exploration/Actions Mean                                0.00813488\n",
      "exploration/Actions Std                                 0.581256\n",
      "exploration/Actions Max                                 0.996725\n",
      "exploration/Actions Min                                -0.995829\n",
      "exploration/Num Paths                                  20\n",
      "exploration/Average Returns                           -15.0671\n",
      "exploration/env_infos/final/reward_forward Mean         0.248345\n",
      "exploration/env_infos/final/reward_forward Std          0.786559\n",
      "exploration/env_infos/final/reward_forward Max          1.5297\n",
      "exploration/env_infos/final/reward_forward Min         -1.69641\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0525585\n",
      "exploration/env_infos/initial/reward_forward Std        0.136478\n",
      "exploration/env_infos/initial/reward_forward Max        0.202354\n",
      "exploration/env_infos/initial/reward_forward Min       -0.317593\n",
      "exploration/env_infos/reward_forward Mean               0.0557829\n",
      "exploration/env_infos/reward_forward Std                0.747786\n",
      "exploration/env_infos/reward_forward Max                3.00539\n",
      "exploration/env_infos/reward_forward Min               -3.08386\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.37494\n",
      "exploration/env_infos/final/reward_ctrl Std             0.45739\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.504032\n",
      "exploration/env_infos/final/reward_ctrl Min            -2.04958\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.47903\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.449227\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.610457\n",
      "exploration/env_infos/initial/reward_ctrl Min          -2.30206\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.3517\n",
      "exploration/env_infos/reward_ctrl Std                   0.416821\n",
      "exploration/env_infos/reward_ctrl Max                  -0.240033\n",
      "exploration/env_infos/reward_ctrl Min                  -2.5948\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.611322\n",
      "exploration/env_infos/final/torso_velocity Std          1.03999\n",
      "exploration/env_infos/final/torso_velocity Max          4.34835\n",
      "exploration/env_infos/final/torso_velocity Min         -2.19677\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.159405\n",
      "exploration/env_infos/initial/torso_velocity Std        0.256093\n",
      "exploration/env_infos/initial/torso_velocity Max        0.606971\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.317593\n",
      "exploration/env_infos/torso_velocity Mean               0.0747526\n",
      "exploration/env_infos/torso_velocity Std                0.8312\n",
      "exploration/env_infos/torso_velocity Max                4.34835\n",
      "exploration/env_infos/torso_velocity Min               -3.08386\n",
      "evaluation/num steps total                         175000\n",
      "evaluation/num paths total                            175\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.986026\n",
      "evaluation/Rewards Std                                  0.0354384\n",
      "evaluation/Rewards Max                                  2.55697\n",
      "evaluation/Rewards Min                                  0.933588\n",
      "evaluation/Returns Mean                               986.026\n",
      "evaluation/Returns Std                                  2.32168\n",
      "evaluation/Returns Max                                992.182\n",
      "evaluation/Returns Min                                981.169\n",
      "evaluation/Actions Mean                                 0.011168\n",
      "evaluation/Actions Std                                  0.0612977\n",
      "evaluation/Actions Max                                  0.246526\n",
      "evaluation/Actions Min                                 -0.17591\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            986.026\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000479891\n",
      "evaluation/env_infos/final/reward_forward Std           0.0172442\n",
      "evaluation/env_infos/final/reward_forward Max           0.0636378\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0581204\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0280765\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14654\n",
      "evaluation/env_infos/initial/reward_forward Max         0.260015\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.355807\n",
      "evaluation/env_infos/reward_forward Mean               -0.00234528\n",
      "evaluation/env_infos/reward_forward Std                 0.074416\n",
      "evaluation/env_infos/reward_forward Max                 1.46139\n",
      "evaluation/env_infos/reward_forward Min                -1.72163\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0152421\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00219748\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0119799\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0205356\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0172252\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00469644\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0122079\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0277512\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0155285\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00275238\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0104219\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.0664118\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.20093e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0122783\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0636378\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0581204\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.131407\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248962\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.701594\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.355807\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00232505\n",
      "evaluation/env_infos/torso_velocity Std                 0.0733979\n",
      "evaluation/env_infos/torso_velocity Max                 1.46139\n",
      "evaluation/env_infos/torso_velocity Min                -2.02991\n",
      "time/data storing (s)                                   0.630582\n",
      "time/evaluation sampling (s)                           45.9002\n",
      "time/exploration sampling (s)                           1.77629\n",
      "time/logging (s)                                        0.281326\n",
      "time/saving (s)                                         0.0278214\n",
      "time/training (s)                                       3.36683\n",
      "time/epoch (s)                                         51.983\n",
      "time/total (s)                                        372.013\n",
      "Epoch                                                   6\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:31:07.875007 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 7 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  18000\n",
      "trainer/QF1 Loss                                        0.499725\n",
      "trainer/QF2 Loss                                        0.5001\n",
      "trainer/Policy Loss                                    -8.92661\n",
      "trainer/Q1 Predictions Mean                             3.60127\n",
      "trainer/Q1 Predictions Std                              0.474266\n",
      "trainer/Q1 Predictions Max                              5.38138\n",
      "trainer/Q1 Predictions Min                              2.26538\n",
      "trainer/Q2 Predictions Mean                             3.58948\n",
      "trainer/Q2 Predictions Std                              0.472508\n",
      "trainer/Q2 Predictions Max                              5.04199\n",
      "trainer/Q2 Predictions Min                              2.0331\n",
      "trainer/Q Targets Mean                                  3.78885\n",
      "trainer/Q Targets Std                                   0.859815\n",
      "trainer/Q Targets Max                                   7.95179\n",
      "trainer/Q Targets Min                                  -0.611172\n",
      "trainer/Log Pis Mean                                   -5.36989\n",
      "trainer/Log Pis Std                                     0.640339\n",
      "trainer/Log Pis Max                                    -2.95203\n",
      "trainer/Log Pis Min                                    -8.30281\n",
      "trainer/Policy mu Mean                                  0.0148649\n",
      "trainer/Policy mu Std                                   0.142477\n",
      "trainer/Policy mu Max                                   0.632426\n",
      "trainer/Policy mu Min                                  -0.482921\n",
      "trainer/Policy log std Mean                            -0.17976\n",
      "trainer/Policy log std Std                              0.0315491\n",
      "trainer/Policy log std Max                             -0.0943957\n",
      "trainer/Policy log std Min                             -0.390167\n",
      "trainer/Alpha                                           0.122292\n",
      "trainer/Alpha Loss                                    -28.0548\n",
      "exploration/num steps total                          9000\n",
      "exploration/num paths total                            87\n",
      "exploration/path length Mean                          333.333\n",
      "exploration/path length Std                           411.787\n",
      "exploration/path length Max                           915\n",
      "exploration/path length Min                            18\n",
      "exploration/Rewards Mean                               -0.312348\n",
      "exploration/Rewards Std                                 0.461752\n",
      "exploration/Rewards Max                                 1.62829\n",
      "exploration/Rewards Min                                -1.79034\n",
      "exploration/Returns Mean                             -104.116\n",
      "exploration/Returns Std                               129.134\n",
      "exploration/Returns Max                                -8.76512\n",
      "exploration/Returns Min                              -286.679\n",
      "exploration/Actions Mean                                0.0182657\n",
      "exploration/Actions Std                                 0.582318\n",
      "exploration/Actions Max                                 0.993007\n",
      "exploration/Actions Min                                -0.993944\n",
      "exploration/Num Paths                                   3\n",
      "exploration/Average Returns                          -104.116\n",
      "exploration/env_infos/final/reward_forward Mean         0.416842\n",
      "exploration/env_infos/final/reward_forward Std          0.708341\n",
      "exploration/env_infos/final/reward_forward Max          1.41752\n",
      "exploration/env_infos/final/reward_forward Min         -0.12356\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0220241\n",
      "exploration/env_infos/initial/reward_forward Std        0.184685\n",
      "exploration/env_infos/initial/reward_forward Max        0.154276\n",
      "exploration/env_infos/initial/reward_forward Min       -0.239153\n",
      "exploration/env_infos/reward_forward Mean              -0.0262263\n",
      "exploration/env_infos/reward_forward Std                0.478498\n",
      "exploration/env_infos/reward_forward Max                2.21531\n",
      "exploration/env_infos/reward_forward Min               -1.66673\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.56687\n",
      "exploration/env_infos/final/reward_ctrl Std             0.323868\n",
      "exploration/env_infos/final/reward_ctrl Max            -1.10917\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.81052\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.08645\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.10107\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.948944\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.189\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.35771\n",
      "exploration/env_infos/reward_ctrl Std                   0.42042\n",
      "exploration/env_infos/reward_ctrl Max                  -0.18268\n",
      "exploration/env_infos/reward_ctrl Min                  -2.79034\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.85312\n",
      "exploration/env_infos/final/torso_velocity Std          1.39419\n",
      "exploration/env_infos/final/torso_velocity Max          4.46384\n",
      "exploration/env_infos/final/torso_velocity Min         -0.12356\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.173367\n",
      "exploration/env_infos/initial/torso_velocity Std        0.198001\n",
      "exploration/env_infos/initial/torso_velocity Max        0.44593\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.239153\n",
      "exploration/env_infos/torso_velocity Mean              -0.00533652\n",
      "exploration/env_infos/torso_velocity Std                0.470255\n",
      "exploration/env_infos/torso_velocity Max                4.46384\n",
      "exploration/env_infos/torso_velocity Min               -2.41984\n",
      "evaluation/num steps total                         200000\n",
      "evaluation/num paths total                            200\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.966592\n",
      "evaluation/Rewards Std                                  0.0198248\n",
      "evaluation/Rewards Max                                  2.31345\n",
      "evaluation/Rewards Min                                  0.654831\n",
      "evaluation/Returns Mean                               966.592\n",
      "evaluation/Returns Std                                  4.44628\n",
      "evaluation/Returns Max                                972.866\n",
      "evaluation/Returns Min                                957.698\n",
      "evaluation/Actions Mean                                 0.0134493\n",
      "evaluation/Actions Std                                  0.0910867\n",
      "evaluation/Actions Max                                  0.461056\n",
      "evaluation/Actions Min                                 -0.437624\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            966.592\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.33631e-06\n",
      "evaluation/env_infos/final/reward_forward Std           4.72074e-06\n",
      "evaluation/env_infos/final/reward_forward Max           8.85679e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.3053e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0359791\n",
      "evaluation/env_infos/initial/reward_forward Std         0.11289\n",
      "evaluation/env_infos/initial/reward_forward Max         0.196939\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.254316\n",
      "evaluation/env_infos/reward_forward Mean               -0.00512404\n",
      "evaluation/env_infos/reward_forward Std                 0.0624973\n",
      "evaluation/env_infos/reward_forward Max                 0.678497\n",
      "evaluation/env_infos/reward_forward Min                -1.43577\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0333239\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00471579\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0260903\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0422733\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0338687\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00838183\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0213324\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0489097\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0339106\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00990509\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0114451\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.345169\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.1553e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           4.75845e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           9.05184e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -3.11066e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.142379\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.224545\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.657781\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.254316\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00261747\n",
      "evaluation/env_infos/torso_velocity Std                 0.0580302\n",
      "evaluation/env_infos/torso_velocity Max                 1.20564\n",
      "evaluation/env_infos/torso_velocity Min                -1.72368\n",
      "time/data storing (s)                                   0.355406\n",
      "time/evaluation sampling (s)                           43.9179\n",
      "time/exploration sampling (s)                           1.85332\n",
      "time/logging (s)                                        0.291298\n",
      "time/saving (s)                                         0.0276431\n",
      "time/training (s)                                       3.85438\n",
      "time/epoch (s)                                         50.3\n",
      "time/total (s)                                        422.541\n",
      "Epoch                                                   7\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:32:00.103074 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 8 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  19898\n",
      "trainer/QF1 Loss                                        0.473494\n",
      "trainer/QF2 Loss                                        0.463465\n",
      "trainer/Policy Loss                                    -8.77111\n",
      "trainer/Q1 Predictions Mean                             3.65271\n",
      "trainer/Q1 Predictions Std                              0.513714\n",
      "trainer/Q1 Predictions Max                              6.14823\n",
      "trainer/Q1 Predictions Min                              2.32566\n",
      "trainer/Q2 Predictions Mean                             3.6805\n",
      "trainer/Q2 Predictions Std                              0.531541\n",
      "trainer/Q2 Predictions Max                              6.0057\n",
      "trainer/Q2 Predictions Min                              2.44788\n",
      "trainer/Q Targets Mean                                  3.68043\n",
      "trainer/Q Targets Std                                   0.879784\n",
      "trainer/Q Targets Max                                   7.92817\n",
      "trainer/Q Targets Min                                  -0.284326\n",
      "trainer/Log Pis Mean                                   -5.11336\n",
      "trainer/Log Pis Std                                     0.809162\n",
      "trainer/Log Pis Max                                    -2.12241\n",
      "trainer/Log Pis Min                                    -8.28169\n",
      "trainer/Policy mu Mean                                 -0.0141595\n",
      "trainer/Policy mu Std                                   0.179397\n",
      "trainer/Policy mu Max                                   0.698627\n",
      "trainer/Policy mu Min                                  -0.721621\n",
      "trainer/Policy log std Mean                            -0.299264\n",
      "trainer/Policy log std Std                              0.0804342\n",
      "trainer/Policy log std Max                             -0.135542\n",
      "trainer/Policy log std Min                             -0.640015\n",
      "trainer/Alpha                                           0.0909251\n",
      "trainer/Alpha Loss                                    -31.4035\n",
      "exploration/num steps total                         10000\n",
      "exploration/num paths total                            91\n",
      "exploration/path length Mean                          250\n",
      "exploration/path length Std                           289.048\n",
      "exploration/path length Max                           750\n",
      "exploration/path length Min                            61\n",
      "exploration/Rewards Mean                               -0.13624\n",
      "exploration/Rewards Std                                 0.461484\n",
      "exploration/Rewards Max                                 2.02638\n",
      "exploration/Rewards Min                                -1.48098\n",
      "exploration/Returns Mean                              -34.06\n",
      "exploration/Returns Std                                46.3176\n",
      "exploration/Returns Max                                 4.0348\n",
      "exploration/Returns Min                              -112.981\n",
      "exploration/Actions Mean                               -0.00148663\n",
      "exploration/Actions Std                                 0.54768\n",
      "exploration/Actions Max                                 0.9962\n",
      "exploration/Actions Min                                -0.994429\n",
      "exploration/Num Paths                                   4\n",
      "exploration/Average Returns                           -34.06\n",
      "exploration/env_infos/final/reward_forward Mean         0.205483\n",
      "exploration/env_infos/final/reward_forward Std          0.65084\n",
      "exploration/env_infos/final/reward_forward Max          1.09849\n",
      "exploration/env_infos/final/reward_forward Min         -0.468885\n",
      "exploration/env_infos/initial/reward_forward Mean       0.194822\n",
      "exploration/env_infos/initial/reward_forward Std        0.0928333\n",
      "exploration/env_infos/initial/reward_forward Max        0.30401\n",
      "exploration/env_infos/initial/reward_forward Min        0.0552452\n",
      "exploration/env_infos/reward_forward Mean              -0.0568107\n",
      "exploration/env_infos/reward_forward Std                0.525848\n",
      "exploration/env_infos/reward_forward Max                1.89813\n",
      "exploration/env_infos/reward_forward Min               -2.19641\n",
      "exploration/env_infos/final/reward_ctrl Mean           -1.11689\n",
      "exploration/env_infos/final/reward_ctrl Std             0.372732\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.659328\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.51911\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -1.27339\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.317373\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.798066\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.66417\n",
      "exploration/env_infos/reward_ctrl Mean                 -1.19982\n",
      "exploration/env_infos/reward_ctrl Std                   0.383693\n",
      "exploration/env_infos/reward_ctrl Max                  -0.149912\n",
      "exploration/env_infos/reward_ctrl Min                  -2.48098\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.421297\n",
      "exploration/env_infos/final/torso_velocity Std          0.911197\n",
      "exploration/env_infos/final/torso_velocity Max          1.72529\n",
      "exploration/env_infos/final/torso_velocity Min         -0.587407\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.158716\n",
      "exploration/env_infos/initial/torso_velocity Std        0.264726\n",
      "exploration/env_infos/initial/torso_velocity Max        0.563558\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.186372\n",
      "exploration/env_infos/torso_velocity Mean              -0.0239313\n",
      "exploration/env_infos/torso_velocity Std                0.556216\n",
      "exploration/env_infos/torso_velocity Max                2.96383\n",
      "exploration/env_infos/torso_velocity Min               -2.67023\n",
      "evaluation/num steps total                         225000\n",
      "evaluation/num paths total                            225\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.974861\n",
      "evaluation/Rewards Std                                  0.0647811\n",
      "evaluation/Rewards Max                                  2.41408\n",
      "evaluation/Rewards Min                                  0.7378\n",
      "evaluation/Returns Mean                               974.861\n",
      "evaluation/Returns Std                                 10.6603\n",
      "evaluation/Returns Max                               1004.34\n",
      "evaluation/Returns Min                                945.731\n",
      "evaluation/Actions Mean                                -0.0428223\n",
      "evaluation/Actions Std                                  0.0803328\n",
      "evaluation/Actions Max                                  0.427879\n",
      "evaluation/Actions Min                                 -0.47254\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            974.861\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0216119\n",
      "evaluation/env_infos/final/reward_forward Std           0.0889578\n",
      "evaluation/env_infos/final/reward_forward Max           0.396903\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0663068\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00593698\n",
      "evaluation/env_infos/initial/reward_forward Std         0.113457\n",
      "evaluation/env_infos/initial/reward_forward Max         0.249669\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.167452\n",
      "evaluation/env_infos/reward_forward Mean                0.00674514\n",
      "evaluation/env_infos/reward_forward Std                 0.121384\n",
      "evaluation/env_infos/reward_forward Max                 1.30775\n",
      "evaluation/env_infos/reward_forward Min                -1.74399\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0317983\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0081929\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.023939\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0601232\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0439563\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.025373\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0211979\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.10735\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0331484\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0165917\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00940272\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.2622\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00329052\n",
      "evaluation/env_infos/final/torso_velocity Std           0.069606\n",
      "evaluation/env_infos/final/torso_velocity Max           0.396903\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.289919\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.15081\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.23941\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.582037\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.228131\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00354083\n",
      "evaluation/env_infos/torso_velocity Std                 0.121472\n",
      "evaluation/env_infos/torso_velocity Max                 1.57947\n",
      "evaluation/env_infos/torso_velocity Min                -1.96547\n",
      "time/data storing (s)                                   0.368822\n",
      "time/evaluation sampling (s)                           45.6813\n",
      "time/exploration sampling (s)                           1.88562\n",
      "time/logging (s)                                        0.271478\n",
      "time/saving (s)                                         0.0257064\n",
      "time/training (s)                                       3.74066\n",
      "time/epoch (s)                                         51.9736\n",
      "time/total (s)                                        474.749\n",
      "Epoch                                                   8\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:401: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_latents\": np.array(self.relabeled_latents),\n",
      "/Users/shanliyu/School/CSE257/own/generalized-hindsight/rlkit/data_management/task_relabeling_replay_buffer.py:404: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"relabeled_rewards\": np.array(self.relabeled_rewards)}\n",
      "2021-05-25 13:32:51.391502 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 9 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  21898\n",
      "trainer/QF1 Loss                                        0.508486\n",
      "trainer/QF2 Loss                                        0.4734\n",
      "trainer/Policy Loss                                    -7.63662\n",
      "trainer/Q1 Predictions Mean                             3.51381\n",
      "trainer/Q1 Predictions Std                              0.601701\n",
      "trainer/Q1 Predictions Max                              6.01044\n",
      "trainer/Q1 Predictions Min                              1.74471\n",
      "trainer/Q2 Predictions Mean                             3.5413\n",
      "trainer/Q2 Predictions Std                              0.571513\n",
      "trainer/Q2 Predictions Max                              5.88351\n",
      "trainer/Q2 Predictions Min                              2.03878\n",
      "trainer/Q Targets Mean                                  3.51963\n",
      "trainer/Q Targets Std                                   0.924831\n",
      "trainer/Q Targets Max                                   7.84807\n",
      "trainer/Q Targets Min                                  -0.428705\n",
      "trainer/Log Pis Mean                                   -4.01522\n",
      "trainer/Log Pis Std                                     1.25466\n",
      "trainer/Log Pis Max                                    -0.855057\n",
      "trainer/Log Pis Min                                    -8.58965\n",
      "trainer/Policy mu Mean                                  0.011955\n",
      "trainer/Policy mu Std                                   0.240384\n",
      "trainer/Policy mu Max                                   0.861386\n",
      "trainer/Policy mu Min                                  -0.866307\n",
      "trainer/Policy log std Mean                            -0.581958\n",
      "trainer/Policy log std Std                              0.171328\n",
      "trainer/Policy log std Max                             -0.238256\n",
      "trainer/Policy log std Min                             -1.24949\n",
      "trainer/Alpha                                           0.0683325\n",
      "trainer/Alpha Loss                                    -32.2082\n",
      "exploration/num steps total                         11000\n",
      "exploration/num paths total                           103\n",
      "exploration/path length Mean                           83.3333\n",
      "exploration/path length Std                            78.5242\n",
      "exploration/path length Max                           291\n",
      "exploration/path length Min                            18\n",
      "exploration/Rewards Mean                                0.170474\n",
      "exploration/Rewards Std                                 0.491419\n",
      "exploration/Rewards Max                                 3.57781\n",
      "exploration/Rewards Min                                -1.08224\n",
      "exploration/Returns Mean                               14.2061\n",
      "exploration/Returns Std                                15.3785\n",
      "exploration/Returns Max                                46.563\n",
      "exploration/Returns Min                                 0.92729\n",
      "exploration/Actions Mean                                0.0262044\n",
      "exploration/Actions Std                                 0.476851\n",
      "exploration/Actions Max                                 0.988844\n",
      "exploration/Actions Min                                -0.99086\n",
      "exploration/Num Paths                                  12\n",
      "exploration/Average Returns                            14.2061\n",
      "exploration/env_infos/final/reward_forward Mean         0.473362\n",
      "exploration/env_infos/final/reward_forward Std          0.992078\n",
      "exploration/env_infos/final/reward_forward Max          2.73296\n",
      "exploration/env_infos/final/reward_forward Min         -1.11121\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.02549\n",
      "exploration/env_infos/initial/reward_forward Std        0.14862\n",
      "exploration/env_infos/initial/reward_forward Max        0.166414\n",
      "exploration/env_infos/initial/reward_forward Min       -0.224352\n",
      "exploration/env_infos/reward_forward Mean               0.00441718\n",
      "exploration/env_infos/reward_forward Std                0.779362\n",
      "exploration/env_infos/reward_forward Max                3.24935\n",
      "exploration/env_infos/reward_forward Min               -2.20701\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.848558\n",
      "exploration/env_infos/final/reward_ctrl Std             0.252051\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.444257\n",
      "exploration/env_infos/final/reward_ctrl Min            -1.3946\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.792501\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.304736\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.43066\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.60698\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.912295\n",
      "exploration/env_infos/reward_ctrl Std                   0.316962\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0628764\n",
      "exploration/env_infos/reward_ctrl Min                  -2.08224\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.428355\n",
      "exploration/env_infos/final/torso_velocity Std          1.02731\n",
      "exploration/env_infos/final/torso_velocity Max          2.73296\n",
      "exploration/env_infos/final/torso_velocity Min         -1.91969\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.12506\n",
      "exploration/env_infos/initial/torso_velocity Std        0.249411\n",
      "exploration/env_infos/initial/torso_velocity Max        0.519909\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.276792\n",
      "exploration/env_infos/torso_velocity Mean               0.00548838\n",
      "exploration/env_infos/torso_velocity Std                0.793066\n",
      "exploration/env_infos/torso_velocity Max                3.31621\n",
      "exploration/env_infos/torso_velocity Min               -2.33459\n",
      "evaluation/num steps total                         250000\n",
      "evaluation/num paths total                            250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.887196\n",
      "evaluation/Rewards Std                                  0.0827141\n",
      "evaluation/Rewards Max                                  2.35701\n",
      "evaluation/Rewards Min                                  0.486319\n",
      "evaluation/Returns Mean                               887.196\n",
      "evaluation/Returns Std                                 37.9511\n",
      "evaluation/Returns Max                                990.861\n",
      "evaluation/Returns Min                                832.817\n",
      "evaluation/Actions Mean                                 0.0366199\n",
      "evaluation/Actions Std                                  0.174564\n",
      "evaluation/Actions Max                                  0.610031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/Actions Min                                 -0.565684\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            887.196\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0236147\n",
      "evaluation/env_infos/final/reward_forward Std           0.121397\n",
      "evaluation/env_infos/final/reward_forward Max           0.223192\n",
      "evaluation/env_infos/final/reward_forward Min          -0.473458\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0142015\n",
      "evaluation/env_infos/initial/reward_forward Std         0.145997\n",
      "evaluation/env_infos/initial/reward_forward Max         0.306621\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.197572\n",
      "evaluation/env_infos/reward_forward Mean               -0.000741717\n",
      "evaluation/env_infos/reward_forward Std                 0.188726\n",
      "evaluation/env_infos/reward_forward Max                 1.07456\n",
      "evaluation/env_infos/reward_forward Min                -1.69891\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.125372\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0198153\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.102134\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.166442\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.143125\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0239671\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.103131\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.186186\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.127254\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0234371\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0390833\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.513681\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00330835\n",
      "evaluation/env_infos/final/torso_velocity Std           0.127327\n",
      "evaluation/env_infos/final/torso_velocity Max           0.476065\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.473458\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.147165\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253488\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.731967\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.244665\n",
      "evaluation/env_infos/torso_velocity Mean                0.00287563\n",
      "evaluation/env_infos/torso_velocity Std                 0.16885\n",
      "evaluation/env_infos/torso_velocity Max                 1.71392\n",
      "evaluation/env_infos/torso_velocity Min                -1.89396\n",
      "time/data storing (s)                                   0.500193\n",
      "time/evaluation sampling (s)                           44.906\n",
      "time/exploration sampling (s)                           1.72385\n",
      "time/logging (s)                                        0.284922\n",
      "time/saving (s)                                         0.0273371\n",
      "time/training (s)                                       3.61934\n",
      "time/epoch (s)                                         51.0616\n",
      "time/total (s)                                        526.05\n",
      "Epoch                                                   9\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:33:42.048256 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 10 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  23898\n",
      "trainer/QF1 Loss                                        0.633375\n",
      "trainer/QF2 Loss                                        0.63613\n",
      "trainer/Policy Loss                                    -5.81978\n",
      "trainer/Q1 Predictions Mean                             3.50726\n",
      "trainer/Q1 Predictions Std                              0.607892\n",
      "trainer/Q1 Predictions Max                              6.23893\n",
      "trainer/Q1 Predictions Min                              1.87135\n",
      "trainer/Q2 Predictions Mean                             3.50571\n",
      "trainer/Q2 Predictions Std                              0.627303\n",
      "trainer/Q2 Predictions Max                              6.71542\n",
      "trainer/Q2 Predictions Min                              1.8865\n",
      "trainer/Q Targets Mean                                  3.50993\n",
      "trainer/Q Targets Std                                   1.04576\n",
      "trainer/Q Targets Max                                   9.53987\n",
      "trainer/Q Targets Min                                  -0.909907\n",
      "trainer/Log Pis Mean                                   -1.97424\n",
      "trainer/Log Pis Std                                     1.76075\n",
      "trainer/Log Pis Max                                     1.60442\n",
      "trainer/Log Pis Min                                    -8.75823\n",
      "trainer/Policy mu Mean                                 -0.0448002\n",
      "trainer/Policy mu Std                                   0.268204\n",
      "trainer/Policy mu Max                                   0.900727\n",
      "trainer/Policy mu Min                                  -1.28695\n",
      "trainer/Policy log std Mean                            -0.960009\n",
      "trainer/Policy log std Std                              0.216713\n",
      "trainer/Policy log std Max                             -0.375564\n",
      "trainer/Policy log std Min                             -1.50291\n",
      "trainer/Alpha                                           0.0529586\n",
      "trainer/Alpha Loss                                    -29.2832\n",
      "exploration/num steps total                         12000\n",
      "exploration/num paths total                           106\n",
      "exploration/path length Mean                          333.333\n",
      "exploration/path length Std                           248.952\n",
      "exploration/path length Max                           676\n",
      "exploration/path length Min                            92\n",
      "exploration/Rewards Mean                                0.457874\n",
      "exploration/Rewards Std                                 0.329527\n",
      "exploration/Rewards Max                                 2.63095\n",
      "exploration/Rewards Min                                -0.767457\n",
      "exploration/Returns Mean                              152.625\n",
      "exploration/Returns Std                               100.615\n",
      "exploration/Returns Max                               290.322\n",
      "exploration/Returns Min                                52.719\n",
      "exploration/Actions Mean                               -0.0383625\n",
      "exploration/Actions Std                                 0.383061\n",
      "exploration/Actions Max                                 0.950907\n",
      "exploration/Actions Min                                -0.946427\n",
      "exploration/Num Paths                                   3\n",
      "exploration/Average Returns                           152.625\n",
      "exploration/env_infos/final/reward_forward Mean         0.414369\n",
      "exploration/env_infos/final/reward_forward Std          0.817636\n",
      "exploration/env_infos/final/reward_forward Max          1.28776\n",
      "exploration/env_infos/final/reward_forward Min         -0.678593\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00564109\n",
      "exploration/env_infos/initial/reward_forward Std        0.0748328\n",
      "exploration/env_infos/initial/reward_forward Max        0.110117\n",
      "exploration/env_infos/initial/reward_forward Min       -0.06121\n",
      "exploration/env_infos/reward_forward Mean               0.0545963\n",
      "exploration/env_infos/reward_forward Std                0.561276\n",
      "exploration/env_infos/reward_forward Max                2.97066\n",
      "exploration/env_infos/reward_forward Min               -1.81569\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.538367\n",
      "exploration/env_infos/final/reward_ctrl Std             0.197914\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.273754\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.749669\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.634201\n",
      "exploration/env_infos/initial/reward_ctrl Std           0.336684\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.302633\n",
      "exploration/env_infos/initial/reward_ctrl Min          -1.09593\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.592831\n",
      "exploration/env_infos/reward_ctrl Std                   0.246504\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0804873\n",
      "exploration/env_infos/reward_ctrl Min                  -1.76746\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.530082\n",
      "exploration/env_infos/final/torso_velocity Std          0.671764\n",
      "exploration/env_infos/final/torso_velocity Max          1.54973\n",
      "exploration/env_infos/final/torso_velocity Min         -0.678593\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0919986\n",
      "exploration/env_infos/initial/torso_velocity Std        0.21568\n",
      "exploration/env_infos/initial/torso_velocity Max        0.384895\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.250554\n",
      "exploration/env_infos/torso_velocity Mean              -0.00686038\n",
      "exploration/env_infos/torso_velocity Std                0.540387\n",
      "exploration/env_infos/torso_velocity Max                2.97066\n",
      "exploration/env_infos/torso_velocity Min               -2.44799\n",
      "evaluation/num steps total                         275000\n",
      "evaluation/num paths total                            275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.915625\n",
      "evaluation/Rewards Std                                  0.0555865\n",
      "evaluation/Rewards Max                                  2.20285\n",
      "evaluation/Rewards Min                                  0.360569\n",
      "evaluation/Returns Mean                               915.625\n",
      "evaluation/Returns Std                                 26.1796\n",
      "evaluation/Returns Max                                933.26\n",
      "evaluation/Returns Min                                799.773\n",
      "evaluation/Actions Mean                                 0.0181337\n",
      "evaluation/Actions Std                                  0.146218\n",
      "evaluation/Actions Max                                  0.639886\n",
      "evaluation/Actions Min                                 -0.665847\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            915.625\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.000210142\n",
      "evaluation/env_infos/final/reward_forward Std           0.00102974\n",
      "evaluation/env_infos/final/reward_forward Max           5.4562e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00525484\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.01458\n",
      "evaluation/env_infos/initial/reward_forward Std         0.157393\n",
      "evaluation/env_infos/initial/reward_forward Max         0.35219\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.240871\n",
      "evaluation/env_infos/reward_forward Mean                0.00230675\n",
      "evaluation/env_infos/reward_forward Std                 0.195403\n",
      "evaluation/env_infos/reward_forward Max                 2.40869\n",
      "evaluation/env_infos/reward_forward Min                -2.25121\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0809853\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0260044\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0637939\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.199559\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.13938\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0453501\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0957397\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.27066\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0868337\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0414864\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0301744\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.639431\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -2.61569e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000684318\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00267989\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00525484\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130733\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244545\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.593988\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.331567\n",
      "evaluation/env_infos/torso_velocity Mean                0.00162877\n",
      "evaluation/env_infos/torso_velocity Std                 0.155371\n",
      "evaluation/env_infos/torso_velocity Max                 2.40869\n",
      "evaluation/env_infos/torso_velocity Min                -2.25121\n",
      "time/data storing (s)                                   0.372183\n",
      "time/evaluation sampling (s)                           44.0649\n",
      "time/exploration sampling (s)                           1.91412\n",
      "time/logging (s)                                        0.277849\n",
      "time/saving (s)                                         0.0274978\n",
      "time/training (s)                                       3.73441\n",
      "time/epoch (s)                                         50.391\n",
      "time/total (s)                                        576.7\n",
      "Epoch                                                  10\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:34:33.358345 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 11 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  25898\n",
      "trainer/QF1 Loss                                        0.629497\n",
      "trainer/QF2 Loss                                        0.628642\n",
      "trainer/Policy Loss                                    -2.98993\n",
      "trainer/Q1 Predictions Mean                             3.63928\n",
      "trainer/Q1 Predictions Std                              0.718234\n",
      "trainer/Q1 Predictions Max                              5.92428\n",
      "trainer/Q1 Predictions Min                              1.6866\n",
      "trainer/Q2 Predictions Mean                             3.66859\n",
      "trainer/Q2 Predictions Std                              0.72409\n",
      "trainer/Q2 Predictions Max                              6.3401\n",
      "trainer/Q2 Predictions Min                              1.6063\n",
      "trainer/Q Targets Mean                                  3.78505\n",
      "trainer/Q Targets Std                                   1.07509\n",
      "trainer/Q Targets Max                                   7.63951\n",
      "trainer/Q Targets Min                                  -0.738386\n",
      "trainer/Log Pis Mean                                    1.25351\n",
      "trainer/Log Pis Std                                     2.11695\n",
      "trainer/Log Pis Max                                     5.30205\n",
      "trainer/Log Pis Min                                    -7.41946\n",
      "trainer/Policy mu Mean                                 -0.0159539\n",
      "trainer/Policy mu Std                                   0.179897\n",
      "trainer/Policy mu Max                                   0.89759\n",
      "trainer/Policy mu Min                                  -0.86132\n",
      "trainer/Policy log std Mean                            -1.48566\n",
      "trainer/Policy log std Std                              0.215168\n",
      "trainer/Policy log std Max                             -0.762562\n",
      "trainer/Policy log std Min                             -2.03414\n",
      "trainer/Alpha                                           0.0433713\n",
      "trainer/Alpha Loss                                    -21.1587\n",
      "exploration/num steps total                         13000\n",
      "exploration/num paths total                           107\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.81791\n",
      "exploration/Rewards Std                                 0.190484\n",
      "exploration/Rewards Max                                 2.0848\n",
      "exploration/Rewards Min                                 0.145131\n",
      "exploration/Returns Mean                              817.91\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               817.91\n",
      "exploration/Returns Min                               817.91\n",
      "exploration/Actions Mean                               -0.0106782\n",
      "exploration/Actions Std                                 0.234745\n",
      "exploration/Actions Max                                 0.752368\n",
      "exploration/Actions Min                                -0.817483\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           817.91\n",
      "exploration/env_infos/final/reward_forward Mean         0.0565867\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0565867\n",
      "exploration/env_infos/final/reward_forward Min          0.0565867\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0393555\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0393555\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0393555\n",
      "exploration/env_infos/reward_forward Mean              -0.0122318\n",
      "exploration/env_infos/reward_forward Std                0.455383\n",
      "exploration/env_infos/reward_forward Max                1.15783\n",
      "exploration/env_infos/reward_forward Min               -1.61146\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.265358\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.265358\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.265358\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.10266\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.10266\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.10266\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.220878\n",
      "exploration/env_infos/reward_ctrl Std                   0.111401\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00630899\n",
      "exploration/env_infos/reward_ctrl Min                  -0.854869\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0105973\n",
      "exploration/env_infos/final/torso_velocity Std          0.0833172\n",
      "exploration/env_infos/final/torso_velocity Max          0.0565867\n",
      "exploration/env_infos/final/torso_velocity Min         -0.128019\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.174686\n",
      "exploration/env_infos/initial/torso_velocity Std        0.266625\n",
      "exploration/env_infos/initial/torso_velocity Max        0.550544\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0393555\n",
      "exploration/env_infos/torso_velocity Mean              -0.0183271\n",
      "exploration/env_infos/torso_velocity Std                0.486939\n",
      "exploration/env_infos/torso_velocity Max                2.00098\n",
      "exploration/env_infos/torso_velocity Min               -2.00537\n",
      "evaluation/num steps total                         300000\n",
      "evaluation/num paths total                            300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.988032\n",
      "evaluation/Rewards Std                                  0.078204\n",
      "evaluation/Rewards Max                                  2.12183\n",
      "evaluation/Rewards Min                                  0.784595\n",
      "evaluation/Returns Mean                               988.032\n",
      "evaluation/Returns Std                                 19.403\n",
      "evaluation/Returns Max                               1066.93\n",
      "evaluation/Returns Min                                970.654\n",
      "evaluation/Actions Mean                                 0.0163147\n",
      "evaluation/Actions Std                                  0.0787733\n",
      "evaluation/Actions Max                                  0.461936\n",
      "evaluation/Actions Min                                 -0.542331\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            988.032\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0178313\n",
      "evaluation/env_infos/final/reward_forward Std           0.125587\n",
      "evaluation/env_infos/final/reward_forward Max           0.379451\n",
      "evaluation/env_infos/final/reward_forward Min          -0.372756\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0257976\n",
      "evaluation/env_infos/initial/reward_forward Std         0.126794\n",
      "evaluation/env_infos/initial/reward_forward Max         0.210496\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.219296\n",
      "evaluation/env_infos/reward_forward Mean                0.00465616\n",
      "evaluation/env_infos/reward_forward Std                 0.19066\n",
      "evaluation/env_infos/reward_forward Max                 1.31456\n",
      "evaluation/env_infos/reward_forward Min                -1.44761\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0233811\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00952056\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0153479\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0595021\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0340381\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0148089\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0119482\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0584456\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0258856\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0157843\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00645215\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.22999\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00231655\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0931854\n",
      "evaluation/env_infos/final/torso_velocity Max           0.379451\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.372756\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.162717\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218244\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.592746\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.294193\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00145289\n",
      "evaluation/env_infos/torso_velocity Std                 0.163647\n",
      "evaluation/env_infos/torso_velocity Max                 1.31456\n",
      "evaluation/env_infos/torso_velocity Min                -1.96166\n",
      "time/data storing (s)                                   0.333958\n",
      "time/evaluation sampling (s)                           44.8851\n",
      "time/exploration sampling (s)                           1.85026\n",
      "time/logging (s)                                        0.284591\n",
      "time/saving (s)                                         0.0272461\n",
      "time/training (s)                                       3.69222\n",
      "time/epoch (s)                                         51.0733\n",
      "time/total (s)                                        628.016\n",
      "Epoch                                                  11\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:35:24.136058 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 12 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  27898\n",
      "trainer/QF1 Loss                                        0.314617\n",
      "trainer/QF2 Loss                                        0.292678\n",
      "trainer/Policy Loss                                    -1.65575\n",
      "trainer/Q1 Predictions Mean                             3.87759\n",
      "trainer/Q1 Predictions Std                              0.79004\n",
      "trainer/Q1 Predictions Max                              7.92674\n",
      "trainer/Q1 Predictions Min                              2.28313\n",
      "trainer/Q2 Predictions Mean                             3.91896\n",
      "trainer/Q2 Predictions Std                              0.809434\n",
      "trainer/Q2 Predictions Max                              7.86731\n",
      "trainer/Q2 Predictions Min                              2.11159\n",
      "trainer/Q Targets Mean                                  4.03256\n",
      "trainer/Q Targets Std                                   0.991639\n",
      "trainer/Q Targets Max                                   9.96589\n",
      "trainer/Q Targets Min                                   0.828013\n",
      "trainer/Log Pis Mean                                    3.04403\n",
      "trainer/Log Pis Std                                     2.21039\n",
      "trainer/Log Pis Max                                     8.84864\n",
      "trainer/Log Pis Min                                    -6.59655\n",
      "trainer/Policy mu Mean                                 -0.0202438\n",
      "trainer/Policy mu Std                                   0.16442\n",
      "trainer/Policy mu Max                                   0.584074\n",
      "trainer/Policy mu Min                                  -0.835329\n",
      "trainer/Policy log std Mean                            -1.76972\n",
      "trainer/Policy log std Std                              0.196508\n",
      "trainer/Policy log std Max                             -1.10381\n",
      "trainer/Policy log std Min                             -2.42232\n",
      "trainer/Alpha                                           0.0375355\n",
      "trainer/Alpha Loss                                    -16.2614\n",
      "exploration/num steps total                         14000\n",
      "exploration/num paths total                           108\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.902311\n",
      "exploration/Rewards Std                                 0.147051\n",
      "exploration/Rewards Max                                 2.10785\n",
      "exploration/Rewards Min                                 0.531136\n",
      "exploration/Returns Mean                              902.311\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               902.311\n",
      "exploration/Returns Min                               902.311\n",
      "exploration/Actions Mean                               -0.03146\n",
      "exploration/Actions Std                                 0.179077\n",
      "exploration/Actions Max                                 0.630083\n",
      "exploration/Actions Min                                -0.639489\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           902.311\n",
      "exploration/env_infos/final/reward_forward Mean        -0.3853\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.3853\n",
      "exploration/env_infos/final/reward_forward Min         -0.3853\n",
      "exploration/env_infos/initial/reward_forward Mean       0.104683\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.104683\n",
      "exploration/env_infos/initial/reward_forward Min        0.104683\n",
      "exploration/env_infos/reward_forward Mean              -0.0143559\n",
      "exploration/env_infos/reward_forward Std                0.391659\n",
      "exploration/env_infos/reward_forward Max                1.76996\n",
      "exploration/env_infos/reward_forward Min               -1.49667\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.108556\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.108556\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.108556\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.155043\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.155043\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.155043\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.132234\n",
      "exploration/env_infos/reward_ctrl Std                   0.066031\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0141423\n",
      "exploration/env_infos/reward_ctrl Min                  -0.468864\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.140698\n",
      "exploration/env_infos/final/torso_velocity Std          0.174695\n",
      "exploration/env_infos/final/torso_velocity Max          0.0116842\n",
      "exploration/env_infos/final/torso_velocity Min         -0.3853\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0909847\n",
      "exploration/env_infos/initial/torso_velocity Std        0.305596\n",
      "exploration/env_infos/initial/torso_velocity Max        0.458224\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.289953\n",
      "exploration/env_infos/torso_velocity Mean              -0.0196198\n",
      "exploration/env_infos/torso_velocity Std                0.403364\n",
      "exploration/env_infos/torso_velocity Max                1.85735\n",
      "exploration/env_infos/torso_velocity Min               -1.79608\n",
      "evaluation/num steps total                         325000\n",
      "evaluation/num paths total                            325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.967751\n",
      "evaluation/Rewards Std                                  0.0269436\n",
      "evaluation/Rewards Max                                  2.60959\n",
      "evaluation/Rewards Min                                  0.728364\n",
      "evaluation/Returns Mean                               967.751\n",
      "evaluation/Returns Std                                  2.76051\n",
      "evaluation/Returns Max                                973.864\n",
      "evaluation/Returns Min                                961.33\n",
      "evaluation/Actions Mean                                -0.0137101\n",
      "evaluation/Actions Std                                  0.0897382\n",
      "evaluation/Actions Max                                  0.496933\n",
      "evaluation/Actions Min                                 -0.528228\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            967.751\n",
      "evaluation/env_infos/final/reward_forward Mean          7.62313e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.14417e-07\n",
      "evaluation/env_infos/final/reward_forward Max           5.60971e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.17092e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0210973\n",
      "evaluation/env_infos/initial/reward_forward Std         0.132457\n",
      "evaluation/env_infos/initial/reward_forward Max         0.226346\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.283088\n",
      "evaluation/env_infos/reward_forward Mean               -0.000503098\n",
      "evaluation/env_infos/reward_forward Std                 0.0691441\n",
      "evaluation/env_infos/reward_forward Max                 1.5259\n",
      "evaluation/env_infos/reward_forward Min                -2.07928\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0323607\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0023919\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.029137\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0381105\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.035562\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.011183\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0209203\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0647375\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0329636\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00797485\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.015575\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.271636\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -8.60406e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.43367e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.60971e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -5.35667e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.164863\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.248344\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.726137\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.283088\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00248795\n",
      "evaluation/env_infos/torso_velocity Std                 0.0715125\n",
      "evaluation/env_infos/torso_velocity Max                 1.60891\n",
      "evaluation/env_infos/torso_velocity Min                -2.07928\n",
      "time/data storing (s)                                   0.329809\n",
      "time/evaluation sampling (s)                           44.1212\n",
      "time/exploration sampling (s)                           2.11438\n",
      "time/logging (s)                                        0.283531\n",
      "time/saving (s)                                         0.0277148\n",
      "time/training (s)                                       3.63543\n",
      "time/epoch (s)                                         50.512\n",
      "time/total (s)                                        678.792\n",
      "Epoch                                                  12\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:36:16.538361 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 13 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  29898\n",
      "trainer/QF1 Loss                                        0.373483\n",
      "trainer/QF2 Loss                                        0.326564\n",
      "trainer/Policy Loss                                    -1.10294\n",
      "trainer/Q1 Predictions Mean                             4.35594\n",
      "trainer/Q1 Predictions Std                              0.792363\n",
      "trainer/Q1 Predictions Max                              6.87973\n",
      "trainer/Q1 Predictions Min                              2.29526\n",
      "trainer/Q2 Predictions Mean                             4.37498\n",
      "trainer/Q2 Predictions Std                              0.819854\n",
      "trainer/Q2 Predictions Max                              6.70069\n",
      "trainer/Q2 Predictions Min                              2.03683\n",
      "trainer/Q Targets Mean                                  4.24977\n",
      "trainer/Q Targets Std                                   0.944913\n",
      "trainer/Q Targets Max                                   8.24851\n",
      "trainer/Q Targets Min                                  -0.901142\n",
      "trainer/Log Pis Mean                                    4.16686\n",
      "trainer/Log Pis Std                                     2.07953\n",
      "trainer/Log Pis Max                                     8.54244\n",
      "trainer/Log Pis Min                                    -2.79266\n",
      "trainer/Policy mu Mean                                 -0.0142141\n",
      "trainer/Policy mu Std                                   0.136443\n",
      "trainer/Policy mu Max                                   0.571637\n",
      "trainer/Policy mu Min                                  -0.59301\n",
      "trainer/Policy log std Mean                            -1.88947\n",
      "trainer/Policy log std Std                              0.19364\n",
      "trainer/Policy log std Max                             -1.27887\n",
      "trainer/Policy log std Min                             -2.40921\n",
      "trainer/Alpha                                           0.0334125\n",
      "trainer/Alpha Loss                                    -13.0241\n",
      "exploration/num steps total                         15000\n",
      "exploration/num paths total                           109\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938356\n",
      "exploration/Rewards Std                                 0.143321\n",
      "exploration/Rewards Max                                 2.00578\n",
      "exploration/Rewards Min                                 0.678028\n",
      "exploration/Returns Mean                              938.356\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.356\n",
      "exploration/Returns Min                               938.356\n",
      "exploration/Actions Mean                               -0.0224913\n",
      "exploration/Actions Std                                 0.153498\n",
      "exploration/Actions Max                                 0.531285\n",
      "exploration/Actions Min                                -0.53591\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.356\n",
      "exploration/env_infos/final/reward_forward Mean        -0.310058\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.310058\n",
      "exploration/env_infos/final/reward_forward Min         -0.310058\n",
      "exploration/env_infos/initial/reward_forward Mean       0.101752\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.101752\n",
      "exploration/env_infos/initial/reward_forward Min        0.101752\n",
      "exploration/env_infos/reward_forward Mean              -0.00704552\n",
      "exploration/env_infos/reward_forward Std                0.397773\n",
      "exploration/env_infos/reward_forward Max                1.47206\n",
      "exploration/env_infos/reward_forward Min               -1.36454\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.13175\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.13175\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.13175\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0579659\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0579659\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0579659\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0962705\n",
      "exploration/env_infos/reward_ctrl Std                   0.0477922\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0131664\n",
      "exploration/env_infos/reward_ctrl Min                  -0.321972\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.359252\n",
      "exploration/env_infos/final/torso_velocity Std          0.164373\n",
      "exploration/env_infos/final/torso_velocity Max         -0.187094\n",
      "exploration/env_infos/final/torso_velocity Min         -0.580605\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.294197\n",
      "exploration/env_infos/initial/torso_velocity Std        0.254476\n",
      "exploration/env_infos/initial/torso_velocity Max        0.653785\n",
      "exploration/env_infos/initial/torso_velocity Min        0.101752\n",
      "exploration/env_infos/torso_velocity Mean              -0.0133887\n",
      "exploration/env_infos/torso_velocity Std                0.345845\n",
      "exploration/env_infos/torso_velocity Max                1.55508\n",
      "exploration/env_infos/torso_velocity Min               -1.61183\n",
      "evaluation/num steps total                         350000\n",
      "evaluation/num paths total                            350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.994909\n",
      "evaluation/Rewards Std                                  0.101746\n",
      "evaluation/Rewards Max                                  2.10769\n",
      "evaluation/Rewards Min                                  0.819391\n",
      "evaluation/Returns Mean                               994.909\n",
      "evaluation/Returns Std                                 33.8569\n",
      "evaluation/Returns Max                               1084.6\n",
      "evaluation/Returns Min                                968.805\n",
      "evaluation/Actions Mean                                -0.0212159\n",
      "evaluation/Actions Std                                  0.0828845\n",
      "evaluation/Actions Max                                  0.328618\n",
      "evaluation/Actions Min                                 -0.387148\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            994.909\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0837632\n",
      "evaluation/env_infos/final/reward_forward Std           0.280018\n",
      "evaluation/env_infos/final/reward_forward Max           0.536246\n",
      "evaluation/env_infos/final/reward_forward Min          -0.511089\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00733696\n",
      "evaluation/env_infos/initial/reward_forward Std         0.10382\n",
      "evaluation/env_infos/initial/reward_forward Max         0.188953\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.290073\n",
      "evaluation/env_infos/reward_forward Mean               -0.000595178\n",
      "evaluation/env_infos/reward_forward Std                 0.317714\n",
      "evaluation/env_infos/reward_forward Max                 1.30675\n",
      "evaluation/env_infos/reward_forward Min                -1.28517\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0275165\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00557441\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0188225\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0435951\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0233694\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0203257\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00668655\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0773105\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0292798\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00799876\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00668655\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.180609\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0272013\n",
      "evaluation/env_infos/final/torso_velocity Std           0.195212\n",
      "evaluation/env_infos/final/torso_velocity Max           0.577276\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.511089\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.122568\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.227608\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.648766\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.290073\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00973588\n",
      "evaluation/env_infos/torso_velocity Std                 0.234925\n",
      "evaluation/env_infos/torso_velocity Max                 1.30675\n",
      "evaluation/env_infos/torso_velocity Min                -1.75865\n",
      "time/data storing (s)                                   0.315379\n",
      "time/evaluation sampling (s)                           45.9038\n",
      "time/exploration sampling (s)                           1.83751\n",
      "time/logging (s)                                        0.283439\n",
      "time/saving (s)                                         0.0260243\n",
      "time/training (s)                                       3.76662\n",
      "time/epoch (s)                                         52.1328\n",
      "time/total (s)                                        731.194\n",
      "Epoch                                                  13\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:37:08.194194 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 14 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  31898\n",
      "trainer/QF1 Loss                                        0.438303\n",
      "trainer/QF2 Loss                                        0.409085\n",
      "trainer/Policy Loss                                    -0.96586\n",
      "trainer/Q1 Predictions Mean                             4.49312\n",
      "trainer/Q1 Predictions Std                              0.898379\n",
      "trainer/Q1 Predictions Max                              8.03226\n",
      "trainer/Q1 Predictions Min                              2.34739\n",
      "trainer/Q2 Predictions Mean                             4.5098\n",
      "trainer/Q2 Predictions Std                              0.88944\n",
      "trainer/Q2 Predictions Max                              8.36783\n",
      "trainer/Q2 Predictions Min                              2.21963\n",
      "trainer/Q Targets Mean                                  4.60809\n",
      "trainer/Q Targets Std                                   1.03541\n",
      "trainer/Q Targets Max                                   7.97544\n",
      "trainer/Q Targets Min                                  -0.738386\n",
      "trainer/Log Pis Mean                                    4.38836\n",
      "trainer/Log Pis Std                                     2.47718\n",
      "trainer/Log Pis Max                                     9.12454\n",
      "trainer/Log Pis Min                                    -4.6088\n",
      "trainer/Policy mu Mean                                 -0.00208276\n",
      "trainer/Policy mu Std                                   0.11999\n",
      "trainer/Policy mu Max                                   0.522537\n",
      "trainer/Policy mu Min                                  -0.53486\n",
      "trainer/Policy log std Mean                            -1.95063\n",
      "trainer/Policy log std Std                              0.193974\n",
      "trainer/Policy log std Max                             -1.18238\n",
      "trainer/Policy log std Min                             -2.54964\n",
      "trainer/Alpha                                           0.030191\n",
      "trainer/Alpha Loss                                    -12.6379\n",
      "exploration/num steps total                         16000\n",
      "exploration/num paths total                           110\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.956221\n",
      "exploration/Rewards Std                                 0.118066\n",
      "exploration/Rewards Max                                 1.87369\n",
      "exploration/Rewards Min                                 0.620225\n",
      "exploration/Returns Mean                              956.221\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               956.221\n",
      "exploration/Returns Min                               956.221\n",
      "exploration/Actions Mean                               -0.0112559\n",
      "exploration/Actions Std                                 0.138868\n",
      "exploration/Actions Max                                 0.480199\n",
      "exploration/Actions Min                                -0.523456\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           956.221\n",
      "exploration/env_infos/final/reward_forward Mean         0.0164082\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0164082\n",
      "exploration/env_infos/final/reward_forward Min          0.0164082\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.202674\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.202674\n",
      "exploration/env_infos/initial/reward_forward Min       -0.202674\n",
      "exploration/env_infos/reward_forward Mean              -0.0351217\n",
      "exploration/env_infos/reward_forward Std                0.221881\n",
      "exploration/env_infos/reward_forward Max                1.09099\n",
      "exploration/env_infos/reward_forward Min               -0.876212\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0881703\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0881703\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0881703\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0475767\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0475767\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0475767\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0776442\n",
      "exploration/env_infos/reward_ctrl Std                   0.0399574\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0067247\n",
      "exploration/env_infos/reward_ctrl Min                  -0.379775\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.110993\n",
      "exploration/env_infos/final/torso_velocity Std          0.141507\n",
      "exploration/env_infos/final/torso_velocity Max          0.311016\n",
      "exploration/env_infos/final/torso_velocity Min          0.00555616\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.148919\n",
      "exploration/env_infos/initial/torso_velocity Std        0.253949\n",
      "exploration/env_infos/initial/torso_velocity Max        0.388132\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.202674\n",
      "exploration/env_infos/torso_velocity Mean              -0.0241539\n",
      "exploration/env_infos/torso_velocity Std                0.247595\n",
      "exploration/env_infos/torso_velocity Max                1.09099\n",
      "exploration/env_infos/torso_velocity Min               -1.5057\n",
      "evaluation/num steps total                         375000\n",
      "evaluation/num paths total                            375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.980539\n",
      "evaluation/Rewards Std                                  0.026346\n",
      "evaluation/Rewards Max                                  2.66991\n",
      "evaluation/Rewards Min                                  0.828862\n",
      "evaluation/Returns Mean                               980.539\n",
      "evaluation/Returns Std                                  2.62527\n",
      "evaluation/Returns Max                                985.864\n",
      "evaluation/Returns Min                                976.551\n",
      "evaluation/Actions Mean                                -0.0218321\n",
      "evaluation/Actions Std                                  0.0677616\n",
      "evaluation/Actions Max                                  0.269577\n",
      "evaluation/Actions Min                                 -0.477821\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            980.539\n",
      "evaluation/env_infos/final/reward_forward Mean          1.74093e-07\n",
      "evaluation/env_infos/final/reward_forward Std           6.95315e-07\n",
      "evaluation/env_infos/final/reward_forward Max           2.24396e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -8.44553e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.050482\n",
      "evaluation/env_infos/initial/reward_forward Std         0.118717\n",
      "evaluation/env_infos/initial/reward_forward Max         0.310913\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.164778\n",
      "evaluation/env_infos/reward_forward Mean                0.00244526\n",
      "evaluation/env_infos/reward_forward Std                 0.0669399\n",
      "evaluation/env_infos/reward_forward Max                 1.36974\n",
      "evaluation/env_infos/reward_forward Min                -1.33631\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0201074\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00214916\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0154517\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0233749\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.00831674\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00690991\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00217536\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.027841\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0202731\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00479069\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00217536\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.171138\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.6099e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           6.8599e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           2.62218e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -2.70046e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.153028\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218828\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.737995\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.272898\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00180695\n",
      "evaluation/env_infos/torso_velocity Std                 0.0669451\n",
      "evaluation/env_infos/torso_velocity Max                 1.36974\n",
      "evaluation/env_infos/torso_velocity Min                -1.7321\n",
      "time/data storing (s)                                   0.339544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time/evaluation sampling (s)                           44.8626\n",
      "time/exploration sampling (s)                           1.97624\n",
      "time/logging (s)                                        0.2745\n",
      "time/saving (s)                                         0.0260533\n",
      "time/training (s)                                       3.88769\n",
      "time/epoch (s)                                         51.3667\n",
      "time/total (s)                                        782.84\n",
      "Epoch                                                  14\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:37:59.749113 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 15 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  33898\n",
      "trainer/QF1 Loss                                        0.340557\n",
      "trainer/QF2 Loss                                        0.370858\n",
      "trainer/Policy Loss                                    -0.890931\n",
      "trainer/Q1 Predictions Mean                             5.00994\n",
      "trainer/Q1 Predictions Std                              0.977982\n",
      "trainer/Q1 Predictions Max                              8.46271\n",
      "trainer/Q1 Predictions Min                              2.24595\n",
      "trainer/Q2 Predictions Mean                             5.00213\n",
      "trainer/Q2 Predictions Std                              1.00442\n",
      "trainer/Q2 Predictions Max                              7.44841\n",
      "trainer/Q2 Predictions Min                              2.50058\n",
      "trainer/Q Targets Mean                                  5.02623\n",
      "trainer/Q Targets Std                                   1.10393\n",
      "trainer/Q Targets Max                                   7.66153\n",
      "trainer/Q Targets Min                                  -0.698472\n",
      "trainer/Log Pis Mean                                    4.92627\n",
      "trainer/Log Pis Std                                     2.18739\n",
      "trainer/Log Pis Max                                     9.75372\n",
      "trainer/Log Pis Min                                    -4.61951\n",
      "trainer/Policy mu Mean                                 -0.00708693\n",
      "trainer/Policy mu Std                                   0.143338\n",
      "trainer/Policy mu Max                                   0.597415\n",
      "trainer/Policy mu Min                                  -0.607115\n",
      "trainer/Policy log std Mean                            -1.98081\n",
      "trainer/Policy log std Std                              0.179931\n",
      "trainer/Policy log std Max                             -1.01741\n",
      "trainer/Policy log std Min                             -2.5457\n",
      "trainer/Alpha                                           0.0274323\n",
      "trainer/Alpha Loss                                    -11.0505\n",
      "exploration/num steps total                         17000\n",
      "exploration/num paths total                           111\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.901316\n",
      "exploration/Rewards Std                                 0.0610151\n",
      "exploration/Rewards Max                                 1.36662\n",
      "exploration/Rewards Min                                 0.667308\n",
      "exploration/Returns Mean                              901.316\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               901.316\n",
      "exploration/Returns Min                               901.316\n",
      "exploration/Actions Mean                               -0.0365034\n",
      "exploration/Actions Std                                 0.158397\n",
      "exploration/Actions Max                                 0.596147\n",
      "exploration/Actions Min                                -0.600281\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           901.316\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0201911\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0201911\n",
      "exploration/env_infos/final/reward_forward Min         -0.0201911\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00381427\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00381427\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00381427\n",
      "exploration/env_infos/reward_forward Mean              -0.0106816\n",
      "exploration/env_infos/reward_forward Std                0.236485\n",
      "exploration/env_infos/reward_forward Max                1.11241\n",
      "exploration/env_infos/reward_forward Min               -0.782265\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.209734\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.209734\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.209734\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0844586\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0844586\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0844586\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.105688\n",
      "exploration/env_infos/reward_ctrl Std                   0.0467975\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0162398\n",
      "exploration/env_infos/reward_ctrl Min                  -0.332692\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0465733\n",
      "exploration/env_infos/final/torso_velocity Std          0.05511\n",
      "exploration/env_infos/final/torso_velocity Max          0.114777\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0201911\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.00667242\n",
      "exploration/env_infos/initial/torso_velocity Std        0.134443\n",
      "exploration/env_infos/initial/torso_velocity Max        0.176324\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.152492\n",
      "exploration/env_infos/torso_velocity Mean              -0.0152328\n",
      "exploration/env_infos/torso_velocity Std                0.193687\n",
      "exploration/env_infos/torso_velocity Max                1.11241\n",
      "exploration/env_infos/torso_velocity Min               -1.50007\n",
      "evaluation/num steps total                         400000\n",
      "evaluation/num paths total                            400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.947234\n",
      "evaluation/Rewards Std                                  0.0223932\n",
      "evaluation/Rewards Max                                  2.1703\n",
      "evaluation/Rewards Min                                  0.774969\n",
      "evaluation/Returns Mean                               947.234\n",
      "evaluation/Returns Std                                  5.14395\n",
      "evaluation/Returns Max                                962.795\n",
      "evaluation/Returns Min                                940.125\n",
      "evaluation/Actions Mean                                -0.0408555\n",
      "evaluation/Actions Std                                  0.108199\n",
      "evaluation/Actions Max                                  0.291613\n",
      "evaluation/Actions Min                                 -0.566172\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            947.234\n",
      "evaluation/env_infos/final/reward_forward Mean          2.53628e-07\n",
      "evaluation/env_infos/final/reward_forward Std           5.26991e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.06457e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.08409e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0627343\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0937122\n",
      "evaluation/env_infos/initial/reward_forward Max         0.132214\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.241751\n",
      "evaluation/env_infos/reward_forward Mean                0.00451976\n",
      "evaluation/env_infos/reward_forward Std                 0.0754555\n",
      "evaluation/env_infos/reward_forward Max                 1.64057\n",
      "evaluation/env_infos/reward_forward Min                -0.867274\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.053392\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0049302\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0385959\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0598444\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0172356\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00721269\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00974875\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0368447\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0535044\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00757206\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00974875\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.233765\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.03947e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           3.37902e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.06457e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.08409e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.106477\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258814\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.765489\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310319\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000567467\n",
      "evaluation/env_infos/torso_velocity Std                 0.0686992\n",
      "evaluation/env_infos/torso_velocity Max                 1.64057\n",
      "evaluation/env_infos/torso_velocity Min                -1.76881\n",
      "time/data storing (s)                                   0.333716\n",
      "time/evaluation sampling (s)                           44.6497\n",
      "time/exploration sampling (s)                           2.11036\n",
      "time/logging (s)                                        0.271071\n",
      "time/saving (s)                                         0.0265713\n",
      "time/training (s)                                       3.88202\n",
      "time/epoch (s)                                         51.2734\n",
      "time/total (s)                                        834.391\n",
      "Epoch                                                  15\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:38:50.575731 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 16 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  35898\n",
      "trainer/QF1 Loss                                        0.368239\n",
      "trainer/QF2 Loss                                        0.400337\n",
      "trainer/Policy Loss                                    -0.561885\n",
      "trainer/Q1 Predictions Mean                             5.72147\n",
      "trainer/Q1 Predictions Std                              0.937824\n",
      "trainer/Q1 Predictions Max                              8.06231\n",
      "trainer/Q1 Predictions Min                              3.36045\n",
      "trainer/Q2 Predictions Mean                             5.77215\n",
      "trainer/Q2 Predictions Std                              0.928177\n",
      "trainer/Q2 Predictions Max                              8.24002\n",
      "trainer/Q2 Predictions Min                              3.53718\n",
      "trainer/Q Targets Mean                                  5.66483\n",
      "trainer/Q Targets Std                                   1.18556\n",
      "trainer/Q Targets Max                                   9.39941\n",
      "trainer/Q Targets Min                                  -0.130904\n",
      "trainer/Log Pis Mean                                    5.91422\n",
      "trainer/Log Pis Std                                     2.20255\n",
      "trainer/Log Pis Max                                    10.9932\n",
      "trainer/Log Pis Min                                    -1.12611\n",
      "trainer/Policy mu Mean                                 -0.0070317\n",
      "trainer/Policy mu Std                                   0.134868\n",
      "trainer/Policy mu Max                                   0.679944\n",
      "trainer/Policy mu Min                                  -0.505882\n",
      "trainer/Policy log std Mean                            -2.1177\n",
      "trainer/Policy log std Std                              0.163174\n",
      "trainer/Policy log std Max                             -1.40684\n",
      "trainer/Policy log std Min                             -2.83177\n",
      "trainer/Alpha                                           0.025237\n",
      "trainer/Alpha Loss                                     -7.67284\n",
      "exploration/num steps total                         18000\n",
      "exploration/num paths total                           112\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.934978\n",
      "exploration/Rewards Std                                 0.0879806\n",
      "exploration/Rewards Max                                 1.53643\n",
      "exploration/Rewards Min                                 0.695596\n",
      "exploration/Returns Mean                              934.978\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               934.978\n",
      "exploration/Returns Min                               934.978\n",
      "exploration/Actions Mean                               -0.0114345\n",
      "exploration/Actions Std                                 0.141393\n",
      "exploration/Actions Max                                 0.513733\n",
      "exploration/Actions Min                                -0.481291\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           934.978\n",
      "exploration/env_infos/final/reward_forward Mean        -0.539305\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.539305\n",
      "exploration/env_infos/final/reward_forward Min         -0.539305\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.284924\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.284924\n",
      "exploration/env_infos/initial/reward_forward Min       -0.284924\n",
      "exploration/env_infos/reward_forward Mean               0.0027589\n",
      "exploration/env_infos/reward_forward Std                0.333469\n",
      "exploration/env_infos/reward_forward Max                1.04635\n",
      "exploration/env_infos/reward_forward Min               -0.990459\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0679869\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0679869\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0679869\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0848553\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0848553\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0848553\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0804911\n",
      "exploration/env_infos/reward_ctrl Std                   0.0407154\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0091593\n",
      "exploration/env_infos/reward_ctrl Min                  -0.304404\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0553497\n",
      "exploration/env_infos/final/torso_velocity Std          0.426356\n",
      "exploration/env_infos/final/torso_velocity Max          0.43904\n",
      "exploration/env_infos/final/torso_velocity Min         -0.539305\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0700723\n",
      "exploration/env_infos/initial/torso_velocity Std        0.27957\n",
      "exploration/env_infos/initial/torso_velocity Max        0.398308\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.284924\n",
      "exploration/env_infos/torso_velocity Mean              -0.0149247\n",
      "exploration/env_infos/torso_velocity Std                0.350142\n",
      "exploration/env_infos/torso_velocity Max                1.43569\n",
      "exploration/env_infos/torso_velocity Min               -1.77708\n",
      "evaluation/num steps total                         425000\n",
      "evaluation/num paths total                            425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.964076\n",
      "evaluation/Rewards Std                                  0.0270606\n",
      "evaluation/Rewards Max                                  2.09466\n",
      "evaluation/Rewards Min                                  0.760487\n",
      "evaluation/Returns Mean                               964.076\n",
      "evaluation/Returns Std                                  6.34753\n",
      "evaluation/Returns Max                                978.25\n",
      "evaluation/Returns Min                                956.334\n",
      "evaluation/Actions Mean                                -0.0338158\n",
      "evaluation/Actions Std                                  0.0901781\n",
      "evaluation/Actions Max                                  0.479255\n",
      "evaluation/Actions Min                                 -0.599763\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            964.076\n",
      "evaluation/env_infos/final/reward_forward Mean          1.37489e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.60119e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.03177e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.53985e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0267532\n",
      "evaluation/env_infos/initial/reward_forward Std         0.116936\n",
      "evaluation/env_infos/initial/reward_forward Max         0.20619\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.207798\n",
      "evaluation/env_infos/reward_forward Mean                0.00730166\n",
      "evaluation/env_infos/reward_forward Std                 0.0987295\n",
      "evaluation/env_infos/reward_forward Max                 1.69182\n",
      "evaluation/env_infos/reward_forward Min                -1.47487\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0367486\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0062462\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0214592\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0436758\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0231497\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0113713\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0113575\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0485524\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0371024\n",
      "evaluation/env_infos/reward_ctrl Std                    0.00978324\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00704153\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.257677\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.26508e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.20753e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.03177e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.53985e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14339\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230986\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.585292\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.273791\n",
      "evaluation/env_infos/torso_velocity Mean                0.000672744\n",
      "evaluation/env_infos/torso_velocity Std                 0.0888166\n",
      "evaluation/env_infos/torso_velocity Max                 1.69182\n",
      "evaluation/env_infos/torso_velocity Min                -1.9009\n",
      "time/data storing (s)                                   0.33928\n",
      "time/evaluation sampling (s)                           44.0617\n",
      "time/exploration sampling (s)                           1.96327\n",
      "time/logging (s)                                        0.29053\n",
      "time/saving (s)                                         0.0268349\n",
      "time/training (s)                                       3.88886\n",
      "time/epoch (s)                                         50.5705\n",
      "time/total (s)                                        885.237\n",
      "Epoch                                                  16\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:39:41.455026 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 17 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  37898\n",
      "trainer/QF1 Loss                                        0.432691\n",
      "trainer/QF2 Loss                                        0.442422\n",
      "trainer/Policy Loss                                    -0.477686\n",
      "trainer/Q1 Predictions Mean                             5.88902\n",
      "trainer/Q1 Predictions Std                              1.09212\n",
      "trainer/Q1 Predictions Max                              9.23918\n",
      "trainer/Q1 Predictions Min                              2.68762\n",
      "trainer/Q2 Predictions Mean                             5.8474\n",
      "trainer/Q2 Predictions Std                              1.07789\n",
      "trainer/Q2 Predictions Max                              9.12701\n",
      "trainer/Q2 Predictions Min                              2.88432\n",
      "trainer/Q Targets Mean                                  6.04181\n",
      "trainer/Q Targets Std                                   1.26156\n",
      "trainer/Q Targets Max                                  11.8813\n",
      "trainer/Q Targets Min                                  -0.0626612\n",
      "trainer/Log Pis Mean                                    6.10836\n",
      "trainer/Log Pis Std                                     2.14959\n",
      "trainer/Log Pis Max                                    11.4984\n",
      "trainer/Log Pis Min                                    -1.0831\n",
      "trainer/Policy mu Mean                                  0.00298827\n",
      "trainer/Policy mu Std                                   0.135525\n",
      "trainer/Policy mu Max                                   0.53307\n",
      "trainer/Policy mu Min                                  -0.723461\n",
      "trainer/Policy log std Mean                            -2.15131\n",
      "trainer/Policy log std Std                              0.148194\n",
      "trainer/Policy log std Max                             -1.53997\n",
      "trainer/Policy log std Min                             -2.67753\n",
      "trainer/Alpha                                           0.0234125\n",
      "trainer/Alpha Loss                                     -7.10079\n",
      "exploration/num steps total                         19000\n",
      "exploration/num paths total                           113\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938776\n",
      "exploration/Rewards Std                                 0.0825063\n",
      "exploration/Rewards Max                                 1.53357\n",
      "exploration/Rewards Min                                 0.505527\n",
      "exploration/Returns Mean                              938.776\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.776\n",
      "exploration/Returns Min                               938.776\n",
      "exploration/Actions Mean                               -0.0241948\n",
      "exploration/Actions Std                                 0.139782\n",
      "exploration/Actions Max                                 0.402196\n",
      "exploration/Actions Min                                -0.778841\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.776\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0568132\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0568132\n",
      "exploration/env_infos/final/reward_forward Min         -0.0568132\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0691676\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0691676\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0691676\n",
      "exploration/env_infos/reward_forward Mean              -0.0209171\n",
      "exploration/env_infos/reward_forward Std                0.177388\n",
      "exploration/env_infos/reward_forward Max                1.28076\n",
      "exploration/env_infos/reward_forward Min               -0.625197\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.126505\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.126505\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.126505\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.00920625\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.00920625\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.00920625\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0804981\n",
      "exploration/env_infos/reward_ctrl Std                   0.0405011\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00843743\n",
      "exploration/env_infos/reward_ctrl Min                  -0.494473\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0089983\n",
      "exploration/env_infos/final/torso_velocity Std          0.0603305\n",
      "exploration/env_infos/final/torso_velocity Max          0.0761051\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0568132\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.210085\n",
      "exploration/env_infos/initial/torso_velocity Std        0.287883\n",
      "exploration/env_infos/initial/torso_velocity Max        0.606281\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0691676\n",
      "exploration/env_infos/torso_velocity Mean              -0.0171407\n",
      "exploration/env_infos/torso_velocity Std                0.158906\n",
      "exploration/env_infos/torso_velocity Max                1.28076\n",
      "exploration/env_infos/torso_velocity Min               -1.60862\n",
      "evaluation/num steps total                         450000\n",
      "evaluation/num paths total                            450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.970838\n",
      "evaluation/Rewards Std                                  0.0459026\n",
      "evaluation/Rewards Max                                  2.90856\n",
      "evaluation/Rewards Min                                  0.688961\n",
      "evaluation/Returns Mean                               970.838\n",
      "evaluation/Returns Std                                  9.7238\n",
      "evaluation/Returns Max                                988.994\n",
      "evaluation/Returns Min                                953.5\n",
      "evaluation/Actions Mean                                -0.0287758\n",
      "evaluation/Actions Std                                  0.0833098\n",
      "evaluation/Actions Max                                  0.397859\n",
      "evaluation/Actions Min                                 -0.63051\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            970.838\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.99433e-07\n",
      "evaluation/env_infos/final/reward_forward Std           6.37699e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.65673e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.49042e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0270557\n",
      "evaluation/env_infos/initial/reward_forward Std         0.13966\n",
      "evaluation/env_infos/initial/reward_forward Max         0.285124\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.215184\n",
      "evaluation/env_infos/reward_forward Mean                0.00524484\n",
      "evaluation/env_infos/reward_forward Std                 0.0846566\n",
      "evaluation/env_infos/reward_forward Max                 1.62882\n",
      "evaluation/env_infos/reward_forward Min                -1.73035\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0304888\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00961551\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0120373\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0465383\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0154207\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00372359\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00837372\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0247026\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0310743\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0143189\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00700376\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.311039\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.34722e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           7.02114e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           4.52655e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.49042e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141771\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.243545\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.609612\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.215184\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00104826\n",
      "evaluation/env_infos/torso_velocity Std                 0.0818091\n",
      "evaluation/env_infos/torso_velocity Max                 1.62882\n",
      "evaluation/env_infos/torso_velocity Min                -1.78784\n",
      "time/data storing (s)                                   0.325997\n",
      "time/evaluation sampling (s)                           43.8874\n",
      "time/exploration sampling (s)                           1.95723\n",
      "time/logging (s)                                        0.283294\n",
      "time/saving (s)                                         0.026768\n",
      "time/training (s)                                       4.09914\n",
      "time/epoch (s)                                         50.5798\n",
      "time/total (s)                                        936.109\n",
      "Epoch                                                  17\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:40:35.958222 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 18 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  39898\n",
      "trainer/QF1 Loss                                        0.595118\n",
      "trainer/QF2 Loss                                        0.543043\n",
      "trainer/Policy Loss                                    -0.788982\n",
      "trainer/Q1 Predictions Mean                             6.56777\n",
      "trainer/Q1 Predictions Std                              1.07621\n",
      "trainer/Q1 Predictions Max                             11.4067\n",
      "trainer/Q1 Predictions Min                              3.51405\n",
      "trainer/Q2 Predictions Mean                             6.36873\n",
      "trainer/Q2 Predictions Std                              1.09579\n",
      "trainer/Q2 Predictions Max                             11.354\n",
      "trainer/Q2 Predictions Min                              3.55056\n",
      "trainer/Q Targets Mean                                  6.4558\n",
      "trainer/Q Targets Std                                   1.3065\n",
      "trainer/Q Targets Max                                  12.6159\n",
      "trainer/Q Targets Min                                  -0.288285\n",
      "trainer/Log Pis Mean                                    6.25719\n",
      "trainer/Log Pis Std                                     2.32609\n",
      "trainer/Log Pis Max                                    11.1968\n",
      "trainer/Log Pis Min                                    -4.46104\n",
      "trainer/Policy mu Mean                                 -0.0429631\n",
      "trainer/Policy mu Std                                   0.135731\n",
      "trainer/Policy mu Max                                   0.771497\n",
      "trainer/Policy mu Min                                  -0.859518\n",
      "trainer/Policy log std Mean                            -2.17025\n",
      "trainer/Policy log std Std                              0.183599\n",
      "trainer/Policy log std Max                             -1.33918\n",
      "trainer/Policy log std Min                             -2.80448\n",
      "trainer/Alpha                                           0.0218607\n",
      "trainer/Alpha Loss                                     -6.66159\n",
      "exploration/num steps total                         20000\n",
      "exploration/num paths total                           114\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.924897\n",
      "exploration/Rewards Std                                 0.0748295\n",
      "exploration/Rewards Max                                 1.6162\n",
      "exploration/Rewards Min                                 0.623742\n",
      "exploration/Returns Mean                              924.897\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               924.897\n",
      "exploration/Returns Min                               924.897\n",
      "exploration/Actions Mean                               -0.0539339\n",
      "exploration/Actions Std                                 0.135648\n",
      "exploration/Actions Max                                 0.54682\n",
      "exploration/Actions Min                                -0.486765\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           924.897\n",
      "exploration/env_infos/final/reward_forward Mean         0.0888629\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0888629\n",
      "exploration/env_infos/final/reward_forward Min          0.0888629\n",
      "exploration/env_infos/initial/reward_forward Mean       0.113134\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.113134\n",
      "exploration/env_infos/initial/reward_forward Min        0.113134\n",
      "exploration/env_infos/reward_forward Mean              -0.0456071\n",
      "exploration/env_infos/reward_forward Std                0.199514\n",
      "exploration/env_infos/reward_forward Max                1.00104\n",
      "exploration/env_infos/reward_forward Min               -0.847141\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.154168\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.154168\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.154168\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0204597\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0204597\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0204597\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.085237\n",
      "exploration/env_infos/reward_ctrl Std                   0.043332\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00688017\n",
      "exploration/env_infos/reward_ctrl Min                  -0.376258\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0938759\n",
      "exploration/env_infos/final/torso_velocity Std          0.132412\n",
      "exploration/env_infos/final/torso_velocity Max          0.0888629\n",
      "exploration/env_infos/final/torso_velocity Min         -0.220661\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.259757\n",
      "exploration/env_infos/initial/torso_velocity Std        0.170513\n",
      "exploration/env_infos/initial/torso_velocity Max        0.498864\n",
      "exploration/env_infos/initial/torso_velocity Min        0.113134\n",
      "exploration/env_infos/torso_velocity Mean              -0.0231199\n",
      "exploration/env_infos/torso_velocity Std                0.248645\n",
      "exploration/env_infos/torso_velocity Max                1.04047\n",
      "exploration/env_infos/torso_velocity Min               -1.52518\n",
      "evaluation/num steps total                         475000\n",
      "evaluation/num paths total                            475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.950463\n",
      "evaluation/Rewards Std                                  0.0235099\n",
      "evaluation/Rewards Max                                  2.08574\n",
      "evaluation/Rewards Min                                  0.724959\n",
      "evaluation/Returns Mean                               950.463\n",
      "evaluation/Returns Std                                  8.75336\n",
      "evaluation/Returns Max                                970.645\n",
      "evaluation/Returns Min                                939.525\n",
      "evaluation/Actions Mean                                -0.0745994\n",
      "evaluation/Actions Std                                  0.0837368\n",
      "evaluation/Actions Max                                  0.249443\n",
      "evaluation/Actions Min                                 -0.587499\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            950.463\n",
      "evaluation/env_infos/final/reward_forward Mean          1.30258e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.10082e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.00046e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -6.84226e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0721425\n",
      "evaluation/env_infos/initial/reward_forward Std         0.13104\n",
      "evaluation/env_infos/initial/reward_forward Max         0.279852\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.194077\n",
      "evaluation/env_infos/reward_forward Mean                0.00342531\n",
      "evaluation/env_infos/reward_forward Std                 0.0620868\n",
      "evaluation/env_infos/reward_forward Max                 1.35182\n",
      "evaluation/env_infos/reward_forward Min                -1.34747\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0501088\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.00904015\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0287137\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0604442\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0112712\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0043213\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00508538\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0214218\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0503077\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0116188\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00508538\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.287545\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          5.05861e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.74499e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.00046e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -6.84226e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.154283\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.213525\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614999\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.20064\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00118886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/torso_velocity Std                 0.0622378\n",
      "evaluation/env_infos/torso_velocity Max                 1.47263\n",
      "evaluation/env_infos/torso_velocity Min                -1.6904\n",
      "time/data storing (s)                                   0.326466\n",
      "time/evaluation sampling (s)                           47.4519\n",
      "time/exploration sampling (s)                           1.94283\n",
      "time/logging (s)                                        0.287122\n",
      "time/saving (s)                                         0.0269477\n",
      "time/training (s)                                       4.1152\n",
      "time/epoch (s)                                         54.1505\n",
      "time/total (s)                                        990.615\n",
      "Epoch                                                  18\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 13:41:27.023608 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 19 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  41898\n",
      "trainer/QF1 Loss                                        0.344626\n",
      "trainer/QF2 Loss                                        0.33073\n",
      "trainer/Policy Loss                                    -0.859075\n",
      "trainer/Q1 Predictions Mean                             6.8705\n",
      "trainer/Q1 Predictions Std                              0.993433\n",
      "trainer/Q1 Predictions Max                              9.59699\n",
      "trainer/Q1 Predictions Min                              4.02179\n",
      "trainer/Q2 Predictions Mean                             6.88148\n",
      "trainer/Q2 Predictions Std                              0.999307\n",
      "trainer/Q2 Predictions Max                              9.8264\n",
      "trainer/Q2 Predictions Min                              4.19659\n",
      "trainer/Q Targets Mean                                  6.73031\n",
      "trainer/Q Targets Std                                   1.10614\n",
      "trainer/Q Targets Max                                   9.53165\n",
      "trainer/Q Targets Min                                   0.0206765\n",
      "trainer/Log Pis Mean                                    6.67661\n",
      "trainer/Log Pis Std                                     2.19719\n",
      "trainer/Log Pis Max                                    10.8282\n",
      "trainer/Log Pis Min                                    -4.22374\n",
      "trainer/Policy mu Mean                                 -0.0304023\n",
      "trainer/Policy mu Std                                   0.156609\n",
      "trainer/Policy mu Max                                   0.715584\n",
      "trainer/Policy mu Min                                  -0.790836\n",
      "trainer/Policy log std Mean                            -2.21015\n",
      "trainer/Policy log std Std                              0.163869\n",
      "trainer/Policy log std Max                             -1.55704\n",
      "trainer/Policy log std Min                             -2.74045\n",
      "trainer/Alpha                                           0.0205708\n",
      "trainer/Alpha Loss                                     -5.13908\n",
      "exploration/num steps total                         21000\n",
      "exploration/num paths total                           115\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.888843\n",
      "exploration/Rewards Std                                 0.058801\n",
      "exploration/Rewards Max                                 1.5754\n",
      "exploration/Rewards Min                                 0.571573\n",
      "exploration/Returns Mean                              888.843\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               888.843\n",
      "exploration/Returns Min                               888.843\n",
      "exploration/Actions Mean                               -0.0602608\n",
      "exploration/Actions Std                                 0.158983\n",
      "exploration/Actions Max                                 0.562601\n",
      "exploration/Actions Min                                -0.666528\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           888.843\n",
      "exploration/env_infos/final/reward_forward Mean         0.00340258\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00340258\n",
      "exploration/env_infos/final/reward_forward Min          0.00340258\n",
      "exploration/env_infos/initial/reward_forward Mean       0.188676\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.188676\n",
      "exploration/env_infos/initial/reward_forward Min        0.188676\n",
      "exploration/env_infos/reward_forward Mean              -0.0146621\n",
      "exploration/env_infos/reward_forward Std                0.0738281\n",
      "exploration/env_infos/reward_forward Max                0.460587\n",
      "exploration/env_infos/reward_forward Min               -0.475264\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0578052\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0578052\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0578052\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0357849\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0357849\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0357849\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.115627\n",
      "exploration/env_infos/reward_ctrl Std                   0.0472379\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0204141\n",
      "exploration/env_infos/reward_ctrl Min                  -0.428427\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0223779\n",
      "exploration/env_infos/final/torso_velocity Std          0.0259165\n",
      "exploration/env_infos/final/torso_velocity Max          0.00340258\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0578298\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.238172\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0613618\n",
      "exploration/env_infos/initial/torso_velocity Max        0.324649\n",
      "exploration/env_infos/initial/torso_velocity Min        0.188676\n",
      "exploration/env_infos/torso_velocity Mean              -0.00931124\n",
      "exploration/env_infos/torso_velocity Std                0.0718786\n",
      "exploration/env_infos/torso_velocity Max                0.726958\n",
      "exploration/env_infos/torso_velocity Min               -1.28046\n",
      "evaluation/num steps total                         500000\n",
      "evaluation/num paths total                            500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.925899\n",
      "evaluation/Rewards Std                                  0.0230015\n",
      "evaluation/Rewards Max                                  2.20453\n",
      "evaluation/Rewards Min                                  0.642997\n",
      "evaluation/Returns Mean                               925.899\n",
      "evaluation/Returns Std                                 15.7245\n",
      "evaluation/Returns Max                                965.346\n",
      "evaluation/Returns Min                                907.109\n",
      "evaluation/Actions Mean                                -0.0620253\n",
      "evaluation/Actions Std                                  0.121554\n",
      "evaluation/Actions Max                                  0.518537\n",
      "evaluation/Actions Min                                 -0.566287\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            925.899\n",
      "evaluation/env_infos/final/reward_forward Mean          5.90362e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.2462e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.89023e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.24088e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0424186\n",
      "evaluation/env_infos/initial/reward_forward Std         0.135912\n",
      "evaluation/env_infos/initial/reward_forward Max         0.243076\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.198691\n",
      "evaluation/env_infos/reward_forward Mean                0.00212489\n",
      "evaluation/env_infos/reward_forward Std                 0.0553376\n",
      "evaluation/env_infos/reward_forward Max                 1.39225\n",
      "evaluation/env_infos/reward_forward Min                -1.32777\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0743111\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0157984\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0341436\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0931451\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0146127\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00529781\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00799213\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0327176\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0744898\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0170943\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00799213\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.357003\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.89778e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.75361e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.89023e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.24088e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.150023\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.212714\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.575719\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.198691\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00161701\n",
      "evaluation/env_infos/torso_velocity Std                 0.0563418\n",
      "evaluation/env_infos/torso_velocity Max                 1.39225\n",
      "evaluation/env_infos/torso_velocity Min                -1.63909\n",
      "time/data storing (s)                                   0.342637\n",
      "time/evaluation sampling (s)                           44.0722\n",
      "time/exploration sampling (s)                           2.01903\n",
      "time/logging (s)                                        0.287135\n",
      "time/saving (s)                                         0.0289531\n",
      "time/training (s)                                       4.01095\n",
      "time/epoch (s)                                         50.7609\n",
      "time/total (s)                                       1041.68\n",
      "Epoch                                                  19\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:42:20.970639 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 20 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  43898\n",
      "trainer/QF1 Loss                                        0.514131\n",
      "trainer/QF2 Loss                                        0.462041\n",
      "trainer/Policy Loss                                    -1.0637\n",
      "trainer/Q1 Predictions Mean                             6.83441\n",
      "trainer/Q1 Predictions Std                              1.08839\n",
      "trainer/Q1 Predictions Max                              9.46783\n",
      "trainer/Q1 Predictions Min                              3.63331\n",
      "trainer/Q2 Predictions Mean                             6.87491\n",
      "trainer/Q2 Predictions Std                              1.1057\n",
      "trainer/Q2 Predictions Max                              9.43877\n",
      "trainer/Q2 Predictions Min                              3.30394\n",
      "trainer/Q Targets Mean                                  6.94609\n",
      "trainer/Q Targets Std                                   1.32016\n",
      "trainer/Q Targets Max                                  11.5468\n",
      "trainer/Q Targets Min                                  -0.961324\n",
      "trainer/Log Pis Mean                                    6.47187\n",
      "trainer/Log Pis Std                                     2.18894\n",
      "trainer/Log Pis Max                                    11.7632\n",
      "trainer/Log Pis Min                                     0.639917\n",
      "trainer/Policy mu Mean                                 -0.0370376\n",
      "trainer/Policy mu Std                                   0.16163\n",
      "trainer/Policy mu Max                                   0.920144\n",
      "trainer/Policy mu Min                                  -0.792916\n",
      "trainer/Policy log std Mean                            -2.19807\n",
      "trainer/Policy log std Std                              0.171114\n",
      "trainer/Policy log std Max                             -1.5361\n",
      "trainer/Policy log std Min                             -2.82189\n",
      "trainer/Alpha                                           0.0194087\n",
      "trainer/Alpha Loss                                     -6.02312\n",
      "exploration/num steps total                         22000\n",
      "exploration/num paths total                           116\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.869357\n",
      "exploration/Rewards Std                                 0.0558761\n",
      "exploration/Rewards Max                                 1.15987\n",
      "exploration/Rewards Min                                 0.667808\n",
      "exploration/Returns Mean                              869.357\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               869.357\n",
      "exploration/Returns Min                               869.357\n",
      "exploration/Actions Mean                               -0.0529487\n",
      "exploration/Actions Std                                 0.173743\n",
      "exploration/Actions Max                                 0.527082\n",
      "exploration/Actions Min                                -0.599547\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           869.357\n",
      "exploration/env_infos/final/reward_forward Mean         0.0134424\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0134424\n",
      "exploration/env_infos/final/reward_forward Min          0.0134424\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.199301\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.199301\n",
      "exploration/env_infos/initial/reward_forward Min       -0.199301\n",
      "exploration/env_infos/reward_forward Mean              -0.0126748\n",
      "exploration/env_infos/reward_forward Std                0.139776\n",
      "exploration/env_infos/reward_forward Max                0.732875\n",
      "exploration/env_infos/reward_forward Min               -0.554475\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.164977\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.164977\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.164977\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0836501\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0836501\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0836501\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.131961\n",
      "exploration/env_infos/reward_ctrl Std                   0.0537625\n",
      "exploration/env_infos/reward_ctrl Max                  -0.013403\n",
      "exploration/env_infos/reward_ctrl Min                  -0.332192\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00161105\n",
      "exploration/env_infos/final/torso_velocity Std          0.0101766\n",
      "exploration/env_infos/final/torso_velocity Max          0.0134424\n",
      "exploration/env_infos/final/torso_velocity Min         -0.011401\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.014371\n",
      "exploration/env_infos/initial/torso_velocity Std        0.220888\n",
      "exploration/env_infos/initial/torso_velocity Max        0.296127\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.199301\n",
      "exploration/env_infos/torso_velocity Mean              -0.0114408\n",
      "exploration/env_infos/torso_velocity Std                0.171646\n",
      "exploration/env_infos/torso_velocity Max                0.828993\n",
      "exploration/env_infos/torso_velocity Min               -1.35904\n",
      "evaluation/num steps total                         525000\n",
      "evaluation/num paths total                            525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.936612\n",
      "evaluation/Rewards Std                                  0.0350532\n",
      "evaluation/Rewards Max                                  2.05043\n",
      "evaluation/Rewards Min                                  0.601517\n",
      "evaluation/Returns Mean                               936.612\n",
      "evaluation/Returns Std                                 26.098\n",
      "evaluation/Returns Max                                975.406\n",
      "evaluation/Returns Min                                872.866\n",
      "evaluation/Actions Mean                                -0.0483746\n",
      "evaluation/Actions Std                                  0.11718\n",
      "evaluation/Actions Max                                  0.441221\n",
      "evaluation/Actions Min                                 -0.649368\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            936.612\n",
      "evaluation/env_infos/final/reward_forward Mean          2.99455e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.98261e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.04869e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.03674e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0126875\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0945557\n",
      "evaluation/env_infos/initial/reward_forward Max         0.159783\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.160367\n",
      "evaluation/env_infos/reward_forward Mean                0.00451843\n",
      "evaluation/env_infos/reward_forward Std                 0.061116\n",
      "evaluation/env_infos/reward_forward Max                 1.14439\n",
      "evaluation/env_infos/reward_forward Min                -1.06666\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0637451\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0263582\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0271835\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.127523\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.018212\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0155566\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00645073\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0658351\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0642849\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0282529\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00645073\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.398483\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.38136e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.24512e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.04869e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.03674e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.127128\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.225388\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.555143\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.250914\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00108985\n",
      "evaluation/env_infos/torso_velocity Std                 0.058264\n",
      "evaluation/env_infos/torso_velocity Max                 1.14439\n",
      "evaluation/env_infos/torso_velocity Min                -1.98114\n",
      "time/data storing (s)                                   0.332205\n",
      "time/evaluation sampling (s)                           46.6389\n",
      "time/exploration sampling (s)                           2.06622\n",
      "time/logging (s)                                        0.283099\n",
      "time/saving (s)                                         0.027634\n",
      "time/training (s)                                       4.2745\n",
      "time/epoch (s)                                         53.6225\n",
      "time/total (s)                                       1095.62\n",
      "Epoch                                                  20\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:43:12.643901 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 21 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  45898\n",
      "trainer/QF1 Loss                                        0.340369\n",
      "trainer/QF2 Loss                                        0.283031\n",
      "trainer/Policy Loss                                    -1.90688\n",
      "trainer/Q1 Predictions Mean                             7.46984\n",
      "trainer/Q1 Predictions Std                              1.06248\n",
      "trainer/Q1 Predictions Max                             10.9052\n",
      "trainer/Q1 Predictions Min                              3.51927\n",
      "trainer/Q2 Predictions Mean                             7.48872\n",
      "trainer/Q2 Predictions Std                              1.06719\n",
      "trainer/Q2 Predictions Max                             10.4606\n",
      "trainer/Q2 Predictions Min                              3.13386\n",
      "trainer/Q Targets Mean                                  7.39832\n",
      "trainer/Q Targets Std                                   1.22974\n",
      "trainer/Q Targets Max                                  12.06\n",
      "trainer/Q Targets Min                                  -0.10118\n",
      "trainer/Log Pis Mean                                    6.22912\n",
      "trainer/Log Pis Std                                     2.41177\n",
      "trainer/Log Pis Max                                    12.6264\n",
      "trainer/Log Pis Min                                    -3.71881\n",
      "trainer/Policy mu Mean                                 -0.0267538\n",
      "trainer/Policy mu Std                                   0.160986\n",
      "trainer/Policy mu Max                                   0.705404\n",
      "trainer/Policy mu Min                                  -1.09313\n",
      "trainer/Policy log std Mean                            -2.1613\n",
      "trainer/Policy log std Std                              0.200143\n",
      "trainer/Policy log std Max                             -1.05628\n",
      "trainer/Policy log std Min                             -3.00275\n",
      "trainer/Alpha                                           0.0182673\n",
      "trainer/Alpha Loss                                     -7.0871\n",
      "exploration/num steps total                         23000\n",
      "exploration/num paths total                           117\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.943713\n",
      "exploration/Rewards Std                                 0.109189\n",
      "exploration/Rewards Max                                 2.14908\n",
      "exploration/Rewards Min                                 0.642314\n",
      "exploration/Returns Mean                              943.713\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               943.713\n",
      "exploration/Returns Min                               943.713\n",
      "exploration/Actions Mean                               -0.0135475\n",
      "exploration/Actions Std                                 0.137921\n",
      "exploration/Actions Max                                 0.547554\n",
      "exploration/Actions Min                                -0.61116\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           943.713\n",
      "exploration/env_infos/final/reward_forward Mean        -0.119771\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.119771\n",
      "exploration/env_infos/final/reward_forward Min         -0.119771\n",
      "exploration/env_infos/initial/reward_forward Mean       0.143085\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.143085\n",
      "exploration/env_infos/initial/reward_forward Min        0.143085\n",
      "exploration/env_infos/reward_forward Mean              -0.0217731\n",
      "exploration/env_infos/reward_forward Std                0.145937\n",
      "exploration/env_infos/reward_forward Max                0.765661\n",
      "exploration/env_infos/reward_forward Min               -0.516655\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.110921\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.110921\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.110921\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0322784\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0322784\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0322784\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0768227\n",
      "exploration/env_infos/reward_ctrl Std                   0.0403945\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0108418\n",
      "exploration/env_infos/reward_ctrl Min                  -0.357686\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.119718\n",
      "exploration/env_infos/final/torso_velocity Std          0.179883\n",
      "exploration/env_infos/final/torso_velocity Max          0.100619\n",
      "exploration/env_infos/final/torso_velocity Min         -0.340003\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.222051\n",
      "exploration/env_infos/initial/torso_velocity Std        0.113636\n",
      "exploration/env_infos/initial/torso_velocity Max        0.382749\n",
      "exploration/env_infos/initial/torso_velocity Min        0.14032\n",
      "exploration/env_infos/torso_velocity Mean              -0.0187162\n",
      "exploration/env_infos/torso_velocity Std                0.190434\n",
      "exploration/env_infos/torso_velocity Max                0.94448\n",
      "exploration/env_infos/torso_velocity Min               -1.78796\n",
      "evaluation/num steps total                         550000\n",
      "evaluation/num paths total                            550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.954519\n",
      "evaluation/Rewards Std                                  0.0327162\n",
      "evaluation/Rewards Max                                  2.83735\n",
      "evaluation/Rewards Min                                  0.667109\n",
      "evaluation/Returns Mean                               954.519\n",
      "evaluation/Returns Std                                 11.1997\n",
      "evaluation/Returns Max                                968.938\n",
      "evaluation/Returns Min                                927.105\n",
      "evaluation/Actions Mean                                -0.0134885\n",
      "evaluation/Actions Std                                  0.107749\n",
      "evaluation/Actions Max                                  0.46282\n",
      "evaluation/Actions Min                                 -0.697006\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            954.519\n",
      "evaluation/env_infos/final/reward_forward Mean          1.12193e-05\n",
      "evaluation/env_infos/final/reward_forward Std           3.58149e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.00014221\n",
      "evaluation/env_infos/final/reward_forward Min          -5.24737e-05\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0584537\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122577\n",
      "evaluation/env_infos/initial/reward_forward Max         0.293423\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.342328\n",
      "evaluation/env_infos/reward_forward Mean                0.00757497\n",
      "evaluation/env_infos/reward_forward Std                 0.104429\n",
      "evaluation/env_infos/reward_forward Max                 1.9311\n",
      "evaluation/env_infos/reward_forward Min                -1.12045\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0469848\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0115534\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0348707\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0727393\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0214233\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0136051\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0100583\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0696757\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0471675\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0166267\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0100583\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.332891\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.09881e-06\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00040642\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00253937\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00242639\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.113836\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.255638\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.683356\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.391715\n",
      "evaluation/env_infos/torso_velocity Mean                0.000824944\n",
      "evaluation/env_infos/torso_velocity Std                 0.090726\n",
      "evaluation/env_infos/torso_velocity Max                 1.9311\n",
      "evaluation/env_infos/torso_velocity Min                -1.91893\n",
      "time/data storing (s)                                   0.322527\n",
      "time/evaluation sampling (s)                           44.4816\n",
      "time/exploration sampling (s)                           1.95023\n",
      "time/logging (s)                                        0.279217\n",
      "time/saving (s)                                         0.0253372\n",
      "time/training (s)                                       4.2902\n",
      "time/epoch (s)                                         51.3491\n",
      "time/total (s)                                       1147.29\n",
      "Epoch                                                  21\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:44:01.264364 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 22 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  47898\n",
      "trainer/QF1 Loss                                        0.466385\n",
      "trainer/QF2 Loss                                        0.412633\n",
      "trainer/Policy Loss                                    -1.27669\n",
      "trainer/Q1 Predictions Mean                             7.9713\n",
      "trainer/Q1 Predictions Std                              1.23647\n",
      "trainer/Q1 Predictions Max                             14.8239\n",
      "trainer/Q1 Predictions Min                              3.60419\n",
      "trainer/Q2 Predictions Mean                             7.9244\n",
      "trainer/Q2 Predictions Std                              1.28679\n",
      "trainer/Q2 Predictions Max                             14.5483\n",
      "trainer/Q2 Predictions Min                              3.38223\n",
      "trainer/Q Targets Mean                                  7.79394\n",
      "trainer/Q Targets Std                                   1.36106\n",
      "trainer/Q Targets Max                                  15.9511\n",
      "trainer/Q Targets Min                                   1.45055\n",
      "trainer/Log Pis Mean                                    7.31636\n",
      "trainer/Log Pis Std                                     2.29133\n",
      "trainer/Log Pis Max                                    12.2751\n",
      "trainer/Log Pis Min                                    -0.0288773\n",
      "trainer/Policy mu Mean                                 -0.0587128\n",
      "trainer/Policy mu Std                                   0.17949\n",
      "trainer/Policy mu Max                                   0.845463\n",
      "trainer/Policy mu Min                                  -1.2352\n",
      "trainer/Policy log std Mean                            -2.28339\n",
      "trainer/Policy log std Std                              0.196162\n",
      "trainer/Policy log std Max                             -1.43396\n",
      "trainer/Policy log std Min                             -2.93528\n",
      "trainer/Alpha                                           0.0173499\n",
      "trainer/Alpha Loss                                     -2.77123\n",
      "exploration/num steps total                         24000\n",
      "exploration/num paths total                           118\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.922662\n",
      "exploration/Rewards Std                                 0.0875098\n",
      "exploration/Rewards Max                                 1.81331\n",
      "exploration/Rewards Min                                 0.696038\n",
      "exploration/Returns Mean                              922.662\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               922.662\n",
      "exploration/Returns Min                               922.662\n",
      "exploration/Actions Mean                               -0.0472917\n",
      "exploration/Actions Std                                 0.145128\n",
      "exploration/Actions Max                                 0.4452\n",
      "exploration/Actions Min                                -0.518309\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           922.662\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0119917\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0119917\n",
      "exploration/env_infos/final/reward_forward Min         -0.0119917\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0306444\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0306444\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0306444\n",
      "exploration/env_infos/reward_forward Mean              -0.0106602\n",
      "exploration/env_infos/reward_forward Std                0.127664\n",
      "exploration/env_infos/reward_forward Max                0.78222\n",
      "exploration/env_infos/reward_forward Min               -0.381944\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0392442\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0392442\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0392442\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0159718\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0159718\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0159718\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0931943\n",
      "exploration/env_infos/reward_ctrl Std                   0.0412758\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0137241\n",
      "exploration/env_infos/reward_ctrl Min                  -0.303962\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.105462\n",
      "exploration/env_infos/final/torso_velocity Std          0.153561\n",
      "exploration/env_infos/final/torso_velocity Max          0.322381\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0119917\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.128993\n",
      "exploration/env_infos/initial/torso_velocity Std        0.261571\n",
      "exploration/env_infos/initial/torso_velocity Max        0.497804\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0801791\n",
      "exploration/env_infos/torso_velocity Mean              -0.00948047\n",
      "exploration/env_infos/torso_velocity Std                0.113495\n",
      "exploration/env_infos/torso_velocity Max                0.805158\n",
      "exploration/env_infos/torso_velocity Min               -1.04066\n",
      "evaluation/num steps total                         575000\n",
      "evaluation/num paths total                            575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.949096\n",
      "evaluation/Rewards Std                                  0.0312857\n",
      "evaluation/Rewards Max                                  2.45091\n",
      "evaluation/Rewards Min                                  0.603833\n",
      "evaluation/Returns Mean                               949.096\n",
      "evaluation/Returns Std                                 13.04\n",
      "evaluation/Returns Max                                966.965\n",
      "evaluation/Returns Min                                918.357\n",
      "evaluation/Actions Mean                                -0.0441399\n",
      "evaluation/Actions Std                                  0.1048\n",
      "evaluation/Actions Max                                  0.461471\n",
      "evaluation/Actions Min                                 -0.692545\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            949.096\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.94958e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.69823e-07\n",
      "evaluation/env_infos/final/reward_forward Max           3.95129e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.70958e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00659225\n",
      "evaluation/env_infos/initial/reward_forward Std         0.133059\n",
      "evaluation/env_infos/initial/reward_forward Max         0.303715\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.242583\n",
      "evaluation/env_infos/reward_forward Mean                0.00442284\n",
      "evaluation/env_infos/reward_forward Std                 0.0703756\n",
      "evaluation/env_infos/reward_forward Max                 1.41785\n",
      "evaluation/env_infos/reward_forward Min                -1.05855\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0506621\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0130884\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.033902\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0811755\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0176585\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00607462\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00670948\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0279306\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0517255\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0171827\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00670948\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.396167\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.25601e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           2.95035e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.35609e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.70958e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.156666\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.226569\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.625089\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.242583\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00084113\n",
      "evaluation/env_infos/torso_velocity Std                 0.0709597\n",
      "evaluation/env_infos/torso_velocity Max                 1.5482\n",
      "evaluation/env_infos/torso_velocity Min                -2.20659\n",
      "time/data storing (s)                                   0.304437\n",
      "time/evaluation sampling (s)                           42.0677\n",
      "time/exploration sampling (s)                           1.84241\n",
      "time/logging (s)                                        0.277024\n",
      "time/saving (s)                                         0.0256275\n",
      "time/training (s)                                       3.79488\n",
      "time/epoch (s)                                         48.3121\n",
      "time/total (s)                                       1195.91\n",
      "Epoch                                                  22\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:44:50.355984 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 23 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  49898\n",
      "trainer/QF1 Loss                                        0.286201\n",
      "trainer/QF2 Loss                                        0.285114\n",
      "trainer/Policy Loss                                    -1.85197\n",
      "trainer/Q1 Predictions Mean                             8.23613\n",
      "trainer/Q1 Predictions Std                              1.22824\n",
      "trainer/Q1 Predictions Max                             13.7295\n",
      "trainer/Q1 Predictions Min                              2.79069\n",
      "trainer/Q2 Predictions Mean                             8.09433\n",
      "trainer/Q2 Predictions Std                              1.17076\n",
      "trainer/Q2 Predictions Max                             14.2834\n",
      "trainer/Q2 Predictions Min                              3.4625\n",
      "trainer/Q Targets Mean                                  8.26518\n",
      "trainer/Q Targets Std                                   1.16784\n",
      "trainer/Q Targets Max                                  14.8725\n",
      "trainer/Q Targets Min                                   3.9854\n",
      "trainer/Log Pis Mean                                    6.84239\n",
      "trainer/Log Pis Std                                     2.1726\n",
      "trainer/Log Pis Max                                    13.9822\n",
      "trainer/Log Pis Min                                    -0.428034\n",
      "trainer/Policy mu Mean                                 -0.00758424\n",
      "trainer/Policy mu Std                                   0.159728\n",
      "trainer/Policy mu Max                                   0.829488\n",
      "trainer/Policy mu Min                                  -0.720204\n",
      "trainer/Policy log std Mean                            -2.22753\n",
      "trainer/Policy log std Std                              0.220529\n",
      "trainer/Policy log std Max                             -0.855044\n",
      "trainer/Policy log std Min                             -3.09712\n",
      "trainer/Alpha                                           0.0164526\n",
      "trainer/Alpha Loss                                     -4.754\n",
      "exploration/num steps total                         25000\n",
      "exploration/num paths total                           119\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.941615\n",
      "exploration/Rewards Std                                 0.0763151\n",
      "exploration/Rewards Max                                 1.62753\n",
      "exploration/Rewards Min                                 0.587229\n",
      "exploration/Returns Mean                              941.615\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               941.615\n",
      "exploration/Returns Min                               941.615\n",
      "exploration/Actions Mean                                0.0137275\n",
      "exploration/Actions Std                                 0.132996\n",
      "exploration/Actions Max                                 0.555954\n",
      "exploration/Actions Min                                -0.66568\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           941.615\n",
      "exploration/env_infos/final/reward_forward Mean        -0.117129\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.117129\n",
      "exploration/env_infos/final/reward_forward Min         -0.117129\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0747551\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0747551\n",
      "exploration/env_infos/initial/reward_forward Min        0.0747551\n",
      "exploration/env_infos/reward_forward Mean               0.00808219\n",
      "exploration/env_infos/reward_forward Std                0.257071\n",
      "exploration/env_infos/reward_forward Max                1.07787\n",
      "exploration/env_infos/reward_forward Min               -0.895543\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0300898\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0300898\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0300898\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0334789\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0334789\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0334789\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0715059\n",
      "exploration/env_infos/reward_ctrl Std                   0.0438212\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00421988\n",
      "exploration/env_infos/reward_ctrl Min                  -0.412771\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0126908\n",
      "exploration/env_infos/final/torso_velocity Std          0.0912204\n",
      "exploration/env_infos/final/torso_velocity Max          0.105112\n",
      "exploration/env_infos/final/torso_velocity Min         -0.117129\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.17034\n",
      "exploration/env_infos/initial/torso_velocity Std        0.150023\n",
      "exploration/env_infos/initial/torso_velocity Max        0.382169\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0540963\n",
      "exploration/env_infos/torso_velocity Mean              -0.0028053\n",
      "exploration/env_infos/torso_velocity Std                0.269553\n",
      "exploration/env_infos/torso_velocity Max                1.40552\n",
      "exploration/env_infos/torso_velocity Min               -1.77341\n",
      "evaluation/num steps total                         600000\n",
      "evaluation/num paths total                            600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.965457\n",
      "evaluation/Rewards Std                                  0.0498534\n",
      "evaluation/Rewards Max                                  2.53305\n",
      "evaluation/Rewards Min                                  0.429857\n",
      "evaluation/Returns Mean                               965.457\n",
      "evaluation/Returns Std                                 18.3971\n",
      "evaluation/Returns Max                                996.861\n",
      "evaluation/Returns Min                                925.476\n",
      "evaluation/Actions Mean                                 0.00765326\n",
      "evaluation/Actions Std                                  0.0962362\n",
      "evaluation/Actions Max                                  0.488621\n",
      "evaluation/Actions Min                                 -0.7224\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            965.457\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00334018\n",
      "evaluation/env_infos/final/reward_forward Std           0.0152698\n",
      "evaluation/env_infos/final/reward_forward Max           1.96272e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0779605\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.021039\n",
      "evaluation/env_infos/initial/reward_forward Std         0.12488\n",
      "evaluation/env_infos/initial/reward_forward Max         0.201365\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.260856\n",
      "evaluation/env_infos/reward_forward Mean                2.35591e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0990865\n",
      "evaluation/env_infos/reward_forward Max                 1.35055\n",
      "evaluation/env_infos/reward_forward Min                -1.74888\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0355967\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.018697\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.00963633\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.075681\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0227856\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0216386\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00530449\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0854482\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0372799\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0242147\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00530449\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.570143\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00206128\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0106267\n",
      "evaluation/env_infos/final/torso_velocity Max           3.00257e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0779605\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135732\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.218133\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.531063\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.53447\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00335094\n",
      "evaluation/env_infos/torso_velocity Std                 0.0961045\n",
      "evaluation/env_infos/torso_velocity Max                 1.35055\n",
      "evaluation/env_infos/torso_velocity Min                -1.88461\n",
      "time/data storing (s)                                   0.309663\n",
      "time/evaluation sampling (s)                           42.5278\n",
      "time/exploration sampling (s)                           1.78682\n",
      "time/logging (s)                                        0.269753\n",
      "time/saving (s)                                         0.0241845\n",
      "time/training (s)                                       3.84617\n",
      "time/epoch (s)                                         48.7644\n",
      "time/total (s)                                       1244.99\n",
      "Epoch                                                  23\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:45:38.247058 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 24 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  51898\n",
      "trainer/QF1 Loss                                        0.243212\n",
      "trainer/QF2 Loss                                        0.247593\n",
      "trainer/Policy Loss                                    -1.6305\n",
      "trainer/Q1 Predictions Mean                             8.57005\n",
      "trainer/Q1 Predictions Std                              1.15969\n",
      "trainer/Q1 Predictions Max                             12.4323\n",
      "trainer/Q1 Predictions Min                              4.77336\n",
      "trainer/Q2 Predictions Mean                             8.49554\n",
      "trainer/Q2 Predictions Std                              1.08342\n",
      "trainer/Q2 Predictions Max                             12.3071\n",
      "trainer/Q2 Predictions Min                              5.01288\n",
      "trainer/Q Targets Mean                                  8.62405\n",
      "trainer/Q Targets Std                                   1.13393\n",
      "trainer/Q Targets Max                                  12.7826\n",
      "trainer/Q Targets Min                                   4.07759\n",
      "trainer/Log Pis Mean                                    7.37812\n",
      "trainer/Log Pis Std                                     2.34235\n",
      "trainer/Log Pis Max                                    13.1847\n",
      "trainer/Log Pis Min                                    -0.311706\n",
      "trainer/Policy mu Mean                                 -0.0131218\n",
      "trainer/Policy mu Std                                   0.182912\n",
      "trainer/Policy mu Max                                   1.0417\n",
      "trainer/Policy mu Min                                  -1.27358\n",
      "trainer/Policy log std Mean                            -2.29106\n",
      "trainer/Policy log std Std                              0.200939\n",
      "trainer/Policy log std Max                             -0.928794\n",
      "trainer/Policy log std Min                             -3.00012\n",
      "trainer/Alpha                                           0.0155293\n",
      "trainer/Alpha Loss                                     -2.58989\n",
      "exploration/num steps total                         26000\n",
      "exploration/num paths total                           120\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.916673\n",
      "exploration/Rewards Std                                 0.092185\n",
      "exploration/Rewards Max                                 1.67079\n",
      "exploration/Rewards Min                                 0.53364\n",
      "exploration/Returns Mean                              916.673\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               916.673\n",
      "exploration/Returns Min                               916.673\n",
      "exploration/Actions Mean                                0.016378\n",
      "exploration/Actions Std                                 0.156941\n",
      "exploration/Actions Max                                 0.608709\n",
      "exploration/Actions Min                                -0.595843\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           916.673\n",
      "exploration/env_infos/final/reward_forward Mean         0.0305696\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0305696\n",
      "exploration/env_infos/final/reward_forward Min          0.0305696\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0937164\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0937164\n",
      "exploration/env_infos/initial/reward_forward Min        0.0937164\n",
      "exploration/env_infos/reward_forward Mean               0.0246924\n",
      "exploration/env_infos/reward_forward Std                0.31699\n",
      "exploration/env_infos/reward_forward Max                1.44255\n",
      "exploration/env_infos/reward_forward Min               -0.870115\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0864975\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0864975\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0864975\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0459623\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0459623\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0459623\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0995949\n",
      "exploration/env_infos/reward_ctrl Std                   0.0528713\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00439111\n",
      "exploration/env_infos/reward_ctrl Min                  -0.46636\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00632912\n",
      "exploration/env_infos/final/torso_velocity Std          0.0357409\n",
      "exploration/env_infos/final/torso_velocity Max          0.0326201\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0442023\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.272593\n",
      "exploration/env_infos/initial/torso_velocity Std        0.181014\n",
      "exploration/env_infos/initial/torso_velocity Max        0.520624\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0937164\n",
      "exploration/env_infos/torso_velocity Mean              -0.00396348\n",
      "exploration/env_infos/torso_velocity Std                0.31442\n",
      "exploration/env_infos/torso_velocity Max                1.44255\n",
      "exploration/env_infos/torso_velocity Min               -1.68988\n",
      "evaluation/num steps total                         625000\n",
      "evaluation/num paths total                            625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.962553\n",
      "evaluation/Rewards Std                                  0.0356735\n",
      "evaluation/Rewards Max                                  2.90379\n",
      "evaluation/Rewards Min                                  0.555593\n",
      "evaluation/Returns Mean                               962.553\n",
      "evaluation/Returns Std                                 12.7993\n",
      "evaluation/Returns Max                                981.808\n",
      "evaluation/Returns Min                                941.255\n",
      "evaluation/Actions Mean                                 0.00237048\n",
      "evaluation/Actions Std                                  0.0985991\n",
      "evaluation/Actions Max                                  0.590671\n",
      "evaluation/Actions Min                                 -0.715901\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            962.553\n",
      "evaluation/env_infos/final/reward_forward Mean          4.59674e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.21683e-06\n",
      "evaluation/env_infos/final/reward_forward Max           2.09466e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -1.69863e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0330364\n",
      "evaluation/env_infos/initial/reward_forward Std         0.147185\n",
      "evaluation/env_infos/initial/reward_forward Max         0.308792\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.33228\n",
      "evaluation/env_infos/reward_forward Mean                0.00440424\n",
      "evaluation/env_infos/reward_forward Std                 0.0718025\n",
      "evaluation/env_infos/reward_forward Max                 1.56074\n",
      "evaluation/env_infos/reward_forward Min                -1.1581\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.037143\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0134262\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0174587\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0586159\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0273844\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00786935\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.018259\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0447916\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0389096\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0206427\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0157493\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.444407\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -8.72601e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           7.07779e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           2.09466e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.88476e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.153855\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.238111\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.621608\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.392512\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00100637\n",
      "evaluation/env_infos/torso_velocity Std                 0.0695557\n",
      "evaluation/env_infos/torso_velocity Max                 1.56074\n",
      "evaluation/env_infos/torso_velocity Min                -2.18525\n",
      "time/data storing (s)                                   0.313683\n",
      "time/evaluation sampling (s)                           41.3406\n",
      "time/exploration sampling (s)                           1.82708\n",
      "time/logging (s)                                        0.279005\n",
      "time/saving (s)                                         0.026057\n",
      "time/training (s)                                       3.78644\n",
      "time/epoch (s)                                         47.5728\n",
      "time/total (s)                                       1292.89\n",
      "Epoch                                                  24\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:46:27.219484 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 25 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  53898\n",
      "trainer/QF1 Loss                                        0.309413\n",
      "trainer/QF2 Loss                                        0.38811\n",
      "trainer/Policy Loss                                    -2.53605\n",
      "trainer/Q1 Predictions Mean                             8.91846\n",
      "trainer/Q1 Predictions Std                              1.18617\n",
      "trainer/Q1 Predictions Max                             12.9226\n",
      "trainer/Q1 Predictions Min                              4.70476\n",
      "trainer/Q2 Predictions Mean                             8.70848\n",
      "trainer/Q2 Predictions Std                              1.11791\n",
      "trainer/Q2 Predictions Max                             12.4896\n",
      "trainer/Q2 Predictions Min                              5.4192\n",
      "trainer/Q Targets Mean                                  8.95641\n",
      "trainer/Q Targets Std                                   1.24854\n",
      "trainer/Q Targets Max                                  14.7506\n",
      "trainer/Q Targets Min                                   5.23214\n",
      "trainer/Log Pis Mean                                    6.75057\n",
      "trainer/Log Pis Std                                     2.22627\n",
      "trainer/Log Pis Max                                    12.753\n",
      "trainer/Log Pis Min                                    -1.12607\n",
      "trainer/Policy mu Mean                                  0.00881865\n",
      "trainer/Policy mu Std                                   0.170885\n",
      "trainer/Policy mu Max                                   0.911115\n",
      "trainer/Policy mu Min                                  -1.00805\n",
      "trainer/Policy log std Mean                            -2.19373\n",
      "trainer/Policy log std Std                              0.210973\n",
      "trainer/Policy log std Max                             -1.46811\n",
      "trainer/Policy log std Min                             -3.07679\n",
      "trainer/Alpha                                           0.0149986\n",
      "trainer/Alpha Loss                                     -5.24695\n",
      "exploration/num steps total                         27000\n",
      "exploration/num paths total                           121\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.912147\n",
      "exploration/Rewards Std                                 0.110538\n",
      "exploration/Rewards Max                                 2.0342\n",
      "exploration/Rewards Min                                 0.673755\n",
      "exploration/Returns Mean                              912.147\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               912.147\n",
      "exploration/Returns Min                               912.147\n",
      "exploration/Actions Mean                                0.0262698\n",
      "exploration/Actions Std                                 0.163186\n",
      "exploration/Actions Max                                 0.611724\n",
      "exploration/Actions Min                                -0.597802\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           912.147\n",
      "exploration/env_infos/final/reward_forward Mean         0.762464\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.762464\n",
      "exploration/env_infos/final/reward_forward Min          0.762464\n",
      "exploration/env_infos/initial/reward_forward Mean       0.183244\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.183244\n",
      "exploration/env_infos/initial/reward_forward Min        0.183244\n",
      "exploration/env_infos/reward_forward Mean               0.00638262\n",
      "exploration/env_infos/reward_forward Std                0.257245\n",
      "exploration/env_infos/reward_forward Max                1.07881\n",
      "exploration/env_infos/reward_forward Min               -0.818304\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0717738\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0717738\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0717738\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0585685\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0585685\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0585685\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109279\n",
      "exploration/env_infos/reward_ctrl Std                   0.0523228\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0135886\n",
      "exploration/env_infos/reward_ctrl Min                  -0.326245\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.329904\n",
      "exploration/env_infos/final/torso_velocity Std          0.397418\n",
      "exploration/env_infos/final/torso_velocity Max          0.762464\n",
      "exploration/env_infos/final/torso_velocity Min         -0.197149\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.196121\n",
      "exploration/env_infos/initial/torso_velocity Std        0.23096\n",
      "exploration/env_infos/initial/torso_velocity Max        0.485206\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0800881\n",
      "exploration/env_infos/torso_velocity Mean               0.00342079\n",
      "exploration/env_infos/torso_velocity Std                0.25568\n",
      "exploration/env_infos/torso_velocity Max                1.07881\n",
      "exploration/env_infos/torso_velocity Min               -1.72861\n",
      "evaluation/num steps total                         650000\n",
      "evaluation/num paths total                            650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.953363\n",
      "evaluation/Rewards Std                                  0.047443\n",
      "evaluation/Rewards Max                                  2.78237\n",
      "evaluation/Rewards Min                                  0.650555\n",
      "evaluation/Returns Mean                               953.363\n",
      "evaluation/Returns Std                                 13.7451\n",
      "evaluation/Returns Max                                983.455\n",
      "evaluation/Returns Min                                928.017\n",
      "evaluation/Actions Mean                                 0.033865\n",
      "evaluation/Actions Std                                  0.105737\n",
      "evaluation/Actions Max                                  0.557856\n",
      "evaluation/Actions Min                                 -0.611953\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            953.363\n",
      "evaluation/env_infos/final/reward_forward Mean         -5.07158e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000230598\n",
      "evaluation/env_infos/final/reward_forward Max           6.23443e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00117737\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.029312\n",
      "evaluation/env_infos/initial/reward_forward Std         0.146389\n",
      "evaluation/env_infos/initial/reward_forward Max         0.220627\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.252616\n",
      "evaluation/env_infos/reward_forward Mean                0.00769707\n",
      "evaluation/env_infos/reward_forward Std                 0.0864926\n",
      "evaluation/env_infos/reward_forward Max                 1.85327\n",
      "evaluation/env_infos/reward_forward Min                -1.28413\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0478297\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0154817\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0166681\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0827622\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0329378\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0110874\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.018749\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0647261\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.049309\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0196161\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00985934\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.366988\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.60373e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000182126\n",
      "evaluation/env_infos/final/torso_velocity Max           3.21055e-05\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.00117737\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.150622\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.235585\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.633225\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.252616\n",
      "evaluation/env_infos/torso_velocity Mean                0.00117936\n",
      "evaluation/env_infos/torso_velocity Std                 0.0879471\n",
      "evaluation/env_infos/torso_velocity Max                 1.85327\n",
      "evaluation/env_infos/torso_velocity Min                -1.8016\n",
      "time/data storing (s)                                   0.303603\n",
      "time/evaluation sampling (s)                           42.5205\n",
      "time/exploration sampling (s)                           1.77484\n",
      "time/logging (s)                                        0.246485\n",
      "time/saving (s)                                         0.0250911\n",
      "time/training (s)                                       3.76509\n",
      "time/epoch (s)                                         48.6357\n",
      "time/total (s)                                       1341.83\n",
      "Epoch                                                  25\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:47:15.596847 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 26 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  55898\n",
      "trainer/QF1 Loss                                        0.529045\n",
      "trainer/QF2 Loss                                        0.485479\n",
      "trainer/Policy Loss                                    -1.53215\n",
      "trainer/Q1 Predictions Mean                             9.05121\n",
      "trainer/Q1 Predictions Std                              1.12679\n",
      "trainer/Q1 Predictions Max                             12.8838\n",
      "trainer/Q1 Predictions Min                              5.32817\n",
      "trainer/Q2 Predictions Mean                             9.10844\n",
      "trainer/Q2 Predictions Std                              1.14066\n",
      "trainer/Q2 Predictions Max                             13.3811\n",
      "trainer/Q2 Predictions Min                              5.28287\n",
      "trainer/Q Targets Mean                                  9.31113\n",
      "trainer/Q Targets Std                                   1.26841\n",
      "trainer/Q Targets Max                                  13.7479\n",
      "trainer/Q Targets Min                                   2.9756\n",
      "trainer/Log Pis Mean                                    8.01741\n",
      "trainer/Log Pis Std                                     2.31843\n",
      "trainer/Log Pis Max                                    17.2532\n",
      "trainer/Log Pis Min                                     0.139599\n",
      "trainer/Policy mu Mean                                  0.0131101\n",
      "trainer/Policy mu Std                                   0.166062\n",
      "trainer/Policy mu Max                                   1.21239\n",
      "trainer/Policy mu Min                                  -0.952004\n",
      "trainer/Policy log std Mean                            -2.38575\n",
      "trainer/Policy log std Std                              0.191168\n",
      "trainer/Policy log std Max                             -1.61257\n",
      "trainer/Policy log std Min                             -3.44189\n",
      "trainer/Alpha                                           0.0143692\n",
      "trainer/Alpha Loss                                      0.0738482\n",
      "exploration/num steps total                         28000\n",
      "exploration/num paths total                           122\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.917976\n",
      "exploration/Rewards Std                                 0.0944411\n",
      "exploration/Rewards Max                                 1.92641\n",
      "exploration/Rewards Min                                 0.460438\n",
      "exploration/Returns Mean                              917.976\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               917.976\n",
      "exploration/Returns Min                               917.976\n",
      "exploration/Actions Mean                               -0.0163072\n",
      "exploration/Actions Std                                 0.15322\n",
      "exploration/Actions Max                                 0.469104\n",
      "exploration/Actions Min                                -0.68243\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           917.976\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0778608\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0778608\n",
      "exploration/env_infos/final/reward_forward Min         -0.0778608\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.120428\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.120428\n",
      "exploration/env_infos/initial/reward_forward Min       -0.120428\n",
      "exploration/env_infos/reward_forward Mean               0.0151805\n",
      "exploration/env_infos/reward_forward Std                0.415872\n",
      "exploration/env_infos/reward_forward Max                1.57067\n",
      "exploration/env_infos/reward_forward Min               -1.41087\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.076083\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.076083\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.076083\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0203379\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0203379\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0203379\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0949692\n",
      "exploration/env_infos/reward_ctrl Std                   0.0535882\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00926641\n",
      "exploration/env_infos/reward_ctrl Min                  -0.539562\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0176531\n",
      "exploration/env_infos/final/torso_velocity Std          0.0566211\n",
      "exploration/env_infos/final/torso_velocity Max          0.0581694\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0778608\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.106207\n",
      "exploration/env_infos/initial/torso_velocity Std        0.244216\n",
      "exploration/env_infos/initial/torso_velocity Max        0.445223\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.120428\n",
      "exploration/env_infos/torso_velocity Mean               0.00615297\n",
      "exploration/env_infos/torso_velocity Std                0.343593\n",
      "exploration/env_infos/torso_velocity Max                1.57067\n",
      "exploration/env_infos/torso_velocity Min               -1.41087\n",
      "evaluation/num steps total                         675000\n",
      "evaluation/num paths total                            675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.967012\n",
      "evaluation/Rewards Std                                  0.0315899\n",
      "evaluation/Rewards Max                                  2.01921\n",
      "evaluation/Rewards Min                                  0.642881\n",
      "evaluation/Returns Mean                               967.012\n",
      "evaluation/Returns Std                                 12.0677\n",
      "evaluation/Returns Max                                986.696\n",
      "evaluation/Returns Min                                950.142\n",
      "evaluation/Actions Mean                                 0.0019821\n",
      "evaluation/Actions Std                                  0.0932778\n",
      "evaluation/Actions Max                                  0.415114\n",
      "evaluation/Actions Min                                 -0.59781\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            967.012\n",
      "evaluation/env_infos/final/reward_forward Mean          1.04645e-07\n",
      "evaluation/env_infos/final/reward_forward Std           3.47986e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.76597e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.50836e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0608147\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125003\n",
      "evaluation/env_infos/initial/reward_forward Max         0.18176\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.3057\n",
      "evaluation/env_infos/reward_forward Mean                0.00306887\n",
      "evaluation/env_infos/reward_forward Std                 0.0880148\n",
      "evaluation/env_infos/reward_forward Max                 1.30881\n",
      "evaluation/env_infos/reward_forward Min                -1.09741\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0342313\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0131475\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0124278\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.050138\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0316405\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00829151\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0173972\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0591509\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0348187\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0175703\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0102798\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.357119\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          9.40695e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.81461e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.76597e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.98039e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.1175\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.274789\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.782503\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.3057\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00139501\n",
      "evaluation/env_infos/torso_velocity Std                 0.0948186\n",
      "evaluation/env_infos/torso_velocity Max                 1.30881\n",
      "evaluation/env_infos/torso_velocity Min                -1.73848\n",
      "time/data storing (s)                                   0.329674\n",
      "time/evaluation sampling (s)                           41.6654\n",
      "time/exploration sampling (s)                           1.9179\n",
      "time/logging (s)                                        0.282611\n",
      "time/saving (s)                                         0.0261979\n",
      "time/training (s)                                       3.92749\n",
      "time/epoch (s)                                         48.1492\n",
      "time/total (s)                                       1390.24\n",
      "Epoch                                                  26\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:48:04.837602 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 27 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  57898\n",
      "trainer/QF1 Loss                                        0.553912\n",
      "trainer/QF2 Loss                                        0.422443\n",
      "trainer/Policy Loss                                    -2.31977\n",
      "trainer/Q1 Predictions Mean                             9.85681\n",
      "trainer/Q1 Predictions Std                              1.27007\n",
      "trainer/Q1 Predictions Max                             14.782\n",
      "trainer/Q1 Predictions Min                              3.23009\n",
      "trainer/Q2 Predictions Mean                             9.70661\n",
      "trainer/Q2 Predictions Std                              1.30562\n",
      "trainer/Q2 Predictions Max                             16.8978\n",
      "trainer/Q2 Predictions Min                              3.84161\n",
      "trainer/Q Targets Mean                                  9.67628\n",
      "trainer/Q Targets Std                                   1.57639\n",
      "trainer/Q Targets Max                                  17.4535\n",
      "trainer/Q Targets Min                                  -0.820566\n",
      "trainer/Log Pis Mean                                    7.8946\n",
      "trainer/Log Pis Std                                     2.48549\n",
      "trainer/Log Pis Max                                    14.8172\n",
      "trainer/Log Pis Min                                    -0.470638\n",
      "trainer/Policy mu Mean                                 -0.0133188\n",
      "trainer/Policy mu Std                                   0.181741\n",
      "trainer/Policy mu Max                                   0.867886\n",
      "trainer/Policy mu Min                                  -1.3922\n",
      "trainer/Policy log std Mean                            -2.37193\n",
      "trainer/Policy log std Std                              0.234247\n",
      "trainer/Policy log std Max                             -1.40934\n",
      "trainer/Policy log std Min                             -3.35162\n",
      "trainer/Alpha                                           0.0138093\n",
      "trainer/Alpha Loss                                     -0.451336\n",
      "exploration/num steps total                         29000\n",
      "exploration/num paths total                           123\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.944489\n",
      "exploration/Rewards Std                                 0.0739804\n",
      "exploration/Rewards Max                                 1.48577\n",
      "exploration/Rewards Min                                 0.493019\n",
      "exploration/Returns Mean                              944.489\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               944.489\n",
      "exploration/Returns Min                               944.489\n",
      "exploration/Actions Mean                                0.00167985\n",
      "exploration/Actions Std                                 0.135122\n",
      "exploration/Actions Max                                 0.52811\n",
      "exploration/Actions Min                                -0.669302\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           944.489\n",
      "exploration/env_infos/final/reward_forward Mean         0.00458262\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00458262\n",
      "exploration/env_infos/final/reward_forward Min          0.00458262\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0723322\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0723322\n",
      "exploration/env_infos/initial/reward_forward Min        0.0723322\n",
      "exploration/env_infos/reward_forward Mean               0.00333266\n",
      "exploration/env_infos/reward_forward Std                0.190615\n",
      "exploration/env_infos/reward_forward Max                0.937207\n",
      "exploration/env_infos/reward_forward Min               -0.697616\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.141958\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.141958\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.141958\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0209105\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0209105\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0209105\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0730431\n",
      "exploration/env_infos/reward_ctrl Std                   0.0405488\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00817122\n",
      "exploration/env_infos/reward_ctrl Min                  -0.506981\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00968495\n",
      "exploration/env_infos/final/torso_velocity Std          0.00784975\n",
      "exploration/env_infos/final/torso_velocity Max          0.0207744\n",
      "exploration/env_infos/final/torso_velocity Min          0.00369782\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164105\n",
      "exploration/env_infos/initial/torso_velocity Std        0.231092\n",
      "exploration/env_infos/initial/torso_velocity Max        0.481633\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0616487\n",
      "exploration/env_infos/torso_velocity Mean              -0.0124028\n",
      "exploration/env_infos/torso_velocity Std                0.186493\n",
      "exploration/env_infos/torso_velocity Max                0.937207\n",
      "exploration/env_infos/torso_velocity Min               -1.76497\n",
      "evaluation/num steps total                         700000\n",
      "evaluation/num paths total                            700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.963104\n",
      "evaluation/Rewards Std                                  0.0346028\n",
      "evaluation/Rewards Max                                  2.58225\n",
      "evaluation/Rewards Min                                  0.506972\n",
      "evaluation/Returns Mean                               963.104\n",
      "evaluation/Returns Std                                 11.3709\n",
      "evaluation/Returns Max                                976.924\n",
      "evaluation/Returns Min                                912.058\n",
      "evaluation/Actions Mean                                 0.00694517\n",
      "evaluation/Actions Std                                  0.0970491\n",
      "evaluation/Actions Max                                  0.515317\n",
      "evaluation/Actions Min                                 -0.692487\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            963.104\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.22671e-07\n",
      "evaluation/env_infos/final/reward_forward Std           5.08053e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.49495e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -1.01835e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0285713\n",
      "evaluation/env_infos/initial/reward_forward Std         0.126941\n",
      "evaluation/env_infos/initial/reward_forward Max         0.24685\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.217098\n",
      "evaluation/env_infos/reward_forward Mean                0.00162777\n",
      "evaluation/env_infos/reward_forward Std                 0.062407\n",
      "evaluation/env_infos/reward_forward Max                 1.17969\n",
      "evaluation/env_infos/reward_forward Min                -1.38624\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.036425\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0114203\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0219814\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0875558\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0205147\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00433\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0138023\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0287392\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.037867\n",
      "evaluation/env_infos/reward_ctrl Std                    0.019922\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00833058\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.493028\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.48651e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.75875e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.56723e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.01835e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.153858\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.223667\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.648026\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.217098\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00200086\n",
      "evaluation/env_infos/torso_velocity Std                 0.0603734\n",
      "evaluation/env_infos/torso_velocity Max                 1.17969\n",
      "evaluation/env_infos/torso_velocity Min                -1.73982\n",
      "time/data storing (s)                                   0.315822\n",
      "time/evaluation sampling (s)                           42.5911\n",
      "time/exploration sampling (s)                           1.86757\n",
      "time/logging (s)                                        0.282802\n",
      "time/saving (s)                                         0.0261321\n",
      "time/training (s)                                       3.80513\n",
      "time/epoch (s)                                         48.8885\n",
      "time/total (s)                                       1439.48\n",
      "Epoch                                                  27\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:48:52.776774 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 28 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  59898\n",
      "trainer/QF1 Loss                                        0.304758\n",
      "trainer/QF2 Loss                                        0.336107\n",
      "trainer/Policy Loss                                    -2.73603\n",
      "trainer/Q1 Predictions Mean                             9.77267\n",
      "trainer/Q1 Predictions Std                              1.19742\n",
      "trainer/Q1 Predictions Max                             13.8201\n",
      "trainer/Q1 Predictions Min                              4.88645\n",
      "trainer/Q2 Predictions Mean                             9.6537\n",
      "trainer/Q2 Predictions Std                              1.16165\n",
      "trainer/Q2 Predictions Max                             13.1583\n",
      "trainer/Q2 Predictions Min                              4.36522\n",
      "trainer/Q Targets Mean                                  9.97068\n",
      "trainer/Q Targets Std                                   1.19789\n",
      "trainer/Q Targets Max                                  13.5096\n",
      "trainer/Q Targets Min                                   4.68479\n",
      "trainer/Log Pis Mean                                    7.38539\n",
      "trainer/Log Pis Std                                     2.14226\n",
      "trainer/Log Pis Max                                    11.8919\n",
      "trainer/Log Pis Min                                     0.767689\n",
      "trainer/Policy mu Mean                                  0.00546933\n",
      "trainer/Policy mu Std                                   0.154832\n",
      "trainer/Policy mu Max                                   0.810745\n",
      "trainer/Policy mu Min                                  -0.689259\n",
      "trainer/Policy log std Mean                            -2.30838\n",
      "trainer/Policy log std Std                              0.199085\n",
      "trainer/Policy log std Max                             -1.6409\n",
      "trainer/Policy log std Min                             -2.97395\n",
      "trainer/Alpha                                           0.0132698\n",
      "trainer/Alpha Loss                                     -2.65624\n",
      "exploration/num steps total                         30000\n",
      "exploration/num paths total                           124\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.957735\n",
      "exploration/Rewards Std                                 0.161459\n",
      "exploration/Rewards Max                                 2.38443\n",
      "exploration/Rewards Min                                 0.664723\n",
      "exploration/Returns Mean                              957.735\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               957.735\n",
      "exploration/Returns Min                               957.735\n",
      "exploration/Actions Mean                                0.00372363\n",
      "exploration/Actions Std                                 0.145907\n",
      "exploration/Actions Max                                 0.529092\n",
      "exploration/Actions Min                                -0.645759\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           957.735\n",
      "exploration/env_infos/final/reward_forward Mean         0.0882622\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0882622\n",
      "exploration/env_infos/final/reward_forward Min          0.0882622\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0376081\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0376081\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0376081\n",
      "exploration/env_infos/reward_forward Mean               0.0493947\n",
      "exploration/env_infos/reward_forward Std                0.262266\n",
      "exploration/env_infos/reward_forward Max                1.12682\n",
      "exploration/env_infos/reward_forward Min               -1.45383\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0817331\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0817331\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0817331\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.059082\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.059082\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.059082\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0852104\n",
      "exploration/env_infos/reward_ctrl Std                   0.0449763\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00340447\n",
      "exploration/env_infos/reward_ctrl Min                  -0.335277\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0287659\n",
      "exploration/env_infos/final/torso_velocity Std          0.0927229\n",
      "exploration/env_infos/final/torso_velocity Max          0.0882622\n",
      "exploration/env_infos/final/torso_velocity Min         -0.138511\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0415328\n",
      "exploration/env_infos/initial/torso_velocity Std        0.341197\n",
      "exploration/env_infos/initial/torso_velocity Max        0.493324\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.331117\n",
      "exploration/env_infos/torso_velocity Mean               0.0135996\n",
      "exploration/env_infos/torso_velocity Std                0.280696\n",
      "exploration/env_infos/torso_velocity Max                1.12682\n",
      "exploration/env_infos/torso_velocity Min               -1.45383\n",
      "evaluation/num steps total                         725000\n",
      "evaluation/num paths total                            725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.978475\n",
      "evaluation/Rewards Std                                  0.054237\n",
      "evaluation/Rewards Max                                  2.1879\n",
      "evaluation/Rewards Min                                  0.525616\n",
      "evaluation/Returns Mean                               978.475\n",
      "evaluation/Returns Std                                 10.248\n",
      "evaluation/Returns Max                               1015.4\n",
      "evaluation/Returns Min                                959.061\n",
      "evaluation/Actions Mean                                 0.0307483\n",
      "evaluation/Actions Std                                  0.0814655\n",
      "evaluation/Actions Max                                  0.624008\n",
      "evaluation/Actions Min                                 -0.650164\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            978.475\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0100538\n",
      "evaluation/env_infos/final/reward_forward Std           0.0515303\n",
      "evaluation/env_infos/final/reward_forward Max           0.138885\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0823337\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0431533\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0848091\n",
      "evaluation/env_infos/initial/reward_forward Max         0.14456\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.213947\n",
      "evaluation/env_infos/reward_forward Mean                0.0172818\n",
      "evaluation/env_infos/reward_forward Std                 0.136338\n",
      "evaluation/env_infos/reward_forward Max                 1.48815\n",
      "evaluation/env_infos/reward_forward Min                -1.5559\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.027212\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0062695\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0166402\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0410841\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0373059\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0170303\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0194662\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0918018\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0303283\n",
      "evaluation/env_infos/reward_ctrl Std                    0.017531\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00868737\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.474384\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.0303368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/torso_velocity Std           0.107162\n",
      "evaluation/env_infos/final/torso_velocity Max           0.486479\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.201133\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151359\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242934\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.635611\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.213947\n",
      "evaluation/env_infos/torso_velocity Mean                0.0037239\n",
      "evaluation/env_infos/torso_velocity Std                 0.155257\n",
      "evaluation/env_infos/torso_velocity Max                 1.48815\n",
      "evaluation/env_infos/torso_velocity Min                -1.99683\n",
      "time/data storing (s)                                   0.262759\n",
      "time/evaluation sampling (s)                           41.4728\n",
      "time/exploration sampling (s)                           1.80289\n",
      "time/logging (s)                                        0.268493\n",
      "time/saving (s)                                         0.0259459\n",
      "time/training (s)                                       3.73624\n",
      "time/epoch (s)                                         47.5691\n",
      "time/total (s)                                       1487.41\n",
      "Epoch                                                  28\n",
      "-------------------------------------------------  ---------------\n",
      "2021-05-25 13:49:40.708568 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 29 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  61898\n",
      "trainer/QF1 Loss                                        0.888537\n",
      "trainer/QF2 Loss                                        0.744035\n",
      "trainer/Policy Loss                                    -3.58059\n",
      "trainer/Q1 Predictions Mean                            10.357\n",
      "trainer/Q1 Predictions Std                              1.12664\n",
      "trainer/Q1 Predictions Max                             12.9828\n",
      "trainer/Q1 Predictions Min                              6.66497\n",
      "trainer/Q2 Predictions Mean                            10.1675\n",
      "trainer/Q2 Predictions Std                              1.10879\n",
      "trainer/Q2 Predictions Max                             12.9682\n",
      "trainer/Q2 Predictions Min                              5.99321\n",
      "trainer/Q Targets Mean                                 10.2555\n",
      "trainer/Q Targets Std                                   1.55771\n",
      "trainer/Q Targets Max                                  16.0471\n",
      "trainer/Q Targets Min                                  -0.940067\n",
      "trainer/Log Pis Mean                                    7.04448\n",
      "trainer/Log Pis Std                                     2.71343\n",
      "trainer/Log Pis Max                                    13.9752\n",
      "trainer/Log Pis Min                                    -4.6157\n",
      "trainer/Policy mu Mean                                 -0.0386889\n",
      "trainer/Policy mu Std                                   0.175079\n",
      "trainer/Policy mu Max                                   1.01929\n",
      "trainer/Policy mu Min                                  -1.11684\n",
      "trainer/Policy log std Mean                            -2.29054\n",
      "trainer/Policy log std Std                              0.237062\n",
      "trainer/Policy log std Max                             -1.56617\n",
      "trainer/Policy log std Min                             -3.43603\n",
      "trainer/Alpha                                           0.0126415\n",
      "trainer/Alpha Loss                                     -4.1758\n",
      "exploration/num steps total                         31000\n",
      "exploration/num paths total                           125\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.890615\n",
      "exploration/Rewards Std                                 0.123577\n",
      "exploration/Rewards Max                                 2.04774\n",
      "exploration/Rewards Min                                 0.528731\n",
      "exploration/Returns Mean                              890.615\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               890.615\n",
      "exploration/Returns Min                               890.615\n",
      "exploration/Actions Mean                               -0.0453534\n",
      "exploration/Actions Std                                 0.17332\n",
      "exploration/Actions Max                                 0.601073\n",
      "exploration/Actions Min                                -0.703911\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           890.615\n",
      "exploration/env_infos/final/reward_forward Mean        -0.716456\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.716456\n",
      "exploration/env_infos/final/reward_forward Min         -0.716456\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0453307\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0453307\n",
      "exploration/env_infos/initial/reward_forward Min        0.0453307\n",
      "exploration/env_infos/reward_forward Mean               0.00959283\n",
      "exploration/env_infos/reward_forward Std                0.499357\n",
      "exploration/env_infos/reward_forward Max                1.39217\n",
      "exploration/env_infos/reward_forward Min               -1.29877\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.177666\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.177666\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.177666\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0416916\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0416916\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0416916\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.128387\n",
      "exploration/env_infos/reward_ctrl Std                   0.0622826\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0180565\n",
      "exploration/env_infos/reward_ctrl Min                  -0.471269\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.13269\n",
      "exploration/env_infos/final/torso_velocity Std          0.412948\n",
      "exploration/env_infos/final/torso_velocity Max          0.173393\n",
      "exploration/env_infos/final/torso_velocity Min         -0.716456\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.164146\n",
      "exploration/env_infos/initial/torso_velocity Std        0.155376\n",
      "exploration/env_infos/initial/torso_velocity Max        0.38363\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0453307\n",
      "exploration/env_infos/torso_velocity Mean              -0.00091571\n",
      "exploration/env_infos/torso_velocity Std                0.333334\n",
      "exploration/env_infos/torso_velocity Max                1.39217\n",
      "exploration/env_infos/torso_velocity Min               -1.6341\n",
      "evaluation/num steps total                         750000\n",
      "evaluation/num paths total                            750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.938984\n",
      "evaluation/Rewards Std                                  0.0481812\n",
      "evaluation/Rewards Max                                  2.48152\n",
      "evaluation/Rewards Min                                  0.502114\n",
      "evaluation/Returns Mean                               938.984\n",
      "evaluation/Returns Std                                 15.0761\n",
      "evaluation/Returns Max                                966.38\n",
      "evaluation/Returns Min                                911.8\n",
      "evaluation/Actions Mean                                -0.0533633\n",
      "evaluation/Actions Std                                  0.118575\n",
      "evaluation/Actions Max                                  0.581973\n",
      "evaluation/Actions Min                                 -0.700874\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            938.984\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00701748\n",
      "evaluation/env_infos/final/reward_forward Std           0.0812178\n",
      "evaluation/env_infos/final/reward_forward Max           0.324395\n",
      "evaluation/env_infos/final/reward_forward Min          -0.237962\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0108806\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124446\n",
      "evaluation/env_infos/initial/reward_forward Max         0.227872\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.267629\n",
      "evaluation/env_infos/reward_forward Mean                0.00229778\n",
      "evaluation/env_infos/reward_forward Std                 0.1138\n",
      "evaluation/env_infos/reward_forward Max                 1.50099\n",
      "evaluation/env_infos/reward_forward Min                -1.75948\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0693988\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0206269\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0381161\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.129015\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0237433\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00755595\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0113774\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0410814\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0676304\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0224178\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00584526\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.497886\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00172497\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0769679\n",
      "evaluation/env_infos/final/torso_velocity Max           0.324395\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.421889\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.131824\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245046\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.726657\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.321486\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00211117\n",
      "evaluation/env_infos/torso_velocity Std                 0.103003\n",
      "evaluation/env_infos/torso_velocity Max                 1.50099\n",
      "evaluation/env_infos/torso_velocity Min                -1.90465\n",
      "time/data storing (s)                                   0.262297\n",
      "time/evaluation sampling (s)                           41.5499\n",
      "time/exploration sampling (s)                           1.73573\n",
      "time/logging (s)                                        0.281091\n",
      "time/saving (s)                                         0.0267674\n",
      "time/training (s)                                       3.74409\n",
      "time/epoch (s)                                         47.5999\n",
      "time/total (s)                                       1535.35\n",
      "Epoch                                                  29\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:50:29.829929 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 30 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  63898\n",
      "trainer/QF1 Loss                                        0.387953\n",
      "trainer/QF2 Loss                                        0.337069\n",
      "trainer/Policy Loss                                    -2.75217\n",
      "trainer/Q1 Predictions Mean                            10.8195\n",
      "trainer/Q1 Predictions Std                              1.38939\n",
      "trainer/Q1 Predictions Max                             16.7923\n",
      "trainer/Q1 Predictions Min                              4.25899\n",
      "trainer/Q2 Predictions Mean                            10.6578\n",
      "trainer/Q2 Predictions Std                              1.30985\n",
      "trainer/Q2 Predictions Max                             15.7435\n",
      "trainer/Q2 Predictions Min                              4.76036\n",
      "trainer/Q Targets Mean                                 10.8015\n",
      "trainer/Q Targets Std                                   1.41544\n",
      "trainer/Q Targets Max                                  18.7688\n",
      "trainer/Q Targets Min                                   4.52735\n",
      "trainer/Log Pis Mean                                    8.36219\n",
      "trainer/Log Pis Std                                     2.57888\n",
      "trainer/Log Pis Max                                    15.6503\n",
      "trainer/Log Pis Min                                     1.06148\n",
      "trainer/Policy mu Mean                                  0.0219117\n",
      "trainer/Policy mu Std                                   0.177308\n",
      "trainer/Policy mu Max                                   0.779617\n",
      "trainer/Policy mu Min                                  -0.73659\n",
      "trainer/Policy log std Mean                            -2.43518\n",
      "trainer/Policy log std Std                              0.255431\n",
      "trainer/Policy log std Max                             -1.80096\n",
      "trainer/Policy log std Min                             -3.54611\n",
      "trainer/Alpha                                           0.0123136\n",
      "trainer/Alpha Loss                                      1.5925\n",
      "exploration/num steps total                         32000\n",
      "exploration/num paths total                           126\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.960827\n",
      "exploration/Rewards Std                                 0.18707\n",
      "exploration/Rewards Max                                 2.10363\n",
      "exploration/Rewards Min                                 0.53241\n",
      "exploration/Returns Mean                              960.827\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               960.827\n",
      "exploration/Returns Min                               960.827\n",
      "exploration/Actions Mean                                0.0423716\n",
      "exploration/Actions Std                                 0.144349\n",
      "exploration/Actions Max                                 0.594285\n",
      "exploration/Actions Min                                -0.72317\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           960.827\n",
      "exploration/env_infos/final/reward_forward Mean         0.508359\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.508359\n",
      "exploration/env_infos/final/reward_forward Min          0.508359\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.235659\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.235659\n",
      "exploration/env_infos/initial/reward_forward Min       -0.235659\n",
      "exploration/env_infos/reward_forward Mean              -0.0629676\n",
      "exploration/env_infos/reward_forward Std                0.334005\n",
      "exploration/env_infos/reward_forward Max                1.34656\n",
      "exploration/env_infos/reward_forward Min               -1.5482\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.121486\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.121486\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.121486\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0882222\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0882222\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0882222\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0905283\n",
      "exploration/env_infos/reward_ctrl Std                   0.0511944\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0075403\n",
      "exploration/env_infos/reward_ctrl Min                  -0.46759\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.573709\n",
      "exploration/env_infos/final/torso_velocity Std          0.0926177\n",
      "exploration/env_infos/final/torso_velocity Max          0.70469\n",
      "exploration/env_infos/final/torso_velocity Min          0.508077\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0461638\n",
      "exploration/env_infos/initial/torso_velocity Std        0.199465\n",
      "exploration/env_infos/initial/torso_velocity Max        0.197628\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.235659\n",
      "exploration/env_infos/torso_velocity Mean              -0.0308859\n",
      "exploration/env_infos/torso_velocity Std                0.356302\n",
      "exploration/env_infos/torso_velocity Max                1.34656\n",
      "exploration/env_infos/torso_velocity Min               -1.59802\n",
      "evaluation/num steps total                         775000\n",
      "evaluation/num paths total                            775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.954719\n",
      "evaluation/Rewards Std                                  0.0342914\n",
      "evaluation/Rewards Max                                  2.0271\n",
      "evaluation/Rewards Min                                  0.195016\n",
      "evaluation/Returns Mean                               954.719\n",
      "evaluation/Returns Std                                 17.6586\n",
      "evaluation/Returns Max                                975.645\n",
      "evaluation/Returns Min                                909.291\n",
      "evaluation/Actions Mean                                 0.0360918\n",
      "evaluation/Actions Std                                  0.101818\n",
      "evaluation/Actions Max                                  0.778649\n",
      "evaluation/Actions Min                                 -0.726485\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            954.719\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00710298\n",
      "evaluation/env_infos/final/reward_forward Std           0.0347956\n",
      "evaluation/env_infos/final/reward_forward Max           9.2504e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.177566\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0249149\n",
      "evaluation/env_infos/initial/reward_forward Std         0.112115\n",
      "evaluation/env_infos/initial/reward_forward Max         0.211701\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.169321\n",
      "evaluation/env_infos/reward_forward Mean                0.00114531\n",
      "evaluation/env_infos/reward_forward Std                 0.0782932\n",
      "evaluation/env_infos/reward_forward Max                 1.73195\n",
      "evaluation/env_infos/reward_forward Min                -1.46416\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0458051\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0191984\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0251618\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0911928\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0455313\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0189962\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0290006\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.106141\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.046678\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0244801\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0194669\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.804984\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00293599\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0210901\n",
      "evaluation/env_infos/final/torso_velocity Max           0.00668438\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.177566\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128897\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.26011\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.634043\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.28053\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00285517\n",
      "evaluation/env_infos/torso_velocity Std                 0.0784036\n",
      "evaluation/env_infos/torso_velocity Max                 1.73195\n",
      "evaluation/env_infos/torso_velocity Min                -1.81052\n",
      "time/data storing (s)                                   0.306692\n",
      "time/evaluation sampling (s)                           42.7786\n",
      "time/exploration sampling (s)                           1.60971\n",
      "time/logging (s)                                        0.279058\n",
      "time/saving (s)                                         0.0258082\n",
      "time/training (s)                                       3.75502\n",
      "time/epoch (s)                                         48.7549\n",
      "time/total (s)                                       1584.47\n",
      "Epoch                                                  30\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:51:17.726415 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 31 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  65898\n",
      "trainer/QF1 Loss                                        0.352228\n",
      "trainer/QF2 Loss                                        0.308292\n",
      "trainer/Policy Loss                                    -3.41806\n",
      "trainer/Q1 Predictions Mean                            10.9819\n",
      "trainer/Q1 Predictions Std                              1.11446\n",
      "trainer/Q1 Predictions Max                             13.5898\n",
      "trainer/Q1 Predictions Min                              6.95377\n",
      "trainer/Q2 Predictions Mean                            11.0195\n",
      "trainer/Q2 Predictions Std                              1.11257\n",
      "trainer/Q2 Predictions Max                             13.0034\n",
      "trainer/Q2 Predictions Min                              7.36385\n",
      "trainer/Q Targets Mean                                 11.1071\n",
      "trainer/Q Targets Std                                   1.22733\n",
      "trainer/Q Targets Max                                  14.2858\n",
      "trainer/Q Targets Min                                   5.56468\n",
      "trainer/Log Pis Mean                                    7.94716\n",
      "trainer/Log Pis Std                                     2.1566\n",
      "trainer/Log Pis Max                                    13.2841\n",
      "trainer/Log Pis Min                                    -0.345096\n",
      "trainer/Policy mu Mean                                 -0.0963052\n",
      "trainer/Policy mu Std                                   0.142916\n",
      "trainer/Policy mu Max                                   0.609123\n",
      "trainer/Policy mu Min                                  -1.11822\n",
      "trainer/Policy log std Mean                            -2.36624\n",
      "trainer/Policy log std Std                              0.195523\n",
      "trainer/Policy log std Max                             -1.50459\n",
      "trainer/Policy log std Min                             -3.00989\n",
      "trainer/Alpha                                           0.0118577\n",
      "trainer/Alpha Loss                                     -0.234332\n",
      "exploration/num steps total                         33000\n",
      "exploration/num paths total                           127\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.834834\n",
      "exploration/Rewards Std                                 0.0623546\n",
      "exploration/Rewards Max                                 1.23667\n",
      "exploration/Rewards Min                                 0.44067\n",
      "exploration/Returns Mean                              834.834\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               834.834\n",
      "exploration/Returns Min                               834.834\n",
      "exploration/Actions Mean                               -0.122494\n",
      "exploration/Actions Std                                 0.163798\n",
      "exploration/Actions Max                                 0.40921\n",
      "exploration/Actions Min                                -0.63077\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           834.834\n",
      "exploration/env_infos/final/reward_forward Mean         0.00168489\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00168489\n",
      "exploration/env_infos/final/reward_forward Min          0.00168489\n",
      "exploration/env_infos/initial/reward_forward Mean       0.101451\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.101451\n",
      "exploration/env_infos/initial/reward_forward Min        0.101451\n",
      "exploration/env_infos/reward_forward Mean               0.00341541\n",
      "exploration/env_infos/reward_forward Std                0.381005\n",
      "exploration/env_infos/reward_forward Max                1.33098\n",
      "exploration/env_infos/reward_forward Min               -1.397\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.171723\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.171723\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.171723\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0445994\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0445994\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0445994\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.167338\n",
      "exploration/env_infos/reward_ctrl Std                   0.0593424\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0301383\n",
      "exploration/env_infos/reward_ctrl Min                  -0.55933\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0420628\n",
      "exploration/env_infos/final/torso_velocity Std          0.130724\n",
      "exploration/env_infos/final/torso_velocity Max          0.21849\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0939871\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.260421\n",
      "exploration/env_infos/initial/torso_velocity Std        0.169283\n",
      "exploration/env_infos/initial/torso_velocity Max        0.494927\n",
      "exploration/env_infos/initial/torso_velocity Min        0.101451\n",
      "exploration/env_infos/torso_velocity Mean              -0.00513874\n",
      "exploration/env_infos/torso_velocity Std                0.259751\n",
      "exploration/env_infos/torso_velocity Max                1.33098\n",
      "exploration/env_infos/torso_velocity Min               -1.67331\n",
      "evaluation/num steps total                         800000\n",
      "evaluation/num paths total                            800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.851361\n",
      "evaluation/Rewards Std                                  0.0589141\n",
      "evaluation/Rewards Max                                  2.42315\n",
      "evaluation/Rewards Min                                  0.52016\n",
      "evaluation/Returns Mean                               851.361\n",
      "evaluation/Returns Std                                 17.8338\n",
      "evaluation/Returns Max                                901.33\n",
      "evaluation/Returns Min                                834.998\n",
      "evaluation/Actions Mean                                -0.127162\n",
      "evaluation/Actions Std                                  0.1487\n",
      "evaluation/Actions Max                                  0.584374\n",
      "evaluation/Actions Min                                 -0.640066\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            851.361\n",
      "evaluation/env_infos/final/reward_forward Mean          9.36012e-08\n",
      "evaluation/env_infos/final/reward_forward Std           5.10711e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.01533e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -1.00986e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0427742\n",
      "evaluation/env_infos/initial/reward_forward Std         0.106734\n",
      "evaluation/env_infos/initial/reward_forward Max         0.140323\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.288584\n",
      "evaluation/env_infos/reward_forward Mean                0.00225473\n",
      "evaluation/env_infos/reward_forward Std                 0.122122\n",
      "evaluation/env_infos/reward_forward Max                 1.47534\n",
      "evaluation/env_infos/reward_forward Min                -1.38424\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.153419\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.012446\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.119181\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.168451\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0310959\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0132191\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00662939\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0585151\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.153128\n",
      "evaluation/env_infos/reward_ctrl Std                    0.017807\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00662939\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.47984\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          6.92234e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.62067e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.01533e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.00986e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.134997\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258827\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.680708\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.288584\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000435282\n",
      "evaluation/env_infos/torso_velocity Std                 0.0907852\n",
      "evaluation/env_infos/torso_velocity Max                 1.63639\n",
      "evaluation/env_infos/torso_velocity Min                -1.73617\n",
      "time/data storing (s)                                   0.305992\n",
      "time/evaluation sampling (s)                           41.4088\n",
      "time/exploration sampling (s)                           1.77347\n",
      "time/logging (s)                                        0.279325\n",
      "time/saving (s)                                         0.0257478\n",
      "time/training (s)                                       3.73445\n",
      "time/epoch (s)                                         47.5278\n",
      "time/total (s)                                       1632.37\n",
      "Epoch                                                  31\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:52:06.328954 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 32 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  67898\n",
      "trainer/QF1 Loss                                        0.450046\n",
      "trainer/QF2 Loss                                        0.399015\n",
      "trainer/Policy Loss                                    -3.83414\n",
      "trainer/Q1 Predictions Mean                            11.3682\n",
      "trainer/Q1 Predictions Std                              1.41541\n",
      "trainer/Q1 Predictions Max                             17.6993\n",
      "trainer/Q1 Predictions Min                              7.09732\n",
      "trainer/Q2 Predictions Mean                            11.4393\n",
      "trainer/Q2 Predictions Std                              1.41957\n",
      "trainer/Q2 Predictions Max                             18.0291\n",
      "trainer/Q2 Predictions Min                              7.36358\n",
      "trainer/Q Targets Mean                                 11.3889\n",
      "trainer/Q Targets Std                                   1.47124\n",
      "trainer/Q Targets Max                                  20.4308\n",
      "trainer/Q Targets Min                                   6.15287\n",
      "trainer/Log Pis Mean                                    7.93135\n",
      "trainer/Log Pis Std                                     2.45177\n",
      "trainer/Log Pis Max                                    17.4236\n",
      "trainer/Log Pis Min                                    -1.72505\n",
      "trainer/Policy mu Mean                                  0.000748445\n",
      "trainer/Policy mu Std                                   0.168115\n",
      "trainer/Policy mu Max                                   0.919507\n",
      "trainer/Policy mu Min                                  -1.67708\n",
      "trainer/Policy log std Mean                            -2.36233\n",
      "trainer/Policy log std Std                              0.235785\n",
      "trainer/Policy log std Max                             -1.42571\n",
      "trainer/Policy log std Min                             -3.82552\n",
      "trainer/Alpha                                           0.0113866\n",
      "trainer/Alpha Loss                                     -0.307211\n",
      "exploration/num steps total                         34000\n",
      "exploration/num paths total                           128\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.90321\n",
      "exploration/Rewards Std                                 0.0888429\n",
      "exploration/Rewards Max                                 1.91378\n",
      "exploration/Rewards Min                                 0.483914\n",
      "exploration/Returns Mean                              903.21\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               903.21\n",
      "exploration/Returns Min                               903.21\n",
      "exploration/Actions Mean                               -0.00403317\n",
      "exploration/Actions Std                                 0.165067\n",
      "exploration/Actions Max                                 0.594521\n",
      "exploration/Actions Min                                -0.579114\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           903.21\n",
      "exploration/env_infos/final/reward_forward Mean         0.0318306\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0318306\n",
      "exploration/env_infos/final/reward_forward Min          0.0318306\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00804233\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00804233\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00804233\n",
      "exploration/env_infos/reward_forward Mean               0.0569315\n",
      "exploration/env_infos/reward_forward Std                0.364283\n",
      "exploration/env_infos/reward_forward Max                1.8232\n",
      "exploration/env_infos/reward_forward Min               -1.11591\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.122016\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.122016\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.122016\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.063022\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.063022\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.063022\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109054\n",
      "exploration/env_infos/reward_ctrl Std                   0.0524703\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0100901\n",
      "exploration/env_infos/reward_ctrl Min                  -0.516086\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0115092\n",
      "exploration/env_infos/final/torso_velocity Std          0.0543928\n",
      "exploration/env_infos/final/torso_velocity Max          0.0318306\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0882165\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0493601\n",
      "exploration/env_infos/initial/torso_velocity Std        0.19684\n",
      "exploration/env_infos/initial/torso_velocity Max        0.313959\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.157836\n",
      "exploration/env_infos/torso_velocity Mean               0.0254313\n",
      "exploration/env_infos/torso_velocity Std                0.298291\n",
      "exploration/env_infos/torso_velocity Max                1.8232\n",
      "exploration/env_infos/torso_velocity Min               -1.257\n",
      "evaluation/num steps total                         825000\n",
      "evaluation/num paths total                            825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.957238\n",
      "evaluation/Rewards Std                                  0.0402831\n",
      "evaluation/Rewards Max                                  2.53605\n",
      "evaluation/Rewards Min                                  0.52532\n",
      "evaluation/Returns Mean                               957.238\n",
      "evaluation/Returns Std                                 19.4596\n",
      "evaluation/Returns Max                                971.983\n",
      "evaluation/Returns Min                                903.071\n",
      "evaluation/Actions Mean                                 0.0150739\n",
      "evaluation/Actions Std                                  0.105019\n",
      "evaluation/Actions Max                                  0.62452\n",
      "evaluation/Actions Min                                 -0.557194\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            957.238\n",
      "evaluation/env_infos/final/reward_forward Mean          8.72758e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.15142e-07\n",
      "evaluation/env_infos/final/reward_forward Max           6.81852e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.08875e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.000425506\n",
      "evaluation/env_infos/initial/reward_forward Std         0.12851\n",
      "evaluation/env_infos/initial/reward_forward Max         0.279299\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.269255\n",
      "evaluation/env_infos/reward_forward Mean               -7.32706e-05\n",
      "evaluation/env_infos/reward_forward Std                 0.0754647\n",
      "evaluation/env_infos/reward_forward Max                 0.958128\n",
      "evaluation/env_infos/reward_forward Min                -1.56208\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0431594\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0217626\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0264867\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.104253\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0181786\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0115691\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0051143\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0531043\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0450246\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0252017\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0051143\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.47468\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          3.36855e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.62478e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           6.81852e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -4.08875e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.153171\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246064\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.66751\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.269255\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000542571\n",
      "evaluation/env_infos/torso_velocity Std                 0.0779438\n",
      "evaluation/env_infos/torso_velocity Max                 1.38517\n",
      "evaluation/env_infos/torso_velocity Min                -1.88701\n",
      "time/data storing (s)                                   0.31614\n",
      "time/evaluation sampling (s)                           41.6287\n",
      "time/exploration sampling (s)                           1.76216\n",
      "time/logging (s)                                        0.300528\n",
      "time/saving (s)                                         0.0276968\n",
      "time/training (s)                                       4.20806\n",
      "time/epoch (s)                                         48.2433\n",
      "time/total (s)                                       1680.99\n",
      "Epoch                                                  32\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:52:55.298577 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 33 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                  69898\n",
      "trainer/QF1 Loss                                        0.535012\n",
      "trainer/QF2 Loss                                        0.604317\n",
      "trainer/Policy Loss                                    -4.27613\n",
      "trainer/Q1 Predictions Mean                            11.6553\n",
      "trainer/Q1 Predictions Std                              1.27749\n",
      "trainer/Q1 Predictions Max                             14.2519\n",
      "trainer/Q1 Predictions Min                              6.26736\n",
      "trainer/Q2 Predictions Mean                            11.6032\n",
      "trainer/Q2 Predictions Std                              1.25802\n",
      "trainer/Q2 Predictions Max                             14.1444\n",
      "trainer/Q2 Predictions Min                              6.24876\n",
      "trainer/Q Targets Mean                                 11.7221\n",
      "trainer/Q Targets Std                                   1.50657\n",
      "trainer/Q Targets Max                                  16.3802\n",
      "trainer/Q Targets Min                                  -0.343527\n",
      "trainer/Log Pis Mean                                    7.75931\n",
      "trainer/Log Pis Std                                     2.29653\n",
      "trainer/Log Pis Max                                    15.165\n",
      "trainer/Log Pis Min                                     0.717742\n",
      "trainer/Policy mu Mean                                 -0.0239649\n",
      "trainer/Policy mu Std                                   0.17975\n",
      "trainer/Policy mu Max                                   0.86837\n",
      "trainer/Policy mu Min                                  -1.23773\n",
      "trainer/Policy log std Mean                            -2.34119\n",
      "trainer/Policy log std Std                              0.235744\n",
      "trainer/Policy log std Max                             -1.49291\n",
      "trainer/Policy log std Min                             -3.66519\n",
      "trainer/Alpha                                           0.0109796\n",
      "trainer/Alpha Loss                                     -1.08588\n",
      "exploration/num steps total                         35000\n",
      "exploration/num paths total                           129\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.885985\n",
      "exploration/Rewards Std                                 0.135732\n",
      "exploration/Rewards Max                                 1.72476\n",
      "exploration/Rewards Min                                 0.393894\n",
      "exploration/Returns Mean                              885.985\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               885.985\n",
      "exploration/Returns Min                               885.985\n",
      "exploration/Actions Mean                               -0.0581124\n",
      "exploration/Actions Std                                 0.178331\n",
      "exploration/Actions Max                                 0.704303\n",
      "exploration/Actions Min                                -0.722522\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           885.985\n",
      "exploration/env_infos/final/reward_forward Mean        -0.054333\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.054333\n",
      "exploration/env_infos/final/reward_forward Min         -0.054333\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0499039\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0499039\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0499039\n",
      "exploration/env_infos/reward_forward Mean               0.0339029\n",
      "exploration/env_infos/reward_forward Std                0.346235\n",
      "exploration/env_infos/reward_forward Max                1.32636\n",
      "exploration/env_infos/reward_forward Min               -1.53458\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0632847\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0632847\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0632847\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0218451\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0218451\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0218451\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.140716\n",
      "exploration/env_infos/reward_ctrl Std                   0.0797476\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0214608\n",
      "exploration/env_infos/reward_ctrl Min                  -0.606106\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00850234\n",
      "exploration/env_infos/final/torso_velocity Std          0.0401383\n",
      "exploration/env_infos/final/torso_velocity Max          0.0434179\n",
      "exploration/env_infos/final/torso_velocity Min         -0.054333\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0684878\n",
      "exploration/env_infos/initial/torso_velocity Std        0.107361\n",
      "exploration/env_infos/initial/torso_velocity Max        0.210007\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0499039\n",
      "exploration/env_infos/torso_velocity Mean               0.021016\n",
      "exploration/env_infos/torso_velocity Std                0.361635\n",
      "exploration/env_infos/torso_velocity Max                1.35967\n",
      "exploration/env_infos/torso_velocity Min               -1.53458\n",
      "evaluation/num steps total                         850000\n",
      "evaluation/num paths total                            850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.952705\n",
      "evaluation/Rewards Std                                  0.139015\n",
      "evaluation/Rewards Max                                  2.34574\n",
      "evaluation/Rewards Min                                  0.365334\n",
      "evaluation/Returns Mean                               952.705\n",
      "evaluation/Returns Std                                 34.2356\n",
      "evaluation/Returns Max                               1055.48\n",
      "evaluation/Returns Min                                893.997\n",
      "evaluation/Actions Mean                                -0.034055\n",
      "evaluation/Actions Std                                  0.135188\n",
      "evaluation/Actions Max                                  0.657179\n",
      "evaluation/Actions Min                                 -0.720365\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            952.705\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0232902\n",
      "evaluation/env_infos/final/reward_forward Std           0.242199\n",
      "evaluation/env_infos/final/reward_forward Max           0.603455\n",
      "evaluation/env_infos/final/reward_forward Min          -0.662699\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0285698\n",
      "evaluation/env_infos/initial/reward_forward Std         0.142688\n",
      "evaluation/env_infos/initial/reward_forward Max         0.168011\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.382803\n",
      "evaluation/env_infos/reward_forward Mean                0.0452163\n",
      "evaluation/env_infos/reward_forward Std                 0.357689\n",
      "evaluation/env_infos/reward_forward Max                 1.60814\n",
      "evaluation/env_infos/reward_forward Min                -1.23531\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.069668\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0296169\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0431709\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.148193\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.039911\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0188875\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.01368\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.103731\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0777422\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0527433\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0134865\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.634666\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0299164\n",
      "evaluation/env_infos/final/torso_velocity Std           0.267053\n",
      "evaluation/env_infos/final/torso_velocity Max           0.87915\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.666288\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.126611\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246239\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.585723\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.382803\n",
      "evaluation/env_infos/torso_velocity Mean                0.0122724\n",
      "evaluation/env_infos/torso_velocity Std                 0.288191\n",
      "evaluation/env_infos/torso_velocity Max                 1.60814\n",
      "evaluation/env_infos/torso_velocity Min                -1.73613\n",
      "time/data storing (s)                                   0.318443\n",
      "time/evaluation sampling (s)                           42.2946\n",
      "time/exploration sampling (s)                           1.80884\n",
      "time/logging (s)                                        0.279689\n",
      "time/saving (s)                                         0.0259647\n",
      "time/training (s)                                       3.80662\n",
      "time/epoch (s)                                         48.5341\n",
      "time/total (s)                                       1729.94\n",
      "Epoch                                                  33\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:53:43.757285 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 34 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  71898\n",
      "trainer/QF1 Loss                                        0.388402\n",
      "trainer/QF2 Loss                                        0.382054\n",
      "trainer/Policy Loss                                    -5.45309\n",
      "trainer/Q1 Predictions Mean                            12.098\n",
      "trainer/Q1 Predictions Std                              1.42453\n",
      "trainer/Q1 Predictions Max                             17.3727\n",
      "trainer/Q1 Predictions Min                              6.25751\n",
      "trainer/Q2 Predictions Mean                            12.0416\n",
      "trainer/Q2 Predictions Std                              1.42605\n",
      "trainer/Q2 Predictions Max                             18.3835\n",
      "trainer/Q2 Predictions Min                              7.053\n",
      "trainer/Q Targets Mean                                 12.1314\n",
      "trainer/Q Targets Std                                   1.37167\n",
      "trainer/Q Targets Max                                  19.1767\n",
      "trainer/Q Targets Min                                   7.92305\n",
      "trainer/Log Pis Mean                                    6.96757\n",
      "trainer/Log Pis Std                                     2.22161\n",
      "trainer/Log Pis Max                                    12.8087\n",
      "trainer/Log Pis Min                                     0.887493\n",
      "trainer/Policy mu Mean                                  0.000421284\n",
      "trainer/Policy mu Std                                   0.187208\n",
      "trainer/Policy mu Max                                   1.19027\n",
      "trainer/Policy mu Min                                  -0.764185\n",
      "trainer/Policy log std Mean                            -2.22101\n",
      "trainer/Policy log std Std                              0.254734\n",
      "trainer/Policy log std Max                             -1.33008\n",
      "trainer/Policy log std Min                             -3.57324\n",
      "trainer/Alpha                                           0.0108373\n",
      "trainer/Alpha Loss                                     -4.67087\n",
      "exploration/num steps total                         36000\n",
      "exploration/num paths total                           130\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.883747\n",
      "exploration/Rewards Std                                 0.081027\n",
      "exploration/Rewards Max                                 1.63038\n",
      "exploration/Rewards Min                                 0.654721\n",
      "exploration/Returns Mean                              883.747\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               883.747\n",
      "exploration/Returns Min                               883.747\n",
      "exploration/Actions Mean                                0.024501\n",
      "exploration/Actions Std                                 0.178153\n",
      "exploration/Actions Max                                 0.628226\n",
      "exploration/Actions Min                                -0.624788\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           883.747\n",
      "exploration/env_infos/final/reward_forward Mean         0.67984\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.67984\n",
      "exploration/env_infos/final/reward_forward Min          0.67984\n",
      "exploration/env_infos/initial/reward_forward Mean       0.00697414\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.00697414\n",
      "exploration/env_infos/initial/reward_forward Min        0.00697414\n",
      "exploration/env_infos/reward_forward Mean               0.005454\n",
      "exploration/env_infos/reward_forward Std                0.169636\n",
      "exploration/env_infos/reward_forward Max                1.01204\n",
      "exploration/env_infos/reward_forward Min               -0.576217\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.203117\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.203117\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.203117\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.120381\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.120381\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.120381\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.129355\n",
      "exploration/env_infos/reward_ctrl Std                   0.053392\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0124809\n",
      "exploration/env_infos/reward_ctrl Min                  -0.345279\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0292934\n",
      "exploration/env_infos/final/torso_velocity Std          0.471992\n",
      "exploration/env_infos/final/torso_velocity Max          0.67984\n",
      "exploration/env_infos/final/torso_velocity Min         -0.425426\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0393585\n",
      "exploration/env_infos/initial/torso_velocity Std        0.297204\n",
      "exploration/env_infos/initial/torso_velocity Max        0.418468\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.307367\n",
      "exploration/env_infos/torso_velocity Mean               0.00193219\n",
      "exploration/env_infos/torso_velocity Std                0.137797\n",
      "exploration/env_infos/torso_velocity Max                1.01204\n",
      "exploration/env_infos/torso_velocity Min               -1.06817\n",
      "evaluation/num steps total                         875000\n",
      "evaluation/num paths total                            875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.928149\n",
      "evaluation/Rewards Std                                  0.0288463\n",
      "evaluation/Rewards Max                                  2.31013\n",
      "evaluation/Rewards Min                                  0.650434\n",
      "evaluation/Returns Mean                               928.149\n",
      "evaluation/Returns Std                                 18.4365\n",
      "evaluation/Returns Max                                956.561\n",
      "evaluation/Returns Min                                886.122\n",
      "evaluation/Actions Mean                                 0.0113164\n",
      "evaluation/Actions Std                                  0.134338\n",
      "evaluation/Actions Max                                  0.642058\n",
      "evaluation/Actions Min                                 -0.644581\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            928.149\n",
      "evaluation/env_infos/final/reward_forward Mean         -3.46503e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.18733e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.26895e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.0525e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0211039\n",
      "evaluation/env_infos/initial/reward_forward Std         0.104657\n",
      "evaluation/env_infos/initial/reward_forward Max         0.210215\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.187761\n",
      "evaluation/env_infos/reward_forward Mean                0.0017893\n",
      "evaluation/env_infos/reward_forward Std                 0.0545026\n",
      "evaluation/env_infos/reward_forward Max                 1.41623\n",
      "evaluation/env_infos/reward_forward Min                -0.582465\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0724018\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0191099\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0429422\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.115201\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0647733\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.015542\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0455508\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0991151\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0726988\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0203089\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0180241\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.349566\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.7493e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           3.18066e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.9671e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.0525e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.138389\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.230798\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.572536\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.295765\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000838357\n",
      "evaluation/env_infos/torso_velocity Std                 0.0600796\n",
      "evaluation/env_infos/torso_velocity Max                 1.41623\n",
      "evaluation/env_infos/torso_velocity Min                -1.76776\n",
      "time/data storing (s)                                   0.303394\n",
      "time/evaluation sampling (s)                           41.7984\n",
      "time/exploration sampling (s)                           1.86651\n",
      "time/logging (s)                                        0.277691\n",
      "time/saving (s)                                         0.0258813\n",
      "time/training (s)                                       3.78366\n",
      "time/epoch (s)                                         48.0556\n",
      "time/total (s)                                       1778.39\n",
      "Epoch                                                  34\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:54:32.975413 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 35 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  73898\n",
      "trainer/QF1 Loss                                        0.37109\n",
      "trainer/QF2 Loss                                        0.350198\n",
      "trainer/Policy Loss                                    -4.49634\n",
      "trainer/Q1 Predictions Mean                            12.3051\n",
      "trainer/Q1 Predictions Std                              1.23945\n",
      "trainer/Q1 Predictions Max                             14.401\n",
      "trainer/Q1 Predictions Min                              7.46477\n",
      "trainer/Q2 Predictions Mean                            12.3273\n",
      "trainer/Q2 Predictions Std                              1.25042\n",
      "trainer/Q2 Predictions Max                             14.3094\n",
      "trainer/Q2 Predictions Min                              7.5917\n",
      "trainer/Q Targets Mean                                 12.3107\n",
      "trainer/Q Targets Std                                   1.41451\n",
      "trainer/Q Targets Max                                  14.6548\n",
      "trainer/Q Targets Min                                   5.01889\n",
      "trainer/Log Pis Mean                                    8.17259\n",
      "trainer/Log Pis Std                                     2.38973\n",
      "trainer/Log Pis Max                                    20.5556\n",
      "trainer/Log Pis Min                                     0.491351\n",
      "trainer/Policy mu Mean                                 -0.0305804\n",
      "trainer/Policy mu Std                                   0.178848\n",
      "trainer/Policy mu Max                                   0.799399\n",
      "trainer/Policy mu Min                                  -0.99745\n",
      "trainer/Policy log std Mean                            -2.39222\n",
      "trainer/Policy log std Std                              0.237673\n",
      "trainer/Policy log std Max                             -1.72451\n",
      "trainer/Policy log std Min                             -3.87482\n",
      "trainer/Alpha                                           0.0104713\n",
      "trainer/Alpha Loss                                      0.786819\n",
      "exploration/num steps total                         37000\n",
      "exploration/num paths total                           131\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.914287\n",
      "exploration/Rewards Std                                 0.129386\n",
      "exploration/Rewards Max                                 2.1337\n",
      "exploration/Rewards Min                                 0.513413\n",
      "exploration/Returns Mean                              914.287\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               914.287\n",
      "exploration/Returns Min                               914.287\n",
      "exploration/Actions Mean                               -0.0147251\n",
      "exploration/Actions Std                                 0.164913\n",
      "exploration/Actions Max                                 0.692715\n",
      "exploration/Actions Min                                -0.64211\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           914.287\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0256364\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0256364\n",
      "exploration/env_infos/final/reward_forward Min         -0.0256364\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.115032\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.115032\n",
      "exploration/env_infos/initial/reward_forward Min       -0.115032\n",
      "exploration/env_infos/reward_forward Mean               0.00285129\n",
      "exploration/env_infos/reward_forward Std                0.264269\n",
      "exploration/env_infos/reward_forward Max                1.16433\n",
      "exploration/env_infos/reward_forward Min               -1.1059\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0536178\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0536178\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0536178\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0609451\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0609451\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0609451\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.109652\n",
      "exploration/env_infos/reward_ctrl Std                   0.0547941\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0121399\n",
      "exploration/env_infos/reward_ctrl Min                  -0.486587\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00648465\n",
      "exploration/env_infos/final/torso_velocity Std          0.0215907\n",
      "exploration/env_infos/final/torso_velocity Max          0.0236861\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0256364\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.127352\n",
      "exploration/env_infos/initial/torso_velocity Std        0.31583\n",
      "exploration/env_infos/initial/torso_velocity Max        0.573444\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.115032\n",
      "exploration/env_infos/torso_velocity Mean              -0.00833724\n",
      "exploration/env_infos/torso_velocity Std                0.318124\n",
      "exploration/env_infos/torso_velocity Max                1.16433\n",
      "exploration/env_infos/torso_velocity Min               -1.3852\n",
      "evaluation/num steps total                         900000\n",
      "evaluation/num paths total                            900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.93865\n",
      "evaluation/Rewards Std                                  0.0396146\n",
      "evaluation/Rewards Max                                  2.29813\n",
      "evaluation/Rewards Min                                  0.388136\n",
      "evaluation/Returns Mean                               938.65\n",
      "evaluation/Returns Std                                 19.7141\n",
      "evaluation/Returns Max                                967.769\n",
      "evaluation/Returns Min                                906.299\n",
      "evaluation/Actions Mean                                -0.0186984\n",
      "evaluation/Actions Std                                  0.124652\n",
      "evaluation/Actions Max                                  0.563211\n",
      "evaluation/Actions Min                                 -0.708285\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            938.65\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00474189\n",
      "evaluation/env_infos/final/reward_forward Std           0.0254205\n",
      "evaluation/env_infos/final/reward_forward Max           0.12908\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00540332\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00774762\n",
      "evaluation/env_infos/initial/reward_forward Std         0.129882\n",
      "evaluation/env_infos/initial/reward_forward Max         0.292294\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.152612\n",
      "evaluation/env_infos/reward_forward Mean                0.00121504\n",
      "evaluation/env_infos/reward_forward Std                 0.0925478\n",
      "evaluation/env_infos/reward_forward Max                 1.4121\n",
      "evaluation/env_infos/reward_forward Min                -1.60207\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0621372\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0208738\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0254978\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0924568\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0313536\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.017693\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.011385\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0695147\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.063551\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0269441\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00968849\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.611864\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000813094\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0271328\n",
      "evaluation/env_infos/final/torso_velocity Max           0.12908\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.169257\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135022\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.241718\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.603678\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.241045\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00261032\n",
      "evaluation/env_infos/torso_velocity Std                 0.102964\n",
      "evaluation/env_infos/torso_velocity Max                 1.4121\n",
      "evaluation/env_infos/torso_velocity Min                -2.0644\n",
      "time/data storing (s)                                   0.319958\n",
      "time/evaluation sampling (s)                           42.5481\n",
      "time/exploration sampling (s)                           1.82127\n",
      "time/logging (s)                                        0.277371\n",
      "time/saving (s)                                         0.0261707\n",
      "time/training (s)                                       3.84472\n",
      "time/epoch (s)                                         48.8376\n",
      "time/total (s)                                       1827.61\n",
      "Epoch                                                  35\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:55:21.186482 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 36 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  75898\n",
      "trainer/QF1 Loss                                        0.396091\n",
      "trainer/QF2 Loss                                        0.315695\n",
      "trainer/Policy Loss                                    -4.38937\n",
      "trainer/Q1 Predictions Mean                            12.77\n",
      "trainer/Q1 Predictions Std                              1.29861\n",
      "trainer/Q1 Predictions Max                             15.4999\n",
      "trainer/Q1 Predictions Min                              7.52069\n",
      "trainer/Q2 Predictions Mean                            12.7457\n",
      "trainer/Q2 Predictions Std                              1.32526\n",
      "trainer/Q2 Predictions Max                             16.0934\n",
      "trainer/Q2 Predictions Min                              8.37876\n",
      "trainer/Q Targets Mean                                 12.793\n",
      "trainer/Q Targets Std                                   1.29063\n",
      "trainer/Q Targets Max                                  16.3475\n",
      "trainer/Q Targets Min                                   7.98454\n",
      "trainer/Log Pis Mean                                    8.67134\n",
      "trainer/Log Pis Std                                     2.42279\n",
      "trainer/Log Pis Max                                    14.0253\n",
      "trainer/Log Pis Min                                     1.45312\n",
      "trainer/Policy mu Mean                                  0.00362967\n",
      "trainer/Policy mu Std                                   0.163052\n",
      "trainer/Policy mu Max                                   1.0897\n",
      "trainer/Policy mu Min                                  -1.13052\n",
      "trainer/Policy log std Mean                            -2.4609\n",
      "trainer/Policy log std Std                              0.230125\n",
      "trainer/Policy log std Max                             -1.65443\n",
      "trainer/Policy log std Min                             -3.36123\n",
      "trainer/Alpha                                           0.0104509\n",
      "trainer/Alpha Loss                                      3.06213\n",
      "exploration/num steps total                         38000\n",
      "exploration/num paths total                           132\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.882109\n",
      "exploration/Rewards Std                                 0.0825715\n",
      "exploration/Rewards Max                                 1.77003\n",
      "exploration/Rewards Min                                 0.522715\n",
      "exploration/Returns Mean                              882.109\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               882.109\n",
      "exploration/Returns Min                               882.109\n",
      "exploration/Actions Mean                               -0.0388588\n",
      "exploration/Actions Std                                 0.173725\n",
      "exploration/Actions Max                                 0.611699\n",
      "exploration/Actions Min                                -0.622426\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           882.109\n",
      "exploration/env_infos/final/reward_forward Mean         0.0430583\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0430583\n",
      "exploration/env_infos/final/reward_forward Min          0.0430583\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0980053\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0980053\n",
      "exploration/env_infos/initial/reward_forward Min        0.0980053\n",
      "exploration/env_infos/reward_forward Mean              -0.00131763\n",
      "exploration/env_infos/reward_forward Std                0.278323\n",
      "exploration/env_infos/reward_forward Max                1.4674\n",
      "exploration/env_infos/reward_forward Min               -1.10903\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.145801\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.145801\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.145801\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.03777\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.03777\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.03777\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.126761\n",
      "exploration/env_infos/reward_ctrl Std                   0.0454783\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0169621\n",
      "exploration/env_infos/reward_ctrl Min                  -0.477285\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0153559\n",
      "exploration/env_infos/final/torso_velocity Std          0.0209141\n",
      "exploration/env_infos/final/torso_velocity Max          0.0430583\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00746918\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.120656\n",
      "exploration/env_infos/initial/torso_velocity Std        0.163218\n",
      "exploration/env_infos/initial/torso_velocity Max        0.330916\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0669548\n",
      "exploration/env_infos/torso_velocity Mean              -0.00862332\n",
      "exploration/env_infos/torso_velocity Std                0.269196\n",
      "exploration/env_infos/torso_velocity Max                1.4674\n",
      "exploration/env_infos/torso_velocity Min               -1.66402\n",
      "evaluation/num steps total                         925000\n",
      "evaluation/num paths total                            925\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.926957\n",
      "evaluation/Rewards Std                                  0.101075\n",
      "evaluation/Rewards Max                                  2.59368\n",
      "evaluation/Rewards Min                                  0.400476\n",
      "evaluation/Returns Mean                               926.957\n",
      "evaluation/Returns Std                                 25.1474\n",
      "evaluation/Returns Max                               1000.04\n",
      "evaluation/Returns Min                                889.006\n",
      "evaluation/Actions Mean                                -0.0194063\n",
      "evaluation/Actions Std                                  0.143964\n",
      "evaluation/Actions Max                                  0.788397\n",
      "evaluation/Actions Min                                 -0.679836\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            926.957\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0286713\n",
      "evaluation/env_infos/final/reward_forward Std           0.159466\n",
      "evaluation/env_infos/final/reward_forward Max           0.277296\n",
      "evaluation/env_infos/final/reward_forward Min          -0.70355\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0354919\n",
      "evaluation/env_infos/initial/reward_forward Std         0.107932\n",
      "evaluation/env_infos/initial/reward_forward Max         0.177672\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.225177\n",
      "evaluation/env_infos/reward_forward Mean                0.0043374\n",
      "evaluation/env_infos/reward_forward Std                 0.253458\n",
      "evaluation/env_infos/reward_forward Max                 1.82931\n",
      "evaluation/env_infos/reward_forward Min                -1.60342\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0833596\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0275136\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0247518\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.120546\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0195415\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.015213\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.00616509\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0798269\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0844085\n",
      "evaluation/env_infos/reward_ctrl Std                    0.038247\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00616509\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.600955\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000843167\n",
      "evaluation/env_infos/final/torso_velocity Std           0.121623\n",
      "evaluation/env_infos/final/torso_velocity Max           0.5639\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.70355\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.129533\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.249054\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.784124\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.314697\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00061243\n",
      "evaluation/env_infos/torso_velocity Std                 0.190741\n",
      "evaluation/env_infos/torso_velocity Max                 1.82931\n",
      "evaluation/env_infos/torso_velocity Min                -1.89195\n",
      "time/data storing (s)                                   0.320065\n",
      "time/evaluation sampling (s)                           41.4316\n",
      "time/exploration sampling (s)                           1.86412\n",
      "time/logging (s)                                        0.279041\n",
      "time/saving (s)                                         0.0258026\n",
      "time/training (s)                                       3.883\n",
      "time/epoch (s)                                         47.8037\n",
      "time/total (s)                                       1875.82\n",
      "Epoch                                                  36\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:56:10.293809 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 37 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  77898\n",
      "trainer/QF1 Loss                                        0.448635\n",
      "trainer/QF2 Loss                                        0.438381\n",
      "trainer/Policy Loss                                    -6.26669\n",
      "trainer/Q1 Predictions Mean                            13.1056\n",
      "trainer/Q1 Predictions Std                              1.48213\n",
      "trainer/Q1 Predictions Max                             17.8151\n",
      "trainer/Q1 Predictions Min                              7.7592\n",
      "trainer/Q2 Predictions Mean                            13.0688\n",
      "trainer/Q2 Predictions Std                              1.49859\n",
      "trainer/Q2 Predictions Max                             18.1979\n",
      "trainer/Q2 Predictions Min                              7.33754\n",
      "trainer/Q Targets Mean                                 13.1413\n",
      "trainer/Q Targets Std                                   1.49401\n",
      "trainer/Q Targets Max                                  18.8355\n",
      "trainer/Q Targets Min                                   8.14723\n",
      "trainer/Log Pis Mean                                    7.17833\n",
      "trainer/Log Pis Std                                     2.25954\n",
      "trainer/Log Pis Max                                    12.5331\n",
      "trainer/Log Pis Min                                    -0.233894\n",
      "trainer/Policy mu Mean                                 -0.0126083\n",
      "trainer/Policy mu Std                                   0.155522\n",
      "trainer/Policy mu Max                                   0.908958\n",
      "trainer/Policy mu Min                                  -0.845931\n",
      "trainer/Policy log std Mean                            -2.293\n",
      "trainer/Policy log std Std                              0.234353\n",
      "trainer/Policy log std Max                             -1.52217\n",
      "trainer/Policy log std Min                             -3.0275\n",
      "trainer/Alpha                                           0.0105808\n",
      "trainer/Alpha Loss                                     -3.73725\n",
      "exploration/num steps total                         39000\n",
      "exploration/num paths total                           133\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.874915\n",
      "exploration/Rewards Std                                 0.108226\n",
      "exploration/Rewards Max                                 1.85714\n",
      "exploration/Rewards Min                                 0.550982\n",
      "exploration/Returns Mean                              874.915\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               874.915\n",
      "exploration/Returns Min                               874.915\n",
      "exploration/Actions Mean                               -0.0505804\n",
      "exploration/Actions Std                                 0.184884\n",
      "exploration/Actions Max                                 0.45295\n",
      "exploration/Actions Min                                -0.628806\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           874.915\n",
      "exploration/env_infos/final/reward_forward Mean         0.0625838\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0625838\n",
      "exploration/env_infos/final/reward_forward Min          0.0625838\n",
      "exploration/env_infos/initial/reward_forward Mean       0.161334\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.161334\n",
      "exploration/env_infos/initial/reward_forward Min        0.161334\n",
      "exploration/env_infos/reward_forward Mean               0.049423\n",
      "exploration/env_infos/reward_forward Std                0.246635\n",
      "exploration/env_infos/reward_forward Max                1.42641\n",
      "exploration/env_infos/reward_forward Min               -1.06221\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.152049\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.152049\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.152049\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0668281\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0668281\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0668281\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.146962\n",
      "exploration/env_infos/reward_ctrl Std                   0.0548658\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0231478\n",
      "exploration/env_infos/reward_ctrl Min                  -0.449018\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.152831\n",
      "exploration/env_infos/final/torso_velocity Std          0.077236\n",
      "exploration/env_infos/final/torso_velocity Max          0.251243\n",
      "exploration/env_infos/final/torso_velocity Min          0.0625838\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.278925\n",
      "exploration/env_infos/initial/torso_velocity Std        0.202217\n",
      "exploration/env_infos/initial/torso_velocity Max        0.563478\n",
      "exploration/env_infos/initial/torso_velocity Min        0.111963\n",
      "exploration/env_infos/torso_velocity Mean               0.0158959\n",
      "exploration/env_infos/torso_velocity Std                0.237376\n",
      "exploration/env_infos/torso_velocity Max                1.42641\n",
      "exploration/env_infos/torso_velocity Min               -1.22008\n",
      "evaluation/num steps total                         950000\n",
      "evaluation/num paths total                            950\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.928181\n",
      "evaluation/Rewards Std                                  0.0251967\n",
      "evaluation/Rewards Max                                  1.88517\n",
      "evaluation/Rewards Min                                  0.59387\n",
      "evaluation/Returns Mean                               928.181\n",
      "evaluation/Returns Std                                 12.6475\n",
      "evaluation/Returns Max                                945.847\n",
      "evaluation/Returns Min                                899.869\n",
      "evaluation/Actions Mean                                -0.0127542\n",
      "evaluation/Actions Std                                  0.134247\n",
      "evaluation/Actions Max                                  0.523558\n",
      "evaluation/Actions Min                                 -0.656263\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            928.181\n",
      "evaluation/env_infos/final/reward_forward Mean          7.17796e-06\n",
      "evaluation/env_infos/final/reward_forward Std           2.63167e-05\n",
      "evaluation/env_infos/final/reward_forward Max           0.000124445\n",
      "evaluation/env_infos/final/reward_forward Min          -9.85039e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00300133\n",
      "evaluation/env_infos/initial/reward_forward Std         0.128411\n",
      "evaluation/env_infos/initial/reward_forward Max         0.27815\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.283132\n",
      "evaluation/env_infos/reward_forward Mean                0.0024827\n",
      "evaluation/env_infos/reward_forward Std                 0.0861162\n",
      "evaluation/env_infos/reward_forward Max                 1.4107\n",
      "evaluation/env_infos/reward_forward Min                -1.29553\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0721221\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0131544\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0534141\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.102821\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0432887\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.041116\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0162042\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.230805\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0727395\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0164615\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0162042\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.40613\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.36855e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           5.5863e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           0.000396994\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.85039e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.15294\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258867\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.706682\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.34024\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000519874\n",
      "evaluation/env_infos/torso_velocity Std                 0.078614\n",
      "evaluation/env_infos/torso_velocity Max                 1.52768\n",
      "evaluation/env_infos/torso_velocity Min                -1.70295\n",
      "time/data storing (s)                                   0.336926\n",
      "time/evaluation sampling (s)                           41.9518\n",
      "time/exploration sampling (s)                           2.07263\n",
      "time/logging (s)                                        0.27001\n",
      "time/saving (s)                                         0.0255435\n",
      "time/training (s)                                       4.0404\n",
      "time/epoch (s)                                         48.6973\n",
      "time/total (s)                                       1924.92\n",
      "Epoch                                                  37\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:56:58.864395 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 38 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                  79898\n",
      "trainer/QF1 Loss                                        0.704785\n",
      "trainer/QF2 Loss                                        0.551022\n",
      "trainer/Policy Loss                                    -6.82183\n",
      "trainer/Q1 Predictions Mean                            13.4375\n",
      "trainer/Q1 Predictions Std                              1.41148\n",
      "trainer/Q1 Predictions Max                             18.7185\n",
      "trainer/Q1 Predictions Min                              7.94149\n",
      "trainer/Q2 Predictions Mean                            13.2627\n",
      "trainer/Q2 Predictions Std                              1.44797\n",
      "trainer/Q2 Predictions Max                             17.661\n",
      "trainer/Q2 Predictions Min                              8.04088\n",
      "trainer/Q Targets Mean                                 13.348\n",
      "trainer/Q Targets Std                                   1.64051\n",
      "trainer/Q Targets Max                                  19.363\n",
      "trainer/Q Targets Min                                   1.01658\n",
      "trainer/Log Pis Mean                                    6.87107\n",
      "trainer/Log Pis Std                                     2.70005\n",
      "trainer/Log Pis Max                                    18.4666\n",
      "trainer/Log Pis Min                                    -3.12815\n",
      "trainer/Policy mu Mean                                 -0.0204345\n",
      "trainer/Policy mu Std                                   0.181262\n",
      "trainer/Policy mu Max                                   1.07925\n",
      "trainer/Policy mu Min                                  -0.935618\n",
      "trainer/Policy log std Mean                            -2.25604\n",
      "trainer/Policy log std Std                              0.226443\n",
      "trainer/Policy log std Max                             -1.55134\n",
      "trainer/Policy log std Min                             -3.68171\n",
      "trainer/Alpha                                           0.010348\n",
      "trainer/Alpha Loss                                     -5.15943\n",
      "exploration/num steps total                         40000\n",
      "exploration/num paths total                           134\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.95077\n",
      "exploration/Rewards Std                                 0.156036\n",
      "exploration/Rewards Max                                 2.06754\n",
      "exploration/Rewards Min                                 0.576586\n",
      "exploration/Returns Mean                              950.77\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               950.77\n",
      "exploration/Returns Min                               950.77\n",
      "exploration/Actions Mean                               -0.000136472\n",
      "exploration/Actions Std                                 0.147567\n",
      "exploration/Actions Max                                 0.581908\n",
      "exploration/Actions Min                                -0.554103\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           950.77\n",
      "exploration/env_infos/final/reward_forward Mean         0.224789\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.224789\n",
      "exploration/env_infos/final/reward_forward Min          0.224789\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.095096\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.095096\n",
      "exploration/env_infos/initial/reward_forward Min       -0.095096\n",
      "exploration/env_infos/reward_forward Mean              -0.0132996\n",
      "exploration/env_infos/reward_forward Std                0.380021\n",
      "exploration/env_infos/reward_forward Max                1.40091\n",
      "exploration/env_infos/reward_forward Min               -1.42837\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.121762\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.121762\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.121762\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.270384\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.270384\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.270384\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0871043\n",
      "exploration/env_infos/reward_ctrl Std                   0.0509548\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00670364\n",
      "exploration/env_infos/reward_ctrl Min                  -0.423414\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.182099\n",
      "exploration/env_infos/final/torso_velocity Std          0.361951\n",
      "exploration/env_infos/final/torso_velocity Max          0.224789\n",
      "exploration/env_infos/final/torso_velocity Min         -0.654512\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0976555\n",
      "exploration/env_infos/initial/torso_velocity Std        0.200839\n",
      "exploration/env_infos/initial/torso_velocity Max        0.374696\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.095096\n",
      "exploration/env_infos/torso_velocity Mean              -0.000305656\n",
      "exploration/env_infos/torso_velocity Std                0.323046\n",
      "exploration/env_infos/torso_velocity Max                1.40091\n",
      "exploration/env_infos/torso_velocity Min               -1.42837\n",
      "evaluation/num steps total                         975000\n",
      "evaluation/num paths total                            975\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943568\n",
      "evaluation/Rewards Std                                  0.0699504\n",
      "evaluation/Rewards Max                                  2.33726\n",
      "evaluation/Rewards Min                                  0.571113\n",
      "evaluation/Returns Mean                               943.568\n",
      "evaluation/Returns Std                                 19.2355\n",
      "evaluation/Returns Max                                988.192\n",
      "evaluation/Returns Min                                910.671\n",
      "evaluation/Actions Mean                                -0.00277282\n",
      "evaluation/Actions Std                                  0.125102\n",
      "evaluation/Actions Max                                  0.607177\n",
      "evaluation/Actions Min                                 -0.527094\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.568\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0145718\n",
      "evaluation/env_infos/final/reward_forward Std           0.128183\n",
      "evaluation/env_infos/final/reward_forward Max           0.236018\n",
      "evaluation/env_infos/final/reward_forward Min          -0.600312\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00398018\n",
      "evaluation/env_infos/initial/reward_forward Std         0.105212\n",
      "evaluation/env_infos/initial/reward_forward Max         0.215927\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.190137\n",
      "evaluation/env_infos/reward_forward Mean               -0.00392865\n",
      "evaluation/env_infos/reward_forward Std                 0.11441\n",
      "evaluation/env_infos/reward_forward Max                 1.32822\n",
      "evaluation/env_infos/reward_forward Min                -1.75778\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0592375\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0191841\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0251742\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.0896581\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0715961\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0401247\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0149363\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.171289\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0626326\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0233988\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00988742\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.428887\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0128804\n",
      "evaluation/env_infos/final/torso_velocity Std           0.125671\n",
      "evaluation/env_infos/final/torso_velocity Max           0.31642\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.82112\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.118777\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.251341\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.639431\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.344022\n",
      "evaluation/env_infos/torso_velocity Mean               -0.002576\n",
      "evaluation/env_infos/torso_velocity Std                 0.109016\n",
      "evaluation/env_infos/torso_velocity Max                 1.55276\n",
      "evaluation/env_infos/torso_velocity Min                -1.77079\n",
      "time/data storing (s)                                   0.32796\n",
      "time/evaluation sampling (s)                           41.8813\n",
      "time/exploration sampling (s)                           1.80977\n",
      "time/logging (s)                                        0.280953\n",
      "time/saving (s)                                         0.0242464\n",
      "time/training (s)                                       3.85321\n",
      "time/epoch (s)                                         48.1775\n",
      "time/total (s)                                       1973.5\n",
      "Epoch                                                  38\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:57:49.070449 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 39 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 81898\n",
      "trainer/QF1 Loss                                       0.375378\n",
      "trainer/QF2 Loss                                       0.336713\n",
      "trainer/Policy Loss                                   -6.39647\n",
      "trainer/Q1 Predictions Mean                           13.7425\n",
      "trainer/Q1 Predictions Std                             1.22589\n",
      "trainer/Q1 Predictions Max                            17.8787\n",
      "trainer/Q1 Predictions Min                             9.17234\n",
      "trainer/Q2 Predictions Mean                           13.7186\n",
      "trainer/Q2 Predictions Std                             1.30692\n",
      "trainer/Q2 Predictions Max                            17.7264\n",
      "trainer/Q2 Predictions Min                             7.77854\n",
      "trainer/Q Targets Mean                                13.7666\n",
      "trainer/Q Targets Std                                  1.31367\n",
      "trainer/Q Targets Max                                 16.6648\n",
      "trainer/Q Targets Min                                  9.11669\n",
      "trainer/Log Pis Mean                                   7.59762\n",
      "trainer/Log Pis Std                                    2.41218\n",
      "trainer/Log Pis Max                                   15.0234\n",
      "trainer/Log Pis Min                                   -0.184961\n",
      "trainer/Policy mu Mean                                -0.0686719\n",
      "trainer/Policy mu Std                                  0.174353\n",
      "trainer/Policy mu Max                                  0.61197\n",
      "trainer/Policy mu Min                                 -1.13706\n",
      "trainer/Policy log std Mean                           -2.32247\n",
      "trainer/Policy log std Std                             0.238445\n",
      "trainer/Policy log std Max                            -1.60228\n",
      "trainer/Policy log std Min                            -4.14776\n",
      "trainer/Alpha                                          0.0104005\n",
      "trainer/Alpha Loss                                    -1.83732\n",
      "exploration/num steps total                        41000\n",
      "exploration/num paths total                          135\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.935638\n",
      "exploration/Rewards Std                                0.104608\n",
      "exploration/Rewards Max                                2.06584\n",
      "exploration/Rewards Min                                0.601685\n",
      "exploration/Returns Mean                             935.638\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              935.638\n",
      "exploration/Returns Min                              935.638\n",
      "exploration/Actions Mean                              -0.036606\n",
      "exploration/Actions Std                                0.148098\n",
      "exploration/Actions Max                                0.496538\n",
      "exploration/Actions Min                               -0.778549\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          935.638\n",
      "exploration/env_infos/final/reward_forward Mean       -0.140521\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.140521\n",
      "exploration/env_infos/final/reward_forward Min        -0.140521\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0229878\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0229878\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0229878\n",
      "exploration/env_infos/reward_forward Mean              0.00104524\n",
      "exploration/env_infos/reward_forward Std               0.255697\n",
      "exploration/env_infos/reward_forward Max               1.31908\n",
      "exploration/env_infos/reward_forward Min              -0.997734\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.173176\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.173176\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.173176\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0461944\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0461944\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0461944\n",
      "exploration/env_infos/reward_ctrl Mean                -0.093092\n",
      "exploration/env_infos/reward_ctrl Std                  0.0461703\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00876999\n",
      "exploration/env_infos/reward_ctrl Min                 -0.398315\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0665794\n",
      "exploration/env_infos/final/torso_velocity Std         0.0604234\n",
      "exploration/env_infos/final/torso_velocity Max         0.00748555\n",
      "exploration/env_infos/final/torso_velocity Min        -0.140521\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.1251\n",
      "exploration/env_infos/initial/torso_velocity Std       0.187054\n",
      "exploration/env_infos/initial/torso_velocity Max       0.388976\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0229878\n",
      "exploration/env_infos/torso_velocity Mean              0.00356465\n",
      "exploration/env_infos/torso_velocity Std               0.232564\n",
      "exploration/env_infos/torso_velocity Max               1.49749\n",
      "exploration/env_infos/torso_velocity Min              -1.80313\n",
      "evaluation/num steps total                             1e+06\n",
      "evaluation/num paths total                          1000\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.889329\n",
      "evaluation/Rewards Std                                 0.0451778\n",
      "evaluation/Rewards Max                                 2.26436\n",
      "evaluation/Rewards Min                                 0.562971\n",
      "evaluation/Returns Mean                              889.329\n",
      "evaluation/Returns Std                                37.2804\n",
      "evaluation/Returns Max                               962.552\n",
      "evaluation/Returns Min                               827.946\n",
      "evaluation/Actions Mean                               -0.0496362\n",
      "evaluation/Actions Std                                 0.159743\n",
      "evaluation/Actions Max                                 0.587119\n",
      "evaluation/Actions Min                                -0.536853\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           889.329\n",
      "evaluation/env_infos/final/reward_forward Mean         9.33563e-08\n",
      "evaluation/env_infos/final/reward_forward Std          6.18485e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.76663e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.12907e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.01776\n",
      "evaluation/env_infos/initial/reward_forward Std        0.101015\n",
      "evaluation/env_infos/initial/reward_forward Max        0.221286\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.159916\n",
      "evaluation/env_infos/reward_forward Mean              -0.00144514\n",
      "evaluation/env_infos/reward_forward Std                0.0705986\n",
      "evaluation/env_infos/reward_forward Max                1.15763\n",
      "evaluation/env_infos/reward_forward Min               -1.52636\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.111774\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0374411\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0362173\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.173146\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0361231\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0102746\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0203487\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0586847\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.111926\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0390051\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0203487\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.437029\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -1.24912e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          5.46764e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.14422e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.12214e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.134207\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.239222\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.674823\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.349483\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00196972\n",
      "evaluation/env_infos/torso_velocity Std                0.0671608\n",
      "evaluation/env_infos/torso_velocity Max                1.27963\n",
      "evaluation/env_infos/torso_velocity Min               -1.76821\n",
      "time/data storing (s)                                  0.342421\n",
      "time/evaluation sampling (s)                          42.5491\n",
      "time/exploration sampling (s)                          2.01835\n",
      "time/logging (s)                                       0.282308\n",
      "time/saving (s)                                        0.0260991\n",
      "time/training (s)                                      4.57098\n",
      "time/epoch (s)                                        49.7892\n",
      "time/total (s)                                      2023.7\n",
      "Epoch                                                 39\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:58:38.373776 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 40 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 83898\n",
      "trainer/QF1 Loss                                       0.407186\n",
      "trainer/QF2 Loss                                       0.398514\n",
      "trainer/Policy Loss                                   -6.19733\n",
      "trainer/Q1 Predictions Mean                           13.7479\n",
      "trainer/Q1 Predictions Std                             1.83796\n",
      "trainer/Q1 Predictions Max                            17.7575\n",
      "trainer/Q1 Predictions Min                             1.03157\n",
      "trainer/Q2 Predictions Mean                           13.899\n",
      "trainer/Q2 Predictions Std                             1.90264\n",
      "trainer/Q2 Predictions Max                            17.009\n",
      "trainer/Q2 Predictions Min                            -0.341166\n",
      "trainer/Q Targets Mean                                13.8758\n",
      "trainer/Q Targets Std                                  1.99518\n",
      "trainer/Q Targets Max                                 18.1344\n",
      "trainer/Q Targets Min                                 -0.611172\n",
      "trainer/Log Pis Mean                                   7.90272\n",
      "trainer/Log Pis Std                                    2.53372\n",
      "trainer/Log Pis Max                                   17.9367\n",
      "trainer/Log Pis Min                                   -0.337735\n",
      "trainer/Policy mu Mean                                -0.0156561\n",
      "trainer/Policy mu Std                                  0.165046\n",
      "trainer/Policy mu Max                                  1.21293\n",
      "trainer/Policy mu Min                                 -1.28727\n",
      "trainer/Policy log std Mean                           -2.38906\n",
      "trainer/Policy log std Std                             0.253516\n",
      "trainer/Policy log std Max                            -1.40838\n",
      "trainer/Policy log std Min                            -3.71757\n",
      "trainer/Alpha                                          0.0104062\n",
      "trainer/Alpha Loss                                    -0.444107\n",
      "exploration/num steps total                        42000\n",
      "exploration/num paths total                          136\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.949596\n",
      "exploration/Rewards Std                                0.0887327\n",
      "exploration/Rewards Max                                1.8446\n",
      "exploration/Rewards Min                                0.650204\n",
      "exploration/Returns Mean                             949.596\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              949.596\n",
      "exploration/Returns Min                              949.596\n",
      "exploration/Actions Mean                              -0.00147673\n",
      "exploration/Actions Std                                0.13087\n",
      "exploration/Actions Max                                0.499351\n",
      "exploration/Actions Min                               -0.527741\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          949.596\n",
      "exploration/env_infos/final/reward_forward Mean       -0.232921\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.232921\n",
      "exploration/env_infos/final/reward_forward Min        -0.232921\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.24382\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.24382\n",
      "exploration/env_infos/initial/reward_forward Min      -0.24382\n",
      "exploration/env_infos/reward_forward Mean             -0.0237681\n",
      "exploration/env_infos/reward_forward Std               0.308645\n",
      "exploration/env_infos/reward_forward Max               1.51646\n",
      "exploration/env_infos/reward_forward Min              -1.31046\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0527231\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0527231\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0527231\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.104801\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.104801\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.104801\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0685163\n",
      "exploration/env_infos/reward_ctrl Std                  0.038994\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00653959\n",
      "exploration/env_infos/reward_ctrl Min                 -0.349796\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.047869\n",
      "exploration/env_infos/final/torso_velocity Std         0.264851\n",
      "exploration/env_infos/final/torso_velocity Max         0.402942\n",
      "exploration/env_infos/final/torso_velocity Min        -0.232921\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.101867\n",
      "exploration/env_infos/initial/torso_velocity Std       0.318617\n",
      "exploration/env_infos/initial/torso_velocity Max       0.525013\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.24382\n",
      "exploration/env_infos/torso_velocity Mean             -0.00593543\n",
      "exploration/env_infos/torso_velocity Std               0.249192\n",
      "exploration/env_infos/torso_velocity Max               1.51646\n",
      "exploration/env_infos/torso_velocity Min              -1.69053\n",
      "evaluation/num steps total                             1.025e+06\n",
      "evaluation/num paths total                          1025\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.948608\n",
      "evaluation/Rewards Std                                 0.0316514\n",
      "evaluation/Rewards Max                                 2.43644\n",
      "evaluation/Rewards Min                                 0.566184\n",
      "evaluation/Returns Mean                              948.608\n",
      "evaluation/Returns Std                                19.212\n",
      "evaluation/Returns Max                               982.518\n",
      "evaluation/Returns Min                               908.258\n",
      "evaluation/Actions Mean                               -0.0150667\n",
      "evaluation/Actions Std                                 0.113213\n",
      "evaluation/Actions Max                                 0.700863\n",
      "evaluation/Actions Min                                -0.507472\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           948.608\n",
      "evaluation/env_infos/final/reward_forward Mean         8.74337e-07\n",
      "evaluation/env_infos/final/reward_forward Std          4.75927e-06\n",
      "evaluation/env_infos/final/reward_forward Max          2.40988e-05\n",
      "evaluation/env_infos/final/reward_forward Min         -1.03441e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0541498\n",
      "evaluation/env_infos/initial/reward_forward Std        0.126337\n",
      "evaluation/env_infos/initial/reward_forward Max        0.265172\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.165051\n",
      "evaluation/env_infos/reward_forward Mean               0.000221288\n",
      "evaluation/env_infos/reward_forward Std                0.0742764\n",
      "evaluation/env_infos/reward_forward Max                1.28096\n",
      "evaluation/env_infos/reward_forward Min               -1.30842\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0514533\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0196994\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0175065\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.09194\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0642943\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0324237\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0250023\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.168737\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0521772\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0222981\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00666106\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.433816\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         1.30443e-06\n",
      "evaluation/env_infos/final/torso_velocity Std          6.6957e-06\n",
      "evaluation/env_infos/final/torso_velocity Max          4.60612e-05\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.03441e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.133681\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.242695\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.721176\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.338119\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000614712\n",
      "evaluation/env_infos/torso_velocity Std                0.0669238\n",
      "evaluation/env_infos/torso_velocity Max                1.63952\n",
      "evaluation/env_infos/torso_velocity Min               -1.96942\n",
      "time/data storing (s)                                  0.319134\n",
      "time/evaluation sampling (s)                          42.6224\n",
      "time/exploration sampling (s)                          1.82793\n",
      "time/logging (s)                                       0.276886\n",
      "time/saving (s)                                        0.026927\n",
      "time/training (s)                                      3.78587\n",
      "time/epoch (s)                                        48.8592\n",
      "time/total (s)                                      2073\n",
      "Epoch                                                 40\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:59:26.518198 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 41 finished\n",
      "-------------------------------------------------  --------------\n",
      "replay_buffer/size                                 85898\n",
      "trainer/QF1 Loss                                       0.755231\n",
      "trainer/QF2 Loss                                       0.853693\n",
      "trainer/Policy Loss                                   -6.95558\n",
      "trainer/Q1 Predictions Mean                           14.2486\n",
      "trainer/Q1 Predictions Std                             1.70684\n",
      "trainer/Q1 Predictions Max                            18.3864\n",
      "trainer/Q1 Predictions Min                             4.91568\n",
      "trainer/Q2 Predictions Mean                           14.1803\n",
      "trainer/Q2 Predictions Std                             1.63273\n",
      "trainer/Q2 Predictions Max                            18.0361\n",
      "trainer/Q2 Predictions Min                             6.55936\n",
      "trainer/Q Targets Mean                                14.2608\n",
      "trainer/Q Targets Std                                  1.981\n",
      "trainer/Q Targets Max                                 20.7356\n",
      "trainer/Q Targets Min                                  0.0441088\n",
      "trainer/Log Pis Mean                                   7.5877\n",
      "trainer/Log Pis Std                                    2.62568\n",
      "trainer/Log Pis Max                                   17.2263\n",
      "trainer/Log Pis Min                                   -0.235189\n",
      "trainer/Policy mu Mean                                 0.0101213\n",
      "trainer/Policy mu Std                                  0.171249\n",
      "trainer/Policy mu Max                                  1.33798\n",
      "trainer/Policy mu Min                                 -0.877819\n",
      "trainer/Policy log std Mean                           -2.33449\n",
      "trainer/Policy log std Std                             0.258992\n",
      "trainer/Policy log std Max                            -1.36563\n",
      "trainer/Policy log std Min                            -3.38474\n",
      "trainer/Alpha                                          0.0102788\n",
      "trainer/Alpha Loss                                    -1.88707\n",
      "exploration/num steps total                        43000\n",
      "exploration/num paths total                          137\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               1.01098\n",
      "exploration/Rewards Std                                0.300366\n",
      "exploration/Rewards Max                                2.43019\n",
      "exploration/Rewards Min                                0.554707\n",
      "exploration/Returns Mean                            1010.98\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                             1010.98\n",
      "exploration/Returns Min                             1010.98\n",
      "exploration/Actions Mean                              -0.0146177\n",
      "exploration/Actions Std                                0.163453\n",
      "exploration/Actions Max                                0.795895\n",
      "exploration/Actions Min                               -0.576584\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                         1010.98\n",
      "exploration/env_infos/final/reward_forward Mean        0.267734\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.267734\n",
      "exploration/env_infos/final/reward_forward Min         0.267734\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0416702\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0416702\n",
      "exploration/env_infos/initial/reward_forward Min       0.0416702\n",
      "exploration/env_infos/reward_forward Mean              0.0978911\n",
      "exploration/env_infos/reward_forward Std               0.469545\n",
      "exploration/env_infos/reward_forward Max               1.39147\n",
      "exploration/env_infos/reward_forward Min              -1.78321\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.155945\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.155945\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.155945\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0318155\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0318155\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0318155\n",
      "exploration/env_infos/reward_ctrl Mean                -0.107723\n",
      "exploration/env_infos/reward_ctrl Std                  0.0532954\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0120096\n",
      "exploration/env_infos/reward_ctrl Min                 -0.445293\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0823508\n",
      "exploration/env_infos/final/torso_velocity Std         0.330693\n",
      "exploration/env_infos/final/torso_velocity Max         0.361494\n",
      "exploration/env_infos/final/torso_velocity Min        -0.382176\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.175336\n",
      "exploration/env_infos/initial/torso_velocity Std       0.181891\n",
      "exploration/env_infos/initial/torso_velocity Max       0.432501\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0416702\n",
      "exploration/env_infos/torso_velocity Mean              0.0521084\n",
      "exploration/env_infos/torso_velocity Std               0.385138\n",
      "exploration/env_infos/torso_velocity Max               1.39147\n",
      "exploration/env_infos/torso_velocity Min              -1.78321\n",
      "evaluation/num steps total                             1.05e+06\n",
      "evaluation/num paths total                          1050\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.953291\n",
      "evaluation/Rewards Std                                 0.146801\n",
      "evaluation/Rewards Max                                 2.57981\n",
      "evaluation/Rewards Min                                 0.539177\n",
      "evaluation/Returns Mean                              953.291\n",
      "evaluation/Returns Std                                50.3437\n",
      "evaluation/Returns Max                              1084.6\n",
      "evaluation/Returns Min                               883.004\n",
      "evaluation/Actions Mean                                0.0133205\n",
      "evaluation/Actions Std                                 0.131848\n",
      "evaluation/Actions Max                                 0.733784\n",
      "evaluation/Actions Min                                -0.662553\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           953.291\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0437856\n",
      "evaluation/env_infos/final/reward_forward Std          0.264068\n",
      "evaluation/env_infos/final/reward_forward Max          0.771718\n",
      "evaluation/env_infos/final/reward_forward Min         -0.584616\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0619003\n",
      "evaluation/env_infos/initial/reward_forward Std        0.129124\n",
      "evaluation/env_infos/initial/reward_forward Max        0.321501\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.225053\n",
      "evaluation/env_infos/reward_forward Mean               0.0687721\n",
      "evaluation/env_infos/reward_forward Std                0.287949\n",
      "evaluation/env_infos/reward_forward Max                1.7489\n",
      "evaluation/env_infos/reward_forward Min               -1.54382\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0667211\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0275236\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0239642\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.119963\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0391944\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.028436\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0080601\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.11662\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0702451\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0338377\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00732632\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.460823\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.00409624\n",
      "evaluation/env_infos/final/torso_velocity Std          0.196145\n",
      "evaluation/env_infos/final/torso_velocity Max          0.771718\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.584616\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.178008\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.216901\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.639314\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.225053\n",
      "evaluation/env_infos/torso_velocity Mean               0.0276149\n",
      "evaluation/env_infos/torso_velocity Std                0.234059\n",
      "evaluation/env_infos/torso_velocity Max                1.7489\n",
      "evaluation/env_infos/torso_velocity Min               -1.70811\n",
      "time/data storing (s)                                  0.323\n",
      "time/evaluation sampling (s)                          41.4918\n",
      "time/exploration sampling (s)                          1.792\n",
      "time/logging (s)                                       0.279325\n",
      "time/saving (s)                                        0.0255126\n",
      "time/training (s)                                      3.80814\n",
      "time/epoch (s)                                        47.7198\n",
      "time/total (s)                                      2121.15\n",
      "Epoch                                                 41\n",
      "-------------------------------------------------  --------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:00:15.726012 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 42 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 87898\n",
      "trainer/QF1 Loss                                       0.64609\n",
      "trainer/QF2 Loss                                       0.482387\n",
      "trainer/Policy Loss                                   -6.55556\n",
      "trainer/Q1 Predictions Mean                           14.2642\n",
      "trainer/Q1 Predictions Std                             1.59554\n",
      "trainer/Q1 Predictions Max                            16.9192\n",
      "trainer/Q1 Predictions Min                             4.35404\n",
      "trainer/Q2 Predictions Mean                           14.3979\n",
      "trainer/Q2 Predictions Std                             1.44291\n",
      "trainer/Q2 Predictions Max                            17.5989\n",
      "trainer/Q2 Predictions Min                             8.92278\n",
      "trainer/Q Targets Mean                                14.5346\n",
      "trainer/Q Targets Std                                  1.57903\n",
      "trainer/Q Targets Max                                 17.6366\n",
      "trainer/Q Targets Min                                  6.79536\n",
      "trainer/Log Pis Mean                                   8.08114\n",
      "trainer/Log Pis Std                                    2.42807\n",
      "trainer/Log Pis Max                                   15.8617\n",
      "trainer/Log Pis Min                                   -0.741503\n",
      "trainer/Policy mu Mean                                -0.00565468\n",
      "trainer/Policy mu Std                                  0.186246\n",
      "trainer/Policy mu Max                                  0.941783\n",
      "trainer/Policy mu Min                                 -1.13615\n",
      "trainer/Policy log std Mean                           -2.35069\n",
      "trainer/Policy log std Std                             0.263801\n",
      "trainer/Policy log std Max                            -1.28933\n",
      "trainer/Policy log std Min                            -4.04303\n",
      "trainer/Alpha                                          0.010071\n",
      "trainer/Alpha Loss                                     0.373071\n",
      "exploration/num steps total                        44000\n",
      "exploration/num paths total                          138\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.880742\n",
      "exploration/Rewards Std                                0.0744896\n",
      "exploration/Rewards Max                                1.51683\n",
      "exploration/Rewards Min                                0.356925\n",
      "exploration/Returns Mean                             880.742\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              880.742\n",
      "exploration/Returns Min                              880.742\n",
      "exploration/Actions Mean                              -0.0170484\n",
      "exploration/Actions Std                                0.175522\n",
      "exploration/Actions Max                                0.61733\n",
      "exploration/Actions Min                               -0.77197\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          880.742\n",
      "exploration/env_infos/final/reward_forward Mean       -0.203505\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.203505\n",
      "exploration/env_infos/final/reward_forward Min        -0.203505\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0337208\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0337208\n",
      "exploration/env_infos/initial/reward_forward Min       0.0337208\n",
      "exploration/env_infos/reward_forward Mean              0.0656158\n",
      "exploration/env_infos/reward_forward Std               0.504272\n",
      "exploration/env_infos/reward_forward Max               1.82485\n",
      "exploration/env_infos/reward_forward Min              -1.21792\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0993305\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0993305\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0993305\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0626499\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0626499\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0626499\n",
      "exploration/env_infos/reward_ctrl Mean                -0.124395\n",
      "exploration/env_infos/reward_ctrl Std                  0.0623807\n",
      "exploration/env_infos/reward_ctrl Max                 -0.007467\n",
      "exploration/env_infos/reward_ctrl Min                 -0.643075\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0375518\n",
      "exploration/env_infos/final/torso_velocity Std         0.121753\n",
      "exploration/env_infos/final/torso_velocity Max         0.0851806\n",
      "exploration/env_infos/final/torso_velocity Min        -0.203505\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.153404\n",
      "exploration/env_infos/initial/torso_velocity Std       0.365866\n",
      "exploration/env_infos/initial/torso_velocity Max       0.649186\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.222696\n",
      "exploration/env_infos/torso_velocity Mean              0.00570766\n",
      "exploration/env_infos/torso_velocity Std               0.388099\n",
      "exploration/env_infos/torso_velocity Max               1.82485\n",
      "exploration/env_infos/torso_velocity Min              -1.28493\n",
      "evaluation/num steps total                             1.075e+06\n",
      "evaluation/num paths total                          1075\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.929641\n",
      "evaluation/Rewards Std                                 0.0669963\n",
      "evaluation/Rewards Max                                 1.8202\n",
      "evaluation/Rewards Min                                 0.463049\n",
      "evaluation/Returns Mean                              929.641\n",
      "evaluation/Returns Std                                26.5923\n",
      "evaluation/Returns Max                               992.445\n",
      "evaluation/Returns Min                               893.176\n",
      "evaluation/Actions Mean                                0.0032543\n",
      "evaluation/Actions Std                                 0.139683\n",
      "evaluation/Actions Max                                 0.61588\n",
      "evaluation/Actions Min                                -0.661068\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           929.641\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0496481\n",
      "evaluation/env_infos/final/reward_forward Std          0.182239\n",
      "evaluation/env_infos/final/reward_forward Max          0.763383\n",
      "evaluation/env_infos/final/reward_forward Min         -0.0718521\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0449648\n",
      "evaluation/env_infos/initial/reward_forward Std        0.115118\n",
      "evaluation/env_infos/initial/reward_forward Max        0.328286\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.183838\n",
      "evaluation/env_infos/reward_forward Mean               1.10675e-05\n",
      "evaluation/env_infos/reward_forward Std                0.220336\n",
      "evaluation/env_infos/reward_forward Max                1.75799\n",
      "evaluation/env_infos/reward_forward Min               -1.54443\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0773839\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0255877\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.013851\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.114929\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0500006\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0228295\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0125943\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.106156\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0780878\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0314637\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00876071\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.536951\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         0.01123\n",
      "evaluation/env_infos/final/torso_velocity Std          0.128959\n",
      "evaluation/env_infos/final/torso_velocity Max          0.763383\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.428975\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.130168\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.246933\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.624217\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.344186\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00312077\n",
      "evaluation/env_infos/torso_velocity Std                0.170662\n",
      "evaluation/env_infos/torso_velocity Max                1.75799\n",
      "evaluation/env_infos/torso_velocity Min               -1.74457\n",
      "time/data storing (s)                                  0.321363\n",
      "time/evaluation sampling (s)                          42.5106\n",
      "time/exploration sampling (s)                          1.83402\n",
      "time/logging (s)                                       0.279263\n",
      "time/saving (s)                                        0.0271616\n",
      "time/training (s)                                      3.8132\n",
      "time/epoch (s)                                        48.7856\n",
      "time/total (s)                                      2170.35\n",
      "Epoch                                                 42\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:01:04.700505 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 43 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 89898\n",
      "trainer/QF1 Loss                                       0.496308\n",
      "trainer/QF2 Loss                                       0.569086\n",
      "trainer/Policy Loss                                   -6.78644\n",
      "trainer/Q1 Predictions Mean                           14.8797\n",
      "trainer/Q1 Predictions Std                             1.70578\n",
      "trainer/Q1 Predictions Max                            18.8297\n",
      "trainer/Q1 Predictions Min                             6.60268\n",
      "trainer/Q2 Predictions Mean                           15.0012\n",
      "trainer/Q2 Predictions Std                             1.5911\n",
      "trainer/Q2 Predictions Max                            17.5464\n",
      "trainer/Q2 Predictions Min                             7.74728\n",
      "trainer/Q Targets Mean                                14.8928\n",
      "trainer/Q Targets Std                                  1.87353\n",
      "trainer/Q Targets Max                                 19.5453\n",
      "trainer/Q Targets Min                                  0.367078\n",
      "trainer/Log Pis Mean                                   8.50527\n",
      "trainer/Log Pis Std                                    2.58996\n",
      "trainer/Log Pis Max                                   20.9859\n",
      "trainer/Log Pis Min                                   -0.304216\n",
      "trainer/Policy mu Mean                                -0.058719\n",
      "trainer/Policy mu Std                                  0.205701\n",
      "trainer/Policy mu Max                                  1.05893\n",
      "trainer/Policy mu Min                                 -1.12371\n",
      "trainer/Policy log std Mean                           -2.44794\n",
      "trainer/Policy log std Std                             0.253973\n",
      "trainer/Policy log std Max                            -1.23706\n",
      "trainer/Policy log std Min                            -3.94536\n",
      "trainer/Alpha                                          0.0104408\n",
      "trainer/Alpha Loss                                     2.30565\n",
      "exploration/num steps total                        45000\n",
      "exploration/num paths total                          139\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.838758\n",
      "exploration/Rewards Std                                0.0638521\n",
      "exploration/Rewards Max                                1.34992\n",
      "exploration/Rewards Min                                0.589278\n",
      "exploration/Returns Mean                             838.758\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              838.758\n",
      "exploration/Returns Min                              838.758\n",
      "exploration/Actions Mean                              -0.0643083\n",
      "exploration/Actions Std                                0.195008\n",
      "exploration/Actions Max                                0.541371\n",
      "exploration/Actions Min                               -0.540801\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          838.758\n",
      "exploration/env_infos/final/reward_forward Mean        0.0282557\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max         0.0282557\n",
      "exploration/env_infos/final/reward_forward Min         0.0282557\n",
      "exploration/env_infos/initial/reward_forward Mean      0.00500789\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.00500789\n",
      "exploration/env_infos/initial/reward_forward Min       0.00500789\n",
      "exploration/env_infos/reward_forward Mean             -0.00865521\n",
      "exploration/env_infos/reward_forward Std               0.113327\n",
      "exploration/env_infos/reward_forward Max               1.07888\n",
      "exploration/env_infos/reward_forward Min              -1.02838\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.122894\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.122894\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.122894\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0351471\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0351471\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0351471\n",
      "exploration/env_infos/reward_ctrl Mean                -0.168655\n",
      "exploration/env_infos/reward_ctrl Std                  0.049437\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0314685\n",
      "exploration/env_infos/reward_ctrl Min                 -0.410722\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean        0.0251447\n",
      "exploration/env_infos/final/torso_velocity Std         0.00404116\n",
      "exploration/env_infos/final/torso_velocity Max         0.0282557\n",
      "exploration/env_infos/final/torso_velocity Min         0.0194374\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0854248\n",
      "exploration/env_infos/initial/torso_velocity Std       0.273548\n",
      "exploration/env_infos/initial/torso_velocity Max       0.453341\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.202074\n",
      "exploration/env_infos/torso_velocity Mean             -0.007926\n",
      "exploration/env_infos/torso_velocity Std               0.101063\n",
      "exploration/env_infos/torso_velocity Max               1.07888\n",
      "exploration/env_infos/torso_velocity Min              -1.06522\n",
      "evaluation/num steps total                             1.1e+06\n",
      "evaluation/num paths total                          1100\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.870241\n",
      "evaluation/Rewards Std                                 0.0440032\n",
      "evaluation/Rewards Max                                 2.15223\n",
      "evaluation/Rewards Min                                 0.573892\n",
      "evaluation/Returns Mean                              870.241\n",
      "evaluation/Returns Std                                36.6271\n",
      "evaluation/Returns Max                               969.467\n",
      "evaluation/Returns Min                               830.817\n",
      "evaluation/Actions Mean                               -0.0557157\n",
      "evaluation/Actions Std                                 0.171826\n",
      "evaluation/Actions Max                                 0.503664\n",
      "evaluation/Actions Min                                -0.479168\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           870.241\n",
      "evaluation/env_infos/final/reward_forward Mean        -4.14308e-07\n",
      "evaluation/env_infos/final/reward_forward Std          2.77714e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.53958e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -9.32472e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0298856\n",
      "evaluation/env_infos/initial/reward_forward Std        0.132681\n",
      "evaluation/env_infos/initial/reward_forward Max        0.280011\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.183507\n",
      "evaluation/env_infos/reward_forward Mean              -0.00160622\n",
      "evaluation/env_infos/reward_forward Std                0.0641939\n",
      "evaluation/env_infos/reward_forward Max                1.298\n",
      "evaluation/env_infos/reward_forward Min               -1.44725\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.130905\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0367977\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0292706\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.169579\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0357916\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0138455\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0180245\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0807374\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.130514\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0376572\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0167783\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.426108\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -2.14321e-07\n",
      "evaluation/env_infos/final/torso_velocity Std          3.12889e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.87059e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -9.32472e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.15676\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.256967\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.771129\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.293134\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00141229\n",
      "evaluation/env_infos/torso_velocity Std                0.0577544\n",
      "evaluation/env_infos/torso_velocity Max                1.44879\n",
      "evaluation/env_infos/torso_velocity Min               -1.68064\n",
      "time/data storing (s)                                  0.329335\n",
      "time/evaluation sampling (s)                          42.0421\n",
      "time/exploration sampling (s)                          1.96351\n",
      "time/logging (s)                                       0.271786\n",
      "time/saving (s)                                        0.0256376\n",
      "time/training (s)                                      3.90005\n",
      "time/epoch (s)                                        48.5325\n",
      "time/total (s)                                      2219.32\n",
      "Epoch                                                 43\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:01:53.628334 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 44 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 91898\n",
      "trainer/QF1 Loss                                       0.382266\n",
      "trainer/QF2 Loss                                       0.402102\n",
      "trainer/Policy Loss                                   -7.76868\n",
      "trainer/Q1 Predictions Mean                           15.3928\n",
      "trainer/Q1 Predictions Std                             1.60155\n",
      "trainer/Q1 Predictions Max                            20.1399\n",
      "trainer/Q1 Predictions Min                             7.69763\n",
      "trainer/Q2 Predictions Mean                           15.3387\n",
      "trainer/Q2 Predictions Std                             1.65117\n",
      "trainer/Q2 Predictions Max                            19.9547\n",
      "trainer/Q2 Predictions Min                             7.02729\n",
      "trainer/Q Targets Mean                                15.4668\n",
      "trainer/Q Targets Std                                  1.66382\n",
      "trainer/Q Targets Max                                 22.4997\n",
      "trainer/Q Targets Min                                  8.01284\n",
      "trainer/Log Pis Mean                                   7.8207\n",
      "trainer/Log Pis Std                                    2.7763\n",
      "trainer/Log Pis Max                                   21.7646\n",
      "trainer/Log Pis Min                                   -2.78275\n",
      "trainer/Policy mu Mean                                -0.0495238\n",
      "trainer/Policy mu Std                                  0.226042\n",
      "trainer/Policy mu Max                                  1.49811\n",
      "trainer/Policy mu Min                                 -1.45794\n",
      "trainer/Policy log std Mean                           -2.34728\n",
      "trainer/Policy log std Std                             0.249699\n",
      "trainer/Policy log std Max                            -1.48783\n",
      "trainer/Policy log std Min                            -4.21755\n",
      "trainer/Alpha                                          0.0108256\n",
      "trainer/Alpha Loss                                    -0.811438\n",
      "exploration/num steps total                        46000\n",
      "exploration/num paths total                          140\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.75562\n",
      "exploration/Rewards Std                                0.0667972\n",
      "exploration/Rewards Max                                1.03641\n",
      "exploration/Rewards Min                                0.498733\n",
      "exploration/Returns Mean                             755.62\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              755.62\n",
      "exploration/Returns Min                              755.62\n",
      "exploration/Actions Mean                              -0.0950456\n",
      "exploration/Actions Std                                0.228789\n",
      "exploration/Actions Max                                0.56514\n",
      "exploration/Actions Min                               -0.651032\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          755.62\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0145589\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0145589\n",
      "exploration/env_infos/final/reward_forward Min        -0.0145589\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0383446\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0383446\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0383446\n",
      "exploration/env_infos/reward_forward Mean              0.0179059\n",
      "exploration/env_infos/reward_forward Std               0.132996\n",
      "exploration/env_infos/reward_forward Max               0.799294\n",
      "exploration/env_infos/reward_forward Min              -0.664928\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.233348\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.233348\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.233348\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.091632\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.091632\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.091632\n",
      "exploration/env_infos/reward_ctrl Mean                -0.245512\n",
      "exploration/env_infos/reward_ctrl Std                  0.0662144\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0739358\n",
      "exploration/env_infos/reward_ctrl Min                 -0.501267\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00254933\n",
      "exploration/env_infos/final/torso_velocity Std         0.00943692\n",
      "exploration/env_infos/final/torso_velocity Max         0.00849636\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0145589\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.157485\n",
      "exploration/env_infos/initial/torso_velocity Std       0.176094\n",
      "exploration/env_infos/initial/torso_velocity Max       0.388635\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.0383446\n",
      "exploration/env_infos/torso_velocity Mean              0.00693571\n",
      "exploration/env_infos/torso_velocity Std               0.0947181\n",
      "exploration/env_infos/torso_velocity Max               0.799294\n",
      "exploration/env_infos/torso_velocity Min              -0.948786\n",
      "evaluation/num steps total                             1.125e+06\n",
      "evaluation/num paths total                          1125\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.845912\n",
      "evaluation/Rewards Std                                 0.0506159\n",
      "evaluation/Rewards Max                                 2.34592\n",
      "evaluation/Rewards Min                                 0.599691\n",
      "evaluation/Returns Mean                              845.912\n",
      "evaluation/Returns Std                                37.6536\n",
      "evaluation/Returns Max                               919.198\n",
      "evaluation/Returns Min                               774.144\n",
      "evaluation/Actions Mean                               -0.04736\n",
      "evaluation/Actions Std                                 0.191195\n",
      "evaluation/Actions Max                                 0.599842\n",
      "evaluation/Actions Min                                -0.468217\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           845.912\n",
      "evaluation/env_infos/final/reward_forward Mean         8.6072e-08\n",
      "evaluation/env_infos/final/reward_forward Std          4.63618e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.99085e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.79724e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean      -0.0120601\n",
      "evaluation/env_infos/initial/reward_forward Std        0.120097\n",
      "evaluation/env_infos/initial/reward_forward Max        0.213541\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.226567\n",
      "evaluation/env_infos/reward_forward Mean              -0.000445387\n",
      "evaluation/env_infos/reward_forward Std                0.0619022\n",
      "evaluation/env_infos/reward_forward Max                1.29341\n",
      "evaluation/env_infos/reward_forward Min               -1.44043\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.15563\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0385708\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0808938\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.227843\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0796767\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0577596\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0153948\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.213022\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.155193\n",
      "evaluation/env_infos/reward_ctrl Std                   0.039384\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0153948\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.404803\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -9.86348e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          4.15534e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          8.99085e-07\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.07953e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.149204\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.251448\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.687015\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.329704\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00122867\n",
      "evaluation/env_infos/torso_velocity Std                0.0570912\n",
      "evaluation/env_infos/torso_velocity Max                1.34074\n",
      "evaluation/env_infos/torso_velocity Min               -1.72522\n",
      "time/data storing (s)                                  0.319794\n",
      "time/evaluation sampling (s)                          41.8887\n",
      "time/exploration sampling (s)                          2.08672\n",
      "time/logging (s)                                       0.306656\n",
      "time/saving (s)                                        0.0267709\n",
      "time/training (s)                                      3.90053\n",
      "time/epoch (s)                                        48.5291\n",
      "time/total (s)                                      2268.28\n",
      "Epoch                                                 44\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:02:42.826011 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 45 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 93898\n",
      "trainer/QF1 Loss                                       0.49425\n",
      "trainer/QF2 Loss                                       0.45841\n",
      "trainer/Policy Loss                                   -7.76772\n",
      "trainer/Q1 Predictions Mean                           15.7288\n",
      "trainer/Q1 Predictions Std                             1.77836\n",
      "trainer/Q1 Predictions Max                            18.9598\n",
      "trainer/Q1 Predictions Min                             5.66195\n",
      "trainer/Q2 Predictions Mean                           15.6164\n",
      "trainer/Q2 Predictions Std                             1.65722\n",
      "trainer/Q2 Predictions Max                            17.9455\n",
      "trainer/Q2 Predictions Min                             7.68422\n",
      "trainer/Q Targets Mean                                15.5517\n",
      "trainer/Q Targets Std                                  1.78007\n",
      "trainer/Q Targets Max                                 18.3588\n",
      "trainer/Q Targets Min                                  6.96442\n",
      "trainer/Log Pis Mean                                   8.15544\n",
      "trainer/Log Pis Std                                    2.69149\n",
      "trainer/Log Pis Max                                   18.4435\n",
      "trainer/Log Pis Min                                   -0.628948\n",
      "trainer/Policy mu Mean                                -0.0388208\n",
      "trainer/Policy mu Std                                  0.186522\n",
      "trainer/Policy mu Max                                  1.0521\n",
      "trainer/Policy mu Min                                 -1.62617\n",
      "trainer/Policy log std Mean                           -2.39745\n",
      "trainer/Policy log std Std                             0.251926\n",
      "trainer/Policy log std Max                            -1.53354\n",
      "trainer/Policy log std Min                            -3.58528\n",
      "trainer/Alpha                                          0.01039\n",
      "trainer/Alpha Loss                                     0.709914\n",
      "exploration/num steps total                        47000\n",
      "exploration/num paths total                          141\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.966242\n",
      "exploration/Rewards Std                                0.180818\n",
      "exploration/Rewards Max                                2.17219\n",
      "exploration/Rewards Min                                0.707673\n",
      "exploration/Returns Mean                             966.242\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              966.242\n",
      "exploration/Returns Min                              966.242\n",
      "exploration/Actions Mean                              -0.019871\n",
      "exploration/Actions Std                                0.154182\n",
      "exploration/Actions Max                                0.480237\n",
      "exploration/Actions Min                               -0.495325\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          966.242\n",
      "exploration/env_infos/final/reward_forward Mean       -0.075582\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.075582\n",
      "exploration/env_infos/final/reward_forward Min        -0.075582\n",
      "exploration/env_infos/initial/reward_forward Mean      0.040452\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.040452\n",
      "exploration/env_infos/initial/reward_forward Min       0.040452\n",
      "exploration/env_infos/reward_forward Mean             -0.0202111\n",
      "exploration/env_infos/reward_forward Std               0.331566\n",
      "exploration/env_infos/reward_forward Max               1.13281\n",
      "exploration/env_infos/reward_forward Min              -1.23781\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.140972\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.140972\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.140972\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0739648\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0739648\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0739648\n",
      "exploration/env_infos/reward_ctrl Mean                -0.0966673\n",
      "exploration/env_infos/reward_ctrl Std                  0.0415626\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0216698\n",
      "exploration/env_infos/reward_ctrl Min                 -0.292327\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.100256\n",
      "exploration/env_infos/final/torso_velocity Std         0.025131\n",
      "exploration/env_infos/final/torso_velocity Max        -0.075582\n",
      "exploration/env_infos/final/torso_velocity Min        -0.134746\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.189552\n",
      "exploration/env_infos/initial/torso_velocity Std       0.209667\n",
      "exploration/env_infos/initial/torso_velocity Max       0.486064\n",
      "exploration/env_infos/initial/torso_velocity Min       0.040452\n",
      "exploration/env_infos/torso_velocity Mean              0.000392493\n",
      "exploration/env_infos/torso_velocity Std               0.236895\n",
      "exploration/env_infos/torso_velocity Max               1.13281\n",
      "exploration/env_infos/torso_velocity Min              -1.23781\n",
      "evaluation/num steps total                             1.15e+06\n",
      "evaluation/num paths total                          1150\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.909737\n",
      "evaluation/Rewards Std                                 0.0464496\n",
      "evaluation/Rewards Max                                 1.76147\n",
      "evaluation/Rewards Min                                 0.63869\n",
      "evaluation/Returns Mean                              909.737\n",
      "evaluation/Returns Std                                28.3433\n",
      "evaluation/Returns Max                               961.915\n",
      "evaluation/Returns Min                               866.69\n",
      "evaluation/Actions Mean                               -0.01498\n",
      "evaluation/Actions Std                                 0.151242\n",
      "evaluation/Actions Max                                 0.584389\n",
      "evaluation/Actions Min                                -0.545056\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           909.737\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0278672\n",
      "evaluation/env_infos/final/reward_forward Std          0.118811\n",
      "evaluation/env_infos/final/reward_forward Max          0.60608\n",
      "evaluation/env_infos/final/reward_forward Min         -0.00648077\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0194325\n",
      "evaluation/env_infos/initial/reward_forward Std        0.13529\n",
      "evaluation/env_infos/initial/reward_forward Max        0.22305\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.274209\n",
      "evaluation/env_infos/reward_forward Mean              -0.00155935\n",
      "evaluation/env_infos/reward_forward Std                0.116467\n",
      "evaluation/env_infos/reward_forward Max                0.952555\n",
      "evaluation/env_infos/reward_forward Min               -1.33257\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0918569\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0370111\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0382424\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.201541\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0313821\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0196488\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.00804085\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.0754273\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0923944\n",
      "evaluation/env_infos/reward_ctrl Std                   0.03773\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00804085\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.36131\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.00410193\n",
      "evaluation/env_infos/final/torso_velocity Std          0.101412\n",
      "evaluation/env_infos/final/torso_velocity Max          0.60608\n",
      "evaluation/env_infos/final/torso_velocity Min         -0.487687\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.161665\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.255645\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.74143\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.422174\n",
      "evaluation/env_infos/torso_velocity Mean              -0.000749822\n",
      "evaluation/env_infos/torso_velocity Std                0.0950474\n",
      "evaluation/env_infos/torso_velocity Max                1.58073\n",
      "evaluation/env_infos/torso_velocity Min               -1.92734\n",
      "time/data storing (s)                                  0.322723\n",
      "time/evaluation sampling (s)                          42.2135\n",
      "time/exploration sampling (s)                          1.85454\n",
      "time/logging (s)                                       0.287995\n",
      "time/saving (s)                                        0.0263971\n",
      "time/training (s)                                      4.02085\n",
      "time/epoch (s)                                        48.726\n",
      "time/total (s)                                      2317.46\n",
      "Epoch                                                 45\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:03:31.990779 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 46 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 95898\n",
      "trainer/QF1 Loss                                       0.318957\n",
      "trainer/QF2 Loss                                       0.30815\n",
      "trainer/Policy Loss                                   -8.69265\n",
      "trainer/Q1 Predictions Mean                           15.935\n",
      "trainer/Q1 Predictions Std                             1.54774\n",
      "trainer/Q1 Predictions Max                            20.0232\n",
      "trainer/Q1 Predictions Min                             9.2724\n",
      "trainer/Q2 Predictions Mean                           15.9268\n",
      "trainer/Q2 Predictions Std                             1.59545\n",
      "trainer/Q2 Predictions Max                            20.2161\n",
      "trainer/Q2 Predictions Min                             9.35438\n",
      "trainer/Q Targets Mean                                16.0823\n",
      "trainer/Q Targets Std                                  1.57624\n",
      "trainer/Q Targets Max                                 20.6321\n",
      "trainer/Q Targets Min                                  9.17907\n",
      "trainer/Log Pis Mean                                   7.50605\n",
      "trainer/Log Pis Std                                    2.8478\n",
      "trainer/Log Pis Max                                   21.0141\n",
      "trainer/Log Pis Min                                   -2.79755\n",
      "trainer/Policy mu Mean                                -0.026609\n",
      "trainer/Policy mu Std                                  0.184244\n",
      "trainer/Policy mu Max                                  1.30465\n",
      "trainer/Policy mu Min                                 -1.37882\n",
      "trainer/Policy log std Mean                           -2.30128\n",
      "trainer/Policy log std Std                             0.281352\n",
      "trainer/Policy log std Max                            -1.54383\n",
      "trainer/Policy log std Min                            -3.72454\n",
      "trainer/Alpha                                          0.0105334\n",
      "trainer/Alpha Loss                                    -2.24898\n",
      "exploration/num steps total                        48000\n",
      "exploration/num paths total                          142\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.909171\n",
      "exploration/Rewards Std                                0.0604748\n",
      "exploration/Rewards Max                                2.15315\n",
      "exploration/Rewards Min                                0.737075\n",
      "exploration/Returns Mean                             909.171\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              909.171\n",
      "exploration/Returns Min                              909.171\n",
      "exploration/Actions Mean                               0.0743535\n",
      "exploration/Actions Std                                0.137425\n",
      "exploration/Actions Max                                0.540301\n",
      "exploration/Actions Min                               -0.497214\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          909.171\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0455253\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0455253\n",
      "exploration/env_infos/final/reward_forward Min        -0.0455253\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.115073\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.115073\n",
      "exploration/env_infos/initial/reward_forward Min      -0.115073\n",
      "exploration/env_infos/reward_forward Mean             -0.00167274\n",
      "exploration/env_infos/reward_forward Std               0.125595\n",
      "exploration/env_infos/reward_forward Max               0.935451\n",
      "exploration/env_infos/reward_forward Min              -1.36869\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.0963491\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.0963491\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.0963491\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.175664\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.175664\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.175664\n",
      "exploration/env_infos/reward_ctrl Mean                -0.097656\n",
      "exploration/env_infos/reward_ctrl Std                  0.0443106\n",
      "exploration/env_infos/reward_ctrl Max                 -0.00897797\n",
      "exploration/env_infos/reward_ctrl Min                 -0.655354\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.0602042\n",
      "exploration/env_infos/final/torso_velocity Std         0.0744307\n",
      "exploration/env_infos/final/torso_velocity Max         0.0227243\n",
      "exploration/env_infos/final/torso_velocity Min        -0.157812\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.0637431\n",
      "exploration/env_infos/initial/torso_velocity Std       0.222775\n",
      "exploration/env_infos/initial/torso_velocity Max       0.377787\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.115073\n",
      "exploration/env_infos/torso_velocity Mean             -0.00472487\n",
      "exploration/env_infos/torso_velocity Std               0.127092\n",
      "exploration/env_infos/torso_velocity Max               0.935451\n",
      "exploration/env_infos/torso_velocity Min              -1.49623\n",
      "evaluation/num steps total                             1.175e+06\n",
      "evaluation/num paths total                          1175\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.933787\n",
      "evaluation/Rewards Std                                 0.0592759\n",
      "evaluation/Rewards Max                                 2.35314\n",
      "evaluation/Rewards Min                                 0.472048\n",
      "evaluation/Returns Mean                              933.787\n",
      "evaluation/Returns Std                                23.041\n",
      "evaluation/Returns Max                               969.785\n",
      "evaluation/Returns Min                               885.569\n",
      "evaluation/Actions Mean                               -0.0129026\n",
      "evaluation/Actions Std                                 0.131634\n",
      "evaluation/Actions Max                                 0.609128\n",
      "evaluation/Actions Min                                -0.583222\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           933.787\n",
      "evaluation/env_infos/final/reward_forward Mean         0.0264329\n",
      "evaluation/env_infos/final/reward_forward Std          0.0938598\n",
      "evaluation/env_infos/final/reward_forward Max          0.432821\n",
      "evaluation/env_infos/final/reward_forward Min         -0.000358411\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0201892\n",
      "evaluation/env_infos/initial/reward_forward Std        0.147214\n",
      "evaluation/env_infos/initial/reward_forward Max        0.309756\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.318304\n",
      "evaluation/env_infos/reward_forward Mean               0.0157554\n",
      "evaluation/env_infos/reward_forward Std                0.193131\n",
      "evaluation/env_infos/reward_forward Max                1.42938\n",
      "evaluation/env_infos/reward_forward Min               -1.7539\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0727213\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0345824\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0239386\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.140524\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0637781\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0613371\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0115064\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.302709\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0699754\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0372998\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.00698408\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.527952\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean        -0.0114135\n",
      "evaluation/env_infos/final/torso_velocity Std          0.136563\n",
      "evaluation/env_infos/final/torso_velocity Max          0.432821\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.02866\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.150043\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.244346\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.630728\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.318304\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00506755\n",
      "evaluation/env_infos/torso_velocity Std                0.186108\n",
      "evaluation/env_infos/torso_velocity Max                1.42938\n",
      "evaluation/env_infos/torso_velocity Min               -1.78871\n",
      "time/data storing (s)                                  0.320584\n",
      "time/evaluation sampling (s)                          42.265\n",
      "time/exploration sampling (s)                          1.94412\n",
      "time/logging (s)                                       0.276956\n",
      "time/saving (s)                                        0.0261625\n",
      "time/training (s)                                      3.83891\n",
      "time/epoch (s)                                        48.6717\n",
      "time/total (s)                                      2366.61\n",
      "Epoch                                                 46\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:04:21.392350 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 47 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 97898\n",
      "trainer/QF1 Loss                                       0.800016\n",
      "trainer/QF2 Loss                                       0.820391\n",
      "trainer/Policy Loss                                   -8.41625\n",
      "trainer/Q1 Predictions Mean                           16.2759\n",
      "trainer/Q1 Predictions Std                             2.16509\n",
      "trainer/Q1 Predictions Max                            19.0025\n",
      "trainer/Q1 Predictions Min                             2.70785\n",
      "trainer/Q2 Predictions Mean                           16.1675\n",
      "trainer/Q2 Predictions Std                             2.09738\n",
      "trainer/Q2 Predictions Max                            18.4999\n",
      "trainer/Q2 Predictions Min                             2.07897\n",
      "trainer/Q Targets Mean                                16.109\n",
      "trainer/Q Targets Std                                  2.52312\n",
      "trainer/Q Targets Max                                 21.1875\n",
      "trainer/Q Targets Min                                 -1.40656\n",
      "trainer/Log Pis Mean                                   8.1258\n",
      "trainer/Log Pis Std                                    2.27839\n",
      "trainer/Log Pis Max                                   14.5823\n",
      "trainer/Log Pis Min                                    0.811672\n",
      "trainer/Policy mu Mean                                -0.0071933\n",
      "trainer/Policy mu Std                                  0.225552\n",
      "trainer/Policy mu Max                                  1.1176\n",
      "trainer/Policy mu Min                                 -2.05781\n",
      "trainer/Policy log std Mean                           -2.36394\n",
      "trainer/Policy log std Std                             0.254474\n",
      "trainer/Policy log std Max                            -0.714798\n",
      "trainer/Policy log std Min                            -3.93868\n",
      "trainer/Alpha                                          0.0102027\n",
      "trainer/Alpha Loss                                     0.576774\n",
      "exploration/num steps total                        49000\n",
      "exploration/num paths total                          143\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.924837\n",
      "exploration/Rewards Std                                0.0671343\n",
      "exploration/Rewards Max                                1.89051\n",
      "exploration/Rewards Min                                0.607545\n",
      "exploration/Returns Mean                             924.837\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              924.837\n",
      "exploration/Returns Min                              924.837\n",
      "exploration/Actions Mean                               0.00368613\n",
      "exploration/Actions Std                                0.149383\n",
      "exploration/Actions Max                                0.542258\n",
      "exploration/Actions Min                               -0.79698\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          924.837\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0282127\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0282127\n",
      "exploration/env_infos/final/reward_forward Min        -0.0282127\n",
      "exploration/env_infos/initial/reward_forward Mean      0.0363636\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max       0.0363636\n",
      "exploration/env_infos/initial/reward_forward Min       0.0363636\n",
      "exploration/env_infos/reward_forward Mean             -0.00640388\n",
      "exploration/env_infos/reward_forward Std               0.109304\n",
      "exploration/env_infos/reward_forward Max               0.441451\n",
      "exploration/env_infos/reward_forward Min              -1.07466\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.109282\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.109282\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.109282\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.0434039\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.0434039\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.0434039\n",
      "exploration/env_infos/reward_ctrl Mean                -0.089316\n",
      "exploration/env_infos/reward_ctrl Std                  0.0377923\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0129026\n",
      "exploration/env_infos/reward_ctrl Min                 -0.392455\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00997213\n",
      "exploration/env_infos/final/torso_velocity Std         0.013014\n",
      "exploration/env_infos/final/torso_velocity Max         0.0012706\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0282127\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.159217\n",
      "exploration/env_infos/initial/torso_velocity Std       0.182392\n",
      "exploration/env_infos/initial/torso_velocity Max       0.417063\n",
      "exploration/env_infos/initial/torso_velocity Min       0.0242255\n",
      "exploration/env_infos/torso_velocity Mean             -0.00499656\n",
      "exploration/env_infos/torso_velocity Std               0.0879893\n",
      "exploration/env_infos/torso_velocity Max               0.928304\n",
      "exploration/env_infos/torso_velocity Min              -1.07466\n",
      "evaluation/num steps total                             1.2e+06\n",
      "evaluation/num paths total                          1200\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.906847\n",
      "evaluation/Rewards Std                                 0.0609935\n",
      "evaluation/Rewards Max                                 2.08221\n",
      "evaluation/Rewards Min                                 0.443632\n",
      "evaluation/Returns Mean                              906.847\n",
      "evaluation/Returns Std                                52.6922\n",
      "evaluation/Returns Max                               957.872\n",
      "evaluation/Returns Min                               790.12\n",
      "evaluation/Actions Mean                                0.0327547\n",
      "evaluation/Actions Std                                 0.150164\n",
      "evaluation/Actions Max                                 0.733623\n",
      "evaluation/Actions Min                                -0.787274\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           906.847\n",
      "evaluation/env_infos/final/reward_forward Mean         1.03916e-07\n",
      "evaluation/env_infos/final/reward_forward Std          5.20223e-07\n",
      "evaluation/env_infos/final/reward_forward Max          1.07827e-06\n",
      "evaluation/env_infos/final/reward_forward Min         -9.45121e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.000393326\n",
      "evaluation/env_infos/initial/reward_forward Std        0.125436\n",
      "evaluation/env_infos/initial/reward_forward Max        0.320859\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.202419\n",
      "evaluation/env_infos/reward_forward Mean              -9.2188e-05\n",
      "evaluation/env_infos/reward_forward Std                0.0828182\n",
      "evaluation/env_infos/reward_forward Max                1.86936\n",
      "evaluation/env_infos/reward_forward Min               -1.08236\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.0940358\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0538908\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.0450732\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.211313\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0599093\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0261485\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0221743\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.110215\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.0944886\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0554939\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.013021\n",
      "evaluation/env_infos/reward_ctrl Min                  -0.556368\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         2.20285e-09\n",
      "evaluation/env_infos/final/torso_velocity Std          3.73427e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          1.07827e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -1.18361e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.128794\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.249316\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.643237\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.335092\n",
      "evaluation/env_infos/torso_velocity Mean              -0.00168575\n",
      "evaluation/env_infos/torso_velocity Std                0.0789365\n",
      "evaluation/env_infos/torso_velocity Max                1.86936\n",
      "evaluation/env_infos/torso_velocity Min               -1.76328\n",
      "time/data storing (s)                                  0.317848\n",
      "time/evaluation sampling (s)                          42.597\n",
      "time/exploration sampling (s)                          1.93651\n",
      "time/logging (s)                                       0.275187\n",
      "time/saving (s)                                        0.0280016\n",
      "time/training (s)                                      3.77973\n",
      "time/epoch (s)                                        48.9342\n",
      "time/total (s)                                      2416.01\n",
      "Epoch                                                 47\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:05:09.630221 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 48 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 99898\n",
      "trainer/QF1 Loss                                       0.894116\n",
      "trainer/QF2 Loss                                       0.475462\n",
      "trainer/Policy Loss                                   -8.93167\n",
      "trainer/Q1 Predictions Mean                           16.4899\n",
      "trainer/Q1 Predictions Std                             2.08366\n",
      "trainer/Q1 Predictions Max                            21.6702\n",
      "trainer/Q1 Predictions Min                             2.21719\n",
      "trainer/Q2 Predictions Mean                           16.6026\n",
      "trainer/Q2 Predictions Std                             2.20791\n",
      "trainer/Q2 Predictions Max                            21.1182\n",
      "trainer/Q2 Predictions Min                            -0.973373\n",
      "trainer/Q Targets Mean                                16.4331\n",
      "trainer/Q Targets Std                                  2.23877\n",
      "trainer/Q Targets Max                                 20.4302\n",
      "trainer/Q Targets Min                                 -0.394603\n",
      "trainer/Log Pis Mean                                   7.89476\n",
      "trainer/Log Pis Std                                    2.77478\n",
      "trainer/Log Pis Max                                   24.09\n",
      "trainer/Log Pis Min                                   -0.25729\n",
      "trainer/Policy mu Mean                                 0.0253945\n",
      "trainer/Policy mu Std                                  0.257009\n",
      "trainer/Policy mu Max                                  1.70323\n",
      "trainer/Policy mu Min                                 -1.68819\n",
      "trainer/Policy log std Mean                           -2.35498\n",
      "trainer/Policy log std Std                             0.2718\n",
      "trainer/Policy log std Max                            -1.4883\n",
      "trainer/Policy log std Min                            -4.35695\n",
      "trainer/Alpha                                          0.0101698\n",
      "trainer/Alpha Loss                                    -0.48272\n",
      "exploration/num steps total                        50000\n",
      "exploration/num paths total                          144\n",
      "exploration/path length Mean                        1000\n",
      "exploration/path length Std                            0\n",
      "exploration/path length Max                         1000\n",
      "exploration/path length Min                         1000\n",
      "exploration/Rewards Mean                               0.84211\n",
      "exploration/Rewards Std                                0.0781243\n",
      "exploration/Rewards Max                                0.975006\n",
      "exploration/Rewards Min                               -0.348148\n",
      "exploration/Returns Mean                             842.11\n",
      "exploration/Returns Std                                0\n",
      "exploration/Returns Max                              842.11\n",
      "exploration/Returns Min                              842.11\n",
      "exploration/Actions Mean                               0.0305368\n",
      "exploration/Actions Std                                0.197973\n",
      "exploration/Actions Max                                0.813983\n",
      "exploration/Actions Min                               -0.815742\n",
      "exploration/Num Paths                                  1\n",
      "exploration/Average Returns                          842.11\n",
      "exploration/env_infos/final/reward_forward Mean       -0.0237902\n",
      "exploration/env_infos/final/reward_forward Std         0\n",
      "exploration/env_infos/final/reward_forward Max        -0.0237902\n",
      "exploration/env_infos/final/reward_forward Min        -0.0237902\n",
      "exploration/env_infos/initial/reward_forward Mean     -0.0279889\n",
      "exploration/env_infos/initial/reward_forward Std       0\n",
      "exploration/env_infos/initial/reward_forward Max      -0.0279889\n",
      "exploration/env_infos/initial/reward_forward Min      -0.0279889\n",
      "exploration/env_infos/reward_forward Mean              0.00935054\n",
      "exploration/env_infos/reward_forward Std               0.181483\n",
      "exploration/env_infos/reward_forward Max               1.74501\n",
      "exploration/env_infos/reward_forward Min              -1.49926\n",
      "exploration/env_infos/final/reward_ctrl Mean          -0.147184\n",
      "exploration/env_infos/final/reward_ctrl Std            0\n",
      "exploration/env_infos/final/reward_ctrl Max           -0.147184\n",
      "exploration/env_infos/final/reward_ctrl Min           -0.147184\n",
      "exploration/env_infos/initial/reward_ctrl Mean        -0.160682\n",
      "exploration/env_infos/initial/reward_ctrl Std          0\n",
      "exploration/env_infos/initial/reward_ctrl Max         -0.160682\n",
      "exploration/env_infos/initial/reward_ctrl Min         -0.160682\n",
      "exploration/env_infos/reward_ctrl Mean                -0.160503\n",
      "exploration/env_infos/reward_ctrl Std                  0.0771234\n",
      "exploration/env_infos/reward_ctrl Max                 -0.0336313\n",
      "exploration/env_infos/reward_ctrl Min                 -1.34815\n",
      "exploration/env_infos/final/reward_contact Mean        0\n",
      "exploration/env_infos/final/reward_contact Std         0\n",
      "exploration/env_infos/final/reward_contact Max        -0\n",
      "exploration/env_infos/final/reward_contact Min        -0\n",
      "exploration/env_infos/initial/reward_contact Mean      0\n",
      "exploration/env_infos/initial/reward_contact Std       0\n",
      "exploration/env_infos/initial/reward_contact Max      -0\n",
      "exploration/env_infos/initial/reward_contact Min      -0\n",
      "exploration/env_infos/reward_contact Mean              0\n",
      "exploration/env_infos/reward_contact Std               0\n",
      "exploration/env_infos/reward_contact Max              -0\n",
      "exploration/env_infos/reward_contact Min              -0\n",
      "exploration/env_infos/final/reward_survive Mean        1\n",
      "exploration/env_infos/final/reward_survive Std         0\n",
      "exploration/env_infos/final/reward_survive Max         1\n",
      "exploration/env_infos/final/reward_survive Min         1\n",
      "exploration/env_infos/initial/reward_survive Mean      1\n",
      "exploration/env_infos/initial/reward_survive Std       0\n",
      "exploration/env_infos/initial/reward_survive Max       1\n",
      "exploration/env_infos/initial/reward_survive Min       1\n",
      "exploration/env_infos/reward_survive Mean              1\n",
      "exploration/env_infos/reward_survive Std               0\n",
      "exploration/env_infos/reward_survive Max               1\n",
      "exploration/env_infos/reward_survive Min               1\n",
      "exploration/env_infos/final/torso_velocity Mean       -0.00788619\n",
      "exploration/env_infos/final/torso_velocity Std         0.0138638\n",
      "exploration/env_infos/final/torso_velocity Max         0.00999571\n",
      "exploration/env_infos/final/torso_velocity Min        -0.0237902\n",
      "exploration/env_infos/initial/torso_velocity Mean      0.120627\n",
      "exploration/env_infos/initial/torso_velocity Std       0.27092\n",
      "exploration/env_infos/initial/torso_velocity Max       0.500764\n",
      "exploration/env_infos/initial/torso_velocity Min      -0.110893\n",
      "exploration/env_infos/torso_velocity Mean              0.000459818\n",
      "exploration/env_infos/torso_velocity Std               0.134301\n",
      "exploration/env_infos/torso_velocity Max               1.74501\n",
      "exploration/env_infos/torso_velocity Min              -1.49926\n",
      "evaluation/num steps total                             1.225e+06\n",
      "evaluation/num paths total                          1225\n",
      "evaluation/path length Mean                         1000\n",
      "evaluation/path length Std                             0\n",
      "evaluation/path length Max                          1000\n",
      "evaluation/path length Min                          1000\n",
      "evaluation/Rewards Mean                                0.838883\n",
      "evaluation/Rewards Std                                 0.0526301\n",
      "evaluation/Rewards Max                                 2.14581\n",
      "evaluation/Rewards Min                                -0.361579\n",
      "evaluation/Returns Mean                              838.883\n",
      "evaluation/Returns Std                                23.3258\n",
      "evaluation/Returns Max                               868.493\n",
      "evaluation/Returns Min                               779.696\n",
      "evaluation/Actions Mean                                0.0580914\n",
      "evaluation/Actions Std                                 0.192733\n",
      "evaluation/Actions Max                                 0.771339\n",
      "evaluation/Actions Min                                -0.805296\n",
      "evaluation/Num Paths                                  25\n",
      "evaluation/Average Returns                           838.883\n",
      "evaluation/env_infos/final/reward_forward Mean         1.01178e-07\n",
      "evaluation/env_infos/final/reward_forward Std          3.29398e-07\n",
      "evaluation/env_infos/final/reward_forward Max          8.9158e-07\n",
      "evaluation/env_infos/final/reward_forward Min         -5.48418e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       0.0613334\n",
      "evaluation/env_infos/initial/reward_forward Std        0.116109\n",
      "evaluation/env_infos/initial/reward_forward Max        0.25069\n",
      "evaluation/env_infos/initial/reward_forward Min       -0.168512\n",
      "evaluation/env_infos/reward_forward Mean              -0.00349638\n",
      "evaluation/env_infos/reward_forward Std                0.0667481\n",
      "evaluation/env_infos/reward_forward Max                1.13775\n",
      "evaluation/env_infos/reward_forward Min               -1.83472\n",
      "evaluation/env_infos/final/reward_ctrl Mean           -0.160093\n",
      "evaluation/env_infos/final/reward_ctrl Std             0.0234441\n",
      "evaluation/env_infos/final/reward_ctrl Max            -0.128317\n",
      "evaluation/env_infos/final/reward_ctrl Min            -0.219099\n",
      "evaluation/env_infos/initial/reward_ctrl Mean         -0.0910081\n",
      "evaluation/env_infos/initial/reward_ctrl Std           0.0248726\n",
      "evaluation/env_infos/initial/reward_ctrl Max          -0.0573051\n",
      "evaluation/env_infos/initial/reward_ctrl Min          -0.149321\n",
      "evaluation/env_infos/reward_ctrl Mean                 -0.162083\n",
      "evaluation/env_infos/reward_ctrl Std                   0.0474792\n",
      "evaluation/env_infos/reward_ctrl Max                  -0.0573051\n",
      "evaluation/env_infos/reward_ctrl Min                  -1.36158\n",
      "evaluation/env_infos/final/reward_contact Mean         0\n",
      "evaluation/env_infos/final/reward_contact Std          0\n",
      "evaluation/env_infos/final/reward_contact Max         -0\n",
      "evaluation/env_infos/final/reward_contact Min         -0\n",
      "evaluation/env_infos/initial/reward_contact Mean       0\n",
      "evaluation/env_infos/initial/reward_contact Std        0\n",
      "evaluation/env_infos/initial/reward_contact Max       -0\n",
      "evaluation/env_infos/initial/reward_contact Min       -0\n",
      "evaluation/env_infos/reward_contact Mean               0\n",
      "evaluation/env_infos/reward_contact Std                0\n",
      "evaluation/env_infos/reward_contact Max               -0\n",
      "evaluation/env_infos/reward_contact Min               -0\n",
      "evaluation/env_infos/final/reward_survive Mean         1\n",
      "evaluation/env_infos/final/reward_survive Std          0\n",
      "evaluation/env_infos/final/reward_survive Max          1\n",
      "evaluation/env_infos/final/reward_survive Min          1\n",
      "evaluation/env_infos/initial/reward_survive Mean       1\n",
      "evaluation/env_infos/initial/reward_survive Std        0\n",
      "evaluation/env_infos/initial/reward_survive Max        1\n",
      "evaluation/env_infos/initial/reward_survive Min        1\n",
      "evaluation/env_infos/reward_survive Mean               1\n",
      "evaluation/env_infos/reward_survive Std                0\n",
      "evaluation/env_infos/reward_survive Max                1\n",
      "evaluation/env_infos/reward_survive Min                1\n",
      "evaluation/env_infos/final/torso_velocity Mean         6.67374e-08\n",
      "evaluation/env_infos/final/torso_velocity Std          4.05702e-07\n",
      "evaluation/env_infos/final/torso_velocity Max          2.10112e-06\n",
      "evaluation/env_infos/final/torso_velocity Min         -8.31898e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean       0.145745\n",
      "evaluation/env_infos/initial/torso_velocity Std        0.25859\n",
      "evaluation/env_infos/initial/torso_velocity Max        0.614292\n",
      "evaluation/env_infos/initial/torso_velocity Min       -0.457625\n",
      "evaluation/env_infos/torso_velocity Mean              -0.0031954\n",
      "evaluation/env_infos/torso_velocity Std                0.0649547\n",
      "evaluation/env_infos/torso_velocity Max                1.26824\n",
      "evaluation/env_infos/torso_velocity Min               -1.83472\n",
      "time/data storing (s)                                  0.321892\n",
      "time/evaluation sampling (s)                          41.4261\n",
      "time/exploration sampling (s)                          1.88292\n",
      "time/logging (s)                                       0.279551\n",
      "time/saving (s)                                        0.0262465\n",
      "time/training (s)                                      3.82897\n",
      "time/epoch (s)                                        47.7656\n",
      "time/total (s)                                      2464.25\n",
      "Epoch                                                 48\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:05:58.274783 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 49 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 101898\n",
      "trainer/QF1 Loss                                        0.51251\n",
      "trainer/QF2 Loss                                        0.731081\n",
      "trainer/Policy Loss                                    -9.09343\n",
      "trainer/Q1 Predictions Mean                            16.4528\n",
      "trainer/Q1 Predictions Std                              1.68917\n",
      "trainer/Q1 Predictions Max                             20.513\n",
      "trainer/Q1 Predictions Min                             10.1505\n",
      "trainer/Q2 Predictions Mean                            16.313\n",
      "trainer/Q2 Predictions Std                              1.75745\n",
      "trainer/Q2 Predictions Max                             20.3427\n",
      "trainer/Q2 Predictions Min                             10.1619\n",
      "trainer/Q Targets Mean                                 16.8964\n",
      "trainer/Q Targets Std                                   1.80052\n",
      "trainer/Q Targets Max                                  21.8294\n",
      "trainer/Q Targets Min                                   9.51214\n",
      "trainer/Log Pis Mean                                    7.62824\n",
      "trainer/Log Pis Std                                     2.97833\n",
      "trainer/Log Pis Max                                    20.0012\n",
      "trainer/Log Pis Min                                    -0.687929\n",
      "trainer/Policy mu Mean                                 -0.00411874\n",
      "trainer/Policy mu Std                                   0.255\n",
      "trainer/Policy mu Max                                   0.952919\n",
      "trainer/Policy mu Min                                  -1.9684\n",
      "trainer/Policy log std Mean                            -2.32868\n",
      "trainer/Policy log std Std                              0.270442\n",
      "trainer/Policy log std Max                             -1.30397\n",
      "trainer/Policy log std Min                             -3.55447\n",
      "trainer/Alpha                                           0.00940329\n",
      "trainer/Alpha Loss                                     -1.73443\n",
      "exploration/num steps total                         51000\n",
      "exploration/num paths total                           145\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.811225\n",
      "exploration/Rewards Std                                 0.0679252\n",
      "exploration/Rewards Max                                 1.54389\n",
      "exploration/Rewards Min                                 0.623829\n",
      "exploration/Returns Mean                              811.225\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               811.225\n",
      "exploration/Returns Min                               811.225\n",
      "exploration/Actions Mean                                0.0382519\n",
      "exploration/Actions Std                                 0.216381\n",
      "exploration/Actions Max                                 0.598842\n",
      "exploration/Actions Min                                -0.517892\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           811.225\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00116841\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00116841\n",
      "exploration/env_infos/final/reward_forward Min         -0.00116841\n",
      "exploration/env_infos/initial/reward_forward Mean       0.058215\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.058215\n",
      "exploration/env_infos/initial/reward_forward Min        0.058215\n",
      "exploration/env_infos/reward_forward Mean              -0.00552125\n",
      "exploration/env_infos/reward_forward Std                0.069729\n",
      "exploration/env_infos/reward_forward Max                0.47629\n",
      "exploration/env_infos/reward_forward Min               -0.570144\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.280064\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.280064\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.280064\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0760524\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0760524\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0760524\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.193137\n",
      "exploration/env_infos/reward_ctrl Std                   0.0508841\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0690116\n",
      "exploration/env_infos/reward_ctrl Min                  -0.376171\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.000305874\n",
      "exploration/env_infos/final/torso_velocity Std          0.000692528\n",
      "exploration/env_infos/final/torso_velocity Max          0.000527156\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00116841\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.165807\n",
      "exploration/env_infos/initial/torso_velocity Std        0.190104\n",
      "exploration/env_infos/initial/torso_velocity Max        0.432975\n",
      "exploration/env_infos/initial/torso_velocity Min        0.00623247\n",
      "exploration/env_infos/torso_velocity Mean              -0.000523138\n",
      "exploration/env_infos/torso_velocity Std                0.0726991\n",
      "exploration/env_infos/torso_velocity Max                0.47629\n",
      "exploration/env_infos/torso_velocity Min               -1.02323\n",
      "evaluation/num steps total                              1.25e+06\n",
      "evaluation/num paths total                           1250\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.840084\n",
      "evaluation/Rewards Std                                  0.0607149\n",
      "evaluation/Rewards Max                                  1.87768\n",
      "evaluation/Rewards Min                                  0.067971\n",
      "evaluation/Returns Mean                               840.084\n",
      "evaluation/Returns Std                                 55.3428\n",
      "evaluation/Returns Max                                889.891\n",
      "evaluation/Returns Min                                679.516\n",
      "evaluation/Actions Mean                                 0.00785853\n",
      "evaluation/Actions Std                                  0.200378\n",
      "evaluation/Actions Max                                  0.587986\n",
      "evaluation/Actions Min                                 -0.870114\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            840.084\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.00527e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.95269e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.76266e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.78973e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0685809\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121407\n",
      "evaluation/env_infos/initial/reward_forward Max         0.265541\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.186169\n",
      "evaluation/env_infos/reward_forward Mean                0.00210224\n",
      "evaluation/env_infos/reward_forward Std                 0.0639707\n",
      "evaluation/env_infos/reward_forward Max                 1.70285\n",
      "evaluation/env_infos/reward_forward Min                -1.12734\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.159946\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0555818\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.110668\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.321016\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0983743\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0311216\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0688881\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.181047\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.160852\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0567095\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0688881\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.932029\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.82785e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.88486e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.23299e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -7.90096e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.172129\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.222522\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.622119\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.194899\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00126515\n",
      "evaluation/env_infos/torso_velocity Std                 0.067444\n",
      "evaluation/env_infos/torso_velocity Max                 1.70285\n",
      "evaluation/env_infos/torso_velocity Min                -2.05734\n",
      "time/data storing (s)                                   0.319619\n",
      "time/evaluation sampling (s)                           41.7308\n",
      "time/exploration sampling (s)                           1.95516\n",
      "time/logging (s)                                        0.278981\n",
      "time/saving (s)                                         0.0250195\n",
      "time/training (s)                                       3.81142\n",
      "time/epoch (s)                                         48.121\n",
      "time/total (s)                                       2512.89\n",
      "Epoch                                                  49\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:06:47.166751 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 50 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 103898\n",
      "trainer/QF1 Loss                                        0.62703\n",
      "trainer/QF2 Loss                                        0.381011\n",
      "trainer/Policy Loss                                   -10.4767\n",
      "trainer/Q1 Predictions Mean                            17.4997\n",
      "trainer/Q1 Predictions Std                              1.97921\n",
      "trainer/Q1 Predictions Max                             27.7872\n",
      "trainer/Q1 Predictions Min                              5.4486\n",
      "trainer/Q2 Predictions Mean                            17.3819\n",
      "trainer/Q2 Predictions Std                              2.06867\n",
      "trainer/Q2 Predictions Max                             26.7731\n",
      "trainer/Q2 Predictions Min                              3.96073\n",
      "trainer/Q Targets Mean                                 17.2382\n",
      "trainer/Q Targets Std                                   2.31346\n",
      "trainer/Q Targets Max                                  27.1054\n",
      "trainer/Q Targets Min                                  -0.87284\n",
      "trainer/Log Pis Mean                                    7.17939\n",
      "trainer/Log Pis Std                                     2.54442\n",
      "trainer/Log Pis Max                                    18.6653\n",
      "trainer/Log Pis Min                                    -3.6684\n",
      "trainer/Policy mu Mean                                 -0.0498539\n",
      "trainer/Policy mu Std                                   0.202007\n",
      "trainer/Policy mu Max                                   0.748501\n",
      "trainer/Policy mu Min                                  -1.49967\n",
      "trainer/Policy log std Mean                            -2.25802\n",
      "trainer/Policy log std Std                              0.249388\n",
      "trainer/Policy log std Max                             -0.507515\n",
      "trainer/Policy log std Min                             -3.98693\n",
      "trainer/Alpha                                           0.00931599\n",
      "trainer/Alpha Loss                                     -3.83699\n",
      "exploration/num steps total                         52000\n",
      "exploration/num paths total                           146\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.850047\n",
      "exploration/Rewards Std                                 0.0886002\n",
      "exploration/Rewards Max                                 1.40369\n",
      "exploration/Rewards Min                                 0.622166\n",
      "exploration/Returns Mean                              850.047\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               850.047\n",
      "exploration/Returns Min                               850.047\n",
      "exploration/Actions Mean                               -0.0455232\n",
      "exploration/Actions Std                                 0.203332\n",
      "exploration/Actions Max                                 0.709494\n",
      "exploration/Actions Min                                -0.604756\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           850.047\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0650625\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0650625\n",
      "exploration/env_infos/final/reward_forward Min         -0.0650625\n",
      "exploration/env_infos/initial/reward_forward Mean       0.143022\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.143022\n",
      "exploration/env_infos/initial/reward_forward Min        0.143022\n",
      "exploration/env_infos/reward_forward Mean               0.0249601\n",
      "exploration/env_infos/reward_forward Std                0.166476\n",
      "exploration/env_infos/reward_forward Max                0.885153\n",
      "exploration/env_infos/reward_forward Min               -0.507892\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0958456\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0958456\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0958456\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.227904\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.227904\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.227904\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.173666\n",
      "exploration/env_infos/reward_ctrl Std                   0.0582571\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0377597\n",
      "exploration/env_infos/reward_ctrl Min                  -0.377834\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0191487\n",
      "exploration/env_infos/final/torso_velocity Std          0.0331027\n",
      "exploration/env_infos/final/torso_velocity Max          0.011722\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0650625\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.172564\n",
      "exploration/env_infos/initial/torso_velocity Std        0.264218\n",
      "exploration/env_infos/initial/torso_velocity Max        0.509922\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.135251\n",
      "exploration/env_infos/torso_velocity Mean               0.00077803\n",
      "exploration/env_infos/torso_velocity Std                0.129972\n",
      "exploration/env_infos/torso_velocity Max                0.885153\n",
      "exploration/env_infos/torso_velocity Min               -1.08116\n",
      "evaluation/num steps total                              1.275e+06\n",
      "evaluation/num paths total                           1275\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.889777\n",
      "evaluation/Rewards Std                                  0.0347578\n",
      "evaluation/Rewards Max                                  2.16316\n",
      "evaluation/Rewards Min                                  0.400878\n",
      "evaluation/Returns Mean                               889.777\n",
      "evaluation/Returns Std                                 25.3263\n",
      "evaluation/Returns Max                                939.958\n",
      "evaluation/Returns Min                                852.529\n",
      "evaluation/Actions Mean                                -0.0365708\n",
      "evaluation/Actions Std                                  0.162444\n",
      "evaluation/Actions Max                                  0.507563\n",
      "evaluation/Actions Min                                 -0.747188\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            889.777\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.62039e-07\n",
      "evaluation/env_infos/final/reward_forward Std           1.49071e-06\n",
      "evaluation/env_infos/final/reward_forward Max           6.61288e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -7.25957e-06\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0120029\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0994969\n",
      "evaluation/env_infos/initial/reward_forward Max         0.216534\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.193948\n",
      "evaluation/env_infos/reward_forward Mean                0.00196022\n",
      "evaluation/env_infos/reward_forward Std                 0.0588348\n",
      "evaluation/env_infos/reward_forward Max                 1.1927\n",
      "evaluation/env_infos/reward_forward Min                -1.36848\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.110158\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0249482\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0602183\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.147442\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0699797\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0395066\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0181769\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.165809\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.110902\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0287487\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0181769\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.599122\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.27208e-07\n",
      "evaluation/env_infos/final/torso_velocity Std           1.54759e-06\n",
      "evaluation/env_infos/final/torso_velocity Max           6.61288e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.11401e-05\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128154\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.22634\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.549084\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.253216\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00157544\n",
      "evaluation/env_infos/torso_velocity Std                 0.058932\n",
      "evaluation/env_infos/torso_velocity Max                 1.1927\n",
      "evaluation/env_infos/torso_velocity Min                -1.72561\n",
      "time/data storing (s)                                   0.318797\n",
      "time/evaluation sampling (s)                           41.9653\n",
      "time/exploration sampling (s)                           1.89799\n",
      "time/logging (s)                                        0.270171\n",
      "time/saving (s)                                         0.0542325\n",
      "time/training (s)                                       3.89133\n",
      "time/epoch (s)                                         48.3978\n",
      "time/total (s)                                       2561.77\n",
      "Epoch                                                  50\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:07:35.944231 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 51 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 105898\n",
      "trainer/QF1 Loss                                        0.919668\n",
      "trainer/QF2 Loss                                        1.15785\n",
      "trainer/Policy Loss                                    -9.7938\n",
      "trainer/Q1 Predictions Mean                            17.2867\n",
      "trainer/Q1 Predictions Std                              1.91833\n",
      "trainer/Q1 Predictions Max                             24.0661\n",
      "trainer/Q1 Predictions Min                              2.66813\n",
      "trainer/Q2 Predictions Mean                            17.3563\n",
      "trainer/Q2 Predictions Std                              1.82744\n",
      "trainer/Q2 Predictions Max                             22.9181\n",
      "trainer/Q2 Predictions Min                              3.23601\n",
      "trainer/Q Targets Mean                                 17.4544\n",
      "trainer/Q Targets Std                                   2.41834\n",
      "trainer/Q Targets Max                                  24.3029\n",
      "trainer/Q Targets Min                                   0.0206765\n",
      "trainer/Log Pis Mean                                    7.80701\n",
      "trainer/Log Pis Std                                     2.42556\n",
      "trainer/Log Pis Max                                    14.5587\n",
      "trainer/Log Pis Min                                     1.2218\n",
      "trainer/Policy mu Mean                                 -0.045333\n",
      "trainer/Policy mu Std                                   0.200649\n",
      "trainer/Policy mu Max                                   0.843747\n",
      "trainer/Policy mu Min                                  -1.38698\n",
      "trainer/Policy log std Mean                            -2.3312\n",
      "trainer/Policy log std Std                              0.256134\n",
      "trainer/Policy log std Max                             -0.666169\n",
      "trainer/Policy log std Min                             -3.26263\n",
      "trainer/Alpha                                           0.00949198\n",
      "trainer/Alpha Loss                                     -0.898965\n",
      "exploration/num steps total                         53000\n",
      "exploration/num paths total                           147\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.885759\n",
      "exploration/Rewards Std                                 0.131696\n",
      "exploration/Rewards Max                                 2.19171\n",
      "exploration/Rewards Min                                 0.569305\n",
      "exploration/Returns Mean                              885.759\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               885.759\n",
      "exploration/Returns Min                               885.759\n",
      "exploration/Actions Mean                               -0.0229959\n",
      "exploration/Actions Std                                 0.194976\n",
      "exploration/Actions Max                                 0.64837\n",
      "exploration/Actions Min                                -0.650581\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           885.759\n",
      "exploration/env_infos/final/reward_forward Mean         0.0203481\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0203481\n",
      "exploration/env_infos/final/reward_forward Min          0.0203481\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.256498\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.256498\n",
      "exploration/env_infos/initial/reward_forward Min       -0.256498\n",
      "exploration/env_infos/reward_forward Mean               0.0188434\n",
      "exploration/env_infos/reward_forward Std                0.170114\n",
      "exploration/env_infos/reward_forward Max                1.29405\n",
      "exploration/env_infos/reward_forward Min               -1.27765\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0922042\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0922042\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0922042\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0904168\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0904168\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0904168\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.154178\n",
      "exploration/env_infos/reward_ctrl Std                   0.0555851\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0308452\n",
      "exploration/env_infos/reward_ctrl Min                  -0.430695\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0114092\n",
      "exploration/env_infos/final/torso_velocity Std          0.0126665\n",
      "exploration/env_infos/final/torso_velocity Max          0.0203834\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00650402\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0292766\n",
      "exploration/env_infos/initial/torso_velocity Std        0.210034\n",
      "exploration/env_infos/initial/torso_velocity Max        0.242314\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.256498\n",
      "exploration/env_infos/torso_velocity Mean               0.00188878\n",
      "exploration/env_infos/torso_velocity Std                0.129448\n",
      "exploration/env_infos/torso_velocity Max                1.29405\n",
      "exploration/env_infos/torso_velocity Min               -1.27765\n",
      "evaluation/num steps total                              1.3e+06\n",
      "evaluation/num paths total                           1300\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.912826\n",
      "evaluation/Rewards Std                                  0.0480751\n",
      "evaluation/Rewards Max                                  2.20902\n",
      "evaluation/Rewards Min                                  0.0657306\n",
      "evaluation/Returns Mean                               912.826\n",
      "evaluation/Returns Std                                 20.4169\n",
      "evaluation/Returns Max                                970.618\n",
      "evaluation/Returns Min                                879.684\n",
      "evaluation/Actions Mean                                -0.0245765\n",
      "evaluation/Actions Std                                  0.148133\n",
      "evaluation/Actions Max                                  0.755172\n",
      "evaluation/Actions Min                                 -0.66487\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            912.826\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.67434e-05\n",
      "evaluation/env_infos/final/reward_forward Std           8.01478e-05\n",
      "evaluation/env_infos/final/reward_forward Max           8.82152e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000409328\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0401303\n",
      "evaluation/env_infos/initial/reward_forward Std         0.159045\n",
      "evaluation/env_infos/initial/reward_forward Max         0.263435\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.316723\n",
      "evaluation/env_infos/reward_forward Mean                0.00106109\n",
      "evaluation/env_infos/reward_forward Std                 0.107592\n",
      "evaluation/env_infos/reward_forward Max                 1.50518\n",
      "evaluation/env_infos/reward_forward Min                -1.46122\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0890005\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0231186\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0278945\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.124351\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.085474\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.031192\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0417147\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.15068\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.090189\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0292989\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0124796\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.934269\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -1.91688e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           9.29515e-05\n",
      "evaluation/env_infos/final/torso_velocity Max           1.11918e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.000504681\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.14794\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.262528\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.672281\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.316723\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00253352\n",
      "evaluation/env_infos/torso_velocity Std                 0.0923991\n",
      "evaluation/env_infos/torso_velocity Max                 1.50518\n",
      "evaluation/env_infos/torso_velocity Min                -1.7036\n",
      "time/data storing (s)                                   0.319433\n",
      "time/evaluation sampling (s)                           41.9768\n",
      "time/exploration sampling (s)                           1.91067\n",
      "time/logging (s)                                        0.279911\n",
      "time/saving (s)                                         0.0263145\n",
      "time/training (s)                                       3.80206\n",
      "time/epoch (s)                                         48.3152\n",
      "time/total (s)                                       2610.56\n",
      "Epoch                                                  51\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 14:08:27.025641 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 52 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 107898\n",
      "trainer/QF1 Loss                                        0.34074\n",
      "trainer/QF2 Loss                                        0.350369\n",
      "trainer/Policy Loss                                   -10.1076\n",
      "trainer/Q1 Predictions Mean                            18.0297\n",
      "trainer/Q1 Predictions Std                              1.53788\n",
      "trainer/Q1 Predictions Max                             21.133\n",
      "trainer/Q1 Predictions Min                             11.1654\n",
      "trainer/Q2 Predictions Mean                            17.8288\n",
      "trainer/Q2 Predictions Std                              1.56927\n",
      "trainer/Q2 Predictions Max                             20.7754\n",
      "trainer/Q2 Predictions Min                             11.1756\n",
      "trainer/Q Targets Mean                                 17.9355\n",
      "trainer/Q Targets Std                                   1.5585\n",
      "trainer/Q Targets Max                                  20.6706\n",
      "trainer/Q Targets Min                                  11.0788\n",
      "trainer/Log Pis Mean                                    8.04334\n",
      "trainer/Log Pis Std                                     2.91542\n",
      "trainer/Log Pis Max                                    19.9215\n",
      "trainer/Log Pis Min                                    -3.95083\n",
      "trainer/Policy mu Mean                                  0.0140356\n",
      "trainer/Policy mu Std                                   0.204074\n",
      "trainer/Policy mu Max                                   0.935015\n",
      "trainer/Policy mu Min                                  -1.48593\n",
      "trainer/Policy log std Mean                            -2.39992\n",
      "trainer/Policy log std Std                              0.271828\n",
      "trainer/Policy log std Max                             -1.61825\n",
      "trainer/Policy log std Min                             -3.91534\n",
      "trainer/Alpha                                           0.00904686\n",
      "trainer/Alpha Loss                                      0.203935\n",
      "exploration/num steps total                         54000\n",
      "exploration/num paths total                           148\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.900057\n",
      "exploration/Rewards Std                                 0.0842688\n",
      "exploration/Rewards Max                                 1.66389\n",
      "exploration/Rewards Min                                 0.550775\n",
      "exploration/Returns Mean                              900.057\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               900.057\n",
      "exploration/Returns Min                               900.057\n",
      "exploration/Actions Mean                               -0.00978707\n",
      "exploration/Actions Std                                 0.169817\n",
      "exploration/Actions Max                                 0.541729\n",
      "exploration/Actions Min                                -0.623839\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           900.057\n",
      "exploration/env_infos/final/reward_forward Mean        -0.398678\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.398678\n",
      "exploration/env_infos/final/reward_forward Min         -0.398678\n",
      "exploration/env_infos/initial/reward_forward Mean       0.185386\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.185386\n",
      "exploration/env_infos/initial/reward_forward Min        0.185386\n",
      "exploration/env_infos/reward_forward Mean              -0.0393193\n",
      "exploration/env_infos/reward_forward Std                0.194984\n",
      "exploration/env_infos/reward_forward Max                0.562284\n",
      "exploration/env_infos/reward_forward Min               -1.07881\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.137537\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.137537\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.137537\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0305618\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0305618\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0305618\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.115734\n",
      "exploration/env_infos/reward_ctrl Std                   0.0466426\n",
      "exploration/env_infos/reward_ctrl Max                  -0.025422\n",
      "exploration/env_infos/reward_ctrl Min                  -0.449225\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0935098\n",
      "exploration/env_infos/final/torso_velocity Std          0.406215\n",
      "exploration/env_infos/final/torso_velocity Max          0.596177\n",
      "exploration/env_infos/final/torso_velocity Min         -0.398678\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.225516\n",
      "exploration/env_infos/initial/torso_velocity Std        0.145161\n",
      "exploration/env_infos/initial/torso_velocity Max        0.419936\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0712249\n",
      "exploration/env_infos/torso_velocity Mean              -0.031008\n",
      "exploration/env_infos/torso_velocity Std                0.226732\n",
      "exploration/env_infos/torso_velocity Max                0.796089\n",
      "exploration/env_infos/torso_velocity Min               -1.62639\n",
      "evaluation/num steps total                              1.325e+06\n",
      "evaluation/num paths total                           1325\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.886419\n",
      "evaluation/Rewards Std                                  0.0414877\n",
      "evaluation/Rewards Max                                  1.86563\n",
      "evaluation/Rewards Min                                  0.432632\n",
      "evaluation/Returns Mean                               886.419\n",
      "evaluation/Returns Std                                 33.7367\n",
      "evaluation/Returns Max                                927.072\n",
      "evaluation/Returns Min                                827.224\n",
      "evaluation/Actions Mean                                 0.00980454\n",
      "evaluation/Actions Std                                  0.168967\n",
      "evaluation/Actions Max                                  0.692352\n",
      "evaluation/Actions Min                                 -0.577106\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            886.419\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00383385\n",
      "evaluation/env_infos/final/reward_forward Std           0.0156853\n",
      "evaluation/env_infos/final/reward_forward Max           0.0789459\n",
      "evaluation/env_infos/final/reward_forward Min          -6.87901e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00865359\n",
      "evaluation/env_infos/initial/reward_forward Std         0.145303\n",
      "evaluation/env_infos/initial/reward_forward Max         0.324079\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.24913\n",
      "evaluation/env_infos/reward_forward Mean               -0.0119211\n",
      "evaluation/env_infos/reward_forward Std                 0.0956374\n",
      "evaluation/env_infos/reward_forward Max                 1.70325\n",
      "evaluation/env_infos/reward_forward Min                -1.24121\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.114139\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0343696\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0740512\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.174273\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.037491\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0238962\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0157536\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.119453\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.114584\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0350885\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0124341\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.567368\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0134567\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0898409\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0789459\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.618768\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.130602\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.244608\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.647559\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.269321\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00801816\n",
      "evaluation/env_infos/torso_velocity Std                 0.0909961\n",
      "evaluation/env_infos/torso_velocity Max                 1.70325\n",
      "evaluation/env_infos/torso_velocity Min                -1.77236\n",
      "time/data storing (s)                                   0.316691\n",
      "time/evaluation sampling (s)                           44.1365\n",
      "time/exploration sampling (s)                           1.84719\n",
      "time/logging (s)                                        0.27585\n",
      "time/saving (s)                                         0.0260517\n",
      "time/training (s)                                       3.9273\n",
      "time/epoch (s)                                         50.5296\n",
      "time/total (s)                                       2661.64\n",
      "Epoch                                                  52\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:09:15.773461 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 53 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 109898\n",
      "trainer/QF1 Loss                                        0.488074\n",
      "trainer/QF2 Loss                                        0.445452\n",
      "trainer/Policy Loss                                   -10.698\n",
      "trainer/Q1 Predictions Mean                            18.2698\n",
      "trainer/Q1 Predictions Std                              1.75184\n",
      "trainer/Q1 Predictions Max                             21.6781\n",
      "trainer/Q1 Predictions Min                              9.75535\n",
      "trainer/Q2 Predictions Mean                            18.3677\n",
      "trainer/Q2 Predictions Std                              1.69841\n",
      "trainer/Q2 Predictions Max                             21.4181\n",
      "trainer/Q2 Predictions Min                             10.6798\n",
      "trainer/Q Targets Mean                                 18.2284\n",
      "trainer/Q Targets Std                                   1.66864\n",
      "trainer/Q Targets Max                                  21.9944\n",
      "trainer/Q Targets Min                                  10.1489\n",
      "trainer/Log Pis Mean                                    7.83875\n",
      "trainer/Log Pis Std                                     2.38122\n",
      "trainer/Log Pis Max                                    15.4211\n",
      "trainer/Log Pis Min                                     1.97376\n",
      "trainer/Policy mu Mean                                  0.0586726\n",
      "trainer/Policy mu Std                                   0.201416\n",
      "trainer/Policy mu Max                                   2.24253\n",
      "trainer/Policy mu Min                                  -1.25285\n",
      "trainer/Policy log std Mean                            -2.35899\n",
      "trainer/Policy log std Std                              0.272193\n",
      "trainer/Policy log std Max                             -0.425244\n",
      "trainer/Policy log std Min                             -3.53649\n",
      "trainer/Alpha                                           0.00894002\n",
      "trainer/Alpha Loss                                     -0.760625\n",
      "exploration/num steps total                         55000\n",
      "exploration/num paths total                           149\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.938781\n",
      "exploration/Rewards Std                                 0.11721\n",
      "exploration/Rewards Max                                 2.21082\n",
      "exploration/Rewards Min                                 0.651327\n",
      "exploration/Returns Mean                              938.781\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               938.781\n",
      "exploration/Returns Min                               938.781\n",
      "exploration/Actions Mean                                0.0606898\n",
      "exploration/Actions Std                                 0.137022\n",
      "exploration/Actions Max                                 0.680534\n",
      "exploration/Actions Min                                -0.414445\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           938.781\n",
      "exploration/env_infos/final/reward_forward Mean         0.100638\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.100638\n",
      "exploration/env_infos/final/reward_forward Min          0.100638\n",
      "exploration/env_infos/initial/reward_forward Mean       0.135752\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.135752\n",
      "exploration/env_infos/initial/reward_forward Min        0.135752\n",
      "exploration/env_infos/reward_forward Mean               0.00666181\n",
      "exploration/env_infos/reward_forward Std                0.159622\n",
      "exploration/env_infos/reward_forward Max                1.01994\n",
      "exploration/env_infos/reward_forward Min               -0.810577\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0445074\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0445074\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0445074\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0404638\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0404638\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0404638\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.089833\n",
      "exploration/env_infos/reward_ctrl Std                   0.0496822\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0111644\n",
      "exploration/env_infos/reward_ctrl Min                  -0.428696\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0124152\n",
      "exploration/env_infos/final/torso_velocity Std          0.119691\n",
      "exploration/env_infos/final/torso_velocity Max          0.100638\n",
      "exploration/env_infos/final/torso_velocity Min         -0.178044\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.210726\n",
      "exploration/env_infos/initial/torso_velocity Std        0.138821\n",
      "exploration/env_infos/initial/torso_velocity Max        0.405348\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0910787\n",
      "exploration/env_infos/torso_velocity Mean              -0.00582678\n",
      "exploration/env_infos/torso_velocity Std                0.177016\n",
      "exploration/env_infos/torso_velocity Max                1.1582\n",
      "exploration/env_infos/torso_velocity Min               -1.32973\n",
      "evaluation/num steps total                              1.35e+06\n",
      "evaluation/num paths total                           1350\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.862358\n",
      "evaluation/Rewards Std                                  0.0590886\n",
      "evaluation/Rewards Max                                  2.0424\n",
      "evaluation/Rewards Min                                  0.506752\n",
      "evaluation/Returns Mean                               862.358\n",
      "evaluation/Returns Std                                 54.3886\n",
      "evaluation/Returns Max                                961.99\n",
      "evaluation/Returns Min                                699.586\n",
      "evaluation/Actions Mean                                 0.103973\n",
      "evaluation/Actions Std                                  0.154138\n",
      "evaluation/Actions Max                                  0.723232\n",
      "evaluation/Actions Min                                 -0.554434\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            862.358\n",
      "evaluation/env_infos/final/reward_forward Mean          1.31129e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.52318e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.40397e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -8.29625e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0264946\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122092\n",
      "evaluation/env_infos/initial/reward_forward Max         0.18179\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.29358\n",
      "evaluation/env_infos/reward_forward Mean               -0.00407368\n",
      "evaluation/env_infos/reward_forward Std                 0.0535659\n",
      "evaluation/env_infos/reward_forward Max                 0.569525\n",
      "evaluation/env_infos/reward_forward Min                -1.52096\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.137955\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0549158\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0366767\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.301741\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0623684\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0417774\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0207358\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.171858\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.138276\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0554959\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0207358\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.493248\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          7.11442e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.88598e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.40397e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.29625e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.143008\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.246169\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.656326\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.29358\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00273555\n",
      "evaluation/env_infos/torso_velocity Std                 0.0530842\n",
      "evaluation/env_infos/torso_velocity Max                 1.35552\n",
      "evaluation/env_infos/torso_velocity Min                -2.01114\n",
      "time/data storing (s)                                   0.316024\n",
      "time/evaluation sampling (s)                           41.8701\n",
      "time/exploration sampling (s)                           1.88262\n",
      "time/logging (s)                                        0.270929\n",
      "time/saving (s)                                         0.0261654\n",
      "time/training (s)                                       3.87577\n",
      "time/epoch (s)                                         48.2416\n",
      "time/total (s)                                       2710.38\n",
      "Epoch                                                  53\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:10:05.151994 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 54 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 111898\n",
      "trainer/QF1 Loss                                        0.566353\n",
      "trainer/QF2 Loss                                        0.714887\n",
      "trainer/Policy Loss                                   -10.5842\n",
      "trainer/Q1 Predictions Mean                            18.2873\n",
      "trainer/Q1 Predictions Std                              2.53833\n",
      "trainer/Q1 Predictions Max                             29.6524\n",
      "trainer/Q1 Predictions Min                              2.50585\n",
      "trainer/Q2 Predictions Mean                            18.358\n",
      "trainer/Q2 Predictions Std                              2.81494\n",
      "trainer/Q2 Predictions Max                             26.4409\n",
      "trainer/Q2 Predictions Min                             -8.39588\n",
      "trainer/Q Targets Mean                                 18.4665\n",
      "trainer/Q Targets Std                                   2.58767\n",
      "trainer/Q Targets Max                                  28.1872\n",
      "trainer/Q Targets Min                                  -0.961324\n",
      "trainer/Log Pis Mean                                    7.9973\n",
      "trainer/Log Pis Std                                     2.76907\n",
      "trainer/Log Pis Max                                    20.8396\n",
      "trainer/Log Pis Min                                    -0.756471\n",
      "trainer/Policy mu Mean                                 -0.0301641\n",
      "trainer/Policy mu Std                                   0.206524\n",
      "trainer/Policy mu Max                                   0.938939\n",
      "trainer/Policy mu Min                                  -1.83484\n",
      "trainer/Policy log std Mean                            -2.35996\n",
      "trainer/Policy log std Std                              0.27891\n",
      "trainer/Policy log std Max                             -0.977479\n",
      "trainer/Policy log std Min                             -4.13244\n",
      "trainer/Alpha                                           0.00925926\n",
      "trainer/Alpha Loss                                     -0.0126272\n",
      "exploration/num steps total                         56000\n",
      "exploration/num paths total                           150\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.941362\n",
      "exploration/Rewards Std                                 0.125888\n",
      "exploration/Rewards Max                                 1.74406\n",
      "exploration/Rewards Min                                 0.461114\n",
      "exploration/Returns Mean                              941.362\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               941.362\n",
      "exploration/Returns Min                               941.362\n",
      "exploration/Actions Mean                               -0.032099\n",
      "exploration/Actions Std                                 0.147476\n",
      "exploration/Actions Max                                 0.605894\n",
      "exploration/Actions Min                                -0.717602\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           941.362\n",
      "exploration/env_infos/final/reward_forward Mean         0.0803266\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0803266\n",
      "exploration/env_infos/final/reward_forward Min          0.0803266\n",
      "exploration/env_infos/initial/reward_forward Mean       0.14934\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.14934\n",
      "exploration/env_infos/initial/reward_forward Min        0.14934\n",
      "exploration/env_infos/reward_forward Mean              -0.0318084\n",
      "exploration/env_infos/reward_forward Std                0.166568\n",
      "exploration/env_infos/reward_forward Max                0.620617\n",
      "exploration/env_infos/reward_forward Min               -0.817969\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.196937\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.196937\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.196937\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0775992\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0775992\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0775992\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.091118\n",
      "exploration/env_infos/reward_ctrl Std                   0.0684253\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0142258\n",
      "exploration/env_infos/reward_ctrl Min                  -0.57218\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0256884\n",
      "exploration/env_infos/final/torso_velocity Std          0.0878629\n",
      "exploration/env_infos/final/torso_velocity Max          0.0803266\n",
      "exploration/env_infos/final/torso_velocity Min         -0.134825\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.236339\n",
      "exploration/env_infos/initial/torso_velocity Std        0.23107\n",
      "exploration/env_infos/initial/torso_velocity Max        0.552626\n",
      "exploration/env_infos/initial/torso_velocity Min        0.00705132\n",
      "exploration/env_infos/torso_velocity Mean              -0.0099234\n",
      "exploration/env_infos/torso_velocity Std                0.217861\n",
      "exploration/env_infos/torso_velocity Max                0.965651\n",
      "exploration/env_infos/torso_velocity Min               -1.26526\n",
      "evaluation/num steps total                              1.375e+06\n",
      "evaluation/num paths total                           1375\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.926355\n",
      "evaluation/Rewards Std                                  0.0604096\n",
      "evaluation/Rewards Max                                  2.44577\n",
      "evaluation/Rewards Min                                  0.162773\n",
      "evaluation/Returns Mean                               926.355\n",
      "evaluation/Returns Std                                 32.3996\n",
      "evaluation/Returns Max                                996.958\n",
      "evaluation/Returns Min                                875.342\n",
      "evaluation/Actions Mean                                -0.0074082\n",
      "evaluation/Actions Std                                  0.139907\n",
      "evaluation/Actions Max                                  0.830535\n",
      "evaluation/Actions Min                                 -0.663857\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            926.355\n",
      "evaluation/env_infos/final/reward_forward Mean         -8.92624e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000438126\n",
      "evaluation/env_infos/final/reward_forward Max           9.9248e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.00223563\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0184488\n",
      "evaluation/env_infos/initial/reward_forward Std         0.116843\n",
      "evaluation/env_infos/initial/reward_forward Max         0.278047\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.196303\n",
      "evaluation/env_infos/reward_forward Mean               -0.00391927\n",
      "evaluation/env_infos/reward_forward Std                 0.109318\n",
      "evaluation/env_infos/reward_forward Max                 1.73252\n",
      "evaluation/env_infos/reward_forward Min                -2.02791\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0775501\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0313069\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0175766\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.125038\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0361481\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0155598\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0120626\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0813539\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0785151\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0357333\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0120626\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.837227\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00279978\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0212221\n",
      "evaluation/env_infos/final/torso_velocity Max           1.09863e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.183817\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.135617\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.240098\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.694377\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.325911\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00381333\n",
      "evaluation/env_infos/torso_velocity Std                 0.0937752\n",
      "evaluation/env_infos/torso_velocity Max                 1.73252\n",
      "evaluation/env_infos/torso_velocity Min                -2.02791\n",
      "time/data storing (s)                                   0.317298\n",
      "time/evaluation sampling (s)                           42.0815\n",
      "time/exploration sampling (s)                           1.86703\n",
      "time/logging (s)                                        0.284623\n",
      "time/saving (s)                                         0.026931\n",
      "time/training (s)                                       4.32117\n",
      "time/epoch (s)                                         48.8985\n",
      "time/total (s)                                       2759.77\n",
      "Epoch                                                  54\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:10:55.460906 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 55 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 113898\n",
      "trainer/QF1 Loss                                        0.387867\n",
      "trainer/QF2 Loss                                        0.366825\n",
      "trainer/Policy Loss                                   -10.6148\n",
      "trainer/Q1 Predictions Mean                            18.6807\n",
      "trainer/Q1 Predictions Std                              1.78387\n",
      "trainer/Q1 Predictions Max                             21.2889\n",
      "trainer/Q1 Predictions Min                              9.20868\n",
      "trainer/Q2 Predictions Mean                            18.7793\n",
      "trainer/Q2 Predictions Std                              1.71714\n",
      "trainer/Q2 Predictions Max                             21.7198\n",
      "trainer/Q2 Predictions Min                              9.89298\n",
      "trainer/Q Targets Mean                                 18.7604\n",
      "trainer/Q Targets Std                                   1.77721\n",
      "trainer/Q Targets Max                                  22.5434\n",
      "trainer/Q Targets Min                                  10.4516\n",
      "trainer/Log Pis Mean                                    8.40368\n",
      "trainer/Log Pis Std                                     2.57028\n",
      "trainer/Log Pis Max                                    16.1536\n",
      "trainer/Log Pis Min                                     0.199452\n",
      "trainer/Policy mu Mean                                  0.00300989\n",
      "trainer/Policy mu Std                                   0.233675\n",
      "trainer/Policy mu Max                                   1.14135\n",
      "trainer/Policy mu Min                                  -1.32185\n",
      "trainer/Policy log std Mean                            -2.39817\n",
      "trainer/Policy log std Std                              0.273266\n",
      "trainer/Policy log std Max                             -1.29797\n",
      "trainer/Policy log std Min                             -3.61965\n",
      "trainer/Alpha                                           0.0093265\n",
      "trainer/Alpha Loss                                      1.88742\n",
      "exploration/num steps total                         57000\n",
      "exploration/num paths total                           151\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.942448\n",
      "exploration/Rewards Std                                 0.119792\n",
      "exploration/Rewards Max                                 1.98317\n",
      "exploration/Rewards Min                                 0.626902\n",
      "exploration/Returns Mean                              942.448\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               942.448\n",
      "exploration/Returns Min                               942.448\n",
      "exploration/Actions Mean                               -0.0221768\n",
      "exploration/Actions Std                                 0.14627\n",
      "exploration/Actions Max                                 0.63549\n",
      "exploration/Actions Min                                -0.60626\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           942.448\n",
      "exploration/env_infos/final/reward_forward Mean        -0.339861\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.339861\n",
      "exploration/env_infos/final/reward_forward Min         -0.339861\n",
      "exploration/env_infos/initial/reward_forward Mean       0.164834\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.164834\n",
      "exploration/env_infos/initial/reward_forward Min        0.164834\n",
      "exploration/env_infos/reward_forward Mean              -0.00331759\n",
      "exploration/env_infos/reward_forward Std                0.233597\n",
      "exploration/env_infos/reward_forward Max                0.852071\n",
      "exploration/env_infos/reward_forward Min               -1.21992\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0503152\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0503152\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0503152\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0166428\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0166428\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0166428\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.087547\n",
      "exploration/env_infos/reward_ctrl Std                   0.044661\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0135129\n",
      "exploration/env_infos/reward_ctrl Min                  -0.373098\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.214484\n",
      "exploration/env_infos/final/torso_velocity Std          0.106236\n",
      "exploration/env_infos/final/torso_velocity Max         -0.0801038\n",
      "exploration/env_infos/final/torso_velocity Min         -0.339861\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.230666\n",
      "exploration/env_infos/initial/torso_velocity Std        0.18357\n",
      "exploration/env_infos/initial/torso_velocity Max        0.481059\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0461035\n",
      "exploration/env_infos/torso_velocity Mean              -0.00527612\n",
      "exploration/env_infos/torso_velocity Std                0.234305\n",
      "exploration/env_infos/torso_velocity Max                1.08125\n",
      "exploration/env_infos/torso_velocity Min               -1.39333\n",
      "evaluation/num steps total                              1.4e+06\n",
      "evaluation/num paths total                           1400\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.891951\n",
      "evaluation/Rewards Std                                  0.0770582\n",
      "evaluation/Rewards Max                                  2.3258\n",
      "evaluation/Rewards Min                                  0.478188\n",
      "evaluation/Returns Mean                               891.951\n",
      "evaluation/Returns Std                                 69.0862\n",
      "evaluation/Returns Max                                961.891\n",
      "evaluation/Returns Min                                761.009\n",
      "evaluation/Actions Mean                                -0.00574122\n",
      "evaluation/Actions Std                                  0.165971\n",
      "evaluation/Actions Max                                  0.695676\n",
      "evaluation/Actions Min                                 -0.734949\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            891.951\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00312661\n",
      "evaluation/env_infos/final/reward_forward Std           0.0167885\n",
      "evaluation/env_infos/final/reward_forward Max           0.0851035\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0069372\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0270407\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127398\n",
      "evaluation/env_infos/initial/reward_forward Max         0.338061\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.205078\n",
      "evaluation/env_infos/reward_forward Mean               -0.000724707\n",
      "evaluation/env_infos/reward_forward Std                 0.0832069\n",
      "evaluation/env_infos/reward_forward Max                 1.48076\n",
      "evaluation/env_infos/reward_forward Min                -1.53584\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.109438\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0687435\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0401995\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.239591\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0763191\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0287566\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0359753\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.124954\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.110318\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0699914\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0179063\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.521812\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00181683\n",
      "evaluation/env_infos/final/torso_velocity Std           0.015853\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0949135\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0476716\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146008\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.261297\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.643905\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.350508\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00294587\n",
      "evaluation/env_infos/torso_velocity Std                 0.0769121\n",
      "evaluation/env_infos/torso_velocity Max                 1.48076\n",
      "evaluation/env_infos/torso_velocity Min                -1.96973\n",
      "time/data storing (s)                                   0.318742\n",
      "time/evaluation sampling (s)                           43.3067\n",
      "time/exploration sampling (s)                           1.86525\n",
      "time/logging (s)                                        0.276428\n",
      "time/saving (s)                                         0.0260317\n",
      "time/training (s)                                       3.91351\n",
      "time/epoch (s)                                         49.7067\n",
      "time/total (s)                                       2810.07\n",
      "Epoch                                                  55\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:11:44.975084 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 56 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 115898\n",
      "trainer/QF1 Loss                                        0.442279\n",
      "trainer/QF2 Loss                                        0.341187\n",
      "trainer/Policy Loss                                   -11.4446\n",
      "trainer/Q1 Predictions Mean                            19.2137\n",
      "trainer/Q1 Predictions Std                              2.14826\n",
      "trainer/Q1 Predictions Max                             22.6558\n",
      "trainer/Q1 Predictions Min                              1.51321\n",
      "trainer/Q2 Predictions Mean                            19.1115\n",
      "trainer/Q2 Predictions Std                              1.97886\n",
      "trainer/Q2 Predictions Max                             23.102\n",
      "trainer/Q2 Predictions Min                              8.56608\n",
      "trainer/Q Targets Mean                                 19.0711\n",
      "trainer/Q Targets Std                                   2.12185\n",
      "trainer/Q Targets Max                                  23.3764\n",
      "trainer/Q Targets Min                                   4.42248\n",
      "trainer/Log Pis Mean                                    7.91688\n",
      "trainer/Log Pis Std                                     2.84231\n",
      "trainer/Log Pis Max                                    15.8434\n",
      "trainer/Log Pis Min                                    -2.65171\n",
      "trainer/Policy mu Mean                                  0.0030347\n",
      "trainer/Policy mu Std                                   0.158787\n",
      "trainer/Policy mu Max                                   0.807646\n",
      "trainer/Policy mu Min                                  -0.997403\n",
      "trainer/Policy log std Mean                            -2.40815\n",
      "trainer/Policy log std Std                              0.270803\n",
      "trainer/Policy log std Max                             -1.25295\n",
      "trainer/Policy log std Min                             -3.61749\n",
      "trainer/Alpha                                           0.00917398\n",
      "trainer/Alpha Loss                                     -0.389795\n",
      "exploration/num steps total                         58000\n",
      "exploration/num paths total                           152\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.911077\n",
      "exploration/Rewards Std                                 0.105804\n",
      "exploration/Rewards Max                                 1.78944\n",
      "exploration/Rewards Min                                 0.70858\n",
      "exploration/Returns Mean                              911.077\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               911.077\n",
      "exploration/Returns Min                               911.077\n",
      "exploration/Actions Mean                               -0.00532825\n",
      "exploration/Actions Std                                 0.168632\n",
      "exploration/Actions Max                                 0.472817\n",
      "exploration/Actions Min                                -0.601754\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           911.077\n",
      "exploration/env_infos/final/reward_forward Mean         0.00199524\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.00199524\n",
      "exploration/env_infos/final/reward_forward Min          0.00199524\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0334785\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0334785\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0334785\n",
      "exploration/env_infos/reward_forward Mean               0.0279997\n",
      "exploration/env_infos/reward_forward Std                0.229203\n",
      "exploration/env_infos/reward_forward Max                1.33044\n",
      "exploration/env_infos/reward_forward Min               -0.752508\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0771796\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0771796\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0771796\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.040399\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.040399\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.040399\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.113861\n",
      "exploration/env_infos/reward_ctrl Std                   0.0471543\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0178828\n",
      "exploration/env_infos/reward_ctrl Min                  -0.29142\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0920924\n",
      "exploration/env_infos/final/torso_velocity Std          0.0971868\n",
      "exploration/env_infos/final/torso_velocity Max          0.227029\n",
      "exploration/env_infos/final/torso_velocity Min          0.00199524\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.106736\n",
      "exploration/env_infos/initial/torso_velocity Std        0.265569\n",
      "exploration/env_infos/initial/torso_velocity Max        0.47858\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.124894\n",
      "exploration/env_infos/torso_velocity Mean              -0.0170529\n",
      "exploration/env_infos/torso_velocity Std                0.224231\n",
      "exploration/env_infos/torso_velocity Max                1.33044\n",
      "exploration/env_infos/torso_velocity Min               -1.21847\n",
      "evaluation/num steps total                              1.425e+06\n",
      "evaluation/num paths total                           1425\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.914982\n",
      "evaluation/Rewards Std                                  0.0435033\n",
      "evaluation/Rewards Max                                  2.3089\n",
      "evaluation/Rewards Min                                  0.551084\n",
      "evaluation/Returns Mean                               914.982\n",
      "evaluation/Returns Std                                 30.1116\n",
      "evaluation/Returns Max                                956.751\n",
      "evaluation/Returns Min                                824.33\n",
      "evaluation/Actions Mean                                 0.00943421\n",
      "evaluation/Actions Std                                  0.146581\n",
      "evaluation/Actions Max                                  0.563876\n",
      "evaluation/Actions Min                                 -0.69066\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            914.982\n",
      "evaluation/env_infos/final/reward_forward Mean          1.62474e-07\n",
      "evaluation/env_infos/final/reward_forward Std           2.75577e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.60465e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -2.703e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0009664\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121128\n",
      "evaluation/env_infos/initial/reward_forward Max         0.177008\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.301294\n",
      "evaluation/env_infos/reward_forward Mean                0.00325621\n",
      "evaluation/env_infos/reward_forward Std                 0.0689802\n",
      "evaluation/env_infos/reward_forward Max                 1.23488\n",
      "evaluation/env_infos/reward_forward Min                -1.70108\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0862166\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0305037\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0489139\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.178985\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0639627\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0363354\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0240258\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.142679\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0862999\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0317182\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0191343\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.448916\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -9.93931e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           3.08098e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.60465e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.10011e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.128976\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.258353\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.651902\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.442376\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00221988\n",
      "evaluation/env_infos/torso_velocity Std                 0.0717108\n",
      "evaluation/env_infos/torso_velocity Max                 1.23488\n",
      "evaluation/env_infos/torso_velocity Min                -1.76869\n",
      "time/data storing (s)                                   0.316864\n",
      "time/evaluation sampling (s)                           42.571\n",
      "time/exploration sampling (s)                           1.86522\n",
      "time/logging (s)                                        0.295283\n",
      "time/saving (s)                                         0.0260962\n",
      "time/training (s)                                       3.91827\n",
      "time/epoch (s)                                         48.9927\n",
      "time/total (s)                                       2859.6\n",
      "Epoch                                                  56\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:12:35.031652 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 57 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 117898\n",
      "trainer/QF1 Loss                                        0.693793\n",
      "trainer/QF2 Loss                                        0.454076\n",
      "trainer/Policy Loss                                   -11.2597\n",
      "trainer/Q1 Predictions Mean                            19.5739\n",
      "trainer/Q1 Predictions Std                              1.7531\n",
      "trainer/Q1 Predictions Max                             23.5526\n",
      "trainer/Q1 Predictions Min                             10.4616\n",
      "trainer/Q2 Predictions Mean                            19.7254\n",
      "trainer/Q2 Predictions Std                              1.71039\n",
      "trainer/Q2 Predictions Max                             23.5736\n",
      "trainer/Q2 Predictions Min                             12.4577\n",
      "trainer/Q Targets Mean                                 19.6615\n",
      "trainer/Q Targets Std                                   1.61039\n",
      "trainer/Q Targets Max                                  23.4253\n",
      "trainer/Q Targets Min                                  12.8821\n",
      "trainer/Log Pis Mean                                    8.61193\n",
      "trainer/Log Pis Std                                     2.37917\n",
      "trainer/Log Pis Max                                    18.0088\n",
      "trainer/Log Pis Min                                     1.61924\n",
      "trainer/Policy mu Mean                                  0.00144623\n",
      "trainer/Policy mu Std                                   0.209624\n",
      "trainer/Policy mu Max                                   1.11007\n",
      "trainer/Policy mu Min                                  -1.68942\n",
      "trainer/Policy log std Mean                            -2.43493\n",
      "trainer/Policy log std Std                              0.251051\n",
      "trainer/Policy log std Max                             -1.71094\n",
      "trainer/Policy log std Min                             -3.62792\n",
      "trainer/Alpha                                           0.00928374\n",
      "trainer/Alpha Loss                                      2.86408\n",
      "exploration/num steps total                         59000\n",
      "exploration/num paths total                           153\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.868611\n",
      "exploration/Rewards Std                                 0.117053\n",
      "exploration/Rewards Max                                 1.73675\n",
      "exploration/Rewards Min                                 0.48707\n",
      "exploration/Returns Mean                              868.611\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               868.611\n",
      "exploration/Returns Min                               868.611\n",
      "exploration/Actions Mean                                0.0280075\n",
      "exploration/Actions Std                                 0.193702\n",
      "exploration/Actions Max                                 0.728611\n",
      "exploration/Actions Min                                -0.729926\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           868.611\n",
      "exploration/env_infos/final/reward_forward Mean        -0.294519\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.294519\n",
      "exploration/env_infos/final/reward_forward Min         -0.294519\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0236011\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0236011\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0236011\n",
      "exploration/env_infos/reward_forward Mean               0.00969175\n",
      "exploration/env_infos/reward_forward Std                0.236078\n",
      "exploration/env_infos/reward_forward Max                1.33634\n",
      "exploration/env_infos/reward_forward Min               -0.844397\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0717925\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0717925\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0717925\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0555151\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0555151\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0555151\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.15322\n",
      "exploration/env_infos/reward_ctrl Std                   0.0625709\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00979762\n",
      "exploration/env_infos/reward_ctrl Min                  -0.51293\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.135057\n",
      "exploration/env_infos/final/torso_velocity Std          0.150213\n",
      "exploration/env_infos/final/torso_velocity Max          0.066226\n",
      "exploration/env_infos/final/torso_velocity Min         -0.294519\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.233129\n",
      "exploration/env_infos/initial/torso_velocity Std        0.274028\n",
      "exploration/env_infos/initial/torso_velocity Max        0.612898\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0236011\n",
      "exploration/env_infos/torso_velocity Mean               0.00734891\n",
      "exploration/env_infos/torso_velocity Std                0.19123\n",
      "exploration/env_infos/torso_velocity Max                1.55653\n",
      "exploration/env_infos/torso_velocity Min               -1.01658\n",
      "evaluation/num steps total                              1.45e+06\n",
      "evaluation/num paths total                           1450\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.912232\n",
      "evaluation/Rewards Std                                  0.0694731\n",
      "evaluation/Rewards Max                                  2.16595\n",
      "evaluation/Rewards Min                                  0.481983\n",
      "evaluation/Returns Mean                               912.232\n",
      "evaluation/Returns Std                                 64.8777\n",
      "evaluation/Returns Max                                979.124\n",
      "evaluation/Returns Min                                757.676\n",
      "evaluation/Actions Mean                                 0.00498324\n",
      "evaluation/Actions Std                                  0.148569\n",
      "evaluation/Actions Max                                  0.635881\n",
      "evaluation/Actions Min                                 -0.718432\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            912.232\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.16035e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.91271e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.813e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.95129e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00636523\n",
      "evaluation/env_infos/initial/reward_forward Std         0.127197\n",
      "evaluation/env_infos/initial/reward_forward Max         0.252517\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.340989\n",
      "evaluation/env_infos/reward_forward Mean                0.0016738\n",
      "evaluation/env_infos/reward_forward Std                 0.0601747\n",
      "evaluation/env_infos/reward_forward Max                 1.29436\n",
      "evaluation/env_infos/reward_forward Min                -1.3561\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0883559\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0666282\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.02213\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.247003\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.112646\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0797669\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0251062\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.312264\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0883907\n",
      "evaluation/env_infos/reward_ctrl Std                    0.067082\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0168331\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.518017\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.40753e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.07037e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.54204e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.95129e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.0908151\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.257181\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.681546\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.352987\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00169795\n",
      "evaluation/env_infos/torso_velocity Std                 0.0637415\n",
      "evaluation/env_infos/torso_velocity Max                 1.29436\n",
      "evaluation/env_infos/torso_velocity Min                -1.9515\n",
      "time/data storing (s)                                   0.320671\n",
      "time/evaluation sampling (s)                           43.0301\n",
      "time/exploration sampling (s)                           1.87342\n",
      "time/logging (s)                                        0.277637\n",
      "time/saving (s)                                         0.0253176\n",
      "time/training (s)                                       3.94366\n",
      "time/epoch (s)                                         49.4708\n",
      "time/total (s)                                       2909.64\n",
      "Epoch                                                  57\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:13:25.041948 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 58 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 119898\n",
      "trainer/QF1 Loss                                        0.269439\n",
      "trainer/QF2 Loss                                        0.289118\n",
      "trainer/Policy Loss                                   -12.4533\n",
      "trainer/Q1 Predictions Mean                            19.9471\n",
      "trainer/Q1 Predictions Std                              1.76157\n",
      "trainer/Q1 Predictions Max                             25.0852\n",
      "trainer/Q1 Predictions Min                             11.1946\n",
      "trainer/Q2 Predictions Mean                            19.8654\n",
      "trainer/Q2 Predictions Std                              1.69163\n",
      "trainer/Q2 Predictions Max                             24.861\n",
      "trainer/Q2 Predictions Min                             11.1293\n",
      "trainer/Q Targets Mean                                 19.7624\n",
      "trainer/Q Targets Std                                   1.84064\n",
      "trainer/Q Targets Max                                  24.3541\n",
      "trainer/Q Targets Min                                  10.2308\n",
      "trainer/Log Pis Mean                                    7.69961\n",
      "trainer/Log Pis Std                                     2.3663\n",
      "trainer/Log Pis Max                                    17.8174\n",
      "trainer/Log Pis Min                                    -0.885635\n",
      "trainer/Policy mu Mean                                 -0.0181046\n",
      "trainer/Policy mu Std                                   0.220983\n",
      "trainer/Policy mu Max                                   1.16874\n",
      "trainer/Policy mu Min                                  -1.75602\n",
      "trainer/Policy log std Mean                            -2.32866\n",
      "trainer/Policy log std Std                              0.240106\n",
      "trainer/Policy log std Max                             -0.355352\n",
      "trainer/Policy log std Min                             -3.68066\n",
      "trainer/Alpha                                           0.00929377\n",
      "trainer/Alpha Loss                                     -1.40534\n",
      "exploration/num steps total                         60000\n",
      "exploration/num paths total                           154\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.896394\n",
      "exploration/Rewards Std                                 0.0478494\n",
      "exploration/Rewards Max                                 1.17854\n",
      "exploration/Rewards Min                                 0.592719\n",
      "exploration/Returns Mean                              896.394\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               896.394\n",
      "exploration/Returns Min                               896.394\n",
      "exploration/Actions Mean                                0.0419925\n",
      "exploration/Actions Std                                 0.159231\n",
      "exploration/Actions Max                                 0.64535\n",
      "exploration/Actions Min                                -0.50338\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           896.394\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0199545\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0199545\n",
      "exploration/env_infos/final/reward_forward Min         -0.0199545\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.13668\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.13668\n",
      "exploration/env_infos/initial/reward_forward Min       -0.13668\n",
      "exploration/env_infos/reward_forward Mean              -0.00367551\n",
      "exploration/env_infos/reward_forward Std                0.0897897\n",
      "exploration/env_infos/reward_forward Max                0.513733\n",
      "exploration/env_infos/reward_forward Min               -0.577906\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0751913\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0751913\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0751913\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.20881\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.20881\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.20881\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.108471\n",
      "exploration/env_infos/reward_ctrl Std                   0.0442061\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0195565\n",
      "exploration/env_infos/reward_ctrl Min                  -0.407281\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0234476\n",
      "exploration/env_infos/final/torso_velocity Std          0.0262413\n",
      "exploration/env_infos/final/torso_velocity Max          0.00680204\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0571904\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.134602\n",
      "exploration/env_infos/initial/torso_velocity Std        0.224541\n",
      "exploration/env_infos/initial/torso_velocity Max        0.413186\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.13668\n",
      "exploration/env_infos/torso_velocity Mean              -0.00375108\n",
      "exploration/env_infos/torso_velocity Std                0.0936703\n",
      "exploration/env_infos/torso_velocity Max                0.513733\n",
      "exploration/env_infos/torso_velocity Min               -1.12208\n",
      "evaluation/num steps total                              1.475e+06\n",
      "evaluation/num paths total                           1475\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.835161\n",
      "evaluation/Rewards Std                                  0.0527417\n",
      "evaluation/Rewards Max                                  2.17364\n",
      "evaluation/Rewards Min                                  0.148084\n",
      "evaluation/Returns Mean                               835.161\n",
      "evaluation/Returns Std                                 47.199\n",
      "evaluation/Returns Max                                918.336\n",
      "evaluation/Returns Min                                765.404\n",
      "evaluation/Actions Mean                                 0.0668533\n",
      "evaluation/Actions Std                                  0.192066\n",
      "evaluation/Actions Max                                  0.751098\n",
      "evaluation/Actions Min                                 -0.649793\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            835.161\n",
      "evaluation/env_infos/final/reward_forward Mean          3.88463e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.08509e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.51779e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.94707e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00732601\n",
      "evaluation/env_infos/initial/reward_forward Std         0.107085\n",
      "evaluation/env_infos/initial/reward_forward Max         0.161753\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.185309\n",
      "evaluation/env_infos/reward_forward Mean               -0.00339546\n",
      "evaluation/env_infos/reward_forward Std                 0.0573462\n",
      "evaluation/env_infos/reward_forward Max                 0.778814\n",
      "evaluation/env_infos/reward_forward Min                -1.35805\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.165332\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0478978\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0806179\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.235858\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0862723\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0389248\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0360113\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.197034\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.165435\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0491813\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0243337\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.851916\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.70652e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           2.60798e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.51779e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.94707e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.154667\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245675\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.647896\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.223966\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00272413\n",
      "evaluation/env_infos/torso_velocity Std                 0.058092\n",
      "evaluation/env_infos/torso_velocity Max                 1.34611\n",
      "evaluation/env_infos/torso_velocity Min                -1.72401\n",
      "time/data storing (s)                                   0.324378\n",
      "time/evaluation sampling (s)                           42.9322\n",
      "time/exploration sampling (s)                           2.00083\n",
      "time/logging (s)                                        0.268156\n",
      "time/saving (s)                                         0.02608\n",
      "time/training (s)                                       3.91561\n",
      "time/epoch (s)                                         49.4673\n",
      "time/total (s)                                       2959.64\n",
      "Epoch                                                  58\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:14:14.635701 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 59 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 121898\n",
      "trainer/QF1 Loss                                        0.395836\n",
      "trainer/QF2 Loss                                        0.367184\n",
      "trainer/Policy Loss                                   -12.9396\n",
      "trainer/Q1 Predictions Mean                            19.6804\n",
      "trainer/Q1 Predictions Std                              1.74111\n",
      "trainer/Q1 Predictions Max                             22.1758\n",
      "trainer/Q1 Predictions Min                             13.0993\n",
      "trainer/Q2 Predictions Mean                            19.7634\n",
      "trainer/Q2 Predictions Std                              1.77145\n",
      "trainer/Q2 Predictions Max                             21.9855\n",
      "trainer/Q2 Predictions Min                             13.2335\n",
      "trainer/Q Targets Mean                                 19.9857\n",
      "trainer/Q Targets Std                                   1.72438\n",
      "trainer/Q Targets Max                                  23.8614\n",
      "trainer/Q Targets Min                                  13.4707\n",
      "trainer/Log Pis Mean                                    6.93746\n",
      "trainer/Log Pis Std                                     2.03775\n",
      "trainer/Log Pis Max                                    14.4387\n",
      "trainer/Log Pis Min                                     0.14142\n",
      "trainer/Policy mu Mean                                 -0.0570959\n",
      "trainer/Policy mu Std                                   0.190601\n",
      "trainer/Policy mu Max                                   1.05367\n",
      "trainer/Policy mu Min                                  -1.95638\n",
      "trainer/Policy log std Mean                            -2.23759\n",
      "trainer/Policy log std Std                              0.20498\n",
      "trainer/Policy log std Max                             -1.05408\n",
      "trainer/Policy log std Min                             -2.94892\n",
      "trainer/Alpha                                           0.00909116\n",
      "trainer/Alpha Loss                                     -4.99204\n",
      "exploration/num steps total                         61000\n",
      "exploration/num paths total                           155\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.850194\n",
      "exploration/Rewards Std                                 0.150798\n",
      "exploration/Rewards Max                                 1.72805\n",
      "exploration/Rewards Min                                 0.432676\n",
      "exploration/Returns Mean                              850.194\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               850.194\n",
      "exploration/Returns Min                               850.194\n",
      "exploration/Actions Mean                               -0.0751049\n",
      "exploration/Actions Std                                 0.201205\n",
      "exploration/Actions Max                                 0.646125\n",
      "exploration/Actions Min                                -0.694065\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           850.194\n",
      "exploration/env_infos/final/reward_forward Mean        -0.244305\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.244305\n",
      "exploration/env_infos/final/reward_forward Min         -0.244305\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0130218\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0130218\n",
      "exploration/env_infos/initial/reward_forward Min        0.0130218\n",
      "exploration/env_infos/reward_forward Mean               0.018694\n",
      "exploration/env_infos/reward_forward Std                0.430885\n",
      "exploration/env_infos/reward_forward Max                1.23919\n",
      "exploration/env_infos/reward_forward Min               -1.08914\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.111088\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.111088\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.111088\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.123749\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.123749\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.123749\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.184496\n",
      "exploration/env_infos/reward_ctrl Std                   0.0852622\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0251314\n",
      "exploration/env_infos/reward_ctrl Min                  -0.567324\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0674037\n",
      "exploration/env_infos/final/torso_velocity Std          0.137299\n",
      "exploration/env_infos/final/torso_velocity Max          0.0903721\n",
      "exploration/env_infos/final/torso_velocity Min         -0.244305\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.209872\n",
      "exploration/env_infos/initial/torso_velocity Std        0.232778\n",
      "exploration/env_infos/initial/torso_velocity Max        0.536805\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0130218\n",
      "exploration/env_infos/torso_velocity Mean              -0.0192717\n",
      "exploration/env_infos/torso_velocity Std                0.305479\n",
      "exploration/env_infos/torso_velocity Max                1.23919\n",
      "exploration/env_infos/torso_velocity Min               -2.04936\n",
      "evaluation/num steps total                              1.5e+06\n",
      "evaluation/num paths total                           1500\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.905633\n",
      "evaluation/Rewards Std                                  0.10752\n",
      "evaluation/Rewards Max                                  2.10731\n",
      "evaluation/Rewards Min                                  0.329606\n",
      "evaluation/Returns Mean                               905.633\n",
      "evaluation/Returns Std                                 36.9788\n",
      "evaluation/Returns Max                                975.515\n",
      "evaluation/Returns Min                                818.736\n",
      "evaluation/Actions Mean                                -0.0613438\n",
      "evaluation/Actions Std                                  0.151026\n",
      "evaluation/Actions Max                                  0.683146\n",
      "evaluation/Actions Min                                 -0.675402\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            905.633\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0277635\n",
      "evaluation/env_infos/final/reward_forward Std           0.320305\n",
      "evaluation/env_infos/final/reward_forward Max           0.700472\n",
      "evaluation/env_infos/final/reward_forward Min          -0.86486\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0414447\n",
      "evaluation/env_infos/initial/reward_forward Std         0.114585\n",
      "evaluation/env_infos/initial/reward_forward Max         0.284494\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.157202\n",
      "evaluation/env_infos/reward_forward Mean               -0.015464\n",
      "evaluation/env_infos/reward_forward Std                 0.303803\n",
      "evaluation/env_infos/reward_forward Max                 1.16423\n",
      "evaluation/env_infos/reward_forward Min                -1.81728\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.110202\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0539604\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0362877\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.244381\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0515853\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0299414\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0168191\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.121764\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.106287\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0574271\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0156553\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.670394\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0449742\n",
      "evaluation/env_infos/final/torso_velocity Std           0.256587\n",
      "evaluation/env_infos/final/torso_velocity Max           0.700472\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.86486\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.141573\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231144\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.631015\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.265247\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0249678\n",
      "evaluation/env_infos/torso_velocity Std                 0.272831\n",
      "evaluation/env_infos/torso_velocity Max                 1.29647\n",
      "evaluation/env_infos/torso_velocity Min                -1.91578\n",
      "time/data storing (s)                                   0.316665\n",
      "time/evaluation sampling (s)                           42.5992\n",
      "time/exploration sampling (s)                           1.84645\n",
      "time/logging (s)                                        0.280245\n",
      "time/saving (s)                                         0.0259579\n",
      "time/training (s)                                       4.01146\n",
      "time/epoch (s)                                         49.08\n",
      "time/total (s)                                       3009.24\n",
      "Epoch                                                  59\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:15:04.533626 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 60 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 123898\n",
      "trainer/QF1 Loss                                        0.346627\n",
      "trainer/QF2 Loss                                        0.307146\n",
      "trainer/Policy Loss                                   -12.1676\n",
      "trainer/Q1 Predictions Mean                            20.3375\n",
      "trainer/Q1 Predictions Std                              1.66399\n",
      "trainer/Q1 Predictions Max                             24.7845\n",
      "trainer/Q1 Predictions Min                             10.9148\n",
      "trainer/Q2 Predictions Mean                            20.2457\n",
      "trainer/Q2 Predictions Std                              1.60472\n",
      "trainer/Q2 Predictions Max                             24.9444\n",
      "trainer/Q2 Predictions Min                             12.1498\n",
      "trainer/Q Targets Mean                                 20.4487\n",
      "trainer/Q Targets Std                                   1.65493\n",
      "trainer/Q Targets Max                                  24.9994\n",
      "trainer/Q Targets Min                                  10.6674\n",
      "trainer/Log Pis Mean                                    8.29453\n",
      "trainer/Log Pis Std                                     2.28097\n",
      "trainer/Log Pis Max                                    15.1192\n",
      "trainer/Log Pis Min                                     0.644508\n",
      "trainer/Policy mu Mean                                 -0.00230013\n",
      "trainer/Policy mu Std                                   0.203949\n",
      "trainer/Policy mu Max                                   1.21782\n",
      "trainer/Policy mu Min                                  -0.856374\n",
      "trainer/Policy log std Mean                            -2.40584\n",
      "trainer/Policy log std Std                              0.231614\n",
      "trainer/Policy log std Max                             -1.41592\n",
      "trainer/Policy log std Min                             -3.43075\n",
      "trainer/Alpha                                           0.00907036\n",
      "trainer/Alpha Loss                                      1.38528\n",
      "exploration/num steps total                         62000\n",
      "exploration/num paths total                           156\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.844528\n",
      "exploration/Rewards Std                                 0.0610268\n",
      "exploration/Rewards Max                                 1.28142\n",
      "exploration/Rewards Min                                 0.628394\n",
      "exploration/Returns Mean                              844.528\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               844.528\n",
      "exploration/Returns Min                               844.528\n",
      "exploration/Actions Mean                                0.0202452\n",
      "exploration/Actions Std                                 0.19672\n",
      "exploration/Actions Max                                 0.686353\n",
      "exploration/Actions Min                                -0.504275\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           844.528\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0507192\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0507192\n",
      "exploration/env_infos/final/reward_forward Min         -0.0507192\n",
      "exploration/env_infos/initial/reward_forward Mean       0.133545\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.133545\n",
      "exploration/env_infos/initial/reward_forward Min        0.133545\n",
      "exploration/env_infos/reward_forward Mean               0.00901487\n",
      "exploration/env_infos/reward_forward Std                0.216323\n",
      "exploration/env_infos/reward_forward Max                1.00088\n",
      "exploration/env_infos/reward_forward Min               -0.869974\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.160553\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.160553\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.160553\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.175884\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.175884\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.175884\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.156434\n",
      "exploration/env_infos/reward_ctrl Std                   0.0590705\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0399354\n",
      "exploration/env_infos/reward_ctrl Min                  -0.371606\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0463479\n",
      "exploration/env_infos/final/torso_velocity Std          0.177644\n",
      "exploration/env_infos/final/torso_velocity Max          0.295555\n",
      "exploration/env_infos/final/torso_velocity Min         -0.105792\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.158008\n",
      "exploration/env_infos/initial/torso_velocity Std        0.280676\n",
      "exploration/env_infos/initial/torso_velocity Max        0.513343\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.172863\n",
      "exploration/env_infos/torso_velocity Mean               0.000163716\n",
      "exploration/env_infos/torso_velocity Std                0.151808\n",
      "exploration/env_infos/torso_velocity Max                1.00088\n",
      "exploration/env_infos/torso_velocity Min               -1.5712\n",
      "evaluation/num steps total                              1.525e+06\n",
      "evaluation/num paths total                           1525\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.852911\n",
      "evaluation/Rewards Std                                  0.0529022\n",
      "evaluation/Rewards Max                                  2.10922\n",
      "evaluation/Rewards Min                                  0.587039\n",
      "evaluation/Returns Mean                               852.911\n",
      "evaluation/Returns Std                                 47.1015\n",
      "evaluation/Returns Max                                932.058\n",
      "evaluation/Returns Min                                788.319\n",
      "evaluation/Actions Mean                                 0.00982704\n",
      "evaluation/Actions Std                                  0.193073\n",
      "evaluation/Actions Max                                  0.590694\n",
      "evaluation/Actions Min                                 -0.718346\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            852.911\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0054543\n",
      "evaluation/env_infos/final/reward_forward Std           0.0267198\n",
      "evaluation/env_infos/final/reward_forward Max           5.59553e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.136354\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00533551\n",
      "evaluation/env_infos/initial/reward_forward Std         0.122902\n",
      "evaluation/env_infos/initial/reward_forward Max         0.361583\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.180959\n",
      "evaluation/env_infos/reward_forward Mean                0.00275622\n",
      "evaluation/env_infos/reward_forward Std                 0.0631867\n",
      "evaluation/env_infos/reward_forward Max                 1.35374\n",
      "evaluation/env_infos/reward_forward Min                -1.08351\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.148326\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0477143\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0648241\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.213468\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0997159\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0275297\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0604687\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.156506\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.149495\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0483548\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0356703\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.720938\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.000188508\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0201589\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0920481\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.136354\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.152499\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.253171\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.691384\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.300559\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000440743\n",
      "evaluation/env_infos/torso_velocity Std                 0.0610248\n",
      "evaluation/env_infos/torso_velocity Max                 1.74757\n",
      "evaluation/env_infos/torso_velocity Min                -2.18828\n",
      "time/data storing (s)                                   0.319105\n",
      "time/evaluation sampling (s)                           42.9323\n",
      "time/exploration sampling (s)                           1.89253\n",
      "time/logging (s)                                        0.28024\n",
      "time/saving (s)                                         0.0264138\n",
      "time/training (s)                                       3.91876\n",
      "time/epoch (s)                                         49.3693\n",
      "time/total (s)                                       3059.14\n",
      "Epoch                                                  60\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:15:53.964773 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 61 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 125898\n",
      "trainer/QF1 Loss                                        0.273831\n",
      "trainer/QF2 Loss                                        0.338836\n",
      "trainer/Policy Loss                                   -13.6513\n",
      "trainer/Q1 Predictions Mean                            20.7896\n",
      "trainer/Q1 Predictions Std                              1.95231\n",
      "trainer/Q1 Predictions Max                             34.8057\n",
      "trainer/Q1 Predictions Min                             10.0459\n",
      "trainer/Q2 Predictions Mean                            20.7184\n",
      "trainer/Q2 Predictions Std                              1.95845\n",
      "trainer/Q2 Predictions Max                             36.0086\n",
      "trainer/Q2 Predictions Min                             10.8564\n",
      "trainer/Q Targets Mean                                 20.7609\n",
      "trainer/Q Targets Std                                   2.16794\n",
      "trainer/Q Targets Max                                  36.7397\n",
      "trainer/Q Targets Min                                   7.40072\n",
      "trainer/Log Pis Mean                                    7.25886\n",
      "trainer/Log Pis Std                                     2.5824\n",
      "trainer/Log Pis Max                                    17\n",
      "trainer/Log Pis Min                                    -0.454144\n",
      "trainer/Policy mu Mean                                 -0.0415516\n",
      "trainer/Policy mu Std                                   0.159383\n",
      "trainer/Policy mu Max                                   0.844351\n",
      "trainer/Policy mu Min                                  -1.75457\n",
      "trainer/Policy log std Mean                            -2.29882\n",
      "trainer/Policy log std Std                              0.258242\n",
      "trainer/Policy log std Max                             -0.655866\n",
      "trainer/Policy log std Min                             -3.51011\n",
      "trainer/Alpha                                           0.00883026\n",
      "trainer/Alpha Loss                                     -3.50449\n",
      "exploration/num steps total                         63000\n",
      "exploration/num paths total                           157\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.897935\n",
      "exploration/Rewards Std                                 0.0403031\n",
      "exploration/Rewards Max                                 1.19495\n",
      "exploration/Rewards Min                                 0.701002\n",
      "exploration/Returns Mean                              897.935\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               897.935\n",
      "exploration/Returns Min                               897.935\n",
      "exploration/Actions Mean                               -0.0256872\n",
      "exploration/Actions Std                                 0.158988\n",
      "exploration/Actions Max                                 0.458084\n",
      "exploration/Actions Min                                -0.540137\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           897.935\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0261308\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0261308\n",
      "exploration/env_infos/final/reward_forward Min         -0.0261308\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0312766\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0312766\n",
      "exploration/env_infos/initial/reward_forward Min        0.0312766\n",
      "exploration/env_infos/reward_forward Mean              -0.00853286\n",
      "exploration/env_infos/reward_forward Std                0.128419\n",
      "exploration/env_infos/reward_forward Max                0.283577\n",
      "exploration/env_infos/reward_forward Min               -1.7382\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.127371\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.127371\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.127371\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.024356\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.024356\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.024356\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.103748\n",
      "exploration/env_infos/reward_ctrl Std                   0.0366824\n",
      "exploration/env_infos/reward_ctrl Max                  -0.024356\n",
      "exploration/env_infos/reward_ctrl Min                  -0.298998\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0516495\n",
      "exploration/env_infos/final/torso_velocity Std          0.0650043\n",
      "exploration/env_infos/final/torso_velocity Max          0.132978\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0261308\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.211319\n",
      "exploration/env_infos/initial/torso_velocity Std        0.22604\n",
      "exploration/env_infos/initial/torso_velocity Max        0.530098\n",
      "exploration/env_infos/initial/torso_velocity Min        0.0312766\n",
      "exploration/env_infos/torso_velocity Mean              -0.00501086\n",
      "exploration/env_infos/torso_velocity Std                0.116067\n",
      "exploration/env_infos/torso_velocity Max                0.578739\n",
      "exploration/env_infos/torso_velocity Min               -1.7382\n",
      "evaluation/num steps total                              1.55e+06\n",
      "evaluation/num paths total                           1550\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.943631\n",
      "evaluation/Rewards Std                                  0.0656282\n",
      "evaluation/Rewards Max                                  2.13481\n",
      "evaluation/Rewards Min                                  0.490343\n",
      "evaluation/Returns Mean                               943.631\n",
      "evaluation/Returns Std                                 25.2746\n",
      "evaluation/Returns Max                               1027.04\n",
      "evaluation/Returns Min                                893.504\n",
      "evaluation/Actions Mean                                -0.0104929\n",
      "evaluation/Actions Std                                  0.124224\n",
      "evaluation/Actions Max                                  0.52775\n",
      "evaluation/Actions Min                                 -0.633737\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            943.631\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0173247\n",
      "evaluation/env_infos/final/reward_forward Std           0.0848734\n",
      "evaluation/env_infos/final/reward_forward Max           9.72746e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -0.433118\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0320099\n",
      "evaluation/env_infos/initial/reward_forward Std         0.107587\n",
      "evaluation/env_infos/initial/reward_forward Max         0.227761\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.183907\n",
      "evaluation/env_infos/reward_forward Mean               -0.00126147\n",
      "evaluation/env_infos/reward_forward Std                 0.134964\n",
      "evaluation/env_infos/reward_forward Max                 1.33378\n",
      "evaluation/env_infos/reward_forward Min                -1.85472\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0604466\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.018686\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0239381\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.106788\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0253282\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.00798941\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0126403\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0459052\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0621666\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0237994\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0126403\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.509657\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          0.00133657\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0690316\n",
      "evaluation/env_infos/final/torso_velocity Max           0.384385\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.433118\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.162054\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.231682\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.742412\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.202686\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0026802\n",
      "evaluation/env_infos/torso_velocity Std                 0.111495\n",
      "evaluation/env_infos/torso_velocity Max                 1.69635\n",
      "evaluation/env_infos/torso_velocity Min                -1.93913\n",
      "time/data storing (s)                                   0.332621\n",
      "time/evaluation sampling (s)                           42.2592\n",
      "time/exploration sampling (s)                           2.09425\n",
      "time/logging (s)                                        0.280228\n",
      "time/saving (s)                                         0.0261111\n",
      "time/training (s)                                       3.90449\n",
      "time/epoch (s)                                         48.8969\n",
      "time/total (s)                                       3108.57\n",
      "Epoch                                                  61\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:16:43.036152 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 62 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 127898\n",
      "trainer/QF1 Loss                                        0.519974\n",
      "trainer/QF2 Loss                                        0.606463\n",
      "trainer/Policy Loss                                   -12.4238\n",
      "trainer/Q1 Predictions Mean                            20.8818\n",
      "trainer/Q1 Predictions Std                              2.02197\n",
      "trainer/Q1 Predictions Max                             24.1631\n",
      "trainer/Q1 Predictions Min                              4.16112\n",
      "trainer/Q2 Predictions Mean                            20.8289\n",
      "trainer/Q2 Predictions Std                              1.84955\n",
      "trainer/Q2 Predictions Max                             24.1728\n",
      "trainer/Q2 Predictions Min                              8.80203\n",
      "trainer/Q Targets Mean                                 20.909\n",
      "trainer/Q Targets Std                                   2.21977\n",
      "trainer/Q Targets Max                                  27.0317\n",
      "trainer/Q Targets Min                                   0.87114\n",
      "trainer/Log Pis Mean                                    8.62047\n",
      "trainer/Log Pis Std                                     2.41013\n",
      "trainer/Log Pis Max                                    21.9326\n",
      "trainer/Log Pis Min                                    -1.39897\n",
      "trainer/Policy mu Mean                                 -0.0802194\n",
      "trainer/Policy mu Std                                   0.21751\n",
      "trainer/Policy mu Max                                   1.8456\n",
      "trainer/Policy mu Min                                  -2.11488\n",
      "trainer/Policy log std Mean                            -2.42607\n",
      "trainer/Policy log std Std                              0.235877\n",
      "trainer/Policy log std Max                             -1.42511\n",
      "trainer/Policy log std Min                             -3.86701\n",
      "trainer/Alpha                                           0.00912584\n",
      "trainer/Alpha Loss                                      2.91454\n",
      "exploration/num steps total                         64000\n",
      "exploration/num paths total                           158\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.920655\n",
      "exploration/Rewards Std                                 0.18275\n",
      "exploration/Rewards Max                                 1.96321\n",
      "exploration/Rewards Min                                 0.507889\n",
      "exploration/Returns Mean                              920.655\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               920.655\n",
      "exploration/Returns Min                               920.655\n",
      "exploration/Actions Mean                               -0.0251578\n",
      "exploration/Actions Std                                 0.180301\n",
      "exploration/Actions Max                                 0.62493\n",
      "exploration/Actions Min                                -0.603911\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           920.655\n",
      "exploration/env_infos/final/reward_forward Mean         0.0797102\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.0797102\n",
      "exploration/env_infos/final/reward_forward Min          0.0797102\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.116404\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.116404\n",
      "exploration/env_infos/initial/reward_forward Min       -0.116404\n",
      "exploration/env_infos/reward_forward Mean              -0.0350256\n",
      "exploration/env_infos/reward_forward Std                0.297449\n",
      "exploration/env_infos/reward_forward Max                1.11992\n",
      "exploration/env_infos/reward_forward Min               -0.844723\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.113291\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.113291\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.113291\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0855903\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0855903\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0855903\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.132565\n",
      "exploration/env_infos/reward_ctrl Std                   0.0732711\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0157563\n",
      "exploration/env_infos/reward_ctrl Min                  -0.492111\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0393064\n",
      "exploration/env_infos/final/torso_velocity Std          0.0286297\n",
      "exploration/env_infos/final/torso_velocity Max          0.0797102\n",
      "exploration/env_infos/final/torso_velocity Min          0.0168379\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0560963\n",
      "exploration/env_infos/initial/torso_velocity Std        0.203317\n",
      "exploration/env_infos/initial/torso_velocity Max        0.341568\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.116404\n",
      "exploration/env_infos/torso_velocity Mean              -0.0187407\n",
      "exploration/env_infos/torso_velocity Std                0.332893\n",
      "exploration/env_infos/torso_velocity Max                1.11992\n",
      "exploration/env_infos/torso_velocity Min               -1.3075\n",
      "evaluation/num steps total                              1.575e+06\n",
      "evaluation/num paths total                           1575\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.851084\n",
      "evaluation/Rewards Std                                  0.079881\n",
      "evaluation/Rewards Max                                  2.06925\n",
      "evaluation/Rewards Min                                  0.419864\n",
      "evaluation/Returns Mean                               851.084\n",
      "evaluation/Returns Std                                 54.8541\n",
      "evaluation/Returns Max                                943.592\n",
      "evaluation/Returns Min                                785.241\n",
      "evaluation/Actions Mean                                -0.0723254\n",
      "evaluation/Actions Std                                  0.182241\n",
      "evaluation/Actions Max                                  0.673479\n",
      "evaluation/Actions Min                                 -0.661683\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            851.084\n",
      "evaluation/env_infos/final/reward_forward Mean          2.10991e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.86926e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.0624e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.77132e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.013776\n",
      "evaluation/env_infos/initial/reward_forward Std         0.134478\n",
      "evaluation/env_infos/initial/reward_forward Max         0.36102\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.31132\n",
      "evaluation/env_infos/reward_forward Mean               -0.000994107\n",
      "evaluation/env_infos/reward_forward Std                 0.11968\n",
      "evaluation/env_infos/reward_forward Max                 1.28612\n",
      "evaluation/env_infos/reward_forward Min                -1.26468\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.150836\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.054894\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0566259\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.217922\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0801034\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0171231\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0544683\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.135499\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.153771\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0568165\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0310354\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.580136\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -5.81812e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.59143e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           9.0624e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.77132e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.160428\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239321\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614025\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.31132\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00226065\n",
      "evaluation/env_infos/torso_velocity Std                 0.132259\n",
      "evaluation/env_infos/torso_velocity Max                 1.44682\n",
      "evaluation/env_infos/torso_velocity Min                -2.17004\n",
      "time/data storing (s)                                   0.319465\n",
      "time/evaluation sampling (s)                           42.13\n",
      "time/exploration sampling (s)                           1.868\n",
      "time/logging (s)                                        0.277192\n",
      "time/saving (s)                                         0.0263382\n",
      "time/training (s)                                       3.89185\n",
      "time/epoch (s)                                         48.5129\n",
      "time/total (s)                                       3157.63\n",
      "Epoch                                                  62\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:17:31.642138 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 63 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 129898\n",
      "trainer/QF1 Loss                                        0.211689\n",
      "trainer/QF2 Loss                                        0.225156\n",
      "trainer/Policy Loss                                   -13.4132\n",
      "trainer/Q1 Predictions Mean                            21.1789\n",
      "trainer/Q1 Predictions Std                              1.65883\n",
      "trainer/Q1 Predictions Max                             24.5864\n",
      "trainer/Q1 Predictions Min                             13.3358\n",
      "trainer/Q2 Predictions Mean                            21.2859\n",
      "trainer/Q2 Predictions Std                              1.69679\n",
      "trainer/Q2 Predictions Max                             24.3114\n",
      "trainer/Q2 Predictions Min                             14.3122\n",
      "trainer/Q Targets Mean                                 21.3063\n",
      "trainer/Q Targets Std                                   1.67524\n",
      "trainer/Q Targets Max                                  25.9565\n",
      "trainer/Q Targets Min                                  14.6211\n",
      "trainer/Log Pis Mean                                    7.97653\n",
      "trainer/Log Pis Std                                     2.55914\n",
      "trainer/Log Pis Max                                    17.1473\n",
      "trainer/Log Pis Min                                    -2.87579\n",
      "trainer/Policy mu Mean                                  0.00878629\n",
      "trainer/Policy mu Std                                   0.190866\n",
      "trainer/Policy mu Max                                   0.763263\n",
      "trainer/Policy mu Min                                  -1.53303\n",
      "trainer/Policy log std Mean                            -2.35849\n",
      "trainer/Policy log std Std                              0.238746\n",
      "trainer/Policy log std Max                             -1.77147\n",
      "trainer/Policy log std Min                             -3.41641\n",
      "trainer/Alpha                                           0.00895742\n",
      "trainer/Alpha Loss                                     -0.110672\n",
      "exploration/num steps total                         65000\n",
      "exploration/num paths total                           159\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.728155\n",
      "exploration/Rewards Std                                 0.106765\n",
      "exploration/Rewards Max                                 2.23468\n",
      "exploration/Rewards Min                                 0.533022\n",
      "exploration/Returns Mean                              728.155\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               728.155\n",
      "exploration/Returns Min                               728.155\n",
      "exploration/Actions Mean                                0.099729\n",
      "exploration/Actions Std                                 0.245803\n",
      "exploration/Actions Max                                 0.781648\n",
      "exploration/Actions Min                                -0.58312\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           728.155\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00298404\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00298404\n",
      "exploration/env_infos/final/reward_forward Min         -0.00298404\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.171947\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.171947\n",
      "exploration/env_infos/initial/reward_forward Min       -0.171947\n",
      "exploration/env_infos/reward_forward Mean              -0.0107143\n",
      "exploration/env_infos/reward_forward Std                0.114845\n",
      "exploration/env_infos/reward_forward Max                1.38008\n",
      "exploration/env_infos/reward_forward Min               -0.810463\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.329913\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.329913\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.329913\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0762802\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0762802\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0762802\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.28146\n",
      "exploration/env_infos/reward_ctrl Std                   0.0653398\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0670966\n",
      "exploration/env_infos/reward_ctrl Min                  -0.466978\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.00453053\n",
      "exploration/env_infos/final/torso_velocity Std          0.00737885\n",
      "exploration/env_infos/final/torso_velocity Max          0.0145584\n",
      "exploration/env_infos/final/torso_velocity Min         -0.00298404\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0312962\n",
      "exploration/env_infos/initial/torso_velocity Std        0.291213\n",
      "exploration/env_infos/initial/torso_velocity Max        0.443122\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.177286\n",
      "exploration/env_infos/torso_velocity Mean              -0.00347838\n",
      "exploration/env_infos/torso_velocity Std                0.0884676\n",
      "exploration/env_infos/torso_velocity Max                1.38008\n",
      "exploration/env_infos/torso_velocity Min               -1.21034\n",
      "evaluation/num steps total                              1.6e+06\n",
      "evaluation/num paths total                           1600\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.807944\n",
      "evaluation/Rewards Std                                  0.0818893\n",
      "evaluation/Rewards Max                                  2.55233\n",
      "evaluation/Rewards Min                                  0.481582\n",
      "evaluation/Returns Mean                               807.944\n",
      "evaluation/Returns Std                                 71.6896\n",
      "evaluation/Returns Max                                950.412\n",
      "evaluation/Returns Min                                716.485\n",
      "evaluation/Actions Mean                                 0.0874472\n",
      "evaluation/Actions Std                                  0.201872\n",
      "evaluation/Actions Max                                  0.690337\n",
      "evaluation/Actions Min                                 -0.588901\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            807.944\n",
      "evaluation/env_infos/final/reward_forward Mean         -1.84148e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.48511e-07\n",
      "evaluation/env_infos/final/reward_forward Max           8.34979e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.83572e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0335831\n",
      "evaluation/env_infos/initial/reward_forward Std         0.135048\n",
      "evaluation/env_infos/initial/reward_forward Max         0.300407\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.335638\n",
      "evaluation/env_infos/reward_forward Mean               -0.000874116\n",
      "evaluation/env_infos/reward_forward Std                 0.0703394\n",
      "evaluation/env_infos/reward_forward Max                 0.98339\n",
      "evaluation/env_infos/reward_forward Min                -1.62459\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.194545\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0725331\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0478243\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.287403\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0625624\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0238941\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0346643\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.126492\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.193598\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0733238\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0166875\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.518418\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -6.14788e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.02602e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           8.34979e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.83572e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.161799\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245517\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.614203\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.335638\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00152571\n",
      "evaluation/env_infos/torso_velocity Std                 0.0637691\n",
      "evaluation/env_infos/torso_velocity Max                 1.01812\n",
      "evaluation/env_infos/torso_velocity Min                -1.82235\n",
      "time/data storing (s)                                   0.320769\n",
      "time/evaluation sampling (s)                           41.5728\n",
      "time/exploration sampling (s)                           1.91642\n",
      "time/logging (s)                                        0.281798\n",
      "time/saving (s)                                         0.0272218\n",
      "time/training (s)                                       3.95226\n",
      "time/epoch (s)                                         48.0712\n",
      "time/total (s)                                       3206.24\n",
      "Epoch                                                  63\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:18:22.181153 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 64 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 131898\n",
      "trainer/QF1 Loss                                        0.374806\n",
      "trainer/QF2 Loss                                        0.339905\n",
      "trainer/Policy Loss                                   -14.7842\n",
      "trainer/Q1 Predictions Mean                            21.7494\n",
      "trainer/Q1 Predictions Std                              2.23472\n",
      "trainer/Q1 Predictions Max                             34.3194\n",
      "trainer/Q1 Predictions Min                              7.44764\n",
      "trainer/Q2 Predictions Mean                            21.6316\n",
      "trainer/Q2 Predictions Std                              2.26593\n",
      "trainer/Q2 Predictions Max                             32.084\n",
      "trainer/Q2 Predictions Min                              6.93863\n",
      "trainer/Q Targets Mean                                 21.64\n",
      "trainer/Q Targets Std                                   2.31267\n",
      "trainer/Q Targets Max                                  36.9144\n",
      "trainer/Q Targets Min                                   8.61664\n",
      "trainer/Log Pis Mean                                    7.0405\n",
      "trainer/Log Pis Std                                     2.55194\n",
      "trainer/Log Pis Max                                    16.3949\n",
      "trainer/Log Pis Min                                    -2.17535\n",
      "trainer/Policy mu Mean                                 -0.0582462\n",
      "trainer/Policy mu Std                                   0.19714\n",
      "trainer/Policy mu Max                                   2.20215\n",
      "trainer/Policy mu Min                                  -1.40306\n",
      "trainer/Policy log std Mean                            -2.25323\n",
      "trainer/Policy log std Std                              0.239068\n",
      "trainer/Policy log std Max                              0.352549\n",
      "trainer/Policy log std Min                             -3.55135\n",
      "trainer/Alpha                                           0.00876719\n",
      "trainer/Alpha Loss                                     -4.54476\n",
      "exploration/num steps total                         66000\n",
      "exploration/num paths total                           160\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.887493\n",
      "exploration/Rewards Std                                 0.0530086\n",
      "exploration/Rewards Max                                 1.33083\n",
      "exploration/Rewards Min                                 0.659231\n",
      "exploration/Returns Mean                              887.493\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               887.493\n",
      "exploration/Returns Min                               887.493\n",
      "exploration/Actions Mean                                0.0014734\n",
      "exploration/Actions Std                                 0.170272\n",
      "exploration/Actions Max                                 0.545019\n",
      "exploration/Actions Min                                -0.495354\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           887.493\n",
      "exploration/env_infos/final/reward_forward Mean         0.33188\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.33188\n",
      "exploration/env_infos/final/reward_forward Min          0.33188\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.100067\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.100067\n",
      "exploration/env_infos/initial/reward_forward Min       -0.100067\n",
      "exploration/env_infos/reward_forward Mean              -0.0119342\n",
      "exploration/env_infos/reward_forward Std                0.249518\n",
      "exploration/env_infos/reward_forward Max                0.93179\n",
      "exploration/env_infos/reward_forward Min               -1.25698\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.127185\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.127185\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.127185\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0797297\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0797297\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0797297\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.115978\n",
      "exploration/env_infos/reward_ctrl Std                   0.0427532\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0127799\n",
      "exploration/env_infos/reward_ctrl Min                  -0.340769\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.140802\n",
      "exploration/env_infos/final/torso_velocity Std          0.137567\n",
      "exploration/env_infos/final/torso_velocity Max          0.33188\n",
      "exploration/env_infos/final/torso_velocity Min          0.0135772\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.096535\n",
      "exploration/env_infos/initial/torso_velocity Std        0.165239\n",
      "exploration/env_infos/initial/torso_velocity Max        0.304228\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.100067\n",
      "exploration/env_infos/torso_velocity Mean              -0.00905226\n",
      "exploration/env_infos/torso_velocity Std                0.172396\n",
      "exploration/env_infos/torso_velocity Max                0.93179\n",
      "exploration/env_infos/torso_velocity Min               -1.28016\n",
      "evaluation/num steps total                              1.625e+06\n",
      "evaluation/num paths total                           1625\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.905267\n",
      "evaluation/Rewards Std                                  0.0996621\n",
      "evaluation/Rewards Max                                  2.66306\n",
      "evaluation/Rewards Min                                  0.450364\n",
      "evaluation/Returns Mean                               905.267\n",
      "evaluation/Returns Std                                 43.5913\n",
      "evaluation/Returns Max                                968.97\n",
      "evaluation/Returns Min                                824.484\n",
      "evaluation/Actions Mean                                -0.0623647\n",
      "evaluation/Actions Std                                  0.147388\n",
      "evaluation/Actions Max                                  0.603909\n",
      "evaluation/Actions Min                                 -0.634106\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            905.267\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0177074\n",
      "evaluation/env_infos/final/reward_forward Std           0.0665004\n",
      "evaluation/env_infos/final/reward_forward Max           0.000128972\n",
      "evaluation/env_infos/final/reward_forward Min          -0.322212\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.013091\n",
      "evaluation/env_infos/initial/reward_forward Std         0.150358\n",
      "evaluation/env_infos/initial/reward_forward Max         0.296599\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.390972\n",
      "evaluation/env_infos/reward_forward Mean               -0.0141903\n",
      "evaluation/env_infos/reward_forward Std                 0.189359\n",
      "evaluation/env_infos/reward_forward Max                 1.38325\n",
      "evaluation/env_infos/reward_forward Min                -1.61971\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0991475\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0430729\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0344914\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.182706\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0648247\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0236208\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0160267\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.122129\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.102451\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0485716\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0160267\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.549636\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00948973\n",
      "evaluation/env_infos/final/torso_velocity Std           0.170636\n",
      "evaluation/env_infos/final/torso_velocity Max           0.848328\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.12887\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.148981\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.228038\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.621424\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.390972\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00542121\n",
      "evaluation/env_infos/torso_velocity Std                 0.179644\n",
      "evaluation/env_infos/torso_velocity Max                 1.8374\n",
      "evaluation/env_infos/torso_velocity Min                -1.93595\n",
      "time/data storing (s)                                   0.32877\n",
      "time/evaluation sampling (s)                           43.2535\n",
      "time/exploration sampling (s)                           1.97022\n",
      "time/logging (s)                                        0.283467\n",
      "time/saving (s)                                         0.0259161\n",
      "time/training (s)                                       4.10893\n",
      "time/epoch (s)                                         49.9708\n",
      "time/total (s)                                       3256.78\n",
      "Epoch                                                  64\n",
      "-------------------------------------------------  ----------------\n",
      "2021-05-25 14:19:11.522257 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 65 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 133898\n",
      "trainer/QF1 Loss                                        0.432633\n",
      "trainer/QF2 Loss                                        0.380431\n",
      "trainer/Policy Loss                                   -14.1513\n",
      "trainer/Q1 Predictions Mean                            21.6139\n",
      "trainer/Q1 Predictions Std                              1.87624\n",
      "trainer/Q1 Predictions Max                             25.0927\n",
      "trainer/Q1 Predictions Min                             12.8207\n",
      "trainer/Q2 Predictions Mean                            21.7314\n",
      "trainer/Q2 Predictions Std                              1.96455\n",
      "trainer/Q2 Predictions Max                             25.8479\n",
      "trainer/Q2 Predictions Min                             11.9619\n",
      "trainer/Q Targets Mean                                 21.831\n",
      "trainer/Q Targets Std                                   1.94426\n",
      "trainer/Q Targets Max                                  26.2921\n",
      "trainer/Q Targets Min                                  13.7733\n",
      "trainer/Log Pis Mean                                    7.71471\n",
      "trainer/Log Pis Std                                     2.64225\n",
      "trainer/Log Pis Max                                    18.558\n",
      "trainer/Log Pis Min                                    -4.37216\n",
      "trainer/Policy mu Mean                                 -0.108091\n",
      "trainer/Policy mu Std                                   0.222618\n",
      "trainer/Policy mu Max                                   1.38279\n",
      "trainer/Policy mu Min                                  -1.58384\n",
      "trainer/Policy log std Mean                            -2.33406\n",
      "trainer/Policy log std Std                              0.225152\n",
      "trainer/Policy log std Max                             -1.56613\n",
      "trainer/Policy log std Min                             -3.88682\n",
      "trainer/Alpha                                           0.00854024\n",
      "trainer/Alpha Loss                                     -1.35896\n",
      "exploration/num steps total                         67000\n",
      "exploration/num paths total                           161\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.721634\n",
      "exploration/Rewards Std                                 0.0816328\n",
      "exploration/Rewards Max                                 0.937629\n",
      "exploration/Rewards Min                                 0.406367\n",
      "exploration/Returns Mean                              721.634\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               721.634\n",
      "exploration/Returns Min                               721.634\n",
      "exploration/Actions Mean                               -0.120071\n",
      "exploration/Actions Std                                 0.235343\n",
      "exploration/Actions Max                                 0.722221\n",
      "exploration/Actions Min                                -0.656739\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           721.634\n",
      "exploration/env_infos/final/reward_forward Mean         0.238845\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.238845\n",
      "exploration/env_infos/final/reward_forward Min          0.238845\n",
      "exploration/env_infos/initial/reward_forward Mean       0.216085\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.216085\n",
      "exploration/env_infos/initial/reward_forward Min        0.216085\n",
      "exploration/env_infos/reward_forward Mean               0.010679\n",
      "exploration/env_infos/reward_forward Std                0.121649\n",
      "exploration/env_infos/reward_forward Max                1.25464\n",
      "exploration/env_infos/reward_forward Min               -0.362345\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.240545\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.240545\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.240545\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0747123\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0747123\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0747123\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.279213\n",
      "exploration/env_infos/reward_ctrl Std                   0.0810738\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0636912\n",
      "exploration/env_infos/reward_ctrl Min                  -0.593633\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0483635\n",
      "exploration/env_infos/final/torso_velocity Std          0.205576\n",
      "exploration/env_infos/final/torso_velocity Max          0.238845\n",
      "exploration/env_infos/final/torso_velocity Min         -0.231032\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.139628\n",
      "exploration/env_infos/initial/torso_velocity Std        0.312605\n",
      "exploration/env_infos/initial/torso_velocity Max        0.478491\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.275693\n",
      "exploration/env_infos/torso_velocity Mean              -0.00210793\n",
      "exploration/env_infos/torso_velocity Std                0.11111\n",
      "exploration/env_infos/torso_velocity Max                1.25464\n",
      "exploration/env_infos/torso_velocity Min               -1.33083\n",
      "evaluation/num steps total                              1.65e+06\n",
      "evaluation/num paths total                           1650\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.800204\n",
      "evaluation/Rewards Std                                  0.0874237\n",
      "evaluation/Rewards Max                                  2.45312\n",
      "evaluation/Rewards Min                                  0.351813\n",
      "evaluation/Returns Mean                               800.204\n",
      "evaluation/Returns Std                                 78.1332\n",
      "evaluation/Returns Max                                922.253\n",
      "evaluation/Returns Min                                608.157\n",
      "evaluation/Actions Mean                                -0.117168\n",
      "evaluation/Actions Std                                  0.191217\n",
      "evaluation/Actions Max                                  0.534281\n",
      "evaluation/Actions Min                                 -0.694517\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            800.204\n",
      "evaluation/env_infos/final/reward_forward Mean          1.0439e-07\n",
      "evaluation/env_infos/final/reward_forward Std           4.39111e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.05804e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -9.22174e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0362469\n",
      "evaluation/env_infos/initial/reward_forward Std         0.0994414\n",
      "evaluation/env_infos/initial/reward_forward Max         0.181963\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.149644\n",
      "evaluation/env_infos/reward_forward Mean                0.0024907\n",
      "evaluation/env_infos/reward_forward Std                 0.0806486\n",
      "evaluation/env_infos/reward_forward Max                 1.33484\n",
      "evaluation/env_infos/reward_forward Min                -1.77073\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.200847\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0804878\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0745615\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.396511\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0651871\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0163554\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0339257\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.098139\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.201169\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0808536\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0339257\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.659318\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          8.03842e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.51833e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.05927e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.22174e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.163391\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.239597\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.628384\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.262053\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00202969\n",
      "evaluation/env_infos/torso_velocity Std                 0.0724855\n",
      "evaluation/env_infos/torso_velocity Max                 1.33484\n",
      "evaluation/env_infos/torso_velocity Min                -1.77073\n",
      "time/data storing (s)                                   0.319344\n",
      "time/evaluation sampling (s)                           42.2308\n",
      "time/exploration sampling (s)                           1.94922\n",
      "time/logging (s)                                        0.278414\n",
      "time/saving (s)                                         0.0261828\n",
      "time/training (s)                                       3.93306\n",
      "time/epoch (s)                                         48.737\n",
      "time/total (s)                                       3306.11\n",
      "Epoch                                                  65\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:20:00.838826 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 66 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 135898\n",
      "trainer/QF1 Loss                                        0.508602\n",
      "trainer/QF2 Loss                                        0.590559\n",
      "trainer/Policy Loss                                   -13.0098\n",
      "trainer/Q1 Predictions Mean                            21.5735\n",
      "trainer/Q1 Predictions Std                              2.40501\n",
      "trainer/Q1 Predictions Max                             25.1931\n",
      "trainer/Q1 Predictions Min                              3.96875\n",
      "trainer/Q2 Predictions Mean                            21.8557\n",
      "trainer/Q2 Predictions Std                              2.68264\n",
      "trainer/Q2 Predictions Max                             25.9522\n",
      "trainer/Q2 Predictions Min                             -4.17297\n",
      "trainer/Q Targets Mean                                 21.9373\n",
      "trainer/Q Targets Std                                   2.56623\n",
      "trainer/Q Targets Max                                  26.0051\n",
      "trainer/Q Targets Min                                   0.282\n",
      "trainer/Log Pis Mean                                    8.85\n",
      "trainer/Log Pis Std                                     3.1848\n",
      "trainer/Log Pis Max                                    20.3619\n",
      "trainer/Log Pis Min                                    -0.0980014\n",
      "trainer/Policy mu Mean                                 -0.00553595\n",
      "trainer/Policy mu Std                                   0.19372\n",
      "trainer/Policy mu Max                                   1.84855\n",
      "trainer/Policy mu Min                                  -1.69477\n",
      "trainer/Policy log std Mean                            -2.49375\n",
      "trainer/Policy log std Std                              0.297458\n",
      "trainer/Policy log std Max                             -1.86868\n",
      "trainer/Policy log std Min                             -3.94168\n",
      "trainer/Alpha                                           0.00818676\n",
      "trainer/Alpha Loss                                      4.08613\n",
      "exploration/num steps total                         68000\n",
      "exploration/num paths total                           162\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.890306\n",
      "exploration/Rewards Std                                 0.0576649\n",
      "exploration/Rewards Max                                 1.17708\n",
      "exploration/Rewards Min                                 0.65431\n",
      "exploration/Returns Mean                              890.306\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               890.306\n",
      "exploration/Returns Min                               890.306\n",
      "exploration/Actions Mean                               -0.0146928\n",
      "exploration/Actions Std                                 0.169572\n",
      "exploration/Actions Max                                 0.576807\n",
      "exploration/Actions Min                                -0.566465\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           890.306\n",
      "exploration/env_infos/final/reward_forward Mean        -0.000226326\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.000226326\n",
      "exploration/env_infos/final/reward_forward Min         -0.000226326\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0108601\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0108601\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0108601\n",
      "exploration/env_infos/reward_forward Mean              -0.00166674\n",
      "exploration/env_infos/reward_forward Std                0.124632\n",
      "exploration/env_infos/reward_forward Max                0.93238\n",
      "exploration/env_infos/reward_forward Min               -0.689971\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.179025\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.179025\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.179025\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0473347\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0473347\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0473347\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.115883\n",
      "exploration/env_infos/reward_ctrl Std                   0.050237\n",
      "exploration/env_infos/reward_ctrl Max                  -0.011737\n",
      "exploration/env_infos/reward_ctrl Min                  -0.34569\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00539778\n",
      "exploration/env_infos/final/torso_velocity Std          0.00413349\n",
      "exploration/env_infos/final/torso_velocity Max         -0.000226326\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0103437\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.162344\n",
      "exploration/env_infos/initial/torso_velocity Std        0.218731\n",
      "exploration/env_infos/initial/torso_velocity Max        0.470903\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0108601\n",
      "exploration/env_infos/torso_velocity Mean              -0.00503336\n",
      "exploration/env_infos/torso_velocity Std                0.10958\n",
      "exploration/env_infos/torso_velocity Max                0.93238\n",
      "exploration/env_infos/torso_velocity Min               -1.1872\n",
      "evaluation/num steps total                              1.675e+06\n",
      "evaluation/num paths total                           1675\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.936073\n",
      "evaluation/Rewards Std                                  0.0328273\n",
      "evaluation/Rewards Max                                  2.77689\n",
      "evaluation/Rewards Min                                  0.439391\n",
      "evaluation/Returns Mean                               936.073\n",
      "evaluation/Returns Std                                 21.3575\n",
      "evaluation/Returns Max                                962.32\n",
      "evaluation/Returns Min                                893.811\n",
      "evaluation/Actions Mean                                -0.00426642\n",
      "evaluation/Actions Std                                  0.127019\n",
      "evaluation/Actions Max                                  0.508561\n",
      "evaluation/Actions Min                                 -0.772513\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            936.073\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.86232e-05\n",
      "evaluation/env_infos/final/reward_forward Std           0.000140079\n",
      "evaluation/env_infos/final/reward_forward Max           1.10543e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -0.000714861\n",
      "evaluation/env_infos/initial/reward_forward Mean        5.09598e-05\n",
      "evaluation/env_infos/initial/reward_forward Std         0.10912\n",
      "evaluation/env_infos/initial/reward_forward Max         0.233547\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.178543\n",
      "evaluation/env_infos/reward_forward Mean                0.003776\n",
      "evaluation/env_infos/reward_forward Std                 0.0676529\n",
      "evaluation/env_infos/reward_forward Max                 1.14024\n",
      "evaluation/env_infos/reward_forward Min                -1.83755\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0636231\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0218931\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.036571\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.105619\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0410785\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0166074\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0163457\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0683186\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0646079\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0258385\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0102531\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.560609\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.67061e-05\n",
      "evaluation/env_infos/final/torso_velocity Std           0.000238143\n",
      "evaluation/env_infos/final/torso_velocity Max           1.10543e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0014813\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.154094\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.214384\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.652133\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.178543\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000703476\n",
      "evaluation/env_infos/torso_velocity Std                 0.0628204\n",
      "evaluation/env_infos/torso_velocity Max                 1.72129\n",
      "evaluation/env_infos/torso_velocity Min                -2.08193\n",
      "time/data storing (s)                                   0.325625\n",
      "time/evaluation sampling (s)                           42.2395\n",
      "time/exploration sampling (s)                           1.98256\n",
      "time/logging (s)                                        0.279705\n",
      "time/saving (s)                                         0.026068\n",
      "time/training (s)                                       3.89504\n",
      "time/epoch (s)                                         48.7485\n",
      "time/total (s)                                       3355.43\n",
      "Epoch                                                  66\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:20:51.384079 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 67 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 137898\n",
      "trainer/QF1 Loss                                        0.386328\n",
      "trainer/QF2 Loss                                        0.402876\n",
      "trainer/Policy Loss                                   -14.2164\n",
      "trainer/Q1 Predictions Mean                            22.2624\n",
      "trainer/Q1 Predictions Std                              2.00335\n",
      "trainer/Q1 Predictions Max                             26.0183\n",
      "trainer/Q1 Predictions Min                             10.1314\n",
      "trainer/Q2 Predictions Mean                            22.3353\n",
      "trainer/Q2 Predictions Std                              1.93951\n",
      "trainer/Q2 Predictions Max                             25.862\n",
      "trainer/Q2 Predictions Min                             12.5355\n",
      "trainer/Q Targets Mean                                 22.2978\n",
      "trainer/Q Targets Std                                   1.97131\n",
      "trainer/Q Targets Max                                  25.5297\n",
      "trainer/Q Targets Min                                   8.14677\n",
      "trainer/Log Pis Mean                                    8.33477\n",
      "trainer/Log Pis Std                                     2.65838\n",
      "trainer/Log Pis Max                                    16.8612\n",
      "trainer/Log Pis Min                                    -1.84159\n",
      "trainer/Policy mu Mean                                  0.0454464\n",
      "trainer/Policy mu Std                                   0.195086\n",
      "trainer/Policy mu Max                                   1.14228\n",
      "trainer/Policy mu Min                                  -0.903268\n",
      "trainer/Policy log std Mean                            -2.41605\n",
      "trainer/Policy log std Std                              0.267795\n",
      "trainer/Policy log std Max                             -1.73987\n",
      "trainer/Policy log std Min                             -3.70968\n",
      "trainer/Alpha                                           0.00796793\n",
      "trainer/Alpha Loss                                      1.61785\n",
      "exploration/num steps total                         69000\n",
      "exploration/num paths total                           163\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.864942\n",
      "exploration/Rewards Std                                 0.113167\n",
      "exploration/Rewards Max                                 1.66645\n",
      "exploration/Rewards Min                                 0.374807\n",
      "exploration/Returns Mean                              864.942\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               864.942\n",
      "exploration/Returns Min                               864.942\n",
      "exploration/Actions Mean                                0.0427257\n",
      "exploration/Actions Std                                 0.189637\n",
      "exploration/Actions Max                                 0.667421\n",
      "exploration/Actions Min                                -0.589118\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           864.942\n",
      "exploration/env_infos/final/reward_forward Mean        -0.255609\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.255609\n",
      "exploration/env_infos/final/reward_forward Min         -0.255609\n",
      "exploration/env_infos/initial/reward_forward Mean       0.177484\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.177484\n",
      "exploration/env_infos/initial/reward_forward Min        0.177484\n",
      "exploration/env_infos/reward_forward Mean              -0.00803089\n",
      "exploration/env_infos/reward_forward Std                0.310875\n",
      "exploration/env_infos/reward_forward Max                1.3843\n",
      "exploration/env_infos/reward_forward Min               -0.913396\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.212693\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.212693\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.212693\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0752186\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0752186\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0752186\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.151151\n",
      "exploration/env_infos/reward_ctrl Std                   0.0796932\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0168965\n",
      "exploration/env_infos/reward_ctrl Min                  -0.625193\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean         0.0556284\n",
      "exploration/env_infos/final/torso_velocity Std          0.224363\n",
      "exploration/env_infos/final/torso_velocity Max          0.264695\n",
      "exploration/env_infos/final/torso_velocity Min         -0.255609\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.178462\n",
      "exploration/env_infos/initial/torso_velocity Std        0.0589505\n",
      "exploration/env_infos/initial/torso_velocity Max        0.251145\n",
      "exploration/env_infos/initial/torso_velocity Min        0.106757\n",
      "exploration/env_infos/torso_velocity Mean              -0.00987332\n",
      "exploration/env_infos/torso_velocity Std                0.252556\n",
      "exploration/env_infos/torso_velocity Max                1.3843\n",
      "exploration/env_infos/torso_velocity Min               -1.05764\n",
      "evaluation/num steps total                              1.7e+06\n",
      "evaluation/num paths total                           1700\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.89887\n",
      "evaluation/Rewards Std                                  0.0408325\n",
      "evaluation/Rewards Max                                  2.17393\n",
      "evaluation/Rewards Min                                  0.373088\n",
      "evaluation/Returns Mean                               898.87\n",
      "evaluation/Returns Std                                 30.6846\n",
      "evaluation/Returns Max                                954.545\n",
      "evaluation/Returns Min                                849.889\n",
      "evaluation/Actions Mean                                 0.0229029\n",
      "evaluation/Actions Std                                  0.158231\n",
      "evaluation/Actions Max                                  0.646074\n",
      "evaluation/Actions Min                                 -0.569968\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            898.87\n",
      "evaluation/env_infos/final/reward_forward Mean          0.000261087\n",
      "evaluation/env_infos/final/reward_forward Std           0.00122731\n",
      "evaluation/env_infos/final/reward_forward Max           0.0062686\n",
      "evaluation/env_infos/final/reward_forward Min          -9.96135e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00495783\n",
      "evaluation/env_infos/initial/reward_forward Std         0.148193\n",
      "evaluation/env_infos/initial/reward_forward Max         0.326637\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.311012\n",
      "evaluation/env_infos/reward_forward Mean                0.000534368\n",
      "evaluation/env_infos/reward_forward Std                 0.0635131\n",
      "evaluation/env_infos/reward_forward Max                 1.35017\n",
      "evaluation/env_infos/reward_forward Min                -1.09796\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.101019\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0309356\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0449969\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.150899\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0842715\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.023687\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.048809\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.143074\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.102247\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0344964\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0139164\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.729199\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000364206\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00368697\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0062686\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.031391\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.146472\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.249724\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.747537\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.311012\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000703102\n",
      "evaluation/env_infos/torso_velocity Std                 0.0665922\n",
      "evaluation/env_infos/torso_velocity Max                 1.47977\n",
      "evaluation/env_infos/torso_velocity Min                -2.03005\n",
      "time/data storing (s)                                   0.318732\n",
      "time/evaluation sampling (s)                           43.4904\n",
      "time/exploration sampling (s)                           1.91317\n",
      "time/logging (s)                                        0.269246\n",
      "time/saving (s)                                         0.0258483\n",
      "time/training (s)                                       3.93983\n",
      "time/epoch (s)                                         49.9572\n",
      "time/total (s)                                       3405.96\n",
      "Epoch                                                  67\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:21:40.660130 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 68 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 139898\n",
      "trainer/QF1 Loss                                        0.374242\n",
      "trainer/QF2 Loss                                        0.37962\n",
      "trainer/Policy Loss                                   -15.9101\n",
      "trainer/Q1 Predictions Mean                            22.7863\n",
      "trainer/Q1 Predictions Std                              1.79181\n",
      "trainer/Q1 Predictions Max                             26.3646\n",
      "trainer/Q1 Predictions Min                             13.565\n",
      "trainer/Q2 Predictions Mean                            22.65\n",
      "trainer/Q2 Predictions Std                              1.80535\n",
      "trainer/Q2 Predictions Max                             26.4195\n",
      "trainer/Q2 Predictions Min                             13.8232\n",
      "trainer/Q Targets Mean                                 22.6723\n",
      "trainer/Q Targets Std                                   1.93228\n",
      "trainer/Q Targets Max                                  28.6135\n",
      "trainer/Q Targets Min                                  14.7163\n",
      "trainer/Log Pis Mean                                    6.9217\n",
      "trainer/Log Pis Std                                     2.30909\n",
      "trainer/Log Pis Max                                    16.3166\n",
      "trainer/Log Pis Min                                     0.125907\n",
      "trainer/Policy mu Mean                                 -0.0589668\n",
      "trainer/Policy mu Std                                   0.188975\n",
      "trainer/Policy mu Max                                   0.75778\n",
      "trainer/Policy mu Min                                  -0.976068\n",
      "trainer/Policy log std Mean                            -2.22838\n",
      "trainer/Policy log std Std                              0.231589\n",
      "trainer/Policy log std Max                             -1.29128\n",
      "trainer/Policy log std Min                             -3.98561\n",
      "trainer/Alpha                                           0.00821138\n",
      "trainer/Alpha Loss                                     -5.17583\n",
      "exploration/num steps total                         70000\n",
      "exploration/num paths total                           164\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.859936\n",
      "exploration/Rewards Std                                 0.116981\n",
      "exploration/Rewards Max                                 1.61786\n",
      "exploration/Rewards Min                                 0.243007\n",
      "exploration/Returns Mean                              859.936\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               859.936\n",
      "exploration/Returns Min                               859.936\n",
      "exploration/Actions Mean                               -0.106963\n",
      "exploration/Actions Std                                 0.175581\n",
      "exploration/Actions Max                                 0.672547\n",
      "exploration/Actions Min                                -0.76276\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           859.936\n",
      "exploration/env_infos/final/reward_forward Mean        -0.16897\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.16897\n",
      "exploration/env_infos/final/reward_forward Min         -0.16897\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.140658\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.140658\n",
      "exploration/env_infos/initial/reward_forward Min       -0.140658\n",
      "exploration/env_infos/reward_forward Mean              -0.0130617\n",
      "exploration/env_infos/reward_forward Std                0.221861\n",
      "exploration/env_infos/reward_forward Max                0.875373\n",
      "exploration/env_infos/reward_forward Min               -1.25343\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.156415\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.156415\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.156415\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.111593\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.111593\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.111593\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.16908\n",
      "exploration/env_infos/reward_ctrl Std                   0.0943408\n",
      "exploration/env_infos/reward_ctrl Max                  -0.00749003\n",
      "exploration/env_infos/reward_ctrl Min                  -0.756993\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0479576\n",
      "exploration/env_infos/final/torso_velocity Std          0.0967244\n",
      "exploration/env_infos/final/torso_velocity Max          0.0677793\n",
      "exploration/env_infos/final/torso_velocity Min         -0.16897\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.120785\n",
      "exploration/env_infos/initial/torso_velocity Std        0.325459\n",
      "exploration/env_infos/initial/torso_velocity Max        0.579563\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.140658\n",
      "exploration/env_infos/torso_velocity Mean               0.00701538\n",
      "exploration/env_infos/torso_velocity Std                0.269968\n",
      "exploration/env_infos/torso_velocity Max                1.2308\n",
      "exploration/env_infos/torso_velocity Min               -1.25343\n",
      "evaluation/num steps total                              1.725e+06\n",
      "evaluation/num paths total                           1725\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.849552\n",
      "evaluation/Rewards Std                                  0.0753214\n",
      "evaluation/Rewards Max                                  2.14857\n",
      "evaluation/Rewards Min                                  0.131588\n",
      "evaluation/Returns Mean                               849.552\n",
      "evaluation/Returns Std                                 67.8885\n",
      "evaluation/Returns Max                                928.122\n",
      "evaluation/Returns Min                                711.615\n",
      "evaluation/Actions Mean                                -0.0253801\n",
      "evaluation/Actions Std                                  0.193092\n",
      "evaluation/Actions Max                                  0.689325\n",
      "evaluation/Actions Min                                 -0.791939\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            849.552\n",
      "evaluation/env_infos/final/reward_forward Mean          8.89893e-08\n",
      "evaluation/env_infos/final/reward_forward Std           4.29659e-07\n",
      "evaluation/env_infos/final/reward_forward Max           7.78295e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -6.95278e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0252362\n",
      "evaluation/env_infos/initial/reward_forward Std         0.125429\n",
      "evaluation/env_infos/initial/reward_forward Max         0.285973\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.210226\n",
      "evaluation/env_infos/reward_forward Mean               -0.00219004\n",
      "evaluation/env_infos/reward_forward Std                 0.0683577\n",
      "evaluation/env_infos/reward_forward Max                 1.34789\n",
      "evaluation/env_infos/reward_forward Min                -1.22727\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.150595\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.070002\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0691989\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.290897\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0780016\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0318286\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.032937\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.136561\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.151714\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0713075\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.032937\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.868412\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.07923e-09\n",
      "evaluation/env_infos/final/torso_velocity Std           2.96098e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           7.78295e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.04749e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.161995\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.212812\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.553309\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.250976\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00201385\n",
      "evaluation/env_infos/torso_velocity Std                 0.0691159\n",
      "evaluation/env_infos/torso_velocity Max                 1.5176\n",
      "evaluation/env_infos/torso_velocity Min                -1.71684\n",
      "time/data storing (s)                                   0.318096\n",
      "time/evaluation sampling (s)                           42.3014\n",
      "time/exploration sampling (s)                           1.85034\n",
      "time/logging (s)                                        0.280578\n",
      "time/saving (s)                                         0.0271316\n",
      "time/training (s)                                       3.90879\n",
      "time/epoch (s)                                         48.6863\n",
      "time/total (s)                                       3455.25\n",
      "Epoch                                                  68\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:22:30.720435 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 69 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 141898\n",
      "trainer/QF1 Loss                                        1.01818\n",
      "trainer/QF2 Loss                                        0.461753\n",
      "trainer/Policy Loss                                   -16.1079\n",
      "trainer/Q1 Predictions Mean                            22.7853\n",
      "trainer/Q1 Predictions Std                              2.16438\n",
      "trainer/Q1 Predictions Max                             29.5166\n",
      "trainer/Q1 Predictions Min                             10.6991\n",
      "trainer/Q2 Predictions Mean                            22.8312\n",
      "trainer/Q2 Predictions Std                              2.43083\n",
      "trainer/Q2 Predictions Max                             29.2968\n",
      "trainer/Q2 Predictions Min                              3.56505\n",
      "trainer/Q Targets Mean                                 22.8812\n",
      "trainer/Q Targets Std                                   2.59913\n",
      "trainer/Q Targets Max                                  30.4642\n",
      "trainer/Q Targets Min                                  -0.841823\n",
      "trainer/Log Pis Mean                                    6.8878\n",
      "trainer/Log Pis Std                                     2.52648\n",
      "trainer/Log Pis Max                                    16.188\n",
      "trainer/Log Pis Min                                    -2.26558\n",
      "trainer/Policy mu Mean                                 -0.0311967\n",
      "trainer/Policy mu Std                                   0.18123\n",
      "trainer/Policy mu Max                                   0.670958\n",
      "trainer/Policy mu Min                                  -0.897309\n",
      "trainer/Policy log std Mean                            -2.22622\n",
      "trainer/Policy log std Std                              0.24154\n",
      "trainer/Policy log std Max                             -1.30635\n",
      "trainer/Policy log std Min                             -3.56575\n",
      "trainer/Alpha                                           0.00823801\n",
      "trainer/Alpha Loss                                     -5.33679\n",
      "exploration/num steps total                         71000\n",
      "exploration/num paths total                           165\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.856839\n",
      "exploration/Rewards Std                                 0.0803734\n",
      "exploration/Rewards Max                                 2.06501\n",
      "exploration/Rewards Min                                 0.555482\n",
      "exploration/Returns Mean                              856.839\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               856.839\n",
      "exploration/Returns Min                               856.839\n",
      "exploration/Actions Mean                               -0.0512185\n",
      "exploration/Actions Std                                 0.185076\n",
      "exploration/Actions Max                                 0.524545\n",
      "exploration/Actions Min                                -0.709058\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           856.839\n",
      "exploration/env_infos/final/reward_forward Mean        -0.174891\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.174891\n",
      "exploration/env_infos/final/reward_forward Min         -0.174891\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.270899\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.270899\n",
      "exploration/env_infos/initial/reward_forward Min       -0.270899\n",
      "exploration/env_infos/reward_forward Mean               0.0148456\n",
      "exploration/env_infos/reward_forward Std                0.260801\n",
      "exploration/env_infos/reward_forward Max                1.24451\n",
      "exploration/env_infos/reward_forward Min               -0.819347\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.101524\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.101524\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.101524\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0355905\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0355905\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0355905\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.147506\n",
      "exploration/env_infos/reward_ctrl Std                   0.0663989\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0202011\n",
      "exploration/env_infos/reward_ctrl Min                  -0.444518\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0966636\n",
      "exploration/env_infos/final/torso_velocity Std          0.0618281\n",
      "exploration/env_infos/final/torso_velocity Max         -0.023721\n",
      "exploration/env_infos/final/torso_velocity Min         -0.174891\n",
      "exploration/env_infos/initial/torso_velocity Mean      -0.0365314\n",
      "exploration/env_infos/initial/torso_velocity Std        0.275765\n",
      "exploration/env_infos/initial/torso_velocity Max        0.350603\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.270899\n",
      "exploration/env_infos/torso_velocity Mean               0.000627761\n",
      "exploration/env_infos/torso_velocity Std                0.20317\n",
      "exploration/env_infos/torso_velocity Max                1.24451\n",
      "exploration/env_infos/torso_velocity Min               -1.56229\n",
      "evaluation/num steps total                              1.75e+06\n",
      "evaluation/num paths total                           1750\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.918944\n",
      "evaluation/Rewards Std                                  0.0453004\n",
      "evaluation/Rewards Max                                  2.28117\n",
      "evaluation/Rewards Min                                  0.605194\n",
      "evaluation/Returns Mean                               918.944\n",
      "evaluation/Returns Std                                 31.3045\n",
      "evaluation/Returns Max                                967.862\n",
      "evaluation/Returns Min                                833.824\n",
      "evaluation/Actions Mean                                -0.0317005\n",
      "evaluation/Actions Std                                  0.140505\n",
      "evaluation/Actions Max                                  0.544247\n",
      "evaluation/Actions Min                                 -0.663346\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            918.944\n",
      "evaluation/env_infos/final/reward_forward Mean          5.7251e-08\n",
      "evaluation/env_infos/final/reward_forward Std           5.73973e-07\n",
      "evaluation/env_infos/final/reward_forward Max           1.03878e-06\n",
      "evaluation/env_infos/final/reward_forward Min          -8.62776e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0731912\n",
      "evaluation/env_infos/initial/reward_forward Std         0.14569\n",
      "evaluation/env_infos/initial/reward_forward Max         0.264435\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.296218\n",
      "evaluation/env_infos/reward_forward Mean                0.000709983\n",
      "evaluation/env_infos/reward_forward Std                 0.0728085\n",
      "evaluation/env_infos/reward_forward Max                 1.00941\n",
      "evaluation/env_infos/reward_forward Min                -1.36694\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0815494\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0307344\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0385517\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.167138\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0777978\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0238826\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0415156\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.144288\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0829864\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0330214\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0275599\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.394806\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean          1.41549e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           4.17098e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.03878e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -8.62776e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.119788\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.26058\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.67116\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.296218\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00188998\n",
      "evaluation/env_infos/torso_velocity Std                 0.0682857\n",
      "evaluation/env_infos/torso_velocity Max                 1.55203\n",
      "evaluation/env_infos/torso_velocity Min                -2.03381\n",
      "time/data storing (s)                                   0.323183\n",
      "time/evaluation sampling (s)                           42.9233\n",
      "time/exploration sampling (s)                           1.88845\n",
      "time/logging (s)                                        0.271825\n",
      "time/saving (s)                                         0.026246\n",
      "time/training (s)                                       4.0127\n",
      "time/epoch (s)                                         49.4457\n",
      "time/total (s)                                       3505.3\n",
      "Epoch                                                  69\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:23:20.000316 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 70 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 143898\n",
      "trainer/QF1 Loss                                        0.434735\n",
      "trainer/QF2 Loss                                        0.328984\n",
      "trainer/Policy Loss                                   -15.2805\n",
      "trainer/Q1 Predictions Mean                            23.3732\n",
      "trainer/Q1 Predictions Std                              2.03929\n",
      "trainer/Q1 Predictions Max                             29.0973\n",
      "trainer/Q1 Predictions Min                             14.6196\n",
      "trainer/Q2 Predictions Mean                            23.3316\n",
      "trainer/Q2 Predictions Std                              2.04993\n",
      "trainer/Q2 Predictions Max                             28.304\n",
      "trainer/Q2 Predictions Min                             14.3905\n",
      "trainer/Q Targets Mean                                 23.2746\n",
      "trainer/Q Targets Std                                   2.06379\n",
      "trainer/Q Targets Max                                  28.7806\n",
      "trainer/Q Targets Min                                  14.2282\n",
      "trainer/Log Pis Mean                                    8.26267\n",
      "trainer/Log Pis Std                                     2.49919\n",
      "trainer/Log Pis Max                                    18.3273\n",
      "trainer/Log Pis Min                                     0.802138\n",
      "trainer/Policy mu Mean                                 -0.00438785\n",
      "trainer/Policy mu Std                                   0.224195\n",
      "trainer/Policy mu Max                                   1.62594\n",
      "trainer/Policy mu Min                                  -1.31743\n",
      "trainer/Policy log std Mean                            -2.38779\n",
      "trainer/Policy log std Std                              0.272748\n",
      "trainer/Policy log std Max                             -1.56917\n",
      "trainer/Policy log std Min                             -3.88835\n",
      "trainer/Alpha                                           0.00826792\n",
      "trainer/Alpha Loss                                      1.25997\n",
      "exploration/num steps total                         72000\n",
      "exploration/num paths total                           166\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.884055\n",
      "exploration/Rewards Std                                 0.0469999\n",
      "exploration/Rewards Max                                 1.1196\n",
      "exploration/Rewards Min                                 0.364562\n",
      "exploration/Returns Mean                              884.055\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               884.055\n",
      "exploration/Returns Min                               884.055\n",
      "exploration/Actions Mean                               -0.0175252\n",
      "exploration/Actions Std                                 0.169838\n",
      "exploration/Actions Max                                 0.518665\n",
      "exploration/Actions Min                                -0.543508\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           884.055\n",
      "exploration/env_infos/final/reward_forward Mean        -0.000786063\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.000786063\n",
      "exploration/env_infos/final/reward_forward Min         -0.000786063\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.178025\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.178025\n",
      "exploration/env_infos/initial/reward_forward Min       -0.178025\n",
      "exploration/env_infos/reward_forward Mean               0.0158162\n",
      "exploration/env_infos/reward_forward Std                0.152227\n",
      "exploration/env_infos/reward_forward Max                1.48297\n",
      "exploration/env_infos/reward_forward Min               -0.510901\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.154303\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.154303\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.154303\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0527645\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0527645\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0527645\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.116608\n",
      "exploration/env_infos/reward_ctrl Std                   0.0464136\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0120034\n",
      "exploration/env_infos/reward_ctrl Min                  -0.635438\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.000705964\n",
      "exploration/env_infos/final/torso_velocity Std          9.4207e-05\n",
      "exploration/env_infos/final/torso_velocity Max         -0.000573716\n",
      "exploration/env_infos/final/torso_velocity Min         -0.000786063\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.124813\n",
      "exploration/env_infos/initial/torso_velocity Std        0.287575\n",
      "exploration/env_infos/initial/torso_velocity Max        0.511319\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.178025\n",
      "exploration/env_infos/torso_velocity Mean              -0.000326734\n",
      "exploration/env_infos/torso_velocity Std                0.134206\n",
      "exploration/env_infos/torso_velocity Max                1.48297\n",
      "exploration/env_infos/torso_velocity Min               -1.68034\n",
      "evaluation/num steps total                              1.775e+06\n",
      "evaluation/num paths total                           1775\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.910528\n",
      "evaluation/Rewards Std                                  0.0631972\n",
      "evaluation/Rewards Max                                  1.92587\n",
      "evaluation/Rewards Min                                  0.13432\n",
      "evaluation/Returns Mean                               910.528\n",
      "evaluation/Returns Std                                 45.519\n",
      "evaluation/Returns Max                                963.663\n",
      "evaluation/Returns Min                                751.887\n",
      "evaluation/Actions Mean                                -0.0132781\n",
      "evaluation/Actions Std                                  0.150799\n",
      "evaluation/Actions Max                                  0.626311\n",
      "evaluation/Actions Min                                 -0.668963\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            910.528\n",
      "evaluation/env_infos/final/reward_forward Mean         -2.68204e-09\n",
      "evaluation/env_infos/final/reward_forward Std           3.24847e-07\n",
      "evaluation/env_infos/final/reward_forward Max           4.95367e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -5.52918e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0448508\n",
      "evaluation/env_infos/initial/reward_forward Std         0.141261\n",
      "evaluation/env_infos/initial/reward_forward Max         0.241566\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.265143\n",
      "evaluation/env_infos/reward_forward Mean                0.00360316\n",
      "evaluation/env_infos/reward_forward Std                 0.0986155\n",
      "evaluation/env_infos/reward_forward Max                 1.47136\n",
      "evaluation/env_infos/reward_forward Min                -1.53592\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0898461\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0470017\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0326279\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.253318\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0401728\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0174324\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0207912\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0879136\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0916672\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0525774\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.010385\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.86568\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -3.11346e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.20999e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           5.17073e-07\n",
      "evaluation/env_infos/final/torso_velocity Min          -9.94451e-07\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.10157\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.265901\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.693359\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.310733\n",
      "evaluation/env_infos/torso_velocity Mean               -0.0020464\n",
      "evaluation/env_infos/torso_velocity Std                 0.0865477\n",
      "evaluation/env_infos/torso_velocity Max                 1.47136\n",
      "evaluation/env_infos/torso_velocity Min                -1.9378\n",
      "time/data storing (s)                                   0.33266\n",
      "time/evaluation sampling (s)                           42.1486\n",
      "time/exploration sampling (s)                           1.96829\n",
      "time/logging (s)                                        0.279263\n",
      "time/saving (s)                                         0.0261932\n",
      "time/training (s)                                       3.93733\n",
      "time/epoch (s)                                         48.6923\n",
      "time/total (s)                                       3554.58\n",
      "Epoch                                                  70\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:24:12.827589 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 71 finished\n",
      "-------------------------------------------------  ---------------\n",
      "replay_buffer/size                                 145898\n",
      "trainer/QF1 Loss                                        0.383073\n",
      "trainer/QF2 Loss                                        0.370441\n",
      "trainer/Policy Loss                                   -15.9632\n",
      "trainer/Q1 Predictions Mean                            23.4948\n",
      "trainer/Q1 Predictions Std                              2.20318\n",
      "trainer/Q1 Predictions Max                             27.1798\n",
      "trainer/Q1 Predictions Min                             13.9147\n",
      "trainer/Q2 Predictions Mean                            23.5395\n",
      "trainer/Q2 Predictions Std                              2.1505\n",
      "trainer/Q2 Predictions Max                             29.0285\n",
      "trainer/Q2 Predictions Min                             13.2647\n",
      "trainer/Q Targets Mean                                 23.3678\n",
      "trainer/Q Targets Std                                   2.11629\n",
      "trainer/Q Targets Max                                  29.7244\n",
      "trainer/Q Targets Min                                  14.1155\n",
      "trainer/Log Pis Mean                                    7.77875\n",
      "trainer/Log Pis Std                                     2.58344\n",
      "trainer/Log Pis Max                                    22.8449\n",
      "trainer/Log Pis Min                                     0.0439036\n",
      "trainer/Policy mu Mean                                 -0.076445\n",
      "trainer/Policy mu Std                                   0.208446\n",
      "trainer/Policy mu Max                                   1.49081\n",
      "trainer/Policy mu Min                                  -2.66042\n",
      "trainer/Policy log std Mean                            -2.29373\n",
      "trainer/Policy log std Std                              0.253483\n",
      "trainer/Policy log std Max                             -1.13002\n",
      "trainer/Policy log std Min                             -4.43752\n",
      "trainer/Alpha                                           0.00856524\n",
      "trainer/Alpha Loss                                     -1.05317\n",
      "exploration/num steps total                         73000\n",
      "exploration/num paths total                           167\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.874993\n",
      "exploration/Rewards Std                                 0.0745837\n",
      "exploration/Rewards Max                                 1.43508\n",
      "exploration/Rewards Min                                 0.0603446\n",
      "exploration/Returns Mean                              874.993\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               874.993\n",
      "exploration/Returns Min                               874.993\n",
      "exploration/Actions Mean                               -0.0648865\n",
      "exploration/Actions Std                                 0.167961\n",
      "exploration/Actions Max                                 0.589328\n",
      "exploration/Actions Min                                -0.712045\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           874.993\n",
      "exploration/env_infos/final/reward_forward Mean         0.19379\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.19379\n",
      "exploration/env_infos/final/reward_forward Min          0.19379\n",
      "exploration/env_infos/initial/reward_forward Mean       0.0412087\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max        0.0412087\n",
      "exploration/env_infos/initial/reward_forward Min        0.0412087\n",
      "exploration/env_infos/reward_forward Mean               0.012292\n",
      "exploration/env_infos/reward_forward Std                0.350093\n",
      "exploration/env_infos/reward_forward Max                1.47866\n",
      "exploration/env_infos/reward_forward Min               -1.44976\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.151309\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.151309\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.151309\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0824982\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0824982\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0824982\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.129685\n",
      "exploration/env_infos/reward_ctrl Std                   0.0644514\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0120808\n",
      "exploration/env_infos/reward_ctrl Min                  -0.939655\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.118888\n",
      "exploration/env_infos/final/torso_velocity Std          0.281277\n",
      "exploration/env_infos/final/torso_velocity Max          0.19379\n",
      "exploration/env_infos/final/torso_velocity Min         -0.488181\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.123213\n",
      "exploration/env_infos/initial/torso_velocity Std        0.179532\n",
      "exploration/env_infos/initial/torso_velocity Max        0.372312\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0438821\n",
      "exploration/env_infos/torso_velocity Mean              -0.0155477\n",
      "exploration/env_infos/torso_velocity Std                0.264615\n",
      "exploration/env_infos/torso_velocity Max                1.47866\n",
      "exploration/env_infos/torso_velocity Min               -1.44976\n",
      "evaluation/num steps total                              1.8e+06\n",
      "evaluation/num paths total                           1800\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.913193\n",
      "evaluation/Rewards Std                                  0.0991006\n",
      "evaluation/Rewards Max                                  2.58903\n",
      "evaluation/Rewards Min                                 -0.418672\n",
      "evaluation/Returns Mean                               913.193\n",
      "evaluation/Returns Std                                 30.7613\n",
      "evaluation/Returns Max                                964.472\n",
      "evaluation/Returns Min                                844.236\n",
      "evaluation/Actions Mean                                -0.0356196\n",
      "evaluation/Actions Std                                  0.150829\n",
      "evaluation/Actions Max                                  0.785797\n",
      "evaluation/Actions Min                                 -0.932673\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            913.193\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.0547162\n",
      "evaluation/env_infos/final/reward_forward Std           0.266221\n",
      "evaluation/env_infos/final/reward_forward Max           0.683194\n",
      "evaluation/env_infos/final/reward_forward Min          -0.858029\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.00910427\n",
      "evaluation/env_infos/initial/reward_forward Std         0.124801\n",
      "evaluation/env_infos/initial/reward_forward Max         0.371562\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.25038\n",
      "evaluation/env_infos/reward_forward Mean               -0.00214412\n",
      "evaluation/env_infos/reward_forward Std                 0.24406\n",
      "evaluation/env_infos/reward_forward Max                 1.16993\n",
      "evaluation/env_infos/reward_forward Min                -1.83337\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0931293\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0421506\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0326673\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.204645\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0430398\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0140565\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0248969\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0698144\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0960726\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0657565\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0195806\n",
      "evaluation/env_infos/reward_ctrl Min                   -1.45409\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0114616\n",
      "evaluation/env_infos/final/torso_velocity Std           0.219043\n",
      "evaluation/env_infos/final/torso_velocity Max           0.717756\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.858029\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.148858\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245228\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.664984\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.267046\n",
      "evaluation/env_infos/torso_velocity Mean               -0.00808382\n",
      "evaluation/env_infos/torso_velocity Std                 0.206892\n",
      "evaluation/env_infos/torso_velocity Max                 1.35495\n",
      "evaluation/env_infos/torso_velocity Min                -1.97505\n",
      "time/data storing (s)                                   0.497334\n",
      "time/evaluation sampling (s)                           44.0371\n",
      "time/exploration sampling (s)                           2.77476\n",
      "time/logging (s)                                        0.304512\n",
      "time/saving (s)                                         0.02675\n",
      "time/training (s)                                       4.6015\n",
      "time/epoch (s)                                         52.2419\n",
      "time/total (s)                                       3607.43\n",
      "Epoch                                                  71\n",
      "-------------------------------------------------  ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:25:08.935027 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 72 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 147898\n",
      "trainer/QF1 Loss                                        0.504327\n",
      "trainer/QF2 Loss                                        0.487883\n",
      "trainer/Policy Loss                                   -16.2152\n",
      "trainer/Q1 Predictions Mean                            23.8648\n",
      "trainer/Q1 Predictions Std                              1.7873\n",
      "trainer/Q1 Predictions Max                             26.8476\n",
      "trainer/Q1 Predictions Min                             13.3692\n",
      "trainer/Q2 Predictions Mean                            24.111\n",
      "trainer/Q2 Predictions Std                              1.79506\n",
      "trainer/Q2 Predictions Max                             27.0589\n",
      "trainer/Q2 Predictions Min                             12.7583\n",
      "trainer/Q Targets Mean                                 23.9712\n",
      "trainer/Q Targets Std                                   2.08211\n",
      "trainer/Q Targets Max                                  26.8167\n",
      "trainer/Q Targets Min                                   4.17267\n",
      "trainer/Log Pis Mean                                    7.85273\n",
      "trainer/Log Pis Std                                     2.05045\n",
      "trainer/Log Pis Max                                    16.006\n",
      "trainer/Log Pis Min                                     1.35972\n",
      "trainer/Policy mu Mean                                 -0.0179164\n",
      "trainer/Policy mu Std                                   0.159106\n",
      "trainer/Policy mu Max                                   1.34144\n",
      "trainer/Policy mu Min                                  -1.1508\n",
      "trainer/Policy log std Mean                            -2.34528\n",
      "trainer/Policy log std Std                              0.242673\n",
      "trainer/Policy log std Max                             -1.5097\n",
      "trainer/Policy log std Min                             -3.41705\n",
      "trainer/Alpha                                           0.00759595\n",
      "trainer/Alpha Loss                                     -0.718623\n",
      "exploration/num steps total                         74000\n",
      "exploration/num paths total                           168\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.866727\n",
      "exploration/Rewards Std                                 0.0533627\n",
      "exploration/Rewards Max                                 1.17801\n",
      "exploration/Rewards Min                                 0.451833\n",
      "exploration/Returns Mean                              866.727\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               866.727\n",
      "exploration/Returns Min                               866.727\n",
      "exploration/Actions Mean                                0.030572\n",
      "exploration/Actions Std                                 0.18345\n",
      "exploration/Actions Max                                 0.52547\n",
      "exploration/Actions Min                                -0.63972\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           866.727\n",
      "exploration/env_infos/final/reward_forward Mean         0.025707\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max          0.025707\n",
      "exploration/env_infos/final/reward_forward Min          0.025707\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.211175\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.211175\n",
      "exploration/env_infos/initial/reward_forward Min       -0.211175\n",
      "exploration/env_infos/reward_forward Mean              -0.00974058\n",
      "exploration/env_infos/reward_forward Std                0.0706415\n",
      "exploration/env_infos/reward_forward Max                0.285475\n",
      "exploration/env_infos/reward_forward Min               -0.608309\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0435036\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0435036\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0435036\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0907486\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0907486\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0907486\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.138354\n",
      "exploration/env_infos/reward_ctrl Std                   0.0486087\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0214695\n",
      "exploration/env_infos/reward_ctrl Min                  -0.548167\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.0272869\n",
      "exploration/env_infos/final/torso_velocity Std          0.0992146\n",
      "exploration/env_infos/final/torso_velocity Max          0.0587285\n",
      "exploration/env_infos/final/torso_velocity Min         -0.166296\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0793072\n",
      "exploration/env_infos/initial/torso_velocity Std        0.279068\n",
      "exploration/env_infos/initial/torso_velocity Max        0.455922\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.211175\n",
      "exploration/env_infos/torso_velocity Mean              -0.00562193\n",
      "exploration/env_infos/torso_velocity Std                0.0846531\n",
      "exploration/env_infos/torso_velocity Max                0.455922\n",
      "exploration/env_infos/torso_velocity Min               -1.67599\n",
      "evaluation/num steps total                              1.825e+06\n",
      "evaluation/num paths total                           1825\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.917462\n",
      "evaluation/Rewards Std                                  0.071076\n",
      "evaluation/Rewards Max                                  2.3295\n",
      "evaluation/Rewards Min                                  0.32081\n",
      "evaluation/Returns Mean                               917.462\n",
      "evaluation/Returns Std                                 41.8798\n",
      "evaluation/Returns Max                                980.382\n",
      "evaluation/Returns Min                                832.406\n",
      "evaluation/Actions Mean                                 0.0111635\n",
      "evaluation/Actions Std                                  0.146973\n",
      "evaluation/Actions Max                                  0.773488\n",
      "evaluation/Actions Min                                 -0.80298\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            917.462\n",
      "evaluation/env_infos/final/reward_forward Mean          7.79718e-08\n",
      "evaluation/env_infos/final/reward_forward Std           3.42604e-07\n",
      "evaluation/env_infos/final/reward_forward Max           9.12915e-07\n",
      "evaluation/env_infos/final/reward_forward Min          -4.68089e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0387726\n",
      "evaluation/env_infos/initial/reward_forward Std         0.105647\n",
      "evaluation/env_infos/initial/reward_forward Max         0.194907\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.257776\n",
      "evaluation/env_infos/reward_forward Mean                0.00247198\n",
      "evaluation/env_infos/reward_forward Std                 0.110934\n",
      "evaluation/env_infos/reward_forward Max                 1.67685\n",
      "evaluation/env_infos/reward_forward Min                -1.63279\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0861702\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0440686\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0192348\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.16926\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0538205\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0189151\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0160367\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0953862\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0869022\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0465196\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.00809999\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.67919\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -4.81049e-08\n",
      "evaluation/env_infos/final/torso_velocity Std           3.26979e-07\n",
      "evaluation/env_infos/final/torso_velocity Max           1.0061e-06\n",
      "evaluation/env_infos/final/torso_velocity Min          -1.33928e-06\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.151454\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.242479\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.61447\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.26645\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000524076\n",
      "evaluation/env_infos/torso_velocity Std                 0.0992097\n",
      "evaluation/env_infos/torso_velocity Max                 1.67685\n",
      "evaluation/env_infos/torso_velocity Min                -2.0461\n",
      "time/data storing (s)                                   0.332444\n",
      "time/evaluation sampling (s)                           48.6402\n",
      "time/exploration sampling (s)                           2.05056\n",
      "time/logging (s)                                        0.280394\n",
      "time/saving (s)                                         0.0268861\n",
      "time/training (s)                                       4.00418\n",
      "time/epoch (s)                                         55.3346\n",
      "time/total (s)                                       3663.52\n",
      "Epoch                                                  72\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:25:59.202703 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 73 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 149898\n",
      "trainer/QF1 Loss                                        0.510818\n",
      "trainer/QF2 Loss                                        0.515252\n",
      "trainer/Policy Loss                                   -15.7212\n",
      "trainer/Q1 Predictions Mean                            23.9292\n",
      "trainer/Q1 Predictions Std                              2.20849\n",
      "trainer/Q1 Predictions Max                             27.042\n",
      "trainer/Q1 Predictions Min                             12.6546\n",
      "trainer/Q2 Predictions Mean                            24.2521\n",
      "trainer/Q2 Predictions Std                              2.23977\n",
      "trainer/Q2 Predictions Max                             28.5323\n",
      "trainer/Q2 Predictions Min                             12.52\n",
      "trainer/Q Targets Mean                                 24.0988\n",
      "trainer/Q Targets Std                                   2.18266\n",
      "trainer/Q Targets Max                                  31.0258\n",
      "trainer/Q Targets Min                                  14.3783\n",
      "trainer/Log Pis Mean                                    8.52113\n",
      "trainer/Log Pis Std                                     2.74145\n",
      "trainer/Log Pis Max                                    24.3865\n",
      "trainer/Log Pis Min                                     1.41628\n",
      "trainer/Policy mu Mean                                  0.00879336\n",
      "trainer/Policy mu Std                                   0.211868\n",
      "trainer/Policy mu Max                                   1.4322\n",
      "trainer/Policy mu Min                                  -2.2246\n",
      "trainer/Policy log std Mean                            -2.42217\n",
      "trainer/Policy log std Std                              0.286466\n",
      "trainer/Policy log std Max                             -1.68833\n",
      "trainer/Policy log std Min                             -4.12615\n",
      "trainer/Alpha                                           0.00796425\n",
      "trainer/Alpha Loss                                      2.51881\n",
      "exploration/num steps total                         75000\n",
      "exploration/num paths total                           169\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.919287\n",
      "exploration/Rewards Std                                 0.0983365\n",
      "exploration/Rewards Max                                 2.0328\n",
      "exploration/Rewards Min                                 0.567663\n",
      "exploration/Returns Mean                              919.287\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               919.287\n",
      "exploration/Returns Min                               919.287\n",
      "exploration/Actions Mean                                0.0210654\n",
      "exploration/Actions Std                                 0.155626\n",
      "exploration/Actions Max                                 0.491043\n",
      "exploration/Actions Min                                -0.542612\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           919.287\n",
      "exploration/env_infos/final/reward_forward Mean        -0.0148978\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.0148978\n",
      "exploration/env_infos/final/reward_forward Min         -0.0148978\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.00480816\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.00480816\n",
      "exploration/env_infos/initial/reward_forward Min       -0.00480816\n",
      "exploration/env_infos/reward_forward Mean               0.0264793\n",
      "exploration/env_infos/reward_forward Std                0.26045\n",
      "exploration/env_infos/reward_forward Max                1.05876\n",
      "exploration/env_infos/reward_forward Min               -1.11247\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.186941\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.186941\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.186941\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0255199\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0255199\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0255199\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0986531\n",
      "exploration/env_infos/reward_ctrl Std                   0.0439219\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0191924\n",
      "exploration/env_infos/reward_ctrl Min                  -0.432337\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00472208\n",
      "exploration/env_infos/final/torso_velocity Std          0.00783208\n",
      "exploration/env_infos/final/torso_velocity Max          0.0041543\n",
      "exploration/env_infos/final/torso_velocity Min         -0.0148978\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.141389\n",
      "exploration/env_infos/initial/torso_velocity Std        0.200608\n",
      "exploration/env_infos/initial/torso_velocity Max        0.425047\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.00480816\n",
      "exploration/env_infos/torso_velocity Mean               0.0161697\n",
      "exploration/env_infos/torso_velocity Std                0.197036\n",
      "exploration/env_infos/torso_velocity Max                1.05876\n",
      "exploration/env_infos/torso_velocity Min               -1.76055\n",
      "evaluation/num steps total                              1.85e+06\n",
      "evaluation/num paths total                           1850\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.924656\n",
      "evaluation/Rewards Std                                  0.044616\n",
      "evaluation/Rewards Max                                  2.3367\n",
      "evaluation/Rewards Min                                  0.691781\n",
      "evaluation/Returns Mean                               924.656\n",
      "evaluation/Returns Std                                 27.3463\n",
      "evaluation/Returns Max                                961.181\n",
      "evaluation/Returns Min                                870.083\n",
      "evaluation/Actions Mean                                -0.00378323\n",
      "evaluation/Actions Std                                  0.138575\n",
      "evaluation/Actions Max                                  0.556024\n",
      "evaluation/Actions Min                                 -0.513125\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            924.656\n",
      "evaluation/env_infos/final/reward_forward Mean         -0.00254848\n",
      "evaluation/env_infos/final/reward_forward Std           0.0124944\n",
      "evaluation/env_infos/final/reward_forward Max           4.33397e-05\n",
      "evaluation/env_infos/final/reward_forward Min          -0.0637583\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.0323492\n",
      "evaluation/env_infos/initial/reward_forward Std         0.131217\n",
      "evaluation/env_infos/initial/reward_forward Max         0.236561\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.191224\n",
      "evaluation/env_infos/reward_forward Mean                0.00427911\n",
      "evaluation/env_infos/reward_forward Std                 0.10478\n",
      "evaluation/env_infos/reward_forward Max                 1.50993\n",
      "evaluation/env_infos/reward_forward Min                -1.73844\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0767352\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.031452\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0353912\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.144795\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0493584\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0119822\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0287668\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0755707\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0768697\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0315412\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.025146\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.308219\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.000412692\n",
      "evaluation/env_infos/final/torso_velocity Std           0.00804209\n",
      "evaluation/env_infos/final/torso_velocity Max           0.0278448\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.0637583\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.158921\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.245327\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.688667\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.315339\n",
      "evaluation/env_infos/torso_velocity Mean               -0.000808433\n",
      "evaluation/env_infos/torso_velocity Std                 0.0801123\n",
      "evaluation/env_infos/torso_velocity Max                 1.50993\n",
      "evaluation/env_infos/torso_velocity Min                -2.08946\n",
      "time/data storing (s)                                   0.351868\n",
      "time/evaluation sampling (s)                           42.6858\n",
      "time/exploration sampling (s)                           2.1384\n",
      "time/logging (s)                                        0.286771\n",
      "time/saving (s)                                         0.0273822\n",
      "time/training (s)                                       4.16271\n",
      "time/epoch (s)                                         49.6529\n",
      "time/total (s)                                       3713.79\n",
      "Epoch                                                  73\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:26:49.612649 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 74 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 151898\n",
      "trainer/QF1 Loss                                        0.517876\n",
      "trainer/QF2 Loss                                        0.328893\n",
      "trainer/Policy Loss                                   -16.6482\n",
      "trainer/Q1 Predictions Mean                            24.1122\n",
      "trainer/Q1 Predictions Std                              2.08731\n",
      "trainer/Q1 Predictions Max                             26.7375\n",
      "trainer/Q1 Predictions Min                              8.00304\n",
      "trainer/Q2 Predictions Mean                            24.3708\n",
      "trainer/Q2 Predictions Std                              2.1633\n",
      "trainer/Q2 Predictions Max                             27.7018\n",
      "trainer/Q2 Predictions Min                              7.6352\n",
      "trainer/Q Targets Mean                                 24.391\n",
      "trainer/Q Targets Std                                   2.21772\n",
      "trainer/Q Targets Max                                  30.9853\n",
      "trainer/Q Targets Min                                   6.39643\n",
      "trainer/Log Pis Mean                                    7.67952\n",
      "trainer/Log Pis Std                                     2.60919\n",
      "trainer/Log Pis Max                                    19.7456\n",
      "trainer/Log Pis Min                                    -1.08743\n",
      "trainer/Policy mu Mean                                 -0.0583334\n",
      "trainer/Policy mu Std                                   0.189267\n",
      "trainer/Policy mu Max                                   2.63579\n",
      "trainer/Policy mu Min                                  -2.74549\n",
      "trainer/Policy log std Mean                            -2.34222\n",
      "trainer/Policy log std Std                              0.21544\n",
      "trainer/Policy log std Max                             -1.77824\n",
      "trainer/Policy log std Min                             -3.55648\n",
      "trainer/Alpha                                           0.00840756\n",
      "trainer/Alpha Loss                                     -1.532\n",
      "exploration/num steps total                         76000\n",
      "exploration/num paths total                           170\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.864756\n",
      "exploration/Rewards Std                                 0.0818758\n",
      "exploration/Rewards Max                                 1.52217\n",
      "exploration/Rewards Min                                 0.643982\n",
      "exploration/Returns Mean                              864.756\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               864.756\n",
      "exploration/Returns Min                               864.756\n",
      "exploration/Actions Mean                               -0.0352826\n",
      "exploration/Actions Std                                 0.191207\n",
      "exploration/Actions Max                                 0.555703\n",
      "exploration/Actions Min                                -0.570263\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           864.756\n",
      "exploration/env_infos/final/reward_forward Mean        -0.00287093\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.00287093\n",
      "exploration/env_infos/final/reward_forward Min         -0.00287093\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.0861685\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.0861685\n",
      "exploration/env_infos/initial/reward_forward Min       -0.0861685\n",
      "exploration/env_infos/reward_forward Mean               0.00781747\n",
      "exploration/env_infos/reward_forward Std                0.216762\n",
      "exploration/env_infos/reward_forward Max                0.884707\n",
      "exploration/env_infos/reward_forward Min               -1.3571\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.144592\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.144592\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.144592\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.040103\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.040103\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.040103\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.15122\n",
      "exploration/env_infos/reward_ctrl Std                   0.055164\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0378767\n",
      "exploration/env_infos/reward_ctrl Min                  -0.356018\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.00323737\n",
      "exploration/env_infos/final/torso_velocity Std          0.110769\n",
      "exploration/env_infos/final/torso_velocity Max          0.132242\n",
      "exploration/env_infos/final/torso_velocity Min         -0.139084\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.169918\n",
      "exploration/env_infos/initial/torso_velocity Std        0.233129\n",
      "exploration/env_infos/initial/torso_velocity Max        0.477791\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.0861685\n",
      "exploration/env_infos/torso_velocity Mean               0.00227874\n",
      "exploration/env_infos/torso_velocity Std                0.163841\n",
      "exploration/env_infos/torso_velocity Max                0.884707\n",
      "exploration/env_infos/torso_velocity Min               -1.3571\n",
      "evaluation/num steps total                              1.875e+06\n",
      "evaluation/num paths total                           1875\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.92876\n",
      "evaluation/Rewards Std                                  0.0591113\n",
      "evaluation/Rewards Max                                  2.44512\n",
      "evaluation/Rewards Min                                  0.171268\n",
      "evaluation/Returns Mean                               928.76\n",
      "evaluation/Returns Std                                 23.8638\n",
      "evaluation/Returns Max                                954.359\n",
      "evaluation/Returns Min                                855.667\n",
      "evaluation/Actions Mean                                -0.0520357\n",
      "evaluation/Actions Std                                  0.12658\n",
      "evaluation/Actions Max                                  0.513899\n",
      "evaluation/Actions Min                                 -0.796486\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            928.76\n",
      "evaluation/env_infos/final/reward_forward Mean          0.0110259\n",
      "evaluation/env_infos/final/reward_forward Std           0.0400766\n",
      "evaluation/env_infos/final/reward_forward Max           0.18882\n",
      "evaluation/env_infos/final/reward_forward Min          -9.84698e-07\n",
      "evaluation/env_infos/initial/reward_forward Mean        0.00829721\n",
      "evaluation/env_infos/initial/reward_forward Std         0.102608\n",
      "evaluation/env_infos/initial/reward_forward Max         0.222327\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.202939\n",
      "evaluation/env_infos/reward_forward Mean                0.0052827\n",
      "evaluation/env_infos/reward_forward Std                 0.132668\n",
      "evaluation/env_infos/reward_forward Max                 2.33798\n",
      "evaluation/env_infos/reward_forward Min                -1.15111\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0725249\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.0253618\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0430748\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.145424\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0390981\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0107596\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0202472\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0584765\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0749207\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0334353\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0202472\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.828732\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.0022268\n",
      "evaluation/env_infos/final/torso_velocity Std           0.0572121\n",
      "evaluation/env_infos/final/torso_velocity Max           0.18882\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.447603\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.144942\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.234434\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.677377\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.202939\n",
      "evaluation/env_infos/torso_velocity Mean                0.00401546\n",
      "evaluation/env_infos/torso_velocity Std                 0.131936\n",
      "evaluation/env_infos/torso_velocity Max                 2.33798\n",
      "evaluation/env_infos/torso_velocity Min                -1.76261\n",
      "time/data storing (s)                                   0.319186\n",
      "time/evaluation sampling (s)                           43.2345\n",
      "time/exploration sampling (s)                           1.88268\n",
      "time/logging (s)                                        0.286171\n",
      "time/saving (s)                                         0.0420624\n",
      "time/training (s)                                       3.99503\n",
      "time/epoch (s)                                         49.7596\n",
      "time/total (s)                                       3764.19\n",
      "Epoch                                                  74\n",
      "-------------------------------------------------  ----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:27:39.146630 PDT | [gher-antdirectionnewsparse-SAC-100e-1000s-disc0.99-horizon1000_2021_05_25_13_24_09_0000--s-10] Epoch 75 finished\n",
      "-------------------------------------------------  ----------------\n",
      "replay_buffer/size                                 153898\n",
      "trainer/QF1 Loss                                        0.255122\n",
      "trainer/QF2 Loss                                        0.279014\n",
      "trainer/Policy Loss                                   -17.0742\n",
      "trainer/Q1 Predictions Mean                            24.7324\n",
      "trainer/Q1 Predictions Std                              1.73593\n",
      "trainer/Q1 Predictions Max                             27.8525\n",
      "trainer/Q1 Predictions Min                             13.3957\n",
      "trainer/Q2 Predictions Mean                            24.7844\n",
      "trainer/Q2 Predictions Std                              1.71647\n",
      "trainer/Q2 Predictions Max                             28.2746\n",
      "trainer/Q2 Predictions Min                             15.8312\n",
      "trainer/Q Targets Mean                                 24.7797\n",
      "trainer/Q Targets Std                                   1.83839\n",
      "trainer/Q Targets Max                                  29.0144\n",
      "trainer/Q Targets Min                                  11.5036\n",
      "trainer/Log Pis Mean                                    7.7808\n",
      "trainer/Log Pis Std                                     2.20571\n",
      "trainer/Log Pis Max                                    16.2074\n",
      "trainer/Log Pis Min                                     1.61007\n",
      "trainer/Policy mu Mean                                  0.0194241\n",
      "trainer/Policy mu Std                                   0.148524\n",
      "trainer/Policy mu Max                                   1.08388\n",
      "trainer/Policy mu Min                                  -1.35287\n",
      "trainer/Policy log std Mean                            -2.36674\n",
      "trainer/Policy log std Std                              0.223414\n",
      "trainer/Policy log std Max                             -1.63535\n",
      "trainer/Policy log std Min                             -3.33643\n",
      "trainer/Alpha                                           0.00819881\n",
      "trainer/Alpha Loss                                     -1.05243\n",
      "exploration/num steps total                         77000\n",
      "exploration/num paths total                           171\n",
      "exploration/path length Mean                         1000\n",
      "exploration/path length Std                             0\n",
      "exploration/path length Max                          1000\n",
      "exploration/path length Min                          1000\n",
      "exploration/Rewards Mean                                0.918968\n",
      "exploration/Rewards Std                                 0.0634685\n",
      "exploration/Rewards Max                                 1.40674\n",
      "exploration/Rewards Min                                 0.630378\n",
      "exploration/Returns Mean                              918.968\n",
      "exploration/Returns Std                                 0\n",
      "exploration/Returns Max                               918.968\n",
      "exploration/Returns Min                               918.968\n",
      "exploration/Actions Mean                                0.0160228\n",
      "exploration/Actions Std                                 0.148691\n",
      "exploration/Actions Max                                 0.542315\n",
      "exploration/Actions Min                                -0.537439\n",
      "exploration/Num Paths                                   1\n",
      "exploration/Average Returns                           918.968\n",
      "exploration/env_infos/final/reward_forward Mean        -0.29028\n",
      "exploration/env_infos/final/reward_forward Std          0\n",
      "exploration/env_infos/final/reward_forward Max         -0.29028\n",
      "exploration/env_infos/final/reward_forward Min         -0.29028\n",
      "exploration/env_infos/initial/reward_forward Mean      -0.133196\n",
      "exploration/env_infos/initial/reward_forward Std        0\n",
      "exploration/env_infos/initial/reward_forward Max       -0.133196\n",
      "exploration/env_infos/initial/reward_forward Min       -0.133196\n",
      "exploration/env_infos/reward_forward Mean               0.0492682\n",
      "exploration/env_infos/reward_forward Std                0.244693\n",
      "exploration/env_infos/reward_forward Max                1.41172\n",
      "exploration/env_infos/reward_forward Min               -0.861723\n",
      "exploration/env_infos/final/reward_ctrl Mean           -0.0647611\n",
      "exploration/env_infos/final/reward_ctrl Std             0\n",
      "exploration/env_infos/final/reward_ctrl Max            -0.0647611\n",
      "exploration/env_infos/final/reward_ctrl Min            -0.0647611\n",
      "exploration/env_infos/initial/reward_ctrl Mean         -0.0824842\n",
      "exploration/env_infos/initial/reward_ctrl Std           0\n",
      "exploration/env_infos/initial/reward_ctrl Max          -0.0824842\n",
      "exploration/env_infos/initial/reward_ctrl Min          -0.0824842\n",
      "exploration/env_infos/reward_ctrl Mean                 -0.0894625\n",
      "exploration/env_infos/reward_ctrl Std                   0.0471548\n",
      "exploration/env_infos/reward_ctrl Max                  -0.0139391\n",
      "exploration/env_infos/reward_ctrl Min                  -0.369622\n",
      "exploration/env_infos/final/reward_contact Mean         0\n",
      "exploration/env_infos/final/reward_contact Std          0\n",
      "exploration/env_infos/final/reward_contact Max         -0\n",
      "exploration/env_infos/final/reward_contact Min         -0\n",
      "exploration/env_infos/initial/reward_contact Mean       0\n",
      "exploration/env_infos/initial/reward_contact Std        0\n",
      "exploration/env_infos/initial/reward_contact Max       -0\n",
      "exploration/env_infos/initial/reward_contact Min       -0\n",
      "exploration/env_infos/reward_contact Mean               0\n",
      "exploration/env_infos/reward_contact Std                0\n",
      "exploration/env_infos/reward_contact Max               -0\n",
      "exploration/env_infos/reward_contact Min               -0\n",
      "exploration/env_infos/final/reward_survive Mean         1\n",
      "exploration/env_infos/final/reward_survive Std          0\n",
      "exploration/env_infos/final/reward_survive Max          1\n",
      "exploration/env_infos/final/reward_survive Min          1\n",
      "exploration/env_infos/initial/reward_survive Mean       1\n",
      "exploration/env_infos/initial/reward_survive Std        0\n",
      "exploration/env_infos/initial/reward_survive Max        1\n",
      "exploration/env_infos/initial/reward_survive Min        1\n",
      "exploration/env_infos/reward_survive Mean               1\n",
      "exploration/env_infos/reward_survive Std                0\n",
      "exploration/env_infos/reward_survive Max                1\n",
      "exploration/env_infos/reward_survive Min                1\n",
      "exploration/env_infos/final/torso_velocity Mean        -0.105749\n",
      "exploration/env_infos/final/torso_velocity Std          0.152819\n",
      "exploration/env_infos/final/torso_velocity Max          0.0839419\n",
      "exploration/env_infos/final/torso_velocity Min         -0.29028\n",
      "exploration/env_infos/initial/torso_velocity Mean       0.0999217\n",
      "exploration/env_infos/initial/torso_velocity Std        0.232339\n",
      "exploration/env_infos/initial/torso_velocity Max        0.417015\n",
      "exploration/env_infos/initial/torso_velocity Min       -0.133196\n",
      "exploration/env_infos/torso_velocity Mean               0.0204841\n",
      "exploration/env_infos/torso_velocity Std                0.186186\n",
      "exploration/env_infos/torso_velocity Max                1.41172\n",
      "exploration/env_infos/torso_velocity Min               -1.14538\n",
      "evaluation/num steps total                              1.9e+06\n",
      "evaluation/num paths total                           1900\n",
      "evaluation/path length Mean                          1000\n",
      "evaluation/path length Std                              0\n",
      "evaluation/path length Max                           1000\n",
      "evaluation/path length Min                           1000\n",
      "evaluation/Rewards Mean                                 0.946131\n",
      "evaluation/Rewards Std                                  0.0586959\n",
      "evaluation/Rewards Max                                  2.24697\n",
      "evaluation/Rewards Min                                  0.518147\n",
      "evaluation/Returns Mean                               946.131\n",
      "evaluation/Returns Std                                 24.9113\n",
      "evaluation/Returns Max                                984.191\n",
      "evaluation/Returns Min                                884.327\n",
      "evaluation/Actions Mean                                 0.0196415\n",
      "evaluation/Actions Std                                  0.117954\n",
      "evaluation/Actions Max                                  0.659793\n",
      "evaluation/Actions Min                                 -0.684176\n",
      "evaluation/Num Paths                                   25\n",
      "evaluation/Average Returns                            946.131\n",
      "evaluation/env_infos/final/reward_forward Mean          0.00338603\n",
      "evaluation/env_infos/final/reward_forward Std           0.131911\n",
      "evaluation/env_infos/final/reward_forward Max           0.518903\n",
      "evaluation/env_infos/final/reward_forward Min          -0.386017\n",
      "evaluation/env_infos/initial/reward_forward Mean       -0.0577291\n",
      "evaluation/env_infos/initial/reward_forward Std         0.121963\n",
      "evaluation/env_infos/initial/reward_forward Max         0.250459\n",
      "evaluation/env_infos/initial/reward_forward Min        -0.295201\n",
      "evaluation/env_infos/reward_forward Mean                0.00636152\n",
      "evaluation/env_infos/reward_forward Std                 0.213477\n",
      "evaluation/env_infos/reward_forward Max                 1.53424\n",
      "evaluation/env_infos/reward_forward Min                -1.81877\n",
      "evaluation/env_infos/final/reward_ctrl Mean            -0.0537624\n",
      "evaluation/env_infos/final/reward_ctrl Std              0.02448\n",
      "evaluation/env_infos/final/reward_ctrl Max             -0.0208402\n",
      "evaluation/env_infos/final/reward_ctrl Min             -0.107426\n",
      "evaluation/env_infos/initial/reward_ctrl Mean          -0.0344091\n",
      "evaluation/env_infos/initial/reward_ctrl Std            0.0112\n",
      "evaluation/env_infos/initial/reward_ctrl Max           -0.0192769\n",
      "evaluation/env_infos/initial/reward_ctrl Min           -0.0683743\n",
      "evaluation/env_infos/reward_ctrl Mean                  -0.0571959\n",
      "evaluation/env_infos/reward_ctrl Std                    0.0379234\n",
      "evaluation/env_infos/reward_ctrl Max                   -0.0132374\n",
      "evaluation/env_infos/reward_ctrl Min                   -0.493859\n",
      "evaluation/env_infos/final/reward_contact Mean          0\n",
      "evaluation/env_infos/final/reward_contact Std           0\n",
      "evaluation/env_infos/final/reward_contact Max          -0\n",
      "evaluation/env_infos/final/reward_contact Min          -0\n",
      "evaluation/env_infos/initial/reward_contact Mean        0\n",
      "evaluation/env_infos/initial/reward_contact Std         0\n",
      "evaluation/env_infos/initial/reward_contact Max        -0\n",
      "evaluation/env_infos/initial/reward_contact Min        -0\n",
      "evaluation/env_infos/reward_contact Mean                0\n",
      "evaluation/env_infos/reward_contact Std                 0\n",
      "evaluation/env_infos/reward_contact Max                -0\n",
      "evaluation/env_infos/reward_contact Min                -0\n",
      "evaluation/env_infos/final/reward_survive Mean          1\n",
      "evaluation/env_infos/final/reward_survive Std           0\n",
      "evaluation/env_infos/final/reward_survive Max           1\n",
      "evaluation/env_infos/final/reward_survive Min           1\n",
      "evaluation/env_infos/initial/reward_survive Mean        1\n",
      "evaluation/env_infos/initial/reward_survive Std         0\n",
      "evaluation/env_infos/initial/reward_survive Max         1\n",
      "evaluation/env_infos/initial/reward_survive Min         1\n",
      "evaluation/env_infos/reward_survive Mean                1\n",
      "evaluation/env_infos/reward_survive Std                 0\n",
      "evaluation/env_infos/reward_survive Max                 1\n",
      "evaluation/env_infos/reward_survive Min                 1\n",
      "evaluation/env_infos/final/torso_velocity Mean         -0.00576831\n",
      "evaluation/env_infos/final/torso_velocity Std           0.112823\n",
      "evaluation/env_infos/final/torso_velocity Max           0.518903\n",
      "evaluation/env_infos/final/torso_velocity Min          -0.588504\n",
      "evaluation/env_infos/initial/torso_velocity Mean        0.11258\n",
      "evaluation/env_infos/initial/torso_velocity Std         0.255694\n",
      "evaluation/env_infos/initial/torso_velocity Max         0.661277\n",
      "evaluation/env_infos/initial/torso_velocity Min        -0.295201\n",
      "evaluation/env_infos/torso_velocity Mean                0.000853331\n",
      "evaluation/env_infos/torso_velocity Std                 0.20371\n",
      "evaluation/env_infos/torso_velocity Max                 1.86091\n",
      "evaluation/env_infos/torso_velocity Min                -1.81877\n",
      "time/data storing (s)                                   0.326855\n",
      "time/evaluation sampling (s)                           42.262\n",
      "time/exploration sampling (s)                           1.93225\n",
      "time/logging (s)                                        0.281125\n",
      "time/saving (s)                                         0.0260086\n",
      "time/training (s)                                       4.07014\n",
      "time/epoch (s)                                         48.8984\n",
      "time/total (s)                                       3813.72\n",
      "Epoch                                                  75\n",
      "-------------------------------------------------  ----------------\n"
     ]
    }
   ],
   "source": [
    "!python launch_gher.py --epochs 100 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 1 --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse\n",
    "!python launch_gher.py --epochs 100 --relabel --n_sampled_latents 100 --use_advantages --env antdirectionnewsparse --cache --irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a953b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
